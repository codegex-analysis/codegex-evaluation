[
  {
    "sha": "836ef8d5200fa304b00889143527136e8e9ad068",
    "filename": "README.md",
    "status": "modified",
    "additions": 10,
    "deletions": 17,
    "changes": 27,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/README.md",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/README.md",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/README.md?ref=e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2",
    "patch": "@@ -376,12 +376,8 @@ Job requires parameters:\n `--yesterdayData`             : file location of previous day's data\n `--todayData`                 : file location of today day's data\n `--N`              \t       : (optional) number of displayed top results\n-`--criticaluri`               : uri to the mongo db collection, to store critical status results\n-`--warninguri`                : uri to the mongo db collection, to store warning status results\n-`--unknownuri`                : uri to the mongo db collection, to store unknown status results\n-\n-`--baseuri`                   : uri to the web-api\n-`--metricProfileUUID`         : uuid of the metric_profile report, to be retrieved from the api request\n+`--mongoUri`                  : uri to the mongo db , to store results\n+`--apiUri`                    : uri to the web-api\n `--key`                       : users's token, used for authentication\n `--proxy`                     : (optional) proxy url \n \n@@ -391,21 +387,18 @@ Job requires parameters:\n `--yesterdayData`              : file location of previous day's data\n `--todayData`                  : file location of today day's data\n `--N`              \t\t: (optional) number of displayed top results\n-`--flipflopuri`                : uri to the mongo db , to store flip flop results\n-`--baseuri`                   : uri to the web-api\n-`--metricProfileUUID`         : uuid of the metric_profile report, to be retrieved from the api request\n-`--key`                       : users's token, used for authentication\n-`--proxy`                     : (optional) proxy url \n+`--mongoUri`                   : uri to the mongo db , to store results\n+`--apiUri`                     : uri to the web-api\n+`--key`                        : users's token, used for authentication\n+`--proxy`                      : (optional) proxy url \n \n Flink batch Job that calculate flip flop trends for service endpoints \n Job requires parameters:\n \n `--yesterdayData`              : file location of previous day's data\n `--todayData`                  : file location of today day's data\n `--N`              \t\t: (optional) number of displayed top results\n-`--servendpflipflopsuri`       : uri to the mongo db , to store flip flop results\n-`--op`                         : the name of the operation that will apply on the timeline\n-`--baseuri`                   : uri to the web-api\n-`--metricProfileUUID`         : uuid of the metric_profile report, to be retrieved from the api request\n-`--key`                       : users's token, used for authentication\n-`--proxy`                     : (optional) proxy url \n+`--mongoUri`                   : uri to the mongo db , to store results\n+`--apiUri`                     : uri to the web-api\n+`--key`                        : users's token, used for authentication\n+`--proxy`                      : (optional) proxy url "
  },
  {
    "sha": "c3fc6461a9f5f3718a03046eff6ffaab533c66db",
    "filename": "flink_jobs/status_trends/src/main/java/argo/batch/BatchFlipFlopTrends.java",
    "status": "modified",
    "additions": 18,
    "deletions": 16,
    "changes": 34,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/batch/BatchFlipFlopTrends.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/batch/BatchFlipFlopTrends.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/batch/BatchFlipFlopTrends.java?ref=e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2",
    "patch": "@@ -17,7 +17,7 @@\n  * the License.\n  */\n import argo.avro.MetricData;\n-import argo.functions.flipfloptrends.CalcServiceEnpointMetricFlipFlop;\n+import argo.functions.calculations.CalcServiceEnpointMetricFlipFlop;\n import argo.functions.timeline.CalcLastTimeStatus;\n \n import argo.functions.timeline.TopologyMetricFilter;\n@@ -41,8 +41,7 @@\n import org.apache.flink.core.fs.Path;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.mapred.JobConf;\n-import parsers.AggregationProfileParser;\n-import parsers.ReportParser;\n+import argo.profileparsers.ProfilesLoader;\n \n /**\n  * Skeleton for a Flink Batch Job.\n@@ -63,46 +62,48 @@\n \n     static Logger LOG = LoggerFactory.getLogger(BatchFlipFlopTrends.class);\n \n-      private static HashMap<String, ArrayList<String>> metricProfileData;\n+    private static HashMap<String, ArrayList<String>> metricProfileData;\n     private static HashMap<String, String> groupEndpointData;\n     private static DataSet<MetricData> yesterdayData;\n     private static DataSet<MetricData> todayData;\n     private static Integer rankNum;\n-\n+ private static final String servEndpointMetricFlipFlopTrends=\"servEndpointMetricFlipFlopTrends\";\n+    private static String mongoUri;\n+    \n     public static void main(String[] args) throws Exception {\n         // set up the batch execution environment\n         final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n \n         final ParameterTool params = ParameterTool.fromArgs(args);\n         //check if all required parameters exist and if not exit program\n-        if (!Utils.checkParameters(params, \"yesterdayData\", \"todayData\", \"flipflopsuri\", \"baseUri\", \"metricProfileUUID\", \"key\", \"reportId\")) {\n+        if (!Utils.checkParameters(params, \"yesterdayData\", \"todayData\",\"mongoUri\", \"apiUri\", \"key\", \"reportId\")) {\n             System.exit(0);\n         }\n \n         env.setParallelism(1);\n-\n+        mongoUri=params.getRequired(\"mongoUri\");\n         if (params.get(\"N\") != null) {\n             rankNum = params.getInt(\"N\");\n         }\n-        ReportParser.loadReportInfo(params.getRequired(\"baseUri\"), params.getRequired(\"key\"), params.get(\"proxy\"), params.getRequired(\"reportId\"));\n-        AggregationProfileParser.loadAggrProfileInfo(params.getRequired(\"baseUri\"), params.getRequired(\"key\"), params.get(\"proxy\"));\n-        metricProfileData = Utils.readMetricDataJson(params.getRequired(\"baseUri\"), params.getRequired(\"metricProfileUUID\"), params.getRequired(\"key\"),params.get(\"proxy\")); //contains the information of the (service, metrics) matches\n-        groupEndpointData = Utils.readGroupEndpointJson(params.getRequired(\"baseUri\"), params.getRequired(\"key\"),params.get(\"proxy\")); //contains the information of the (service, metrics) matches\n+       \n+        ProfilesLoader profilesLoader=new ProfilesLoader(params);\n+        metricProfileData = profilesLoader.getMetricProfileParser().getMetricData();\n+        groupEndpointData = profilesLoader.getTopologyEndpointParser().getTopology(profilesLoader.getAggregationProfileParser().getEndpointGroup().toUpperCase());\n+      \n         yesterdayData = readInputData(env, params.getRequired(\"yesterdayData\"));\n         todayData = readInputData(env, params.getRequired(\"todayData\"));\n \n         DataSet<Tuple5<String, String, String, String, Integer>> criticalData = calcFlipFlops();\n \n-        writeToMongo(params.getRequired(\"flipflopsuri\"), criticalData);\n+        writeToMongo( criticalData);\n // execute program\n         env.execute(\"Flink Batch Java API Skeleton\");\n     }\n \n-    \n     // filter yesterdaydata and exclude the ones not contained in topology and metric profile data and get the last timestamp data for each service endpoint metric\n     // filter todaydata and exclude the ones not contained in topology and metric profile data , union yesterday data and calculate status changes for each service endpoint metric\n     // rank results\n-   private static DataSet<Tuple5<String, String, String, String, Integer>> calcFlipFlops() {\n+    private static DataSet<Tuple5<String, String, String, String, Integer>> calcFlipFlops() {\n \n         DataSet<MetricData> filteredYesterdayData = yesterdayData.filter(new TopologyMetricFilter(metricProfileData, groupEndpointData)).groupBy(\"hostname\", \"service\", \"metric\").reduceGroup(new CalcLastTimeStatus());\n \n@@ -151,10 +152,11 @@ public static void main(String[] args) throws Exception {\n     }\n \n     //write to mongo db\n-    public static void writeToMongo(String uri, DataSet<Tuple5<String, String, String, String, Integer>> data) {\n+    public static void writeToMongo( DataSet<Tuple5<String, String, String, String, Integer>> data) {\n+        String collectionUri=mongoUri+\".\"+servEndpointMetricFlipFlopTrends;\n         DataSet<Tuple2<Text, BSONWritable>> result = convertResultToBSON(data);\n         JobConf conf = new JobConf();\n-        conf.set(\"mongo.output.uri\", uri);\n+        conf.set(\"mongo.output.uri\", collectionUri);\n \n         MongoOutputFormat<Text, BSONWritable> mongoOutputFormat = new MongoOutputFormat<Text, BSONWritable>();\n         result.output(new HadoopOutputFormat<Text, BSONWritable>(mongoOutputFormat, conf));"
  },
  {
    "sha": "20b098ee87bab40fa4ecd166661cc62ec3eceac9",
    "filename": "flink_jobs/status_trends/src/main/java/argo/batch/BatchGroupFlipFlopTrends.java",
    "status": "added",
    "additions": 198,
    "deletions": 0,
    "changes": 198,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/batch/BatchGroupFlipFlopTrends.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/batch/BatchGroupFlipFlopTrends.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/batch/BatchGroupFlipFlopTrends.java?ref=e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2",
    "patch": "@@ -0,0 +1,198 @@\n+package argo.batch;\n+\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with this\n+ * work for additional information regarding copyright ownership. The ASF\n+ * licenses this file to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+import argo.avro.MetricData;\n+import argo.functions.calculations.CalcGroupFlipFlop;\n+import argo.functions.calculations.CalcGroupFunctionFlipFlop;\n+\n+import argo.functions.timeline.CalcMetricTimelineStatus;\n+import argo.functions.calculations.CalcServiceEndpointFlipFlop;\n+import argo.functions.calculations.CalcServiceFlipFlop;\n+import argo.functions.timeline.CalcLastTimeStatus;\n+import argo.functions.timeline.MapServices;\n+import argo.functions.timeline.ServiceFilter;\n+import argo.functions.timeline.TopologyMetricFilter;\n+import argo.pojos.TimelineTrends;\n+import argo.utils.Utils;\n+import com.mongodb.BasicDBObject;\n+import com.mongodb.hadoop.io.BSONWritable;\n+import com.mongodb.hadoop.mapred.MongoOutputFormat;\n+import java.util.ArrayList;\n+\n+import java.util.HashMap;\n+import org.apache.flink.api.common.functions.MapFunction;\n+import org.apache.flink.api.common.operators.Order;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.api.java.ExecutionEnvironment;\n+import org.apache.flink.api.java.hadoop.mapred.HadoopOutputFormat;\n+import org.apache.flink.api.java.io.AvroInputFormat;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.api.java.utils.ParameterTool;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapred.JobConf;\n+import argo.profileparsers.ProfilesLoader;\n+\n+/**\n+ * Skeleton for a Flink Batch Job.\n+ *\n+ * For a full example of a Flink Batch Job, see the WordCountJob.java file in\n+ * the same package/directory or have a look at the website.\n+ *\n+ * You can also generate a .jar file that you can submit on your Flink cluster.\n+ * Just type mvn clean package in the projects root directory. You will find the\n+ * jar in target/argo2932-1.0.jar From the CLI you can then run ./bin/flink run\n+ * -c com.company.argo.BatchJob target/argo2932-1.0.jar\n+ *\n+ * For more information on the CLI see:\n+ *\n+ * http://flink.apache.org/docs/latest/apis/cli.html\n+ */\n+public class BatchGroupFlipFlopTrends {\n+\n+    private static HashMap<String, HashMap<String, String>> opTruthTableMap = new HashMap<>(); // the truth table for the operations to be applied on timeline\n+    private static HashMap<String, ArrayList<String>> metricProfileData;\n+    private static HashMap<String, String> groupEndpointData;\n+    private static DataSet<MetricData> yesterdayData;\n+    private static DataSet<MetricData> todayData;\n+    private static Integer rankNum;\n+    private static final String groupFlipFlopTrends = \"groupFlipFlopTrends\";\n+    private static String mongoUri;\n+    private static ProfilesLoader profilesLoader;\n+\n+    public static void main(String[] args) throws Exception {\n+        // set up the batch execution environment\n+        final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n+\n+        final ParameterTool params = ParameterTool.fromArgs(args);\n+        //check if all required parameters exist and if not exit program\n+        if (!Utils.checkParameters(params, \"yesterdayData\", \"todayData\", \"mongoUri\", \"apiUri\", \"key\")) {\n+            System.exit(0);\n+        }\n+\n+        env.setParallelism(1);\n+        if (params.get(\"N\") != null) {\n+            rankNum = params.getInt(\"N\");\n+        }\n+        mongoUri = params.get(\"mongoUri\");\n+        profilesLoader = new ProfilesLoader(params);\n+        metricProfileData = profilesLoader.getMetricProfileParser().getMetricData();\n+        groupEndpointData = profilesLoader.getTopologyEndpointParser().getTopology(profilesLoader.getAggregationProfileParser().getEndpointGroup().toUpperCase());\n+\n+        yesterdayData = readInputData(env, params, \"yesterdayData\");\n+        todayData = readInputData(env, params, \"todayData\");\n+        String metricOperation = profilesLoader.getAggregationProfileParser().getMetricOp();\n+\n+        // calculate on data \n+        DataSet<TimelineTrends> resultData = calcFlipFlops(profilesLoader.getOperationParser().getOpTruthTable().get(metricOperation));\n+        writeToMongo(resultData);\n+\n+// execute program\n+        env.execute(\"Flink Batch Java API Skeleton\");\n+\n+    }\n+\n+// filter yesterdaydata and exclude the ones not contained in topology and metric profile data and get the last timestamp data for each service endpoint metric\n+// filter todaydata and exclude the ones not contained in topology and metric profile data , union yesterday data and calculate status changes for each service endpoint metric\n+// rank results\n+    private static DataSet<TimelineTrends> calcFlipFlops(HashMap<String, String> truthTable) {\n+\n+        DataSet<MetricData> filteredYesterdayData = yesterdayData.filter(new TopologyMetricFilter(metricProfileData, groupEndpointData)).groupBy(\"hostname\", \"service\", \"metric\").reduceGroup(new CalcLastTimeStatus());\n+        DataSet<MetricData> filteredTodayData = todayData.filter(new TopologyMetricFilter(metricProfileData, groupEndpointData));\n+\n+        //group data by service enpoint metric and return for each group , the necessary info and a treemap containing timestamps and status\n+        DataSet<TimelineTrends> serviceEndpointMetricGroupData = filteredTodayData.union(filteredYesterdayData).groupBy(\"hostname\", \"service\", \"metric\").reduceGroup(new CalcMetricTimelineStatus(groupEndpointData));\n+\n+        //group data by service endpoint  and count flip flops\n+        DataSet<TimelineTrends> serviceEndpointGroupData = serviceEndpointMetricGroupData.groupBy(\"group\", \"endpoint\", \"service\").reduceGroup(new CalcServiceEndpointFlipFlop(profilesLoader.getOperationParser(), profilesLoader.getAggregationProfileParser().getMetricOp()));\n+\n+        //group data by service   and count flip flops\n+        DataSet<TimelineTrends> serviceGroupData = serviceEndpointGroupData.filter(new ServiceFilter(profilesLoader.getAggregationProfileParser().getServiceOperations())).groupBy(\"group\", \"service\").reduceGroup(new CalcServiceFlipFlop(profilesLoader.getOperationParser().getOpTruthTable(), profilesLoader.getAggregationProfileParser().getServiceOperations()));\n+        //flat map data to add function as described in aggregation profile groups\n+        serviceGroupData = serviceGroupData.flatMap(new MapServices(profilesLoader.getAggregationProfileParser().getServiceFunctions()));\n+\n+        //group data by group,function   and count flip flops\n+        DataSet<TimelineTrends> groupData = serviceGroupData.groupBy(\"group\", \"function\").reduceGroup(new CalcGroupFunctionFlipFlop(profilesLoader.getOperationParser().getOpTruthTable(), profilesLoader.getAggregationProfileParser().getFunctionOperations()));\n+\n+        //group data by group   and count flip flops\n+        groupData = groupData.groupBy(\"group\").reduceGroup(new CalcGroupFlipFlop(profilesLoader.getOperationParser().getOpTruthTable(), profilesLoader.getAggregationProfileParser().getProfileOp()));\n+\n+        if (rankNum != null) { //sort and rank data\n+            groupData = groupData.sortPartition(\"flipflops\", Order.DESCENDING).first(rankNum);\n+        } else {\n+            groupData = groupData.sortPartition(\"flipflops\", Order.DESCENDING);\n+        }\n+        return groupData;\n+\n+    }    //read input from file\n+\n+    private static DataSet<MetricData> readInputData(ExecutionEnvironment env, ParameterTool params, String path) {\n+        DataSet<MetricData> inputData;\n+        Path input = new Path(params.getRequired(path));\n+\n+        AvroInputFormat<MetricData> inputAvroFormat = new AvroInputFormat<MetricData>(input, MetricData.class\n+        );\n+        inputData = env.createInput(inputAvroFormat);\n+        return inputData;\n+    }\n+\n+//    //initialize configuaration parameters to be used from functions\n+//    private static void initializeConfigurationParameters(ParameterTool params, ExecutionEnvironment env) {\n+//\n+//        Configuration conf = new Configuration();\n+//        conf.setString(\"groupEndpointsPath\", params.get(\"groupEndpointsPath\"));\n+//        conf.setString(\"metricDataPath\", params.get(\"metricDataPath\"));\n+//        env.getConfig().setGlobalJobParameters(conf);\n+//\n+//    }\n+    //convert the result in bson format\n+    public static DataSet<Tuple2<Text, BSONWritable>> convertResultToBSON(DataSet<TimelineTrends> in) {\n+\n+        return in.map(new MapFunction<TimelineTrends, Tuple2<Text, BSONWritable>>() {\n+            int i = 0;\n+\n+            @Override\n+            public Tuple2<Text, BSONWritable> map(TimelineTrends in) throws Exception {\n+                BasicDBObject dbObject = new BasicDBObject();\n+                dbObject.put(\"group\", in.getGroup().toString());\n+                dbObject.put(\"trend\", in.getFlipflops());\n+\n+                BSONWritable bson = new BSONWritable(dbObject);\n+                i++;\n+                return new Tuple2<Text, BSONWritable>(new Text(String.valueOf(i)), bson);\n+                /* TODO */\n+            }\n+        });\n+    }\n+\n+    //write to mongo db\n+    public static void writeToMongo(DataSet<TimelineTrends> data) {\n+        String collectionUri = mongoUri + \".\" + groupFlipFlopTrends;\n+        DataSet<Tuple2<Text, BSONWritable>> result = convertResultToBSON(data);\n+        JobConf conf = new JobConf();\n+        conf.set(\"mongo.output.uri\", collectionUri);\n+\n+        MongoOutputFormat<Text, BSONWritable> mongoOutputFormat = new MongoOutputFormat<Text, BSONWritable>();\n+        result.output(new HadoopOutputFormat<Text, BSONWritable>(mongoOutputFormat, conf));\n+    }\n+\n+//    public static void createOpTruthTables(String baseUri, String key, String proxy, String operationsId) throws IOException, ParseException {\n+//        \n+//        opTruthTableMap = Utils.readOperationProfileJson(baseUri, key, proxy, operationsId);\n+//    }\n+}"
  },
  {
    "sha": "9c22943ed0bc2b54631cb350c7aec1d6059d7102",
    "filename": "flink_jobs/status_trends/src/main/java/argo/batch/BatchServEndpFlipFlopTrends.java",
    "status": "modified",
    "additions": 51,
    "deletions": 59,
    "changes": 110,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/batch/BatchServEndpFlipFlopTrends.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/batch/BatchServEndpFlipFlopTrends.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/batch/BatchServEndpFlipFlopTrends.java?ref=e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2",
    "patch": "@@ -19,16 +19,14 @@\n import argo.avro.MetricData;\n \n import argo.functions.timeline.CalcMetricTimelineStatus;\n-import argo.functions.servendptrends.CalcServiceEndpointFlipFlop;\n+import argo.functions.calculations.CalcServiceEndpointFlipFlop;\n import argo.functions.timeline.CalcLastTimeStatus;\n import argo.functions.timeline.TopologyMetricFilter;\n-import argo.pojos.MetricTimelinePojo;\n-import argo.pojos.ServEndpFlipFlopPojo;\n+import argo.pojos.TimelineTrends;\n import argo.utils.Utils;\n import com.mongodb.BasicDBObject;\n import com.mongodb.hadoop.io.BSONWritable;\n import com.mongodb.hadoop.mapred.MongoOutputFormat;\n-import java.io.IOException;\n import java.util.ArrayList;\n \n import java.util.HashMap;\n@@ -40,13 +38,10 @@\n import org.apache.flink.api.java.io.AvroInputFormat;\n import org.apache.flink.api.java.tuple.Tuple2;\n import org.apache.flink.api.java.utils.ParameterTool;\n-import org.apache.flink.configuration.Configuration;\n import org.apache.flink.core.fs.Path;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.mapred.JobConf;\n-import org.json.simple.parser.ParseException;\n-import parsers.AggregationProfileParser;\n-import parsers.ReportParser;\n+import argo.profileparsers.ProfilesLoader;\n \n /**\n  * Skeleton for a Flink Batch Job.\n@@ -71,58 +66,53 @@\n     private static DataSet<MetricData> yesterdayData;\n     private static DataSet<MetricData> todayData;\n     private static Integer rankNum;\n-\n+    private static final String servEndpointFlipFlopTrends = \"servEndpointFlipFlopTrends\";\n+    private static String mongoUri;\n+    private static  ProfilesLoader profilesLoader ;\n     public static void main(String[] args) throws Exception {\n         // set up the batch execution environment\n         final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n \n         final ParameterTool params = ParameterTool.fromArgs(args);\n         //check if all required parameters exist and if not exit program\n-        if (!Utils.checkParameters(params, \"yesterdayData\", \"todayData\", \"servendpflipflopsuri\", \"op\", \"baseUri\", \"metricProfileUUID\", \"key\")) {\n+        if (!Utils.checkParameters(params, \"yesterdayData\", \"todayData\", \"mongoUri\",\"apiUri\", \"key\")) {\n             System.exit(0);\n         }\n \n         env.setParallelism(1);\n-        ReportParser.loadReportInfo(params.getRequired(\"baseUri\"), params.getRequired(\"key\"), params.get(\"proxy\"), params.getRequired(\"reportId\"));\n-        AggregationProfileParser.loadAggrProfileInfo(params.getRequired(\"baseUri\"), params.getRequired(\"key\"), params.get(\"proxy\"));\n-\n-        createOpTruthTables(params.getRequired(\"baseUri\"), params.getRequired(\"key\"), params.get(\"proxy\")); // build the truth table hardcode now -fix later....\n-        HashMap<String, String> truthTable = opTruthTableMap.get(params.getRequired(\"op\"));\n-        DataSet<ServEndpFlipFlopPojo> resultData = null;\n-\n-        if (truthTable != null) {\n-            if (params.get(\"N\") != null) {\n-                rankNum = params.getInt(\"N\");\n-            }\n-            //read the data from input\n-            metricProfileData = Utils.readMetricDataJson(params.getRequired(\"baseUri\"), params.getRequired(\"metricProfileUUID\"), params.getRequired(\"key\"), params.get(\"proxy\")); //contains the information of the (service, metrics) matches\n-            groupEndpointData = Utils.readGroupEndpointJson(params.getRequired(\"baseUri\"), params.getRequired(\"key\"), params.get(\"proxy\")); //contains the information of the (service, metrics) matches\n-\n-            yesterdayData = readInputData(env, params, \"yesterdayData\");\n-            todayData = readInputData(env, params, \"todayData\");\n-\n-            // calculate on data \n-            resultData = calcFlipFlops(truthTable);\n-            writeToMongo(params.getRequired(\"servendpflipflopsuri\"), resultData);\n+        if (params.get(\"N\") != null) {\n+            rankNum = params.getInt(\"N\");\n+        }\n+        mongoUri = params.get(\"mongoUri\");\n+        profilesLoader = new ProfilesLoader(params);\n+        metricProfileData = profilesLoader.getMetricProfileParser().getMetricData();\n+        groupEndpointData = profilesLoader.getTopologyEndpointParser().getTopology(profilesLoader.getAggregationProfileParser().getEndpointGroup().toUpperCase());\n+\n+        yesterdayData = readInputData(env, params, \"yesterdayData\");\n+        todayData = readInputData(env, params, \"todayData\");\n+      \n+        // calculate on data \n+        DataSet<TimelineTrends> resultData = calcFlipFlops();\n+        writeToMongo(resultData);\n \n // execute program\n-            env.execute(\"Flink Batch Java API Skeleton\");\n-        }\n+        env.execute(\"Flink Batch Java API Skeleton\");\n+\n     }\n \n-    // filter yesterdaydata and exclude the ones not contained in topology and metric profile data and get the last timestamp data for each service endpoint metric\n-    // filter todaydata and exclude the ones not contained in topology and metric profile data , union yesterday data and calculate status changes for each service endpoint metric\n-    // rank results\n-    private static DataSet<ServEndpFlipFlopPojo> calcFlipFlops(HashMap<String, String> truthTable) {\n+// filter yesterdaydata and exclude the ones not contained in topology and metric profile data and get the last timestamp data for each service endpoint metric\n+// filter todaydata and exclude the ones not contained in topology and metric profile data , union yesterday data and calculate status changes for each service endpoint metric\n+// rank results\n+    private static DataSet<TimelineTrends> calcFlipFlops() {\n \n         DataSet<MetricData> filteredYesterdayData = yesterdayData.filter(new TopologyMetricFilter(metricProfileData, groupEndpointData)).groupBy(\"hostname\", \"service\", \"metric\").reduceGroup(new CalcLastTimeStatus());\n         DataSet<MetricData> filteredTodayData = todayData.filter(new TopologyMetricFilter(metricProfileData, groupEndpointData));\n \n         //group data by service enpoint metric and return for each group , the necessary info and a treemap containing timestamps and status\n-        DataSet<MetricTimelinePojo> serviceEndpointMetricGroupData = filteredTodayData.union(filteredYesterdayData).groupBy(\"hostname\", \"service\", \"metric\").reduceGroup(new CalcMetricTimelineStatus(groupEndpointData));\n+        DataSet<TimelineTrends> serviceEndpointMetricGroupData = filteredTodayData.union(filteredYesterdayData).groupBy(\"hostname\", \"service\", \"metric\").reduceGroup(new CalcMetricTimelineStatus(groupEndpointData));\n \n         //group data by service endpoint  and count flip flops\n-        DataSet<ServEndpFlipFlopPojo> serviceEndpointGroupData = serviceEndpointMetricGroupData.groupBy(\"group\", \"endpoint\", \"service\").reduceGroup(new CalcServiceEndpointFlipFlop(truthTable));\n+        DataSet<TimelineTrends> serviceEndpointGroupData = serviceEndpointMetricGroupData.groupBy(\"group\", \"endpoint\", \"service\").reduceGroup(new CalcServiceEndpointFlipFlop(profilesLoader.getOperationParser(), profilesLoader.getAggregationProfileParser().getMetricOp()));\n \n         if (rankNum != null) { //sort and rank data\n             serviceEndpointGroupData = serviceEndpointGroupData.sortPartition(\"flipflops\", Order.DESCENDING).first(rankNum);\n@@ -137,33 +127,34 @@ public static void main(String[] args) throws Exception {\n         DataSet<MetricData> inputData;\n         Path input = new Path(params.getRequired(path));\n \n-        AvroInputFormat<MetricData> inputAvroFormat = new AvroInputFormat<MetricData>(input, MetricData.class);\n+        AvroInputFormat<MetricData> inputAvroFormat = new AvroInputFormat<MetricData>(input, MetricData.class\n+        );\n         inputData = env.createInput(inputAvroFormat);\n         return inputData;\n     }\n \n-    //initialize configuaration parameters to be used from functions\n-    private static void initializeConfigurationParameters(ParameterTool params, ExecutionEnvironment env) {\n-\n-        Configuration conf = new Configuration();\n-        conf.setString(\"groupEndpointsPath\", params.get(\"groupEndpointsPath\"));\n-        conf.setString(\"metricDataPath\", params.get(\"metricDataPath\"));\n-        env.getConfig().setGlobalJobParameters(conf);\n-\n-    }\n+//    //initialize configuaration parameters to be used from functions\n+//    private static void initializeConfigurationParameters(ParameterTool params, ExecutionEnvironment env) {\n+//\n+//        Configuration conf = new Configuration();\n+//        conf.setString(\"groupEndpointsPath\", params.get(\"groupEndpointsPath\"));\n+//        conf.setString(\"metricDataPath\", params.get(\"metricDataPath\"));\n+//        env.getConfig().setGlobalJobParameters(conf);\n+//\n+//    }\n \n     //convert the result in bson format\n-    public static DataSet<Tuple2<Text, BSONWritable>> convertResultToBSON(DataSet<ServEndpFlipFlopPojo> in) {\n+    public static DataSet<Tuple2<Text, BSONWritable>> convertResultToBSON(DataSet<TimelineTrends> in) {\n \n-        return in.map(new MapFunction<ServEndpFlipFlopPojo, Tuple2<Text, BSONWritable>>() {\n+        return in.map(new MapFunction<TimelineTrends, Tuple2<Text, BSONWritable>>() {\n             int i = 0;\n \n             @Override\n-            public Tuple2<Text, BSONWritable> map(ServEndpFlipFlopPojo in) throws Exception {\n+            public Tuple2<Text, BSONWritable> map(TimelineTrends in) throws Exception {\n                 BasicDBObject dbObject = new BasicDBObject();\n                 dbObject.put(\"group\", in.getGroup().toString());\n                 dbObject.put(\"service\", in.getService().toString());\n-                dbObject.put(\"hostname\", in.getHostname().toString());\n+                dbObject.put(\"hostname\", in.getEndpoint().toString());\n                 dbObject.put(\"trend\", in.getFlipflops());\n \n                 BSONWritable bson = new BSONWritable(dbObject);\n@@ -175,17 +166,18 @@ private static void initializeConfigurationParameters(ParameterTool params, Exec\n     }\n \n     //write to mongo db\n-    public static void writeToMongo(String uri, DataSet<ServEndpFlipFlopPojo> data) {\n+    public static void writeToMongo(DataSet<TimelineTrends> data) {\n+        String collectionUri = mongoUri + \".\" + servEndpointFlipFlopTrends;\n         DataSet<Tuple2<Text, BSONWritable>> result = convertResultToBSON(data);\n         JobConf conf = new JobConf();\n-        conf.set(\"mongo.output.uri\", uri);\n+        conf.set(\"mongo.output.uri\", collectionUri);\n \n         MongoOutputFormat<Text, BSONWritable> mongoOutputFormat = new MongoOutputFormat<Text, BSONWritable>();\n         result.output(new HadoopOutputFormat<Text, BSONWritable>(mongoOutputFormat, conf));\n     }\n \n-    public static void createOpTruthTables(String baseUri, String key, String proxy) throws IOException, ParseException {\n-\n-        opTruthTableMap = Utils.readOperationProfileJson(baseUri, key, proxy);\n-    }\n+//    public static void createOpTruthTables(String baseUri, String key, String proxy, String operationsId) throws IOException, ParseException {\n+//        \n+//        opTruthTableMap = Utils.readOperationProfileJson(baseUri, key, proxy, operationsId);\n+//    }\n }"
  },
  {
    "sha": "81cf2fd9fbd949a07f1090839e3a75e119823deb",
    "filename": "flink_jobs/status_trends/src/main/java/argo/batch/BatchServiceFlipFlopTrends.java",
    "status": "added",
    "additions": 190,
    "deletions": 0,
    "changes": 190,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/batch/BatchServiceFlipFlopTrends.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/batch/BatchServiceFlipFlopTrends.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/batch/BatchServiceFlipFlopTrends.java?ref=e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2",
    "patch": "@@ -0,0 +1,190 @@\n+package argo.batch;\n+\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with this\n+ * work for additional information regarding copyright ownership. The ASF\n+ * licenses this file to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+import argo.avro.MetricData;\n+\n+import argo.functions.timeline.CalcMetricTimelineStatus;\n+import argo.functions.calculations.CalcServiceEndpointFlipFlop;\n+import argo.functions.calculations.CalcServiceFlipFlop;\n+import argo.functions.timeline.CalcLastTimeStatus;\n+import argo.functions.timeline.ServiceFilter;\n+import argo.functions.timeline.TopologyMetricFilter;\n+import argo.pojos.TimelineTrends;\n+import argo.utils.Utils;\n+import com.mongodb.BasicDBObject;\n+import com.mongodb.hadoop.io.BSONWritable;\n+import com.mongodb.hadoop.mapred.MongoOutputFormat;\n+import java.util.ArrayList;\n+\n+import java.util.HashMap;\n+import org.apache.flink.api.common.functions.MapFunction;\n+import org.apache.flink.api.common.operators.Order;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.api.java.ExecutionEnvironment;\n+import org.apache.flink.api.java.hadoop.mapred.HadoopOutputFormat;\n+import org.apache.flink.api.java.io.AvroInputFormat;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.api.java.utils.ParameterTool;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapred.JobConf;\n+import argo.profileparsers.ProfilesLoader;\n+\n+/**\n+ * Skeleton for a Flink Batch Job.\n+ *\n+ * For a full example of a Flink Batch Job, see the WordCountJob.java file in\n+ * the same package/directory or have a look at the website.\n+ *\n+ * You can also generate a .jar file that you can submit on your Flink cluster.\n+ * Just type mvn clean package in the projects root directory. You will find the\n+ * jar in target/argo2932-1.0.jar From the CLI you can then run ./bin/flink run\n+ * -c com.company.argo.BatchJob target/argo2932-1.0.jar\n+ *\n+ * For more information on the CLI see:\n+ *\n+ * http://flink.apache.org/docs/latest/apis/cli.html\n+ */\n+public class BatchServiceFlipFlopTrends {\n+\n+    private static HashMap<String, HashMap<String, String>> opTruthTableMap = new HashMap<>(); // the truth table for the operations to be applied on timeline\n+    private static HashMap<String, ArrayList<String>> metricProfileData;\n+    private static HashMap<String, String> groupEndpointData;\n+    private static DataSet<MetricData> yesterdayData;\n+    private static DataSet<MetricData> todayData;\n+    private static Integer rankNum;\n+    private static final String serviceFlipFlopTrends = \"serviceFlipFlopTrends\";\n+    private static String mongoUri;\n+    private static ProfilesLoader profilesLoader;\n+\n+    public static void main(String[] args) throws Exception {\n+        // set up the batch execution environment\n+        final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n+\n+        final ParameterTool params = ParameterTool.fromArgs(args);\n+        //check if all required parameters exist and if not exit program\n+        if (!Utils.checkParameters(params, \"yesterdayData\", \"todayData\", \"mongoUri\",\"apiUri\", \"key\")) {\n+            System.exit(0);\n+        }\n+\n+        env.setParallelism(1);\n+        if (params.get(\"N\") != null) {\n+            rankNum = params.getInt(\"N\");\n+        }\n+        mongoUri = params.get(\"mongoUri\");\n+        profilesLoader = new ProfilesLoader(params);\n+        metricProfileData = profilesLoader.getMetricProfileParser().getMetricData();\n+        groupEndpointData = profilesLoader.getTopologyEndpointParser().getTopology(profilesLoader.getAggregationProfileParser().getEndpointGroup().toUpperCase());\n+\n+        yesterdayData = readInputData(env, params, \"yesterdayData\");\n+        todayData = readInputData(env, params, \"todayData\");\n+        String metricOperation = profilesLoader.getAggregationProfileParser().getMetricOp();\n+\n+        // calculate on data \n+        DataSet<TimelineTrends> resultData = calcFlipFlops(profilesLoader.getOperationParser().getOpTruthTable().get(metricOperation));\n+        writeToMongo(resultData);\n+\n+// execute program\n+        env.execute(\"Flink Batch Java API Skeleton\");\n+\n+    }\n+\n+// filter yesterdaydata and exclude the ones not contained in topology and metric profile data and get the last timestamp data for each service endpoint metric\n+// filter todaydata and exclude the ones not contained in topology and metric profile data , union yesterday data and calculate status changes for each service endpoint metric\n+// rank results\n+    private static DataSet<TimelineTrends> calcFlipFlops(HashMap<String, String> truthTable) {\n+\n+        DataSet<MetricData> filteredYesterdayData = yesterdayData.filter(new TopologyMetricFilter(metricProfileData, groupEndpointData)).groupBy(\"hostname\", \"service\", \"metric\").reduceGroup(new CalcLastTimeStatus());\n+        DataSet<MetricData> filteredTodayData = todayData.filter(new TopologyMetricFilter(metricProfileData, groupEndpointData));\n+\n+        //group data by service enpoint metric and return for each group , the necessary info and a treemap containing timestamps and status\n+        DataSet<TimelineTrends> serviceEndpointMetricGroupData = filteredTodayData.union(filteredYesterdayData).groupBy(\"hostname\", \"service\", \"metric\").reduceGroup(new CalcMetricTimelineStatus(groupEndpointData));\n+\n+        //group data by service endpoint  and count flip flops\n+        DataSet<TimelineTrends> serviceEndpointGroupData = serviceEndpointMetricGroupData.groupBy(\"group\", \"endpoint\", \"service\").reduceGroup(new CalcServiceEndpointFlipFlop(profilesLoader.getOperationParser(), profilesLoader.getAggregationProfileParser().getMetricOp()));\n+\n+        //group data by service   and count flip flops\n+        DataSet<TimelineTrends> serviceGroupData = serviceEndpointGroupData.filter(new ServiceFilter(profilesLoader.getAggregationProfileParser().getServiceOperations())).groupBy(\"group\", \"service\").reduceGroup(new CalcServiceFlipFlop(profilesLoader.getOperationParser().getOpTruthTable(), profilesLoader.getAggregationProfileParser().getServiceOperations()));\n+\n+        \n+        if (rankNum != null) { //sort and rank data\n+            serviceGroupData = serviceGroupData.sortPartition(\"flipflops\", Order.DESCENDING).first(rankNum);\n+        } else {\n+            serviceGroupData = serviceGroupData.sortPartition(\"flipflops\", Order.DESCENDING);\n+        }\n+        return serviceGroupData;\n+\n+    }    //read input from file\n+\n+    private static DataSet<MetricData> readInputData(ExecutionEnvironment env, ParameterTool params, String path) {\n+        DataSet<MetricData> inputData;\n+        Path input = new Path(params.getRequired(path));\n+\n+        AvroInputFormat<MetricData> inputAvroFormat = new AvroInputFormat<MetricData>(input, MetricData.class\n+        );\n+        inputData = env.createInput(inputAvroFormat);\n+        return inputData;\n+    }\n+\n+//    //initialize configuaration parameters to be used from functions\n+//    private static void initializeConfigurationParameters(ParameterTool params, ExecutionEnvironment env) {\n+//\n+//        Configuration conf = new Configuration();\n+//        conf.setString(\"groupEndpointsPath\", params.get(\"groupEndpointsPath\"));\n+//        conf.setString(\"metricDataPath\", params.get(\"metricDataPath\"));\n+//        env.getConfig().setGlobalJobParameters(conf);\n+//\n+//    }\n+\n+    //convert the result in bson format\n+    public static DataSet<Tuple2<Text, BSONWritable>> convertResultToBSON(DataSet<TimelineTrends> in) {\n+\n+        return in.map(new MapFunction<TimelineTrends, Tuple2<Text, BSONWritable>>() {\n+            int i = 0;\n+\n+            @Override\n+            public Tuple2<Text, BSONWritable> map(TimelineTrends in) throws Exception {\n+                BasicDBObject dbObject = new BasicDBObject();\n+                dbObject.put(\"group\", in.getGroup().toString());\n+                dbObject.put(\"service\", in.getService().toString());\n+                dbObject.put(\"trend\", in.getFlipflops());\n+\n+                BSONWritable bson = new BSONWritable(dbObject);\n+                i++;\n+                return new Tuple2<Text, BSONWritable>(new Text(String.valueOf(i)), bson);\n+                /* TODO */\n+            }\n+        });\n+    }\n+\n+    //write to mongo db\n+    public static void writeToMongo(DataSet<TimelineTrends> data) {\n+        String collectionUri = mongoUri + \".\" + serviceFlipFlopTrends;\n+        DataSet<Tuple2<Text, BSONWritable>> result = convertResultToBSON(data);\n+        JobConf conf = new JobConf();\n+        conf.set(\"mongo.output.uri\", collectionUri);\n+\n+        MongoOutputFormat<Text, BSONWritable> mongoOutputFormat = new MongoOutputFormat<Text, BSONWritable>();\n+        result.output(new HadoopOutputFormat<Text, BSONWritable>(mongoOutputFormat, conf));\n+    }\n+\n+//    public static void createOpTruthTables(String baseUri, String key, String proxy, String operationsId) throws IOException, ParseException {\n+//        \n+//        opTruthTableMap = Utils.readOperationProfileJson(baseUri, key, proxy, operationsId);\n+//    }\n+}"
  },
  {
    "sha": "3dcbb2ad29b9121f9fddcb8c9392d51db12a0145",
    "filename": "flink_jobs/status_trends/src/main/java/argo/batch/BatchStatusTrends.java",
    "status": "modified",
    "additions": 20,
    "deletions": 14,
    "changes": 34,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/batch/BatchStatusTrends.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/batch/BatchStatusTrends.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/batch/BatchStatusTrends.java?ref=e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2",
    "patch": "@@ -17,7 +17,7 @@\n  * the License.\n  */\n import argo.avro.MetricData;\n-import argo.functions.statustrends.CalcServiceEnpointMetricStatus;\n+import argo.functions.calculations.CalcServiceEnpointMetricStatus;\n import argo.functions.timeline.TopologyMetricFilter;\n import argo.functions.timeline.CalcLastTimeStatus;\n import argo.functions.timeline.StatusFilter;\n@@ -41,8 +41,7 @@\n import org.slf4j.LoggerFactory;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.mapred.JobConf;\n-import parsers.AggregationProfileParser;\n-import parsers.ReportParser;\n+import argo.profileparsers.ProfilesLoader;\n \n /**\n  * Skeleton for a Flink Batch Job.\n@@ -69,32 +68,38 @@\n     private static DataSet<MetricData> todayData;\n     private static Integer rankNum;\n \n+    private static final String criticalStatusTrends = \"criticalStatusTrends\";\n+    private static final String warningStatusTrends = \"warningStatusTrends\";\n+    private static final String unknownStatusTrends = \"unknownStatusTrends\";\n+\n+    private static String mongoUri;\n+\n     public static void main(String[] args) throws Exception {\n         // set up the batch execution environment\n         final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n \n         final ParameterTool params = ParameterTool.fromArgs(args);\n         //check if all required parameters exist and if not exit program\n-        if (!Utils.checkParameters(params, \"yesterdayData\", \"todayData\", \"criticaluri\", \"warninguri\", \"unknownuri\", \"baseUri\", \"metricProfileUUID\", \"key\")) {\n+        if (!Utils.checkParameters(params, \"yesterdayData\", \"todayData\", \"apiUri\", \"key\")) {\n             System.exit(0);\n         }\n \n         env.setParallelism(1);\n         if (params.get(\"N\") != null) {\n             rankNum = params.getInt(\"N\");\n         }\n-        ReportParser.loadReportInfo(params.getRequired(\"baseUri\"), params.getRequired(\"key\"), params.get(\"proxy\"), params.getRequired(\"reportId\"));\n-        AggregationProfileParser.loadAggrProfileInfo(params.getRequired(\"baseUri\"), params.getRequired(\"key\"), params.get(\"proxy\"));\n-       \n-        metricProfileData = Utils.readMetricDataJson(params.getRequired(\"baseUri\"), params.getRequired(\"metricProfileUUID\"), params.getRequired(\"key\"),params.get(\"proxy\")); //contains the information of the (service, metrics) matches\n-        groupEndpointData = Utils.readGroupEndpointJson(params.getRequired(\"baseUri\"), params.getRequired(\"key\"),params.get(\"proxy\")); //contains the information of the (service, metrics) matches\n+        mongoUri = params.get(\"mongoUri\");\n+        ProfilesLoader profilesLoader = new ProfilesLoader(params);\n+        metricProfileData = profilesLoader.getMetricProfileParser().getMetricData();\n+        groupEndpointData = profilesLoader.getTopologyEndpointParser().getTopology(profilesLoader.getAggregationProfileParser().getEndpointGroup().toUpperCase());\n+\n         yesterdayData = readInputData(env, params, \"yesterdayData\");\n         todayData = readInputData(env, params, \"todayData\");\n \n         DataSet<Tuple6<String, String, String, String, String, Integer>> rankedData = rankByStatus();\n-        filterByStatusAndWrite(params.getRequired(\"criticaluri\"), rankedData, \"critical\");\n-        filterByStatusAndWrite(params.getRequired(\"warninguri\"), rankedData, \"warning\");\n-        filterByStatusAndWrite(params.getRequired(\"unknownuri\"), rankedData, \"unknown\");\n+        filterByStatusAndWrite(criticalStatusTrends, rankedData, \"critical\");\n+        filterByStatusAndWrite(warningStatusTrends, rankedData, \"warning\");\n+        filterByStatusAndWrite(unknownStatusTrends, rankedData, \"unknown\");\n \n // execute program\n         env.execute(\"Flink Batch Java API Skeleton\");\n@@ -113,6 +118,7 @@ public static void main(String[] args) throws Exception {\n \n     // filter the data based on status (CRITICAL,WARNING,UNKNOWN), rank and write top N in seperate files for each status\n     private static void filterByStatusAndWrite(String uri, DataSet<Tuple6<String, String, String, String, String, Integer>> data, String status) {\n+        String collectionUri = mongoUri + \".\" + uri;\n         DataSet<Tuple6<String, String, String, String, String, Integer>> filteredData = data.filter(new StatusFilter(status));\n \n         if (rankNum != null) {\n@@ -121,7 +127,7 @@ private static void filterByStatusAndWrite(String uri, DataSet<Tuple6<String, St\n             filteredData = filteredData.sortPartition(5, Order.DESCENDING);\n         }\n \n-        writeToMongo(uri, filteredData);\n+        writeToMongo(collectionUri, filteredData);\n     }\n \n     // reads input from file\n@@ -134,7 +140,6 @@ private static void filterByStatusAndWrite(String uri, DataSet<Tuple6<String, St\n         return inputData;\n     }\n \n-\n     //convert the result in bson format\n     public static DataSet<Tuple2<Text, BSONWritable>> convertResultToBSON(DataSet<Tuple6<String, String, String, String, String, Integer>> in) {\n \n@@ -158,6 +163,7 @@ private static void filterByStatusAndWrite(String uri, DataSet<Tuple6<String, St\n             }\n         });\n     }\n+\n     //write to mongo db\n     public static void writeToMongo(String uri, DataSet<Tuple6<String, String, String, String, String, Integer>> data) {\n         DataSet<Tuple2<Text, BSONWritable>> result = convertResultToBSON(data);"
  },
  {
    "sha": "cb223273fbb4698340dbeca718b4f37f7a9f66a5",
    "filename": "flink_jobs/status_trends/src/main/java/argo/functions/calculations/CalcGroupFlipFlop.java",
    "status": "added",
    "additions": 62,
    "deletions": 0,
    "changes": 62,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/functions/calculations/CalcGroupFlipFlop.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/functions/calculations/CalcGroupFlipFlop.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/functions/calculations/CalcGroupFlipFlop.java?ref=e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2",
    "patch": "@@ -0,0 +1,62 @@\n+package argo.functions.calculations;\n+\n+/*\n+ * To change this license header, choose License Headers in Project Properties.\n+ * To change this template file, choose Tools | Templates\n+ * and open the template in the editor.\n+ */\n+import argo.utils.TimelineBuilder;\n+import argo.pojos.TimelineTrends;\n+import argo.profileparsers.AggregationProfileParser.GroupOps;\n+import java.util.ArrayList;\n+import java.util.Date;\n+import java.util.HashMap;\n+import java.util.TreeMap;\n+import org.apache.flink.api.common.functions.GroupReduceFunction;\n+import org.apache.flink.util.Collector;\n+\n+/**\n+ *\n+ * @author cthermolia\n+ *\n+ * CalcServiceEndpointFlipFlop, count status changes for each service endpoint\n+ * group\n+ */\n+public class CalcGroupFlipFlop implements GroupReduceFunction< TimelineTrends, TimelineTrends> {\n+\n+    private HashMap<String,HashMap<String,String>> operationTruthTables;\n+\n+    private String groupOperation;\n+\n+    public CalcGroupFlipFlop(HashMap<String,HashMap<String,String>> operationTruthTables, String groupOperation) {\n+        this.operationTruthTables = operationTruthTables;\n+        this.groupOperation=groupOperation;\n+    }\n+\n+    @Override\n+    public void reduce(Iterable<TimelineTrends> in, Collector< TimelineTrends> out) throws Exception {\n+        String group = null;\n+        String function = null;\n+        ArrayList<TimelineTrends> list = new ArrayList<>();\n+        //construct a timeline containing all the timestamps of each metric timeline\n+\n+        TimelineBuilder timebuilder = new TimelineBuilder();\n+\n+        ArrayList<TimelineTrends> timelist = new ArrayList<>();\n+        for (TimelineTrends time : in) {\n+            group = time.getGroup();\n+            function=time.getFunction();\n+            timelist.add(time);\n+        }\n+        \n+        HashMap<String, String> opTruthTable =  operationTruthTables.get(groupOperation);\n+    \n+        TreeMap<Date, String> resultMap = timebuilder.buildStatusTimeline(timelist, opTruthTable);\n+        int flipflops = timebuilder.calcFlipFlops(resultMap);\n+\n+        TimelineTrends serviceFlipFlop = new TimelineTrends(group, resultMap, flipflops);\n+        out.collect(serviceFlipFlop);\n+\n+    }\n+\n+}"
  },
  {
    "sha": "44074d217a284cc1bf3870d80367e2b49089d10c",
    "filename": "flink_jobs/status_trends/src/main/java/argo/functions/calculations/CalcGroupFunctionFlipFlop.java",
    "status": "added",
    "additions": 63,
    "deletions": 0,
    "changes": 63,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/functions/calculations/CalcGroupFunctionFlipFlop.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/functions/calculations/CalcGroupFunctionFlipFlop.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/functions/calculations/CalcGroupFunctionFlipFlop.java?ref=e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2",
    "patch": "@@ -0,0 +1,63 @@\n+package argo.functions.calculations;\n+\n+/*\n+ * To change this license header, choose License Headers in Project Properties.\n+ * To change this template file, choose Tools | Templates\n+ * and open the template in the editor.\n+ */\n+import argo.utils.TimelineBuilder;\n+import argo.pojos.TimelineTrends;\n+import argo.profileparsers.AggregationProfileParser.GroupOps;\n+import java.util.ArrayList;\n+import java.util.Date;\n+import java.util.HashMap;\n+import java.util.TreeMap;\n+import org.apache.flink.api.common.functions.GroupReduceFunction;\n+import org.apache.flink.util.Collector;\n+\n+/**\n+ *\n+ * @author cthermolia\n+ *\n+ * CalcGroupFunctionFlipFlop, count status changes for each group function\n+ * group\n+ */\n+public class CalcGroupFunctionFlipFlop implements GroupReduceFunction< TimelineTrends, TimelineTrends> {\n+\n+    private HashMap<String,HashMap<String,String>> operationTruthTables;\n+\n+    private HashMap<String,String> functionOperations;\n+\n+    public CalcGroupFunctionFlipFlop(HashMap<String,HashMap<String,String>> operationTruthTables, HashMap<String,String> functionOperations) {\n+        this.operationTruthTables = operationTruthTables;\n+        this.functionOperations=functionOperations;\n+    }\n+\n+    @Override\n+    public void reduce(Iterable<TimelineTrends> in, Collector< TimelineTrends> out) throws Exception {\n+        String group = null;\n+        String function = null;\n+        ArrayList<TimelineTrends> list = new ArrayList<>();\n+        //construct a timeline containing all the timestamps of each metric timeline\n+\n+        TimelineBuilder timebuilder = new TimelineBuilder();\n+\n+        ArrayList<TimelineTrends> timelist = new ArrayList<>();\n+        for (TimelineTrends time : in) {\n+            group = time.getGroup();\n+            function=time.getFunction();\n+            timelist.add(time);\n+        }\n+        String operation=functionOperations.get(function);  //for each function an operation exists , so retrieve the corresponding truth table\n+        \n+        HashMap<String, String> opTruthTable =  operationTruthTables.get(operation);\n+    \n+        TreeMap<Date, String> resultMap = timebuilder.buildStatusTimeline(timelist, opTruthTable);\n+        int flipflops = timebuilder.calcFlipFlops(resultMap);\n+\n+        TimelineTrends serviceFlipFlop = new TimelineTrends(group, function, resultMap, flipflops);\n+        out.collect(serviceFlipFlop);\n+\n+    }\n+\n+}"
  },
  {
    "sha": "9db8f155720cfa7f3fa4435d1a9d215b228f71e4",
    "filename": "flink_jobs/status_trends/src/main/java/argo/functions/calculations/CalcServiceEndpointFlipFlop.java",
    "status": "added",
    "additions": 69,
    "deletions": 0,
    "changes": 69,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/functions/calculations/CalcServiceEndpointFlipFlop.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/functions/calculations/CalcServiceEndpointFlipFlop.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/functions/calculations/CalcServiceEndpointFlipFlop.java?ref=e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2",
    "patch": "@@ -0,0 +1,69 @@\n+package argo.functions.calculations;\n+\n+/*\n+ * To change this license header, choose License Headers in Project Properties.\n+ * To change this template file, choose Tools | Templates\n+ * and open the template in the editor.\n+ */\n+import argo.utils.TimelineBuilder;\n+import argo.pojos.TimelineTrends;\n+import argo.profileparsers.OperationsParser;\n+import argo.utils.Utils;\n+import java.text.ParseException;\n+import java.util.ArrayList;\n+import java.util.Date;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.TreeMap;\n+import org.apache.flink.api.common.functions.GroupReduceFunction;\n+import org.apache.flink.util.Collector;\n+\n+/**\n+ *\n+ * @author cthermolia\n+ *\n+ * CalcServiceEndpointFlipFlop, count status changes for each service endpoint\n+ * group\n+ */\n+public class CalcServiceEndpointFlipFlop implements GroupReduceFunction< TimelineTrends, TimelineTrends> {\n+\n+    private HashMap<String, String> opTruthTable;\n+    private OperationsParser operationsParser;\n+\n+    public CalcServiceEndpointFlipFlop(OperationsParser operationsParser, String operation) {\n+        this.operationsParser = this.operationsParser;\n+        this.opTruthTable=operationsParser.getOpTruthTable().get(operation);\n+    }\n+\n+    @Override\n+    public void reduce(Iterable<TimelineTrends> in, Collector< TimelineTrends> out) throws Exception {\n+        String group = null;\n+        String service = null;\n+        String hostname = null;\n+        ArrayList<TimelineTrends> list = new ArrayList<>();\n+        //construct a timeline containing all the timestamps of each metric timeline\n+         \n+        TimelineBuilder timebuilder = new TimelineBuilder();\n+\n+        ArrayList<TimelineTrends> timelist = new ArrayList<>();\n+        for (TimelineTrends time : in) {\n+            group = time.getGroup();\n+            service = time.getService();\n+            hostname = time.getEndpoint();\n+            timelist.add(time);\n+        }\n+\n+        TreeMap<Date, String> resultMap = timebuilder.buildStatusTimeline(timelist, opTruthTable);\n+        int flipflops = timebuilder.calcFlipFlops(resultMap);\n+\n+        TimelineTrends servEndpFlipFlop = new TimelineTrends(group, service, hostname, resultMap, flipflops);\n+\n+        //Tuple4<String, String, String, Integer> tuple = new Tuple4<String, String, String, Integer>(group, service, hostname, flipflops);\n+        out.collect(servEndpFlipFlop);\n+\n+    }\n+\n+   \n+   \n+}"
  },
  {
    "sha": "6385a5fc67a149e164293d24d287dcb79d8798fc",
    "filename": "flink_jobs/status_trends/src/main/java/argo/functions/calculations/CalcServiceEnpointMetricFlipFlop.java",
    "status": "renamed",
    "additions": 2,
    "deletions": 2,
    "changes": 4,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/functions/calculations/CalcServiceEnpointMetricFlipFlop.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/functions/calculations/CalcServiceEnpointMetricFlipFlop.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/functions/calculations/CalcServiceEnpointMetricFlipFlop.java?ref=e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2",
    "patch": "@@ -1,4 +1,4 @@\n-package argo.functions.flipfloptrends;\n+package argo.functions.calculations;\n \n /*\n  * To change this license header, choose License Headers in Project Properties.\n@@ -57,7 +57,7 @@ public void reduce(Iterable<MetricData> in, Collector<Tuple5<String, String, Str\n         int flipflop = calcFlipFlops(timeStatusMap);\n \n         if (group != null && service != null && hostname != null && metric != null) {\n-            Tuple5<String, String, String, String, Integer> tuple = new Tuple5<String, String, String, String, Integer>(\n+           Tuple5<String, String, String, String, Integer> tuple = new Tuple5<String, String, String, String, Integer>(\n                     group, service, hostname, metric, flipflop\n             );\n             out.collect(tuple);",
    "previous_filename": "flink_jobs/status_trends/src/main/java/argo/functions/flipfloptrends/CalcServiceEnpointMetricFlipFlop.java"
  },
  {
    "sha": "941147c1cd9006ce174005f12acde13ca19b5bdb",
    "filename": "flink_jobs/status_trends/src/main/java/argo/functions/calculations/CalcServiceEnpointMetricStatus.java",
    "status": "renamed",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/functions/calculations/CalcServiceEnpointMetricStatus.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/functions/calculations/CalcServiceEnpointMetricStatus.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/functions/calculations/CalcServiceEnpointMetricStatus.java?ref=e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2",
    "patch": "@@ -1,4 +1,4 @@\n-package argo.functions.statustrends;\n+package argo.functions.calculations;\n \n /*\n  * To change this license header, choose License Headers in Project Properties.",
    "previous_filename": "flink_jobs/status_trends/src/main/java/argo/functions/statustrends/CalcServiceEnpointMetricStatus.java"
  },
  {
    "sha": "2a3a1784e59458a1f2c6d908972ab87fe7c67e3e",
    "filename": "flink_jobs/status_trends/src/main/java/argo/functions/calculations/CalcServiceFlipFlop.java",
    "status": "added",
    "additions": 63,
    "deletions": 0,
    "changes": 63,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/functions/calculations/CalcServiceFlipFlop.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/functions/calculations/CalcServiceFlipFlop.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/functions/calculations/CalcServiceFlipFlop.java?ref=e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2",
    "patch": "@@ -0,0 +1,63 @@\n+package argo.functions.calculations;\n+\n+/*\n+ * To change this license header, choose License Headers in Project Properties.\n+ * To change this template file, choose Tools | Templates\n+ * and open the template in the editor.\n+ */\n+import argo.utils.TimelineBuilder;\n+import argo.pojos.TimelineTrends;\n+import java.util.ArrayList;\n+import java.util.Date;\n+import java.util.HashMap;\n+import java.util.TreeMap;\n+import org.apache.flink.api.common.functions.GroupReduceFunction;\n+import org.apache.flink.util.Collector;\n+\n+/**\n+ *\n+ * @author cthermolia\n+ *\n+ * CalcServiceEndpointFlipFlop, count status changes for each service endpoint\n+ * group\n+ */\n+public class CalcServiceFlipFlop implements GroupReduceFunction< TimelineTrends, TimelineTrends> {\n+\n+    private HashMap<String,HashMap<String,String>> operationTruthTables;\n+\n+    private HashMap<String,String> serviceOperationMap;\n+\n+    public CalcServiceFlipFlop(HashMap<String,HashMap<String,String>> operationTruthTables, HashMap<String,String> serviceOperationMap) {\n+        this.operationTruthTables = operationTruthTables;\n+        this.serviceOperationMap=serviceOperationMap;\n+    }\n+\n+    @Override\n+    public void reduce(Iterable<TimelineTrends> in, Collector< TimelineTrends> out) throws Exception {\n+        String group = null;\n+        String service = null;\n+        String hostname = null;\n+        ArrayList<TimelineTrends> list = new ArrayList<>();\n+        //construct a timeline containing all the timestamps of each metric timeline\n+\n+        TimelineBuilder timebuilder = new TimelineBuilder();\n+\n+        ArrayList<TimelineTrends> timelist = new ArrayList<>();\n+        for (TimelineTrends time : in) {\n+            group = time.getGroup();\n+            service = time.getService();\n+            timelist.add(time);\n+        }\n+        String operation=serviceOperationMap.get(service);\n+        \n+        HashMap<String, String> opTruthTable =  operationTruthTables.get(operation);\n+    \n+        TreeMap<Date, String> resultMap = timebuilder.buildStatusTimeline(timelist, opTruthTable);\n+        int flipflops = timebuilder.calcFlipFlops(resultMap);\n+\n+        TimelineTrends serviceFlipFlop = new TimelineTrends(group, service, resultMap, flipflops);\n+        out.collect(serviceFlipFlop);\n+\n+    }\n+\n+}"
  },
  {
    "sha": "687728934aed39c987a3d3aba9a23514523ea900",
    "filename": "flink_jobs/status_trends/src/main/java/argo/functions/timeline/CalcMetricTimelineStatus.java",
    "status": "modified",
    "additions": 4,
    "deletions": 4,
    "changes": 8,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/functions/timeline/CalcMetricTimelineStatus.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/functions/timeline/CalcMetricTimelineStatus.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/functions/timeline/CalcMetricTimelineStatus.java?ref=e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2",
    "patch": "@@ -6,7 +6,7 @@\n package argo.functions.timeline;\n \n import argo.avro.MetricData;\n-import argo.pojos.MetricTimelinePojo;\n+import argo.pojos.TimelineTrends;\n import argo.utils.Utils;\n import java.util.ArrayList;\n import java.util.Date;\n@@ -21,7 +21,7 @@\n  * CalcMetricTimelineStatus calculates the timeline for each group, service,\n  * endpoint , metric\n  */\n-public class CalcMetricTimelineStatus implements GroupReduceFunction<MetricData, MetricTimelinePojo> {\n+public class CalcMetricTimelineStatus implements GroupReduceFunction<MetricData, TimelineTrends> {\n \n     private HashMap<String, String> groupEndpoints;\n \n@@ -30,7 +30,7 @@ public CalcMetricTimelineStatus(HashMap<String, String> groupEndpoints) {\n     }\n \n     @Override\n-    public void reduce(Iterable<MetricData> in, Collector<MetricTimelinePojo> out) throws Exception {\n+    public void reduce(Iterable<MetricData> in, Collector<TimelineTrends> out) throws Exception {\n \n         String group = null;\n         String service = null;\n@@ -47,7 +47,7 @@ public void reduce(Iterable<MetricData> in, Collector<MetricTimelinePojo> out) t\n             Date timestamp = Utils.convertStringtoDate(md.getTimestamp().toString());\n             mapTimeline.put(timestamp, md.getStatus().toString());\n         }\n-        MetricTimelinePojo mt = new MetricTimelinePojo(group, service, metric, metric, mapTimeline);\n+        TimelineTrends mt = new TimelineTrends(group, service, metric, metric, mapTimeline,0);\n         out.collect(mt);\n     }\n "
  },
  {
    "sha": "e892a8370559958c7dab1b8f68c60402981fa631",
    "filename": "flink_jobs/status_trends/src/main/java/argo/functions/timeline/LastTimeStampGroupReduce.java",
    "status": "removed",
    "additions": 0,
    "deletions": 40,
    "changes": 40,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/9615d67301d4def5b974d5a080bb60d8c3bb1525/flink_jobs/status_trends/src/main/java/argo/functions/timeline/LastTimeStampGroupReduce.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/9615d67301d4def5b974d5a080bb60d8c3bb1525/flink_jobs/status_trends/src/main/java/argo/functions/timeline/LastTimeStampGroupReduce.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/functions/timeline/LastTimeStampGroupReduce.java?ref=9615d67301d4def5b974d5a080bb60d8c3bb1525",
    "patch": "@@ -1,40 +0,0 @@\n-package argo.functions.timeline;\n-\n-/*\n- * To change this license header, choose License Headers in Project Properties.\n- * To change this template file, choose Tools | Templates\n- * and open the template in the editor.\n- */\n-import argo.avro.MetricData;\n-import java.util.TreeMap;\n-import org.apache.flink.api.common.functions.GroupReduceFunction;\n-import org.apache.flink.util.Collector;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n-/**\n- *\n- * @author cthermolia LastTimeStampGroupReduce keeps data of the latest time\n- * entry\n- */\n-public class LastTimeStampGroupReduce implements GroupReduceFunction<MetricData, MetricData> {\n-\n-    static Logger LOG = LoggerFactory.getLogger(LastTimeStampGroupReduce.class);\n-\n-    /**\n-     *\n-     * @param in, the initial dataset of the MetricData\n-     * @param out , the output dataset containing the MetricData of the latest\n-     * timestamp\n-     * @throws Exception\n-     */\n-    @Override\n-    public void reduce(Iterable<MetricData> in, Collector<MetricData> out) throws Exception {\n-        TreeMap<String, MetricData> timeStatusMap = new TreeMap<>();\n-        for (MetricData md : in) {\n-            timeStatusMap.put(md.getTimestamp().toString(), md);\n-\n-        }\n-        out.collect(timeStatusMap.lastEntry().getValue());\n-    }\n-}"
  },
  {
    "sha": "9e19043345027ab41d4c68dc124b43f57c56445b",
    "filename": "flink_jobs/status_trends/src/main/java/argo/functions/timeline/MapServices.java",
    "status": "added",
    "additions": 48,
    "deletions": 0,
    "changes": 48,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/functions/timeline/MapServices.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/functions/timeline/MapServices.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/functions/timeline/MapServices.java?ref=e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2",
    "patch": "@@ -0,0 +1,48 @@\n+/*\n+ * To change this license header, choose License Headers in Project Properties.\n+ * To change this template file, choose Tools | Templates\n+ * and open the template in the editor.\n+ */\n+package argo.functions.timeline;\n+\n+import argo.pojos.TimelineTrends;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import org.apache.flink.api.common.functions.FlatMapFunction;\n+import org.apache.flink.util.Collector;\n+\n+/**\n+ * @author cthermolia\n+ *\n+ * MapServices produces TimelineTrends for each service,that maps to the groups of functions as described in aggregation profile groups\n+ * endpoint , metric\n+ */\n+public class MapServices implements FlatMapFunction<TimelineTrends, TimelineTrends> {\n+\n+    private HashMap<String, ArrayList<String>> serviceFunctions;\n+\n+    public MapServices(HashMap<String, ArrayList<String>> serviceFunctions) {\n+        this.serviceFunctions = serviceFunctions;\n+    }\n+\n+    /**\n+     * if the service exist in one or more function groups ,  timeline trends are produced for each function that the service belongs and the function info is added to the timelinetrend\n+     * @param t\n+     * @param out\n+     * @throws Exception \n+     */\n+    @Override\n+    public void flatMap(TimelineTrends t, Collector<TimelineTrends> out) throws Exception {\n+\n+        String service = t.getService();\n+\n+        ArrayList<String> functionList = serviceFunctions.get(service);\n+        for (String f : functionList) {\n+            TimelineTrends newT=t;\n+            newT.setFunction(f);\n+            out.collect(newT);\n+        }\n+\n+    }\n+\n+}"
  },
  {
    "sha": "e603bf1371d43153b396e049c6fd9689080b668b",
    "filename": "flink_jobs/status_trends/src/main/java/argo/functions/timeline/ServiceFilter.java",
    "status": "added",
    "additions": 41,
    "deletions": 0,
    "changes": 41,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/functions/timeline/ServiceFilter.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/functions/timeline/ServiceFilter.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/functions/timeline/ServiceFilter.java?ref=e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2",
    "patch": "@@ -0,0 +1,41 @@\n+/*\n+ * To change this license header, choose License Headers in Project Properties.\n+ * To change this template file, choose Tools | Templates\n+ * and open the template in the editor.\n+ */\n+package argo.functions.timeline;\n+\n+import argo.pojos.TimelineTrends;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import org.apache.flink.api.common.functions.FilterFunction;\n+import org.apache.flink.api.java.tuple.Tuple6;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ *\n+ * @author cthermolia\n+ *\n+ * StatusFilter, filters data by status\n+ */\n+public class ServiceFilter implements FilterFunction<TimelineTrends> {\n+\n+    static Logger LOG = LoggerFactory.getLogger(ServiceFilter.class);\n+    private HashMap<String, String> serviceOperations;\n+\n+    public ServiceFilter(HashMap<String, String> serviceOperations) {\n+        this.serviceOperations = serviceOperations;\n+    }\n+    //if the status field value in Tuple equals the given status returns true, else returns false\n+\n+    @Override\n+    public boolean filter(TimelineTrends t) throws Exception {\n+        ArrayList<String> services = new ArrayList<>(serviceOperations.keySet());\n+        if (services.contains(t.getService())) {\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+}"
  },
  {
    "sha": "44e2bb3f0ce87245ff19ec730e46d43bf4b502e0",
    "filename": "flink_jobs/status_trends/src/main/java/argo/pojos/MetricTimelinePojo.java",
    "status": "removed",
    "additions": 0,
    "deletions": 75,
    "changes": 75,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/9615d67301d4def5b974d5a080bb60d8c3bb1525/flink_jobs/status_trends/src/main/java/argo/pojos/MetricTimelinePojo.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/9615d67301d4def5b974d5a080bb60d8c3bb1525/flink_jobs/status_trends/src/main/java/argo/pojos/MetricTimelinePojo.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/pojos/MetricTimelinePojo.java?ref=9615d67301d4def5b974d5a080bb60d8c3bb1525",
    "patch": "@@ -1,75 +0,0 @@\n-/*\n- * To change this license header, choose License Headers in Project Properties.\n- * To change this template file, choose Tools | Templates\n- * and open the template in the editor.\n- */\n-package argo.pojos;\n-\n-import java.util.Date;\n-import java.util.TreeMap;\n-\n-/**\n- *\n- * @author cthermolia\n- */\n-public class MetricTimelinePojo {\n-\n-    String group;\n-    String service;\n-    String endpoint;\n-    String metric;\n-    TreeMap<Date, String> timelineMap;\n-\n-    public MetricTimelinePojo() {\n-    }\n-\n-    \n-    public MetricTimelinePojo(String group, String service, String endpoint, String metric, TreeMap<Date, String> timelineMap) {\n-        this.group = group;\n-        this.service = service;\n-        this.endpoint = endpoint;\n-        this.metric = metric;\n-        this.timelineMap = timelineMap;\n-    }\n-\n-    public String getGroup() {\n-        return group;\n-    }\n-\n-    public void setGroup(String group) {\n-        this.group = group;\n-    }\n-\n-    public String getService() {\n-        return service;\n-    }\n-\n-    public void setService(String service) {\n-        this.service = service;\n-    }\n-\n-    public String getEndpoint() {\n-        return endpoint;\n-    }\n-\n-    public void setEndpoint(String endpoint) {\n-        this.endpoint = endpoint;\n-    }\n-\n-    public String getMetric() {\n-        return metric;\n-    }\n-\n-    public void setMetric(String metric) {\n-        this.metric = metric;\n-    }\n-\n-    public TreeMap<Date, String> getTimelineMap() {\n-        return timelineMap;\n-    }\n-\n-    public void setTimelineMap(TreeMap<Date, String> timelineMap) {\n-        this.timelineMap = timelineMap;\n-    }\n-\n-}"
  },
  {
    "sha": "9e2f79476d0308760eabda0724be873250270568",
    "filename": "flink_jobs/status_trends/src/main/java/argo/pojos/ServEndpFlipFlopPojo.java",
    "status": "removed",
    "additions": 0,
    "deletions": 63,
    "changes": 63,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/9615d67301d4def5b974d5a080bb60d8c3bb1525/flink_jobs/status_trends/src/main/java/argo/pojos/ServEndpFlipFlopPojo.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/9615d67301d4def5b974d5a080bb60d8c3bb1525/flink_jobs/status_trends/src/main/java/argo/pojos/ServEndpFlipFlopPojo.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/pojos/ServEndpFlipFlopPojo.java?ref=9615d67301d4def5b974d5a080bb60d8c3bb1525",
    "patch": "@@ -1,63 +0,0 @@\n-/*\n- * To change this license header, choose License Headers in Project Properties.\n- * To change this template file, choose Tools | Templates\n- * and open the template in the editor.\n- */\n-package argo.pojos;\n-\n-/**\n- *\n- * @author cthermolia\n- */\n-public class ServEndpFlipFlopPojo {\n-\n-    String group;\n-    String service;\n-\n-    String hostname;\n-    Integer flipflops;\n-\n-    public ServEndpFlipFlopPojo() {\n-    }\n-\n-    \n-    public ServEndpFlipFlopPojo(String group, String service, String hostname, Integer flipflops) {\n-        this.group = group;\n-        this.service = service;\n-        this.hostname = hostname;\n-        this.flipflops = flipflops;\n-    }\n-\n-    public String getGroup() {\n-        return group;\n-    }\n-\n-    public void setGroup(String group) {\n-        this.group = group;\n-    }\n-\n-    public String getService() {\n-        return service;\n-    }\n-\n-    public void setService(String service) {\n-        this.service = service;\n-    }\n-\n-    public String getHostname() {\n-        return hostname;\n-    }\n-\n-    public void setHostname(String hostname) {\n-        this.hostname = hostname;\n-    }\n-\n-    public Integer getFlipflops() {\n-        return flipflops;\n-    }\n-\n-    public void setFlipflops(Integer flipflops) {\n-        this.flipflops = flipflops;\n-    }\n-\n-}"
  },
  {
    "sha": "e12254d43c160d904b1f934633c7a3c27a8300c6",
    "filename": "flink_jobs/status_trends/src/main/java/argo/pojos/TimelineTrends.java",
    "status": "added",
    "additions": 127,
    "deletions": 0,
    "changes": 127,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/pojos/TimelineTrends.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/pojos/TimelineTrends.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/pojos/TimelineTrends.java?ref=e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2",
    "patch": "@@ -0,0 +1,127 @@\n+/*\n+ * To change this license header, choose License Headers in Project Properties.\n+ * To change this template file, choose Tools | Templates\n+ * and open the template in the editor.\n+ */\n+package argo.pojos;\n+\n+import java.util.Date;\n+import java.util.TreeMap;\n+\n+/**\n+ *\n+ * @author cthermolia\n+ */\n+public class TimelineTrends {\n+    String function;\n+    String group;\n+    String service;\n+    String endpoint;\n+    String metric;\n+    TreeMap<Date, String> timelineMap;\n+    Integer flipflops;\n+\n+    public TimelineTrends() {\n+    }\n+\n+    public TimelineTrends(String function, String group, String service, String endpoint, String metric, TreeMap<Date, String> timelineMap, Integer flipflops) {\n+        this.function = function;\n+        this.group = group;\n+        this.service = service;\n+        this.endpoint = endpoint;\n+        this.metric = metric;\n+        this.timelineMap = timelineMap;\n+        this.flipflops = flipflops;\n+    }\n+\n+    \n+    \n+    public TimelineTrends(String group, TreeMap<Date, String> timelineMap, Integer flipflops) {\n+        this.group = group;\n+        this.timelineMap = timelineMap;\n+        this.flipflops = flipflops;\n+    }\n+\n+    public TimelineTrends(String group, String service, TreeMap<Date, String> timelineMap, Integer flipflops) {\n+        this.group = group;\n+        this.service = service;\n+        this.timelineMap = timelineMap;\n+        this.flipflops = flipflops;\n+    }\n+\n+    public TimelineTrends(String group, String service, String endpoint, TreeMap<Date, String> timelineMap, Integer flipflops) {\n+        this.group = group;\n+        this.service = service;\n+        this.endpoint = endpoint;\n+        this.timelineMap = timelineMap;\n+        this.flipflops = flipflops;\n+    }\n+\n+    public TimelineTrends(String group, String service, String endpoint, String metric, TreeMap<Date, String> timelineMap, Integer flipflops) {\n+        this.group = group;\n+        this.service = service;\n+        this.endpoint = endpoint;\n+        this.metric = metric;\n+        this.timelineMap = timelineMap;\n+        this.flipflops = flipflops;\n+    }\n+\n+    public String getGroup() {\n+        return group;\n+    }\n+\n+    public void setGroup(String group) {\n+        this.group = group;\n+    }\n+\n+    public String getService() {\n+        return service;\n+    }\n+\n+    public void setService(String service) {\n+        this.service = service;\n+    }\n+\n+    public String getEndpoint() {\n+        return endpoint;\n+    }\n+\n+    public void setEndpoint(String endpoint) {\n+        this.endpoint = endpoint;\n+    }\n+\n+    public String getMetric() {\n+        return metric;\n+    }\n+\n+    public void setMetric(String metric) {\n+        this.metric = metric;\n+    }\n+\n+    public TreeMap<Date, String> getTimelineMap() {\n+        return timelineMap;\n+    }\n+\n+    public void setTimelineMap(TreeMap<Date, String> timelineMap) {\n+        this.timelineMap = timelineMap;\n+    }\n+\n+    public Integer getFlipflops() {\n+        return flipflops;\n+    }\n+\n+    public void setFlipflops(Integer flipflops) {\n+        this.flipflops = flipflops;\n+    }\n+\n+    public String getFunction() {\n+        return function;\n+    }\n+\n+    public void setFunction(String function) {\n+        this.function = function;\n+    }\n+\n+    \n+\n+}"
  },
  {
    "sha": "6f8018aaf25568f16528bb2cd4f3f72f8d63329b",
    "filename": "flink_jobs/status_trends/src/main/java/argo/profileparsers/AggregationProfileParser.java",
    "status": "added",
    "additions": 198,
    "deletions": 0,
    "changes": 198,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/profileparsers/AggregationProfileParser.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/profileparsers/AggregationProfileParser.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/profileparsers/AggregationProfileParser.java?ref=e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2",
    "patch": "@@ -0,0 +1,198 @@\n+/*\n+ * To change this license header, choose License Headers in Project Properties.\n+ * To change this template file, choose Tools | Templates\n+ * and open the template in the editor.\n+ */\n+package argo.profileparsers;\n+\n+import argo.utils.RequestManager;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import org.json.simple.JSONArray;\n+import org.json.simple.JSONObject;\n+import org.json.simple.parser.ParseException;\n+\n+/**\n+ *\n+ * @author cthermolia\n+ */\n+public class AggregationProfileParser {\n+\n+    private String id;\n+    private String date;\n+    private String name;\n+    private String namespace;\n+    private String endpointGroup;\n+    private String metricOp;\n+    private String profileOp;\n+    private String[] metricProfile = new String[2];\n+    private ArrayList<GroupOps> groups = new ArrayList<>();\n+\n+    private HashMap<String, String> serviceOperations = new HashMap<>();\n+    private HashMap<String, String> functionOperations = new HashMap<>();\n+  \n+    private HashMap<String, ArrayList<String>> serviceFunctions = new HashMap<>();\n+    private final String url = \"/aggregation_profiles\";\n+\n+    public AggregationProfileParser(String apiUri, String key, String proxy, String aggregationId, String dateStr) throws IOException, ParseException {\n+\n+        String uri = apiUri + url;\n+        if (dateStr == null) {\n+            uri = uri + \"/\" + aggregationId;\n+        } else {\n+            uri = uri + \"?date=\" + dateStr;\n+        }\n+\n+        loadAggrProfileInfo(uri, key, proxy);\n+    }\n+\n+    public void loadAggrProfileInfo(String uri, String key, String proxy) throws IOException, ParseException {\n+\n+        JSONObject jsonObject = RequestManager.request(uri, key, proxy);\n+\n+        JSONArray dataList = (JSONArray) jsonObject.get(\"data\");\n+\n+        Iterator<JSONObject> iterator = dataList.iterator();\n+\n+        while (iterator.hasNext()) {\n+            if (iterator.next() instanceof JSONObject) {\n+                JSONObject dataObject = (JSONObject) iterator.next();\n+\n+                id = (String) dataObject.get(\"id\");\n+                date = (String) dataObject.get(\"date\");\n+                name = (String) dataObject.get(\"name\");\n+                namespace = (String) dataObject.get(\"namespace\");\n+                endpointGroup = (String) dataObject.get(\"endpoint_group\");\n+                metricOp = (String) dataObject.get(\"metric_operation\");\n+                profileOp = (String) dataObject.get(\"profile_operation\");\n+\n+                JSONObject metricProfileObject = (JSONObject) dataObject.get(\"metric_profile\");\n+\n+                metricProfile[0] = (String) metricProfileObject.get(\"id\");\n+                metricProfile[1] = (String) metricProfileObject.get(\"name\");\n+\n+                JSONArray groupArray = (JSONArray) dataObject.get(\"groups\");\n+                Iterator<JSONObject> groupiterator = groupArray.iterator();\n+\n+                while (groupiterator.hasNext()) {\n+                    if (groupiterator.next() instanceof JSONObject) {\n+                        JSONObject groupObject = (JSONObject) groupiterator.next();\n+                        String groupname = (String) groupObject.get(\"name\");\n+                        String groupoperation = (String) groupObject.get(\"operation\");\n+\n+                        JSONArray serviceArray = (JSONArray) groupObject.get(\"services\");\n+                        Iterator<JSONObject> serviceiterator = serviceArray.iterator();\n+                        HashMap<String, String> services = new HashMap<>();\n+                        while (serviceiterator.hasNext()) {\n+                            JSONObject servObject = (JSONObject) serviceiterator.next();\n+                            String servicename = (String) servObject.get(\"name\");\n+                            String serviceoperation = (String) servObject.get(\"operation\");\n+                            serviceOperations.put(servicename, serviceoperation);\n+                            services.put(servicename, serviceoperation);\n+                            ArrayList<String> servFunctionList = new ArrayList();\n+                            if (serviceFunctions.get(servicename) != null) {\n+                                servFunctionList = serviceFunctions.get(servicename);\n+                            }\n+                            servFunctionList.add(groupname);\n+                            serviceFunctions.put(servicename, servFunctionList);\n+\n+                        }\n+                        functionOperations.put(groupname,groupoperation);\n+                        \n+                        groups.add(new GroupOps(groupname, groupoperation, services));\n+\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    public String getId() {\n+        return id;\n+    }\n+\n+    public String getDate() {\n+        return date;\n+    }\n+\n+    public String getName() {\n+        return name;\n+    }\n+\n+    public String getNamespace() {\n+        return namespace;\n+    }\n+\n+    public String getEndpointGroup() {\n+        return endpointGroup;\n+    }\n+\n+    public String getMetricOp() {\n+        return metricOp;\n+    }\n+\n+    public String getProfileOp() {\n+        return profileOp;\n+    }\n+\n+    public String[] getMetricProfile() {\n+        return metricProfile;\n+    }\n+\n+    public ArrayList<GroupOps> getGroups() {\n+        return groups;\n+    }\n+\n+    public HashMap<String, String> getServiceOperations() {\n+        return serviceOperations;\n+    }\n+\n+    public void setServiceOperations(HashMap<String, String> serviceOperations) {\n+        this.serviceOperations = serviceOperations;\n+    }\n+\n+    public HashMap<String, ArrayList<String>> getServiceFunctions() {\n+        return serviceFunctions;\n+    }\n+\n+    public void setServiceFunctions(HashMap<String, ArrayList<String>> serviceFunctions) {\n+        this.serviceFunctions = serviceFunctions;\n+    }\n+\n+    public static class GroupOps {\n+\n+        private String name;\n+        private String operation;\n+        private HashMap<String, String> services;\n+\n+        public GroupOps(String name, String operation, HashMap<String, String> services) {\n+            this.name = name;\n+            this.operation = operation;\n+            this.services = services;\n+        }\n+\n+        public String getName() {\n+            return name;\n+        }\n+\n+        public String getOperation() {\n+            return operation;\n+        }\n+\n+        public HashMap<String, String> getServices() {\n+            return services;\n+        }\n+\n+    }\n+\n+    public HashMap<String, String> getFunctionOperations() {\n+        return functionOperations;\n+    }\n+\n+    public void setFunctionOperations(HashMap<String, String> functionOperations) {\n+        this.functionOperations = functionOperations;\n+    }\n+\n+}"
  },
  {
    "sha": "9803514eef7475cfdbeae15ecfa6ac99a8de3ce3",
    "filename": "flink_jobs/status_trends/src/main/java/argo/profileparsers/MetricProfileParser.java",
    "status": "added",
    "additions": 129,
    "deletions": 0,
    "changes": 129,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/profileparsers/MetricProfileParser.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/profileparsers/MetricProfileParser.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/profileparsers/MetricProfileParser.java?ref=e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2",
    "patch": "@@ -0,0 +1,129 @@\n+/*\n+ * To change this license header, choose License Headers in Project Properties.\n+ * To change this template file, choose Tools | Templates\n+ * and open the template in the editor.\n+ */\n+package argo.profileparsers;\n+\n+import argo.utils.RequestManager;\n+import com.google.common.util.concurrent.Service;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.Map;\n+import org.json.simple.JSONArray;\n+import org.json.simple.JSONObject;\n+import org.json.simple.parser.ParseException;\n+\n+/**\n+ *\n+ * @author cthermolia\n+ */\n+public class MetricProfileParser {\n+\n+    private String id;\n+    private String date;\n+    private String name;\n+    private String description;\n+    private ArrayList<Services> services;\n+    private HashMap<String, ArrayList<String>> metricData;\n+    private final String url=\"/metric_profiles\";\n+    public class Services {\n+\n+        private String service;\n+        private ArrayList<String> metrics;\n+\n+        public Services(String service, ArrayList<String> metrics) {\n+            this.service = service;\n+            this.metrics = metrics;\n+        }\n+\n+        public String getService() {\n+            return service;\n+        }\n+\n+        public ArrayList<String> getMetrics() {\n+            return metrics;\n+        }\n+    }\n+\n+    public MetricProfileParser(String apiUri, String key, String proxy, String metricId, String date) throws IOException, ParseException {\n+        String uri = apiUri +url;\n+        if (date == null) {\n+            uri = uri + \"/\" + metricId;\n+        } else {\n+            uri = uri + \"?date=\" + date;\n+        }\n+        loadMetricProfile(uri, key, proxy);\n+    }\n+\n+    private void loadMetricProfile(String uri, String key, String proxy) throws IOException, org.json.simple.parser.ParseException {\n+        JSONObject jsonObject = RequestManager.request(uri, key, proxy);\n+\n+        services = new ArrayList<>();\n+        metricData = new HashMap<>();\n+        JSONArray data = (JSONArray) jsonObject.get(\"data\");\n+        id = (String) jsonObject.get(\"id\");\n+        date = (String) jsonObject.get(\"date\");\n+        name = (String) jsonObject.get(\"name\");\n+        description = (String) jsonObject.get(\"description\");\n+\n+        Iterator<Object> dataIter = data.iterator();\n+        while (dataIter.hasNext()) {\n+            Object dataobj = dataIter.next();\n+            if (dataobj instanceof JSONObject) {\n+                JSONObject jsonDataObj = new JSONObject((Map) dataobj);\n+\n+                JSONArray servicesArray = (JSONArray) jsonDataObj.get(\"services\");\n+\n+                Iterator<Object> iterator = servicesArray.iterator();\n+\n+                while (iterator.hasNext()) {\n+                    Object obj = iterator.next();\n+                    if (obj instanceof JSONObject) {\n+                        JSONObject servObj = new JSONObject((Map) obj);\n+                        String serviceName = (String) servObj.get(\"service\");\n+                        JSONArray metrics = (JSONArray) servObj.get(\"metrics\");\n+                        Iterator<Object> metrIter = metrics.iterator();\n+                        ArrayList<String> metricList = new ArrayList<>();\n+\n+                        while (metrIter.hasNext()) {\n+                            Object metrObj = metrIter.next();\n+                            metricList.add(metrObj.toString());\n+                        }\n+\n+                        Services service = new Services(serviceName, metricList);\n+                        services.add(service);\n+                        metricData.put(serviceName, metricList);\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    public String getId() {\n+        return id;\n+    }\n+\n+    public String getDate() {\n+        return date;\n+    }\n+\n+    public String getName() {\n+        return name;\n+    }\n+\n+    public String getDescription() {\n+        return description;\n+    }\n+\n+    public ArrayList<Services> getServices() {\n+        return services;\n+    }\n+\n+    public HashMap<String, ArrayList<String>> getMetricData() {\n+        return metricData;\n+    }\n+\n+}"
  },
  {
    "sha": "143dc7ef48329f5621ea8d873b48f4eed21a07d6",
    "filename": "flink_jobs/status_trends/src/main/java/argo/profileparsers/OperationsParser.java",
    "status": "added",
    "additions": 136,
    "deletions": 0,
    "changes": 136,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/profileparsers/OperationsParser.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/profileparsers/OperationsParser.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/profileparsers/OperationsParser.java?ref=e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2",
    "patch": "@@ -0,0 +1,136 @@\n+/*\n+ * To change this license header, choose License Headers in Project Properties.\n+ * To change this template file, choose Tools | Templates\n+ * and open the template in the editor.\n+ */\n+package argo.profileparsers;\n+\n+import argo.utils.RequestManager;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import org.json.simple.JSONArray;\n+import org.json.simple.JSONObject;\n+import org.json.simple.parser.ParseException;\n+\n+/**\n+ *\n+ * @author cthermolia\n+ */\n+public class OperationsParser {\n+\n+    private String id;\n+    private String name;\n+    private ArrayList<String> states;\n+    private DefaultStatus defaults;\n+    private HashMap<String, HashMap<String, String>> opTruthTable;\n+    private final String url = \"/operations_profiles\";\n+\n+    public class DefaultStatus {\n+\n+        private String down;\n+        private String missing;\n+        private String unknown;\n+\n+        public DefaultStatus(String down, String missing, String unknown) {\n+            this.down = down;\n+            this.missing = missing;\n+            this.unknown = unknown;\n+        }\n+\n+        public String getDown() {\n+            return down;\n+        }\n+\n+        public String getMissing() {\n+            return missing;\n+        }\n+\n+        public String getUnknown() {\n+            return unknown;\n+        }\n+\n+    }\n+\n+    public OperationsParser(String apiUri, String key, String proxy, String operationsId, String dateStr) throws IOException, ParseException {\n+        String uri = apiUri + url;\n+        if (dateStr == null) {\n+            uri = uri + operationsId;\n+        } else {\n+            uri = uri + \"?date=\" + dateStr;\n+        }\n+        loadOperationProfile(uri, key, proxy);\n+    }\n+\n+    private HashMap<String, HashMap<String, String>> loadOperationProfile(String uri, String key, String proxy) throws IOException, org.json.simple.parser.ParseException {\n+        JSONObject jsonObject = RequestManager.request(uri, key, proxy);\n+\n+        // A JSON object. Key value pairs are unordered. JSONObject supports java.util.Map interface.\n+        JSONArray dataList = (JSONArray) jsonObject.get(\"data\");\n+\n+        Iterator<JSONObject> iterator = dataList.iterator();\n+        opTruthTable = new HashMap<>();\n+        states = new ArrayList<>();\n+        while (iterator.hasNext()) {\n+            JSONObject dataObject = (JSONObject) iterator.next();\n+            id = (String) dataObject.get(\"id\");\n+            name = (String) dataObject.get(\"name\");\n+\n+            JSONArray stateList = (JSONArray) dataObject.get(\"available_states\");\n+            Iterator<String> stateIter = stateList.iterator();\n+            while (stateIter.hasNext()) {\n+                String state = stateIter.next();\n+                states.add(state);\n+            }\n+\n+            JSONObject defaultObject = (JSONObject) dataObject.get(\"defaults\");\n+            String down = (String) defaultObject.get(\"down\");\n+            String missing = (String) defaultObject.get(\"missing\");\n+            String unknown = (String) defaultObject.get(\"unknown\");\n+            defaults = new DefaultStatus(down, missing, unknown);\n+\n+            JSONArray operationList = (JSONArray) dataObject.get(\"operations\");\n+            Iterator<JSONObject> opIterator = operationList.iterator();\n+            while (opIterator.hasNext()) {\n+                JSONObject operationObject = (JSONObject) opIterator.next();\n+                String opName = (String) operationObject.get(\"name\");\n+                JSONArray truthtable = (JSONArray) operationObject.get(\"truth_table\");\n+                Iterator<JSONObject> truthTableIter = truthtable.iterator();\n+                HashMap<String, String> truthTable = new HashMap<>();\n+                while (truthTableIter.hasNext()) {\n+                    JSONObject truthEntry = (JSONObject) truthTableIter.next();\n+                    String a = (String) truthEntry.get(\"a\");\n+                    String b = (String) truthEntry.get(\"b\");\n+                    String x = (String) truthEntry.get(\"x\");\n+\n+                    truthTable.put(a + \"-\" + b, x);\n+                }\n+                opTruthTable.put(opName, truthTable);\n+            }\n+        }\n+        return opTruthTable;\n+\n+    }\n+\n+    public String getId() {\n+        return id;\n+    }\n+\n+    public String getName() {\n+        return name;\n+    }\n+\n+    public ArrayList<String> getStates() {\n+        return states;\n+    }\n+\n+    public DefaultStatus getDefaults() {\n+        return defaults;\n+    }\n+\n+    public HashMap<String, HashMap<String, String>> getOpTruthTable() {\n+        return opTruthTable;\n+    }\n+\n+}"
  },
  {
    "sha": "d3bfdd066ec71928f0c28201b0ceb4a5a68d4011",
    "filename": "flink_jobs/status_trends/src/main/java/argo/profileparsers/ProfilesLoader.java",
    "status": "added",
    "additions": 85,
    "deletions": 0,
    "changes": 85,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/profileparsers/ProfilesLoader.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/profileparsers/ProfilesLoader.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/profileparsers/ProfilesLoader.java?ref=e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2",
    "patch": "@@ -0,0 +1,85 @@\n+/*\n+ * To change this license header, choose License Headers in Project Properties.\n+ * To change this template file, choose Tools | Templates\n+ * and open the template in the editor.\n+ */\n+package argo.profileparsers;\n+import java.io.IOException;\n+import org.apache.flink.api.java.utils.ParameterTool;\n+import org.json.simple.parser.ParseException;\n+\n+/**\n+ *\n+ * @author cthermolia\n+ */\n+public class ProfilesLoader {\n+\n+    private ReportParser reportParser;\n+    private TopologyEndpointParser topologyEndpointParser;\n+    private MetricProfileParser metricProfileParser;\n+    private OperationsParser operationParser;\n+    private AggregationProfileParser aggregationProfileParser;\n+    private TopologyGroupParser topolGroupParser;\n+\n+    public ProfilesLoader(ParameterTool params) throws IOException, ParseException {\n+\n+        reportParser = new ReportParser(params.getRequired(\"apiUri\"), params.getRequired(\"key\"), params.get(\"proxy\"), params.getRequired(\"reportId\"));\n+        String[] reportInfo = reportParser.getTenantReport().getInfo();\n+        topolGroupParser = new TopologyGroupParser(params.getRequired(\"apiUri\"), params.getRequired(\"key\"), params.get(\"proxy\"), params.getRequired(\"date\"),reportInfo[0]);\n+        //ArrayList<TopologyGroupParser.TopologyGroup> groupsList = topolGroupParser.getTopologyGroups().get(topology);\n+\n+        topologyEndpointParser = new TopologyEndpointParser(params.getRequired(\"apiUri\"), params.getRequired(\"key\"), params.get(\"proxy\"),  params.getRequired(\"date\"),reportInfo[0]);\n+        //  ArrayList<TopologyEndpointParser.EndpointGroup> endpointList = topolEndpointParser.getEndpointGroups().get(topology);\n+\n+        String aggregationId = reportParser.getAggregationReportId();\n+        String metricId = reportParser.getMetricReportId();\n+        String operationsId = reportParser.getOperationReportId();\n+\n+        aggregationProfileParser = new AggregationProfileParser(params.getRequired(\"apiUri\"), params.getRequired(\"key\"), params.get(\"proxy\"), aggregationId, params.get(\"date\"));\n+\n+        metricProfileParser = new MetricProfileParser(params.getRequired(\"apiUri\"), params.getRequired(\"key\"), params.get(\"proxy\"), metricId, params.get(\"date\"));\n+        operationParser = new OperationsParser(params.getRequired(\"apiUri\"), params.getRequired(\"key\"), params.get(\"proxy\"), operationsId, params.get(\"date\"));\n+\n+    }\n+\n+    public ReportParser getReportParser() {\n+        return reportParser;\n+    }\n+\n+    public void setReportParser(ReportParser reportParser) {\n+        this.reportParser = reportParser;\n+    }\n+\n+    public TopologyEndpointParser getTopologyEndpointParser() {\n+        return topologyEndpointParser;\n+    }\n+\n+    public void setTopologyEndpointParser(TopologyEndpointParser topologyEndpointParser) {\n+        this.topologyEndpointParser = topologyEndpointParser;\n+    }\n+\n+    public MetricProfileParser getMetricProfileParser() {\n+        return metricProfileParser;\n+    }\n+\n+    public void setMetricProfileParser(MetricProfileParser metricProfileParser) {\n+        this.metricProfileParser = metricProfileParser;\n+    }\n+\n+    public OperationsParser getOperationParser() {\n+        return operationParser;\n+    }\n+\n+    public void setOperationParser(OperationsParser operationParser) {\n+        this.operationParser = operationParser;\n+    }\n+\n+    public AggregationProfileParser getAggregationProfileParser() {\n+        return aggregationProfileParser;\n+    }\n+\n+    public void setAggregationProfileParser(AggregationProfileParser aggregationProfileParser) {\n+        this.aggregationProfileParser = aggregationProfileParser;\n+    }\n+\n+}"
  },
  {
    "sha": "c5ad392e6f44ddd393c9b1d6365b5776b9f837c6",
    "filename": "flink_jobs/status_trends/src/main/java/argo/profileparsers/ReportParser.java",
    "status": "added",
    "additions": 317,
    "deletions": 0,
    "changes": 317,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/profileparsers/ReportParser.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/profileparsers/ReportParser.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/profileparsers/ReportParser.java?ref=e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2",
    "patch": "@@ -0,0 +1,317 @@\n+/*\n+ * To change this license header, choose License Headers in Project Properties.\n+ * To change this template file, choose Tools | Templates\n+ * and open the template in the editor.\n+ */\n+package argo.profileparsers;\n+\n+import argo.utils.RequestManager;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import org.json.simple.JSONArray;\n+import org.json.simple.JSONObject;\n+import org.json.simple.parser.ParseException;\n+\n+/**\n+ *\n+ * @author cthermolia\n+ */\n+public class ReportParser {\n+\n+    private TenantReport tenantReport;\n+    private final String url = \"/reports/\";\n+\n+    public ReportParser(String apiUri, String key, String proxy, String reportId) throws IOException, ParseException {\n+        String uri = apiUri + url + reportId;\n+        loadReportInfo(uri, key, proxy);\n+    }\n+\n+    public void loadReportInfo(String uri, String key, String proxy) throws IOException, ParseException {\n+\n+        JSONObject jsonObject = RequestManager.request(uri, key, proxy);\n+\n+        JSONArray dataList = (JSONArray) jsonObject.get(\"data\");\n+\n+        Iterator<JSONObject> iterator = dataList.iterator();\n+\n+        while (iterator.hasNext()) {\n+            JSONObject dataObject = (JSONObject) iterator.next();\n+\n+            String id = (String) dataObject.get(\"id\");\n+            String tenant = (String) dataObject.get(\"tenant\");\n+            boolean disabled = (boolean) dataObject.get(\"disabled\");\n+\n+            JSONObject infoObject = (JSONObject) dataObject.get(\"info\");\n+            String[] info = new String[4];\n+            info[0] = (String) infoObject.get(\"name\");\n+            info[1] = (String) infoObject.get(\"description\");\n+            info[2] = (String) infoObject.get(\"created\");\n+            info[3] = (String) infoObject.get(\"updated\");\n+\n+            JSONObject topologyObject = (JSONObject) dataObject.get(\"topology_schema\");\n+            JSONObject groupObject = (JSONObject) topologyObject.get(\"group\");\n+\n+            String type = (String) groupObject.get(\"type\");\n+            JSONObject subGroupObject = (JSONObject) groupObject.get(\"group\");\n+\n+            String grouptype = (String) subGroupObject.get(\"type\");\n+\n+            Topology group = new Topology(grouptype, null);\n+            Topology topologyGroup = new Topology(type, group);\n+\n+            JSONObject thresholdsObject = (JSONObject) dataObject.get(\"thresholds\");\n+\n+            Threshold threshold = new Threshold((Long) thresholdsObject.get(\"availability\"), (Long) thresholdsObject.get(\"reliability\"), (Double) thresholdsObject.get(\"uptime\"),\n+                    (Double) thresholdsObject.get(\"unknown\"), (Double) thresholdsObject.get(\"downtime\"));\n+\n+            JSONArray profiles = (JSONArray) dataObject.get(\"profiles\");\n+\n+            Iterator<JSONObject> profileIter = profiles.iterator();\n+            ArrayList<Profiles> profileList = new ArrayList<>();\n+            while (profileIter.hasNext()) {\n+                JSONObject profileObject = (JSONObject) profileIter.next();\n+                Profiles profile = new Profiles((String) profileObject.get(\"id\"), (String) profileObject.get(\"name\"), (String) profileObject.get(\"type\"));\n+                profileList.add(profile);\n+            }\n+\n+            JSONArray filters = (JSONArray) dataObject.get(\"filter_tags\");\n+            Iterator<JSONObject> filterIter = filters.iterator();\n+            ArrayList<FilterTags> filtersList = new ArrayList<>();\n+            while (filterIter.hasNext()) {\n+                JSONObject filterObject = (JSONObject) filterIter.next();\n+                FilterTags filter = new FilterTags((String) filterObject.get(\"name\"), (String) filterObject.get(\"value\"), (String) filterObject.get(\"context\"));\n+                filtersList.add(filter);\n+            }\n+\n+            tenantReport = new TenantReport(id, tenant, disabled, info, group, threshold, profileList, filtersList);\n+        }\n+\n+    }\n+\n+//    public String getProfileId(String profileName) {\n+//        ArrayList<Profiles> profiles = tenantReport.getProfiles();\n+//        if (profiles != null) {\n+//            for (Profiles profile : profiles) {\n+//                if (profile.getType().equalsIgnoreCase(profileName)) {\n+//                    return profile.id;\n+//                }\n+//            }\n+//        }\n+//        return null;\n+//    }\n+    \n+    public String getAggregationReportId() {\n+        ArrayList<Profiles> profiles = tenantReport.getProfiles();\n+        if (profiles != null) {\n+            for (Profiles profile : profiles) {\n+                if (profile.getType().equalsIgnoreCase(ReportParser.ProfileType.AGGREGATION.name())) {\n+                    return profile.id;\n+                }\n+            }\n+        }\n+        return null;\n+    }\n+    \n+     public String getMetricReportId() {\n+        ArrayList<Profiles> profiles = tenantReport.getProfiles();\n+        if (profiles != null) {\n+            for (Profiles profile : profiles) {\n+                if (profile.getType().equalsIgnoreCase(ReportParser.ProfileType.METRIC.name())) {\n+                    return profile.id;\n+                }\n+            }\n+        }\n+        return null;\n+    }\n+    \n+ public String getOperationReportId() {\n+        ArrayList<Profiles> profiles = tenantReport.getProfiles();\n+        if (profiles != null) {\n+            for (Profiles profile : profiles) {\n+                if (profile.getType().equalsIgnoreCase(ReportParser.ProfileType.OPERATIONS.name())) {\n+                    return profile.id;\n+                }\n+            }\n+        }\n+        return null;\n+    }\n+    public TenantReport getTenantReport() {\n+        return tenantReport;\n+    }\n+\n+    public class Threshold {\n+\n+        private Long availability;\n+        private Long reliability;\n+        private Double uptime;\n+        private Double unknown;\n+        private Double downtime;\n+\n+        public Threshold(Long availability, Long reliability, Double uptime, Double unknown, Double downtime) {\n+            this.availability = availability;\n+            this.reliability = reliability;\n+            this.uptime = uptime;\n+            this.unknown = unknown;\n+            this.downtime = downtime;\n+        }\n+\n+        public Long getAvailability() {\n+            return availability;\n+        }\n+\n+        public Long getReliability() {\n+            return reliability;\n+        }\n+\n+        public Double getUptime() {\n+            return uptime;\n+        }\n+\n+        public Double getUnknown() {\n+            return unknown;\n+        }\n+\n+        public Double getDowntime() {\n+            return downtime;\n+        }\n+\n+    }\n+\n+    private class Profiles {\n+\n+        private String id;\n+        private String name;\n+        private String type;\n+\n+        public Profiles(String id, String name, String type) {\n+            this.id = id;\n+            this.name = name;\n+            this.type = type;\n+        }\n+\n+        public String getId() {\n+            return id;\n+        }\n+\n+        public String getName() {\n+            return name;\n+        }\n+\n+        public String getType() {\n+            return type;\n+        }\n+\n+    }\n+\n+    private class FilterTags {\n+\n+        private String name;\n+        private String value;\n+        private String context;\n+\n+        public FilterTags(String name, String value, String context) {\n+            this.name = name;\n+            this.value = value;\n+            this.context = context;\n+        }\n+\n+        public String getName() {\n+            return name;\n+        }\n+\n+        public String getValue() {\n+            return value;\n+        }\n+\n+        public String getContext() {\n+            return context;\n+        }\n+\n+    }\n+\n+    public class Topology {\n+\n+        private String type;\n+        private Topology group;\n+\n+        public Topology(String type, Topology group) {\n+            this.type = type;\n+            this.group = group;\n+        }\n+\n+        public String getType() {\n+            return type;\n+        }\n+\n+        public Topology getGroup() {\n+            return group;\n+        }\n+\n+    }\n+\n+    public class TenantReport {\n+\n+        private String id;\n+        private String tenant;\n+        private boolean disabled;\n+        private String[] info;\n+        private Topology group;\n+        private Threshold threshold;\n+        private ArrayList<Profiles> profiles;\n+        private ArrayList<FilterTags> filterTags;\n+\n+        public TenantReport(String id, String tenant, boolean disabled, String[] info, Topology group, Threshold threshold, ArrayList<Profiles> profiles, ArrayList<FilterTags> filterTags) {\n+            this.id = id;\n+            this.tenant = tenant;\n+            this.disabled = disabled;\n+            this.info = info;\n+            this.group = group;\n+            this.threshold = threshold;\n+            this.profiles = profiles;\n+            this.filterTags = filterTags;\n+\n+        }\n+\n+        public String getId() {\n+            return id;\n+        }\n+\n+        public String getTenant() {\n+            return tenant;\n+        }\n+\n+        public boolean isDisabled() {\n+            return disabled;\n+        }\n+\n+        public String[] getInfo() {\n+            return info;\n+        }\n+\n+        public Topology getGroup() {\n+            return group;\n+        }\n+\n+        public Threshold getThreshold() {\n+            return threshold;\n+        }\n+\n+        public ArrayList<Profiles> getProfiles() {\n+            return profiles;\n+        }\n+\n+        public ArrayList<FilterTags> getFilterTags() {\n+            return filterTags;\n+        }\n+\n+    }\n+\n+    public enum ProfileType {\n+\n+        METRIC,\n+        AGGREGATION,\n+        OPERATIONS\n+\n+    }\n+}"
  },
  {
    "sha": "cf804de64f70a8a2e11d923d76bcd77ae13ee7aa",
    "filename": "flink_jobs/status_trends/src/main/java/argo/profileparsers/TopologyEndpointParser.java",
    "status": "added",
    "additions": 159,
    "deletions": 0,
    "changes": 159,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/profileparsers/TopologyEndpointParser.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/profileparsers/TopologyEndpointParser.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/profileparsers/TopologyEndpointParser.java?ref=e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2",
    "patch": "@@ -0,0 +1,159 @@\n+/*\n+ * To change this license header, choose License Headers in Project Properties.\n+ * To change this template file, choose Tools | Templates\n+ * and open the template in the editor.\n+ */\n+package argo.profileparsers;\n+\n+import argo.utils.RequestManager;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.Map;\n+import org.json.simple.JSONArray;\n+import org.json.simple.JSONObject;\n+import org.json.simple.parser.ParseException;\n+\n+/**\n+ *\n+ * @author cthermolia\n+ */\n+public class TopologyEndpointParser {\n+\n+    private HashMap<String, ArrayList<EndpointGroup>> endpointGroups;\n+\n+    private HashMap<String, HashMap<String, String>> topologyEndpoint;\n+   private final String url = \"/topology/endpoints/by_report\";\n+// private final String url = \"/topology/endpoints\";\n+\n+    public TopologyEndpointParser(String apiUri, String key, String proxy, String date, String reportname) throws IOException, ParseException {\n+        String uri = apiUri + url + \"/\" + reportname;\n+        //      String uri = apiUri + url;\n+ \n+       if (date != null) {\n+            uri = uri + \"?date=\" + date;\n+        }\n+        loadTopologyEndpoints(uri, key, proxy);\n+    }\n+\n+    public HashMap<String, String> getTopology(String type) throws IOException, org.json.simple.parser.ParseException {\n+\n+        return topologyEndpoint.get(type);\n+    }\n+\n+    private void loadTopologyEndpoints(String uri, String key, String proxy) throws IOException, ParseException {\n+        endpointGroups = new HashMap<>();\n+        topologyEndpoint = new HashMap<>();\n+        JSONObject jsonObject = RequestManager.request(uri, key, proxy);\n+\n+        JSONArray data = (JSONArray) jsonObject.get(\"data\");\n+\n+        Iterator<Object> dataIter = data.iterator();\n+        while (dataIter.hasNext()) {\n+            Object dataobj = dataIter.next();\n+            if (dataobj instanceof JSONObject) {\n+                JSONObject jsonDataObj = new JSONObject((Map) dataobj);\n+                String hostname = (String) jsonDataObj.get(\"hostname\");\n+                String service = (String) jsonDataObj.get(\"service\");\n+                String group = (String) jsonDataObj.get(\"group\");\n+                String type = (String) jsonDataObj.get(\"type\");\n+                JSONObject tagsObj = (JSONObject) jsonDataObj.get(\"tags\");\n+\n+                String scope = (String) tagsObj.get(\"scope\");\n+                String production = (String) tagsObj.get(\"production\");\n+                String monitored = (String) tagsObj.get(\"monitored\");\n+                Tags tag = new Tags(scope, production, monitored);\n+\n+                String topologyEndpointKey = hostname + \"-\" + service;\n+\n+                HashMap<String, String> endpMap = new HashMap<String, String>();\n+                if (topologyEndpoint.get(type) != null) {\n+                    endpMap = topologyEndpoint.get(type);\n+                }\n+\n+                endpMap.put(topologyEndpointKey, group);\n+                topologyEndpoint.put(type, endpMap);\n+\n+                EndpointGroup endpointGroup = new EndpointGroup(group, hostname, service, key, tag);\n+\n+                ArrayList<EndpointGroup> topologies = new ArrayList<>();\n+                if (endpointGroups.get(type) != null) {\n+                    topologies = endpointGroups.get(type);\n+                }\n+                topologies.add(endpointGroup);\n+            }\n+        }\n+\n+    }\n+\n+    public HashMap<String, ArrayList<EndpointGroup>> getEndpointGroups() {\n+        return endpointGroups;\n+    }\n+\n+    public class EndpointGroup {\n+\n+        private String group;\n+\n+        private String hostname;\n+        private String service;\n+        private String type;\n+        private Tags tags;\n+\n+        public EndpointGroup(String group, String hostname, String service, String type, Tags tags) {\n+            this.group = group;\n+            this.hostname = hostname;\n+            this.service = service;\n+            this.type = type;\n+            this.tags = tags;\n+        }\n+\n+        public String getGroup() {\n+            return group;\n+        }\n+\n+        public String getHostname() {\n+            return hostname;\n+        }\n+\n+        public String getService() {\n+            return service;\n+        }\n+\n+        public String getType() {\n+            return type;\n+        }\n+\n+        public Tags getTags() {\n+            return tags;\n+        }\n+\n+    }\n+\n+    public class Tags {\n+\n+        private String scope;\n+        private String production;\n+        private String monitored;\n+\n+        public Tags(String scope, String production, String monitored) {\n+            this.scope = scope;\n+            this.production = production;\n+            this.monitored = monitored;\n+        }\n+\n+        public String getScope() {\n+            return scope;\n+        }\n+\n+        public String getProduction() {\n+            return production;\n+        }\n+\n+        public String getMonitored() {\n+            return monitored;\n+        }\n+\n+    }\n+\n+}"
  },
  {
    "sha": "b7269263926a239d303c6b820262a42531cb8f68",
    "filename": "flink_jobs/status_trends/src/main/java/argo/profileparsers/TopologyGroupParser.java",
    "status": "added",
    "additions": 163,
    "deletions": 0,
    "changes": 163,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/profileparsers/TopologyGroupParser.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/profileparsers/TopologyGroupParser.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/profileparsers/TopologyGroupParser.java?ref=e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2",
    "patch": "@@ -0,0 +1,163 @@\n+/*\n+ * To change this license header, choose License Headers in Project Properties.\n+ * To change this template file, choose Tools | Templates\n+ * and open the template in the editor.\n+ */\n+package argo.profileparsers;\n+\n+import argo.utils.RequestManager;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.Map;\n+import org.json.simple.JSONArray;\n+import org.json.simple.JSONObject;\n+import org.json.simple.parser.ParseException;\n+\n+/**\n+ *\n+ * @author cthermolia\n+ */\n+public class TopologyGroupParser {\n+\n+    private HashMap<String, ArrayList<TopologyGroup>> topologyGroups = new HashMap<>();\n+    //private final String url = \"/topology/groups/by_report\";\n+    private final String url = \"/topology/groups\";\n+\n+    public TopologyGroupParser(String apiUri, String key, String proxy, String date, String reportname) throws IOException, ParseException {\n+     //       String uri = apiUri + url + \"/\" + reportname;\n+        String uri = apiUri + url;\n+        if (date != null) {\n+            uri = uri + \"?date=\" + date;\n+        }\n+        loadTopologyGroups(uri, key, proxy);\n+\n+    }\n+\n+    public void loadTopologyGroups(String uri, String key, String proxy) throws IOException, ParseException {\n+\n+        JSONObject jsonObject = RequestManager.request(uri, key, proxy);\n+        JSONArray data = (JSONArray) jsonObject.get(\"data\");\n+\n+        Iterator<Object> dataIter = data.iterator();\n+        while (dataIter.hasNext()) {\n+            Object dataobj = dataIter.next();\n+            if (dataobj instanceof JSONObject) {\n+                JSONObject jsonDataObj = new JSONObject((Map) dataobj);\n+                String group = (String) jsonDataObj.get(\"group\");\n+                String type = (String) jsonDataObj.get(\"type\");\n+                String subgroup = (String) jsonDataObj.get(\"subgroup\");\n+                JSONObject tagsObj = (JSONObject) jsonDataObj.get(\"tags\");\n+                Tags tag = null;\n+                if (tagsObj != null) {\n+                    String scope = (String) tagsObj.get(\"scope\");\n+                    String production = (String) tagsObj.get(\"production\");\n+                    String monitored = (String) tagsObj.get(\"monitored\");\n+\n+                    tag = new Tags(scope, production, monitored);\n+                }\n+                Notifications notification = null;\n+                JSONObject notificationsObj = (JSONObject) jsonDataObj.get(\"notifications\");\n+                if (notificationsObj != null) {\n+                    String contacts = (String) notificationsObj.get(\"contacts\");\n+                    String enabled = (String) notificationsObj.get(\"enabled\");\n+                    notification = new Notifications(contacts, enabled);\n+\n+                }\n+\n+                TopologyGroup topologyGroup = new TopologyGroup(group, type, subgroup, tag, notification);\n+                ArrayList<TopologyGroup> groupList = new ArrayList<>();\n+                if (topologyGroups.get(type) != null) {\n+                    groupList = topologyGroups.get(type);\n+                }\n+                groupList.add(topologyGroup);\n+                topologyGroups.put(type, groupList);\n+\n+            }\n+\n+        }\n+    }\n+\n+    public HashMap<String, ArrayList<TopologyGroup>> getTopologyGroups() {\n+        return topologyGroups;\n+    }\n+\n+    public class TopologyGroup {\n+\n+        private String group;\n+        private String type;\n+        private String subgroup;\n+\n+        private Tags tags;\n+        private Notifications notifications;\n+\n+        public TopologyGroup(String group, String type, String subgroup, Tags tags, Notifications notifications) {\n+            this.group = group;\n+            this.type = type;\n+            this.subgroup = subgroup;\n+            this.tags = tags;\n+            this.notifications = notifications;\n+        }\n+\n+        public String getGroup() {\n+            return group;\n+        }\n+\n+        public String getType() {\n+            return type;\n+        }\n+\n+        public String getSubgroup() {\n+            return subgroup;\n+        }\n+\n+        public Tags getTags() {\n+            return tags;\n+        }\n+\n+        public Notifications getNotifications() {\n+            return notifications;\n+        }\n+\n+    }\n+\n+    public class Tags {\n+\n+        private String scope;\n+        private String infrastructure;\n+        private String certification;\n+\n+        public Tags(String scope, String infrastructure, String certification) {\n+            this.scope = scope;\n+            this.infrastructure = infrastructure;\n+            this.certification = certification;\n+        }\n+\n+        public String getScope() {\n+            return scope;\n+        }\n+\n+        public String getInfrastructure() {\n+            return infrastructure;\n+        }\n+\n+        public String getCertification() {\n+            return certification;\n+        }\n+\n+    }\n+\n+    public class Notifications {\n+\n+        private String contacts;\n+        private String enabled;\n+\n+        public Notifications(String contacts, String enabled) {\n+            this.contacts = contacts;\n+            this.enabled = enabled;\n+        }\n+\n+    }\n+\n+}"
  },
  {
    "sha": "502dd55782ee2ce20907fb26c16994da037e5cf9",
    "filename": "flink_jobs/status_trends/src/main/java/argo/utils/RequestManager.java",
    "status": "modified",
    "additions": 1,
    "deletions": 42,
    "changes": 43,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/utils/RequestManager.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/utils/RequestManager.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/utils/RequestManager.java?ref=e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2",
    "patch": "@@ -21,7 +21,7 @@\n  */\n public class RequestManager {\n \n-    private static JSONObject request(String uri, String key, String proxy) throws ParseException {\n+    public static JSONObject request(String uri, String key, String proxy) throws ParseException {\n         JSONObject jsonresult = null;\n \n         Request request = Request.Get(uri);\n@@ -46,45 +46,4 @@ private static JSONObject request(String uri, String key, String proxy) throws P\n         return jsonresult;\n     }\n \n-    public static JSONObject getReportRequest(String baseUri, String key, String proxy, String reportId) throws IOException, ParseException {\n-        JSONObject jsonresult = null;\n-        String uri = baseUri + \"/reports/\" + reportId;\n-\n-        jsonresult = request(uri, key, proxy);\n-        return jsonresult;\n-\n-    }\n-\n-    public static JSONObject getAggregationProfileRequest(String baseUri, String key, String proxy) throws IOException, ParseException {\n-        JSONObject jsonresult = null;\n-        String uri = baseUri + \"/aggregation_profiles\";\n-        jsonresult = request(uri, key, proxy);\n-        return jsonresult;\n-\n-    }\n-\n-    public static JSONObject getMetricProfileRequest(String baseUri, String uid, String key, String proxy) throws IOException, ParseException {\n-        JSONObject jsonresult = null;\n-        String uri = baseUri + \"/metric_profiles/\" + uid;\n-        jsonresult = request(uri, key, proxy);\n-        return jsonresult;\n-\n-    }\n-\n-    public static JSONObject getTopologyEndpointRequest(String baseUri, String key, String proxy) throws IOException, ParseException {\n-        JSONObject jsonresult = null;\n-        String uri = baseUri + \"/topology/endpoints\";\n-        jsonresult = request(uri, key, proxy);\n-\n-        return jsonresult;\n-\n-    }\n-\n-    public static JSONObject getOperationProfileRequest(String baseUri, String key, String proxy) throws IOException, ParseException {\n-        JSONObject jsonresult = null;\n-        String uri = baseUri + \"/operations_profiles\";\n-        jsonresult = request(uri, key, proxy);\n-        return jsonresult;\n-\n-    }\n }"
  },
  {
    "sha": "b0aa686eb25a49e7166cf2cb1a3e57ecfc76ef74",
    "filename": "flink_jobs/status_trends/src/main/java/argo/utils/TimelineBuilder.java",
    "status": "renamed",
    "additions": 28,
    "deletions": 45,
    "changes": 73,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/utils/TimelineBuilder.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/utils/TimelineBuilder.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/utils/TimelineBuilder.java?ref=e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2",
    "patch": "@@ -1,12 +1,11 @@\n-package argo.functions.servendptrends;\n-\n /*\n  * To change this license header, choose License Headers in Project Properties.\n  * To change this template file, choose Tools | Templates\n  * and open the template in the editor.\n  */\n-import argo.pojos.MetricTimelinePojo;\n-import argo.pojos.ServEndpFlipFlopPojo;\n+package argo.utils;\n+\n+import argo.pojos.TimelineTrends;\n import argo.utils.Utils;\n import java.text.ParseException;\n import java.util.ArrayList;\n@@ -15,52 +14,34 @@\n import java.util.Iterator;\n import java.util.Map;\n import java.util.TreeMap;\n-import org.apache.flink.api.common.functions.GroupReduceFunction;\n-import org.apache.flink.util.Collector;\n \n /**\n  *\n  * @author cthermolia\n- *\n- * CalcServiceEndpointFlipFlop, count status changes for each service endpoint\n- * group\n  */\n-public class CalcServiceEndpointFlipFlop implements GroupReduceFunction< MetricTimelinePojo, ServEndpFlipFlopPojo> {\n+public class TimelineBuilder {\n+\n+    public TreeMap<Date, String> buildStatusTimeline(ArrayList<TimelineTrends> timelist, HashMap<String, String> opTruthTable) throws ParseException {\n+\n+        ArrayList<Date> timeline = collectTimestampsInTimeline(timelist);\n \n-    private HashMap<String, String> opTruthTable;\n+        TreeMap<Date, ArrayList<String>> statusMap = mergeTimelines(timeline, timelist);\n \n-    public CalcServiceEndpointFlipFlop(HashMap<String, String> opTruthTable) {\n-        this.opTruthTable = opTruthTable;\n+        TreeMap<Date, String> resultMap = operateStatusTimeline(statusMap, opTruthTable);\n+        return resultMap;\n     }\n \n-    @Override\n-    public void reduce(Iterable<MetricTimelinePojo> in, Collector< ServEndpFlipFlopPojo> out) throws Exception {\n-        String group = null;\n-        String service = null;\n-        String hostname = null;\n+    public ArrayList<Date> collectTimestampsInTimeline(ArrayList<TimelineTrends> in) {\n+        ArrayList<TimelineTrends> timelist = new ArrayList<>();\n         ArrayList<Date> timeline = new ArrayList<>();\n-        ArrayList<MetricTimelinePojo> list = new ArrayList<>();\n-        //construct a timeline containing all the timestamps of each metric timeline\n-        for (MetricTimelinePojo t : in) {\n+\n+        for (TimelineTrends t : in) {\n             TreeMap<Date, String> metricTimeline = t.getTimelineMap();\n             for (Date time : metricTimeline.keySet()) {\n                 timeline.add(time);\n             }\n-            group = t.getGroup();\n-            service = t.getService();\n-            hostname = t.getEndpoint();\n-            list.add(t);\n         }\n-        TreeMap<Date, ArrayList<String>> statusMap = createStatusTimeline(timeline, list);\n-\n-        TreeMap<String, String> resultMap = operateStatus(statusMap);\n-        int flipflops = calcFlipFlops(resultMap);\n-\n-        ServEndpFlipFlopPojo servEndpFlipFlop = new ServEndpFlipFlopPojo(group, service, hostname, flipflops);\n-\n-        //Tuple4<String, String, String, Integer> tuple = new Tuple4<String, String, String, Integer>(group, service, hostname, flipflops);\n-        out.collect(servEndpFlipFlop);\n-\n+        return timeline;\n     }\n \n     /**\n@@ -72,11 +53,11 @@ public void reduce(Iterable<MetricTimelinePojo> in, Collector< ServEndpFlipFlopP\n      * @param in\n      * @return\n      */\n-    private TreeMap<Date, ArrayList<String>> createStatusTimeline(ArrayList<Date> timeline, ArrayList<MetricTimelinePojo> in) {\n+    private TreeMap<Date, ArrayList<String>> mergeTimelines(ArrayList<Date> timeline, ArrayList<TimelineTrends> in) {\n         TreeMap<Date, ArrayList<String>> statusMap = new TreeMap<>();\n \n         for (Date time : timeline) { //for each timestamp  T in the overall timeline\n-            for (MetricTimelinePojo t : in) { // for each metric timeline\n+            for (TimelineTrends t : in) { // for each metric timeline\n \n                 TreeMap<Date, String> metricTimeline = t.getTimelineMap();\n                 if (metricTimeline.containsKey(time)) { // if timestamp T is in the metric timeline\n@@ -131,13 +112,15 @@ public void reduce(Iterable<MetricTimelinePojo> in, Collector< ServEndpFlipFlopP\n \n     /**\n      *\n-     * @param timelineStatusMap , the timeline status bucket containing all statuses for each timestamp \n-     * @return , a timeline with one status per timestamp , extracted from the operation's truth table\n+     * @param timelineStatusMap , the timeline status bucket containing all\n+     * statuses for each timestamp\n+     * @return , a timeline with one status per timestamp , extracted from the\n+     * operation's truth table\n      * @throws ParseException\n      */\n-    private TreeMap<String, String> operateStatus(TreeMap<Date, ArrayList<String>> timelineStatusMap) throws ParseException {\n+    private TreeMap<Date, String> operateStatusTimeline(TreeMap<Date, ArrayList<String>> timelineStatusMap, HashMap<String, String> opTruthTable) throws ParseException {\n \n-        TreeMap<String, String> result = new TreeMap<String, String>();\n+        TreeMap<Date, String> result = new TreeMap<Date, String>();\n \n         for (Date dt : timelineStatusMap.keySet()) {\n             String dateStr = Utils.convertDateToString(dt);\n@@ -154,7 +137,7 @@ public void reduce(Iterable<MetricTimelinePojo> in, Collector< ServEndpFlipFlopP\n                 } else {\n                     String status = iter.next();\n                     String key = finalStatus + \"-\" + status;\n-                    if (opTruthTable.containsKey(key)) { \n+                    if (opTruthTable.containsKey(key)) {\n                         finalStatus = opTruthTable.get(key);\n                     } else { //reverse status combination\n                         key = status + \"-\" + finalStatus;\n@@ -163,16 +146,16 @@ public void reduce(Iterable<MetricTimelinePojo> in, Collector< ServEndpFlipFlopP\n                 }\n                 pos++;\n             }\n-            result.put(dateStr, finalStatus);\n+            result.put(dt, finalStatus);\n         }\n         return result;\n     }\n \n-    private int calcFlipFlops(TreeMap<String, String> map) {\n+    public int calcFlipFlops(TreeMap<Date, String> map) {\n \n         String previousStatus = null;\n         int flipflop = 0;\n-        for (Map.Entry<String, String> entry : map.entrySet()) {\n+        for (Map.Entry<Date, String> entry : map.entrySet()) {\n             String status = entry.getValue();\n             if (previousStatus != null && status != null && !status.equalsIgnoreCase(previousStatus)) {\n                 flipflop++;",
    "previous_filename": "flink_jobs/status_trends/src/main/java/argo/functions/servendptrends/CalcServiceEndpointFlipFlop.java"
  },
  {
    "sha": "e11c29418ec0c4168da4a28b96b278fa585f9947",
    "filename": "flink_jobs/status_trends/src/main/java/argo/utils/Utils.java",
    "status": "modified",
    "additions": 85,
    "deletions": 82,
    "changes": 167,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/utils/Utils.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2/flink_jobs/status_trends/src/main/java/argo/utils/Utils.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/argo/utils/Utils.java?ref=e13a1f1d90c8c4a98f798d0d1061cc858e2a56a2",
    "patch": "@@ -21,13 +21,13 @@\n import org.json.simple.parser.JSONParser;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n+import argo.profileparsers.TopologyEndpointParser;\n \n /**\n  *\n  * @author cthermolia\n  */\n public class Utils {\n-\n     static Logger LOG = LoggerFactory.getLogger(Utils.class);\n \n     public static String convertDateToString(Date date) throws ParseException {\n@@ -77,75 +77,75 @@ public static boolean isPreviousDate(String nowDate, String firstDate) throws Pa\n         }\n     }\n \n-    public static HashMap<String, ArrayList<String>> readMetricDataJson(String baseUri, String metricProfileUUID, String key,String proxy) throws IOException, org.json.simple.parser.ParseException {\n-        JSONObject jsonObject = RequestManager.getMetricProfileRequest(baseUri, metricProfileUUID, key, proxy);\n-        HashMap<String, ArrayList<String>> jsonDataMap = new HashMap<String, ArrayList<String>>();\n-\n-        JSONArray data = (JSONArray) jsonObject.get(\"data\");\n-\n-        Iterator<Object> dataIter = data.iterator();\n-        while (dataIter.hasNext()) {\n-            Object dataobj = dataIter.next();\n-            if (dataobj instanceof JSONObject) {\n-                JSONObject jsonDataObj = new JSONObject((Map) dataobj);\n-\n-                JSONArray services = (JSONArray) jsonDataObj.get(\"services\");\n-\n-                Iterator<Object> iterator = services.iterator();\n-\n-                while (iterator.hasNext()) {\n-                    Object obj = iterator.next();\n-                    if (obj instanceof JSONObject) {\n-                        JSONObject servObj = new JSONObject((Map) obj);\n-                        String service = (String) servObj.get(\"service\");\n-                        JSONArray metrics = (JSONArray) servObj.get(\"metrics\");\n-                        Iterator<Object> metrIter = metrics.iterator();\n-                        ArrayList<String> metricList = new ArrayList<>();\n-\n-                        while (metrIter.hasNext()) {\n-                            Object metrObj = metrIter.next();\n-                            metricList.add(metrObj.toString());\n-                        }\n-                        jsonDataMap.put(service, metricList);\n-                    }\n-                }\n-            }\n-        }\n-        return jsonDataMap;\n-    }\n-\n-    public static HashMap<String, HashMap<String, String>> readOperationProfileJson(String baseUri, String key,String proxy) throws IOException, org.json.simple.parser.ParseException {\n-        JSONObject jsonObject = RequestManager.getOperationProfileRequest(baseUri, key, proxy);\n-\n-        // A JSON object. Key value pairs are unordered. JSONObject supports java.util.Map interface.\n-        JSONArray dataList = (JSONArray) jsonObject.get(\"data\");\n-\n-        Iterator<JSONObject> iterator = dataList.iterator();\n-        HashMap<String, HashMap<String, String>> opTruthTable = new HashMap<>();\n-        while (iterator.hasNext()) {\n-            JSONObject dataObject = (JSONObject) iterator.next();\n-            JSONArray operationList = (JSONArray) dataObject.get(\"operations\");\n-            Iterator<JSONObject> opIterator = operationList.iterator();\n-            while (opIterator.hasNext()) {\n-                JSONObject operationObject = (JSONObject) opIterator.next();\n-                String opName = (String) operationObject.get(\"name\");\n-                JSONArray truthtable = (JSONArray) operationObject.get(\"truth_table\");\n-                Iterator<JSONObject> truthTableIter = truthtable.iterator();\n-                HashMap<String, String> truthTable = new HashMap<>();\n-                while (truthTableIter.hasNext()) {\n-                    JSONObject truthEntry = (JSONObject) truthTableIter.next();\n-                    String a = (String) truthEntry.get(\"a\");\n-                    String b = (String) truthEntry.get(\"b\");\n-                    String x = (String) truthEntry.get(\"x\");\n-\n-                    truthTable.put(a + \"-\" + b, x);\n-                }\n-                opTruthTable.put(opName, truthTable);\n-            }\n-        }\n-        return opTruthTable;\n-\n-    }\n+//    public static HashMap<String, ArrayList<String>> readMetricDataJson(String baseUri, String metricProfileUUID, String key,String proxy) throws IOException, org.json.simple.parser.ParseException {\n+//        JSONObject jsonObject = RequestManager.getMetricProfileRequest(baseUri, metricProfileUUID, key, proxy);\n+//        HashMap<String, ArrayList<String>> jsonDataMap = new HashMap<String, ArrayList<String>>();\n+//\n+//        JSONArray data = (JSONArray) jsonObject.get(\"data\");\n+//\n+//        Iterator<Object> dataIter = data.iterator();\n+//        while (dataIter.hasNext()) {\n+//            Object dataobj = dataIter.next();\n+//            if (dataobj instanceof JSONObject) {\n+//                JSONObject jsonDataObj = new JSONObject((Map) dataobj);\n+//\n+//                JSONArray services = (JSONArray) jsonDataObj.get(\"services\");\n+//\n+//                Iterator<Object> iterator = services.iterator();\n+//\n+//                while (iterator.hasNext()) {\n+//                    Object obj = iterator.next();\n+//                    if (obj instanceof JSONObject) {\n+//                        JSONObject servObj = new JSONObject((Map) obj);\n+//                        String service = (String) servObj.get(\"service\");\n+//                        JSONArray metrics = (JSONArray) servObj.get(\"metrics\");\n+//                        Iterator<Object> metrIter = metrics.iterator();\n+//                        ArrayList<String> metricList = new ArrayList<>();\n+//\n+//                        while (metrIter.hasNext()) {\n+//                            Object metrObj = metrIter.next();\n+//                            metricList.add(metrObj.toString());\n+//                        }\n+//                        jsonDataMap.put(service, metricList);\n+//                    }\n+//                }\n+//            }\n+//        }\n+//        return jsonDataMap;\n+//    }\n+\n+//    public static HashMap<String, HashMap<String, String>> readOperationProfileJson(String baseUri, String key,String proxy,String operationsId) throws IOException, org.json.simple.parser.ParseException {\n+//        JSONObject jsonObject = RequestManager.getOperationProfileRequest(baseUri, key, proxy, operationsId);\n+//\n+//        // A JSON object. Key value pairs are unordered. JSONObject supports java.util.Map interface.\n+//        JSONArray dataList = (JSONArray) jsonObject.get(\"data\");\n+//\n+//        Iterator<JSONObject> iterator = dataList.iterator();\n+//        HashMap<String, HashMap<String, String>> opTruthTable = new HashMap<>();\n+//        while (iterator.hasNext()) {\n+//            JSONObject dataObject = (JSONObject) iterator.next();\n+//            JSONArray operationList = (JSONArray) dataObject.get(\"operations\");\n+//            Iterator<JSONObject> opIterator = operationList.iterator();\n+//            while (opIterator.hasNext()) {\n+//                JSONObject operationObject = (JSONObject) opIterator.next();\n+//                String opName = (String) operationObject.get(\"name\");\n+//                JSONArray truthtable = (JSONArray) operationObject.get(\"truth_table\");\n+//                Iterator<JSONObject> truthTableIter = truthtable.iterator();\n+//                HashMap<String, String> truthTable = new HashMap<>();\n+//                while (truthTableIter.hasNext()) {\n+//                    JSONObject truthEntry = (JSONObject) truthTableIter.next();\n+//                    String a = (String) truthEntry.get(\"a\");\n+//                    String b = (String) truthEntry.get(\"b\");\n+//                    String x = (String) truthEntry.get(\"x\");\n+//\n+//                    truthTable.put(a + \"-\" + b, x);\n+//                }\n+//                opTruthTable.put(opName, truthTable);\n+//            }\n+//        }\n+//        return opTruthTable;\n+//\n+//    }\n \n     public static boolean checkParameters(ParameterTool params, String... vars) {\n \n@@ -160,22 +160,25 @@ public static boolean checkParameters(ParameterTool params, String... vars) {\n \n     }\n \n-    public static HashMap<String, String> readGroupEndpointJson(String baseUri, String key,String proxy) throws IOException, org.json.simple.parser.ParseException {\n-        JSONObject jsonObject = RequestManager.getTopologyEndpointRequest(baseUri, key,proxy);\n+    \n+    \n+    \n+    public static HashMap<String, String> getEndpoints(ArrayList<TopologyEndpointParser.EndpointGroup> endpointList) throws IOException, org.json.simple.parser.ParseException {\n+      \n+        \n+        \n         HashMap<String, String> jsonDataMap = new HashMap<>();\n \n-        JSONArray data = (JSONArray) jsonObject.get(\"data\");\n-\n-        Iterator<Object> dataIter = data.iterator();\n+      \n+        Iterator<TopologyEndpointParser.EndpointGroup> dataIter = endpointList.iterator();\n         while (dataIter.hasNext()) {\n-            Object dataobj = dataIter.next();\n-            if (dataobj instanceof JSONObject) {\n-                JSONObject jsonDataObj = new JSONObject((Map) dataobj);\n-                String hostname = (String) jsonDataObj.get(\"hostname\");\n-                String service = (String) jsonDataObj.get(\"service\");\n-                String group = (String) jsonDataObj.get(\"group\");\n+            TopologyEndpointParser.EndpointGroup dataobj = dataIter.next();\n+            \n+            String hostname = dataobj.getHostname();\n+                String service = dataobj.getService();\n+                String group = dataobj.getGroup();\n                 jsonDataMap.put(hostname + \"-\" + service, group);\n-            }\n+            \n         }\n \n         return jsonDataMap;"
  },
  {
    "sha": "6d7b0ee860559064b773341f3506b8d2276a6d32",
    "filename": "flink_jobs/status_trends/src/main/java/parsers/AggregationProfileParser.java",
    "status": "removed",
    "additions": 0,
    "deletions": 99,
    "changes": 99,
    "blob_url": "https://github.com/ARGOeu/argo-streaming/blob/9615d67301d4def5b974d5a080bb60d8c3bb1525/flink_jobs/status_trends/src/main/java/parsers/AggregationProfileParser.java",
    "raw_url": "https://github.com/ARGOeu/argo-streaming/raw/9615d67301d4def5b974d5a080bb60d8c3bb1525/flink_jobs/status_trends/src/main/java/parsers/AggregationProfileParser.java",
    "contents_url": "https://api.github.com/repos/ARGOeu/argo-streaming/contents/flink_jobs/status_trends/src/main/java/parsers/AggregationProfileParser.java?ref=9615d67301d4def5b974d5a080bb60d8c3bb1525",
    "patch": "@@ -1,99 +0,0 @@\n-/*\n- * To change this license header, choose License Headers in Project Properties.\n- * To change this template file, choose Tools | Templates\n- * and open the template in the editor.\n- */\n-package parsers;\n-\n-import argo.utils.RequestManager;\n-import java.io.IOException;\n-import java.util.ArrayList;\n-import java.util.HashMap;\n-import java.util.Iterator;\n-import org.json.simple.JSONArray;\n-import org.json.simple.JSONObject;\n-import org.json.simple.parser.ParseException;\n-\n-/**\n- *\n- * @author cthermolia\n- */\n-public class AggregationProfileParser {\n-\n-    public static String id;\n-    public static String date;\n-    public static String name;\n-    public static String namespace;\n-    public static String endpointGroup;\n-    public static String metricOp;\n-    public static String profileOp;\n-    public static String[] metricProfile = new String[2];\n-    public static ArrayList<GroupOps> groups = new ArrayList<>();\n-\n-    public static void loadAggrProfileInfo(String baseUri, String key, String proxy) throws IOException, ParseException {\n-\n-        JSONObject jsonObject = RequestManager.getAggregationProfileRequest(baseUri, key, proxy);\n-\n-        JSONArray dataList = (JSONArray) jsonObject.get(\"data\");\n-\n-        Iterator<JSONObject> iterator = dataList.iterator();\n-\n-        while (iterator.hasNext()) {\n-            if (iterator.next() instanceof JSONObject) {\n-                JSONObject dataObject = (JSONObject) iterator.next();\n-\n-                id = (String) dataObject.get(\"id\");\n-                date = (String) dataObject.get(\"date\");\n-                name = (String) dataObject.get(\"name\");\n-                namespace = (String) dataObject.get(\"namespace\");\n-                endpointGroup = (String) dataObject.get(\"endpoint_group\");\n-                metricOp = (String) dataObject.get(\"metric_operation\");\n-                profileOp = (String) dataObject.get(\"profile_operation\");\n-\n-                JSONObject metricProfileObject = (JSONObject) dataObject.get(\"metric_profile\");\n-\n-                metricProfile[0] = (String) metricProfileObject.get(\"id\");\n-                metricProfile[1] = (String) metricProfileObject.get(\"name\");\n-\n-                JSONArray groupArray = (JSONArray) dataObject.get(\"groups\");\n-                Iterator<JSONObject> groupiterator = groupArray.iterator();\n-\n-                while (groupiterator.hasNext()) {\n-                    if (groupiterator.next() instanceof JSONObject) {\n-                        JSONObject groupObject = (JSONObject) groupiterator.next();\n-                        String groupname = (String) groupObject.get(\"name\");\n-                        String groupoperation = (String) groupObject.get(\"operation\");\n-\n-                        JSONArray serviceArray = (JSONArray) groupObject.get(\"services\");\n-                        Iterator<JSONObject> serviceiterator = serviceArray.iterator();\n-                        HashMap<String, String> services = new HashMap<>();\n-                        while (serviceiterator.hasNext()) {\n-                            JSONObject servObject = (JSONObject) serviceiterator.next();\n-                            String servicename = (String) servObject.get(\"name\");\n-                            String serviceoperation = (String) servObject.get(\"operation\");\n-                            services.put(servicename, serviceoperation);\n-\n-                        }\n-                        groups.add(new GroupOps(groupname, groupoperation, services));\n-\n-                    }\n-                }\n-            }\n-        }\n-    }\n-\n-    private static class GroupOps {\n-\n-        public String name;\n-        public String operation;\n-        public HashMap<String, String> services;\n-\n-        public GroupOps(String name, String operation, HashMap<String, String> services) {\n-            this.name = name;\n-            this.operation = operation;\n-            this.services = services;\n-        }\n-\n-    }\n-\n-}"
  }
]
