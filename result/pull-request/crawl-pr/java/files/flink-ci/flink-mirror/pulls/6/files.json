[
  {
    "sha": "92b14f7e69e0bb07dcfc9ba29d6c6263cbdc5091",
    "filename": "azure-pipelines.yml",
    "status": "modified",
    "additions": 1,
    "deletions": 0,
    "changes": 1,
    "blob_url": "https://github.com/flink-ci/flink-mirror/blob/f18a7de3388ed18cf9b763d891c16bc098667abb/azure-pipelines.yml",
    "raw_url": "https://github.com/flink-ci/flink-mirror/raw/f18a7de3388ed18cf9b763d891c16bc098667abb/azure-pipelines.yml",
    "contents_url": "https://api.github.com/repos/flink-ci/flink-mirror/contents/azure-pipelines.yml?ref=f18a7de3388ed18cf9b763d891c16bc098667abb",
    "patch": "@@ -49,6 +49,7 @@ resources:\n variables:\n   MAVEN_CACHE_FOLDER: $(Pipeline.Workspace)/.m2/repository\n   E2E_CACHE_FOLDER: $(Pipeline.Workspace)/e2e_cache\n+  E2E_TARBALL_CACHE: $(Pipeline.Workspace)/e2e_artifact_cache\n   MAVEN_OPTS: '-Dmaven.repo.local=$(MAVEN_CACHE_FOLDER)'\n   CACHE_KEY: maven | $(Agent.OS) | **/pom.xml, !**/target/**\n   CACHE_FALLBACK_KEY: maven | $(Agent.OS)"
  },
  {
    "sha": "59d0636181d966f429141e28f66d3b01ba904768",
    "filename": "flink-end-to-end-tests/run-nightly-tests.sh",
    "status": "modified",
    "additions": 1,
    "deletions": 173,
    "changes": 174,
    "blob_url": "https://github.com/flink-ci/flink-mirror/blob/f18a7de3388ed18cf9b763d891c16bc098667abb/flink-end-to-end-tests/run-nightly-tests.sh",
    "raw_url": "https://github.com/flink-ci/flink-mirror/raw/f18a7de3388ed18cf9b763d891c16bc098667abb/flink-end-to-end-tests/run-nightly-tests.sh",
    "contents_url": "https://api.github.com/repos/flink-ci/flink-mirror/contents/flink-end-to-end-tests/run-nightly-tests.sh?ref=f18a7de3388ed18cf9b763d891c16bc098667abb",
    "patch": "@@ -94,189 +94,17 @@ printf \"========================================================================\n # Checkpointing tests\n ################################################################################\n \n-\n-# Remove this condition once FLINK-21333 is done\n-if [[ ${PROFILE} != *\"enable-adaptive-scheduler\"* ]]; then\n-\trun_test \"Resuming Savepoint (hashmap, async, no parallelism change) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_savepoint.sh 2 2 hashmap true\"\n-\trun_test \"Resuming Savepoint (hashmap, sync, no parallelism change) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_savepoint.sh 2 2 hashmap false\"\n-\trun_test \"Resuming Savepoint (hashmap, async, scale up) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_savepoint.sh 2 4 hashmap true\"\n-\trun_test \"Resuming Savepoint (hashmap, sync, scale up) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_savepoint.sh 2 4 hashmap false\"\n-\trun_test \"Resuming Savepoint (hashmap, async, scale down) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_savepoint.sh 4 2 hashmap true\"\n-\trun_test \"Resuming Savepoint (hashmap, sync, scale down) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_savepoint.sh 4 2 hashmap false\"\n-\trun_test \"Resuming Savepoint (rocks, no parallelism change, heap timers) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_savepoint.sh 2 2 rocks false heap\"\n-\trun_test \"Resuming Savepoint (rocks, scale up, heap timers) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_savepoint.sh 2 4 rocks false heap\"\n-\trun_test \"Resuming Savepoint (rocks, scale down, heap timers) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_savepoint.sh 4 2 rocks false heap\"\n-\trun_test \"Resuming Savepoint (rocks, no parallelism change, rocks timers) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_savepoint.sh 2 2 rocks false rocks\"\n-\trun_test \"Resuming Savepoint (rocks, scale up, rocks timers) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_savepoint.sh 2 4 rocks false rocks\"\n-\trun_test \"Resuming Savepoint (rocks, scale down, rocks timers) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_savepoint.sh 4 2 rocks false rocks\"\n-\n-\trun_test \"Resuming Externalized Checkpoint (hashmap, async, no parallelism change) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_externalized_checkpoints.sh 2 2 hashmap true true\" \"skip_check_exceptions\"\n-\trun_test \"Resuming Externalized Checkpoint (hashmap, sync, no parallelism change) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_externalized_checkpoints.sh 2 2 hashmap false true\" \"skip_check_exceptions\"\n-\trun_test \"Resuming Externalized Checkpoint (hashmap, async, scale up) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_externalized_checkpoints.sh 2 4 hashmap true true\" \"skip_check_exceptions\"\n-\trun_test \"Resuming Externalized Checkpoint (hashmap, sync, scale up) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_externalized_checkpoints.sh 2 4 hashmap false true\" \"skip_check_exceptions\"\n-\trun_test \"Resuming Externalized Checkpoint (hashmap, async, scale down) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_externalized_checkpoints.sh 4 2 hashmap true true\" \"skip_check_exceptions\"\n-\trun_test \"Resuming Externalized Checkpoint (hashmap, sync, scale down) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_externalized_checkpoints.sh 4 2 hashmap false true\" \"skip_check_exceptions\"\n-\trun_test \"Resuming Externalized Checkpoint (rocks, non-incremental, no parallelism change) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_externalized_checkpoints.sh 2 2 rocks true false\" \"skip_check_exceptions\"\n-\trun_test \"Resuming Externalized Checkpoint (rocks, incremental, no parallelism change) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_externalized_checkpoints.sh 2 2 rocks true true\" \"skip_check_exceptions\"\n-\trun_test \"Resuming Externalized Checkpoint (rocks, non-incremental, scale up) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_externalized_checkpoints.sh 2 4 rocks true false\" \"skip_check_exceptions\"\n-\trun_test \"Resuming Externalized Checkpoint (rocks, incremental, scale up) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_externalized_checkpoints.sh 2 4 rocks true true\" \"skip_check_exceptions\"\n-\trun_test \"Resuming Externalized Checkpoint (rocks, non-incremental, scale down) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_externalized_checkpoints.sh 4 2 rocks true false\" \"skip_check_exceptions\"\n-\trun_test \"Resuming Externalized Checkpoint (rocks, incremental, scale down) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_externalized_checkpoints.sh 4 2 rocks true true\" \"skip_check_exceptions\"\n-\n-\trun_test \"Resuming Externalized Checkpoint after terminal failure (hashmap, async) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_externalized_checkpoints.sh 2 2 hashmap true false true\" \"skip_check_exceptions\"\n-\trun_test \"Resuming Externalized Checkpoint after terminal failure (hashmap, sync) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_externalized_checkpoints.sh 2 2 hashmap false false true\" \"skip_check_exceptions\"\n-\trun_test \"Resuming Externalized Checkpoint after terminal failure (rocks, non-incremental) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_externalized_checkpoints.sh 2 2 rocks true false true\" \"skip_check_exceptions\"\n-\trun_test \"Resuming Externalized Checkpoint after terminal failure (rocks, incremental) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_resume_externalized_checkpoints.sh 2 2 rocks true true true\" \"skip_check_exceptions\"\n-fi\n-\n-run_test \"RocksDB Memory Management end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_rocksdb_state_memory_control.sh\"\n-\n-################################################################################\n-# Docker / Container / Kubernetes / Mesos tests\n-################################################################################\n-\n-# These tests are known to fail on JDK11. See FLINK-13719\n-if [[ ${PROFILE} != *\"jdk11\"* && ${PROFILE} != *\"enable-adaptive-scheduler\"* ]]; then\n-\trun_test \"Wordcount on Docker test (custom fs plugin)\" \"$END_TO_END_DIR/test-scripts/test_docker_embedded_job.sh dummy-fs\"\n-\n-\trun_test \"Run Kubernetes test\" \"$END_TO_END_DIR/test-scripts/test_kubernetes_embedded_job.sh\"\n-\trun_test \"Run kubernetes session test (default input)\" \"$END_TO_END_DIR/test-scripts/test_kubernetes_session.sh\"\n-\trun_test \"Run kubernetes session test (custom fs plugin)\" \"$END_TO_END_DIR/test-scripts/test_kubernetes_session.sh dummy-fs\"\n-\trun_test \"Run kubernetes application test\" \"$END_TO_END_DIR/test-scripts/test_kubernetes_application.sh\"\n-\trun_test \"Run Kubernetes IT test\" \"$END_TO_END_DIR/test-scripts/test_kubernetes_itcases.sh\"\n-\n-\trun_test \"Running Flink over NAT end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_nat.sh\" \"skip_check_exceptions\"\n-\n-\tif [[ `uname -i` != 'aarch64' ]]; then\n-\t\t# Skip PyFlink e2e test, because MiniConda and Pyarrow which Pyflink depends doesn't support aarch64 currently.\n-\t\trun_test \"Run kubernetes pyflink application test\" \"$END_TO_END_DIR/test-scripts/test_kubernetes_pyflink_application.sh\"\n-\t\t\n-\t\t# Hadoop YARN deosn't support aarch64 at this moment. See: https://issues.apache.org/jira/browse/HADOOP-16723\n-\n-# disabled because of e2e timeouts\n-#\t\trun_test \"Running Kerberized YARN per-job on Docker test (default input)\" \"$END_TO_END_DIR/test-scripts/test_yarn_job_kerberos_docker.sh\"\n-#\t\trun_test \"Running Kerberized YARN per-job on Docker test (custom fs plugin)\" \"$END_TO_END_DIR/test-scripts/test_yarn_job_kerberos_docker.sh dummy-fs\"\n-#\t\trun_test \"Running Kerberized YARN application on Docker test (default input)\" \"$END_TO_END_DIR/test-scripts/test_yarn_application_kerberos_docker.sh\"\n-#\t\trun_test \"Running Kerberized YARN application on Docker test (custom fs plugin)\" \"$END_TO_END_DIR/test-scripts/test_yarn_application_kerberos_docker.sh dummy-fs\"\n-\n-\t\trun_test \"Run Mesos WordCount test\" \"$END_TO_END_DIR/test-scripts/test_mesos_wordcount.sh\"\n-\t\trun_test \"Run Mesos multiple submission test\" \"$END_TO_END_DIR/test-scripts/test_mesos_multiple_submissions.sh\"\n-\n-\t\t# `google/cloud-sdk` docker image doesn't support aarch64 currently.\n-\t\trun_test \"Test PubSub connector with Docker based Google PubSub Emulator\" \"$END_TO_END_DIR/test-scripts/test_streaming_gcp_pubsub.sh\"\n-\tfi\n-fi\n-\n-################################################################################\n-# High Availability\n-################################################################################\n-\n-run_test \"Running HA dataset end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_ha_dataset.sh\" \"skip_check_exceptions\"\n-\n-run_test \"Running HA (hashmap, async) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_ha_datastream.sh hashmap true false 3.4\" \"skip_check_exceptions\"\n-run_test \"Running HA (hashmap, sync) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_ha_datastream.sh hashmap false false 3.5\" \"skip_check_exceptions\"\n-run_test \"Running HA (rocks, non-incremental) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_ha_datastream.sh rocks true false 3.4\" \"skip_check_exceptions\"\n-run_test \"Running HA (rocks, incremental) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_ha_datastream.sh rocks true true 3.5\" \"skip_check_exceptions\"\n-\n-run_test \"Running HA per-job cluster (hashmap, async) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_ha_per_job_cluster_datastream.sh hashmap true false 3.4\" \"skip_check_exceptions\"\n-run_test \"Running HA per-job cluster (hashmap, sync) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_ha_per_job_cluster_datastream.sh hashmap false false 3.5\" \"skip_check_exceptions\"\n-run_test \"Running HA per-job cluster (rocks, non-incremental) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_ha_per_job_cluster_datastream.sh rocks true false 3.4\" \"skip_check_exceptions\"\n-run_test \"Running HA per-job cluster (rocks, incremental) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_ha_per_job_cluster_datastream.sh rocks true true 3.5\" \"skip_check_exceptions\"\n-\n-################################################################################\n-# Miscellaneous\n-################################################################################\n-\n-run_test \"Flink CLI end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_cli.sh\"\n-\n-run_test \"Queryable state (rocksdb) end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_queryable_state.sh rocksdb\"\n-run_test \"Queryable state (rocksdb) with TM restart end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_queryable_state_restart_tm.sh\" \"skip_check_exceptions\"\n-\n-run_test \"DataSet allround end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_batch_allround.sh\"\n-run_test \"Batch SQL end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_batch_sql.sh\"\n-run_test \"Streaming SQL end-to-end test (Old planner)\" \"$END_TO_END_DIR/test-scripts/test_streaming_sql.sh old\" \"skip_check_exceptions\"\n-run_test \"Streaming SQL end-to-end test (Blink planner)\" \"$END_TO_END_DIR/test-scripts/test_streaming_sql.sh blink\" \"skip_check_exceptions\"\n-\n-if [[ ${PROFILE} != *\"enable-adaptive-scheduler\"* ]]; then # FLINK-21400\n-  run_test \"Streaming File Sink end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_file_sink.sh local StreamingFileSink\" \"skip_check_exceptions\"\n-  run_test \"Streaming File Sink s3 end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_file_sink.sh s3 StreamingFileSink\" \"skip_check_exceptions\"\n-  run_test \"New File Sink end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_file_sink.sh local FileSink\" \"skip_check_exceptions\"\n-  run_test \"New File Sink s3 end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_file_sink.sh s3 FileSink\" \"skip_check_exceptions\"\n-\n-  run_test \"Stateful stream job upgrade end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_stateful_stream_job_upgrade.sh 2 4\"\n-fi\n-\n-run_test \"Netty shuffle direct memory consumption end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_netty_shuffle_memory_control.sh\"\n-\n-run_test \"Elasticsearch (v5.3.3) sink end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_streaming_elasticsearch.sh 5 https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.3.3.tar.gz\"\n-run_test \"Elasticsearch (v6.3.1) sink end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_streaming_elasticsearch.sh 6 https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.3.1.tar.gz\"\n-\n-run_test \"Quickstarts Java nightly end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_quickstarts.sh java\"\n-run_test \"Quickstarts Scala nightly end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_quickstarts.sh scala\"\n-\n-run_test \"Walkthrough DataStream Java nightly end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_datastream_walkthroughs.sh java\"\n-run_test \"Walkthrough DataStream Scala nightly end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_datastream_walkthroughs.sh scala\"\n-\n-if [[ ${PROFILE} != *\"jdk11\"* ]]; then\n-\trun_test \"Avro Confluent Schema Registry nightly end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_confluent_schema_registry.sh\"\n-fi\n-\n-run_test \"State TTL Heap backend end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_stream_state_ttl.sh hashmap\" \"skip_check_exceptions\"\n-run_test \"State TTL RocksDb backend end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_stream_state_ttl.sh rocks\" \"skip_check_exceptions\"\n-\n-run_test \"SQL Client end-to-end test (Old planner) Elasticsearch (v7.5.1)\" \"$END_TO_END_DIR/test-scripts/test_sql_client.sh old\"\n-run_test \"SQL Client end-to-end test (Blink planner) Elasticsearch (v7.5.1)\" \"$END_TO_END_DIR/test-scripts/test_sql_client.sh blink\"\n-\n-run_test \"TPC-H end-to-end test (Blink planner)\" \"$END_TO_END_DIR/test-scripts/test_tpch.sh\"\n-run_test \"TPC-DS end-to-end test (Blink planner)\" \"$END_TO_END_DIR/test-scripts/test_tpcds.sh\"\n-\n-run_test \"Heavy deployment end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_heavy_deployment.sh\" \"skip_check_exceptions\"\n-\n-run_test \"ConnectedComponents iterations with high parallelism end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_high_parallelism_iterations.sh 25\"\n-\n-run_test \"Dependency shading of table modules test\" \"$END_TO_END_DIR/test-scripts/test_table_shaded_dependencies.sh\"\n-\n-run_test \"Shaded Hadoop S3A with credentials provider end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_batch_wordcount.sh hadoop_with_provider\"\n+echo \"E2E cache dir prior to test run: $E2E_TARBALL_CACHE\"\n \n if [[ `uname -i` != 'aarch64' ]]; then\n     run_test \"PyFlink end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_pyflink.sh\" \"skip_check_exceptions\"\n fi\n-# These tests are known to fail on JDK11. See FLINK-13719\n-# disabled because of e2e timeouts\n-#if [[ ${PROFILE} != *\"jdk11\"* ]] && [[ `uname -i` != 'aarch64' ]]; then\n-#    run_test \"PyFlink YARN per-job on Docker test\" \"$END_TO_END_DIR/test-scripts/test_pyflink_yarn.sh\" \"skip_check_exceptions\"\n-#fi\n-\n-################################################################################\n-# Sticky Scheduling\n-################################################################################\n-\n-if [[ ${PROFILE} != *\"enable-adaptive-scheduler\"* ]]; then #FLINK-21450\n-\trun_test \"Local recovery and sticky scheduling end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_local_recovery_and_scheduling.sh 4 3 hashmap false false\" \"skip_check_exceptions\"\n-\trun_test \"Local recovery and sticky scheduling end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_local_recovery_and_scheduling.sh 4 3 hashmap false true\" \"skip_check_exceptions\"\n-\trun_test \"Local recovery and sticky scheduling end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_local_recovery_and_scheduling.sh 4 10 rocks false false\" \"skip_check_exceptions\"\n-\trun_test \"Local recovery and sticky scheduling end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_local_recovery_and_scheduling.sh 4 10 rocks true false\" \"skip_check_exceptions\"\n-\trun_test \"Local recovery and sticky scheduling end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_local_recovery_and_scheduling.sh 4 10 rocks false true\" \"skip_check_exceptions\"\n-\trun_test \"Local recovery and sticky scheduling end-to-end test\" \"$END_TO_END_DIR/test-scripts/test_local_recovery_and_scheduling.sh 4 10 rocks true true\" \"skip_check_exceptions\"\n-fi\n-\n-printf \"\\n[PASS] All bash e2e-tests passed\\n\"\n \n printf \"\\n\\n==============================================================================\\n\"\n printf \"Running Java end-to-end tests\\n\"\n printf \"==============================================================================\\n\"\n \n \n-LOG4J_PROPERTIES=${END_TO_END_DIR}/../tools/ci/log4j.properties\n-\n-MVN_LOGGING_OPTIONS=\"-Dlog.dir=${ARTIFACTS_DIR} -DlogBackupDir=${ARTIFACTS_DIR} -Dlog4j.configurationFile=file://$LOG4J_PROPERTIES\"\n-MVN_COMMON_OPTIONS=\"-Dflink.forkCount=2 -Dflink.forkCountTestPackage=2 -Dfast -Pskip-webui-build\"\n-e2e_modules=$(find flink-end-to-end-tests -mindepth 2 -maxdepth 5 -name 'pom.xml' -printf '%h\\n' | sort -u | tr '\\n' ',')\n-e2e_modules=\"${e2e_modules},$(find flink-walkthroughs -mindepth 2 -maxdepth 2 -name 'pom.xml' -printf '%h\\n' | sort -u | tr '\\n' ',')\"\n-\n-PROFILE=\"$PROFILE -Pe2e-travis1 -Pe2e-travis2 -Pe2e-travis3 -Pe2e-travis4 -Pe2e-travis5 -Pe2e-travis6\"\n-run_mvn ${MVN_COMMON_OPTIONS} ${MVN_LOGGING_OPTIONS} ${PROFILE} verify -pl ${e2e_modules} -DdistDir=$(readlink -e build-target) -Dcache-dir=$E2E_CACHE_FOLDER -Dcache-ttl=P1M -Dcache-download-attempt-timeout=4min -Dcache-download-global-timeout=10min\n-\n EXIT_CODE=$?\n \n "
  },
  {
    "sha": "34bbd8fdcb004a502af6932722ff5da4e3fa5a1a",
    "filename": "flink-end-to-end-tests/test-scripts/common_artifact_download_cacher.sh",
    "status": "added",
    "additions": 54,
    "deletions": 0,
    "changes": 54,
    "blob_url": "https://github.com/flink-ci/flink-mirror/blob/f18a7de3388ed18cf9b763d891c16bc098667abb/flink-end-to-end-tests/test-scripts/common_artifact_download_cacher.sh",
    "raw_url": "https://github.com/flink-ci/flink-mirror/raw/f18a7de3388ed18cf9b763d891c16bc098667abb/flink-end-to-end-tests/test-scripts/common_artifact_download_cacher.sh",
    "contents_url": "https://api.github.com/repos/flink-ci/flink-mirror/contents/flink-end-to-end-tests/test-scripts/common_artifact_download_cacher.sh?ref=f18a7de3388ed18cf9b763d891c16bc098667abb",
    "patch": "@@ -0,0 +1,54 @@\n+#!/usr/bin/env bash\n+################################################################################\n+#  Licensed to the Apache Software Foundation (ASF) under one\n+#  or more contributor license agreements.  See the NOTICE file\n+#  distributed with this work for additional information\n+#  regarding copyright ownership.  The ASF licenses this file\n+#  to you under the Apache License, Version 2.0 (the\n+#  \"License\"); you may not use this file except in compliance\n+#  with the License.  You may obtain a copy of the License at\n+#\n+#      http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#  Unless required by applicable law or agreed to in writing, software\n+#  distributed under the License is distributed on an \"AS IS\" BASIS,\n+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#  See the License for the specific language governing permissions and\n+# limitations under the License.\n+################################################################################\n+\n+# This bash script aims to predownload dependency tarballs for the E2E tests.\n+\n+if [ -z \"$E2E_TARBALL_CACHE\" ] ; then\n+    echo \"You have to export the E2E Tarball Cache as E2E_TARBALL_CACHE\"\n+    exit 1\n+fi\n+\n+mkdir -p $E2E_TARBALL_CACHE\n+\n+# Given a variable name and a URL, checks whether the file exists in the E2E_TARBALL_CACHE,\n+# otherwise retrieves from source, and echos the path to the cached file.\n+# For example:\n+# result=$(get_artifact https://archive.apache.org/artifact.tar.gz)\n+# echo $result\n+\n+function get_artifact {\n+    local __result=$1\n+    local ARTIFACT_URL=$2\n+    BASENAME=\"`basename $ARTIFACT_URL`\"\n+    echo \"RUN ARTIFACT GET\"\n+    echo $ARTIFACT_URL\n+    echo $BASENAME\n+    echo \"CONTENTS OF ${E2E_TARBALL_CACHE}\"\n+    ls $E2E_TARBALL_CACHE\n+    if [ ! -f \"$E2E_TARBALL_CACHE/$BASENAME\" ]; then\n+        echo \"Not present. Downloading.\"\n+        curl $ARTIFACT_URL --retry 10 --retry-max-time 120 --output $E2E_TARBALL_CACHE/$BASENAME\n+        local res=$?\n+        echo \"Result is $res\"\n+        if [ ! 0 -eq $res ]; then\n+            exit 1\n+        fi\n+    fi\n+    eval $__result=\"$E2E_TARBALL_CACHE/$BASENAME\"\n+}"
  },
  {
    "sha": "a4ffca7ad28e09ab03c454f7ac6798011c228a40",
    "filename": "flink-end-to-end-tests/test-scripts/common_yarn_docker.sh",
    "status": "modified",
    "additions": 5,
    "deletions": 0,
    "changes": 5,
    "blob_url": "https://github.com/flink-ci/flink-mirror/blob/f18a7de3388ed18cf9b763d891c16bc098667abb/flink-end-to-end-tests/test-scripts/common_yarn_docker.sh",
    "raw_url": "https://github.com/flink-ci/flink-mirror/raw/f18a7de3388ed18cf9b763d891c16bc098667abb/flink-end-to-end-tests/test-scripts/common_yarn_docker.sh",
    "contents_url": "https://api.github.com/repos/flink-ci/flink-mirror/contents/flink-end-to-end-tests/test-scripts/common_yarn_docker.sh?ref=f18a7de3388ed18cf9b763d891c16bc098667abb",
    "patch": "@@ -20,6 +20,7 @@ set -o pipefail\n \n source \"$(dirname \"$0\")\"/common.sh\n source \"$(dirname \"$0\")\"/common_docker.sh\n+source \"$(dirname \"$0\")\"/common_artifact_download_cacher.sh\n \n FLINK_TARBALL_DIR=$TEST_DATA_DIR\n FLINK_TARBALL=flink.tar.gz\n@@ -93,6 +94,10 @@ function start_hadoop_cluster() {\n }\n \n function build_image() {\n+    echo \"Predownloading Hadoop tarball\"\n+    cache_path=$(get_artifact \"http://archive.apache.org/dist/hadoop/common/hadoop-2.8.4/hadoop-2.8.4.tar.gz\")\n+    ln \"$cache_path\" \"$END_TO_END_DIR/test-scripts/docker-hadoop-secure-cluster/hadoop-2.8.4.tar.gz\"\n+\n     echo \"Building Hadoop Docker container\"\n     docker build --build-arg HADOOP_VERSION=2.8.4 \\\n         -f $END_TO_END_DIR/test-scripts/docker-hadoop-secure-cluster/Dockerfile \\"
  },
  {
    "sha": "cdb6c4809941c673dac4ca4efa77d1949e477167",
    "filename": "flink-end-to-end-tests/test-scripts/docker-hadoop-secure-cluster/Dockerfile",
    "status": "modified",
    "additions": 2,
    "deletions": 2,
    "changes": 4,
    "blob_url": "https://github.com/flink-ci/flink-mirror/blob/f18a7de3388ed18cf9b763d891c16bc098667abb/flink-end-to-end-tests/test-scripts/docker-hadoop-secure-cluster/Dockerfile",
    "raw_url": "https://github.com/flink-ci/flink-mirror/raw/f18a7de3388ed18cf9b763d891c16bc098667abb/flink-end-to-end-tests/test-scripts/docker-hadoop-secure-cluster/Dockerfile",
    "contents_url": "https://api.github.com/repos/flink-ci/flink-mirror/contents/flink-end-to-end-tests/test-scripts/docker-hadoop-secure-cluster/Dockerfile?ref=f18a7de3388ed18cf9b763d891c16bc098667abb",
    "patch": "@@ -55,9 +55,9 @@ RUN sed -i 's/^#crypto.policy=unlimited/crypto.policy=unlimited/' $JAVA_HOME/jre\n \n ARG HADOOP_VERSION=2.8.4\n \n-ENV HADOOP_URL http://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz\n+COPY hadoop-${HADOOP_VERSION}.tar.gz /tmp/hadoop.tar.gz\n+\n RUN set -x \\\n-    && curl -fSL \"$HADOOP_URL\" -o /tmp/hadoop.tar.gz \\\n     && tar -xf /tmp/hadoop.tar.gz -C /usr/local/ \\\n     && rm /tmp/hadoop.tar.gz*\n "
  },
  {
    "sha": "84640ebd53b48bada02128235ac22f9a256543b8",
    "filename": "flink-end-to-end-tests/test-scripts/kafka-common.sh",
    "status": "modified",
    "additions": 10,
    "deletions": 2,
    "changes": 12,
    "blob_url": "https://github.com/flink-ci/flink-mirror/blob/f18a7de3388ed18cf9b763d891c16bc098667abb/flink-end-to-end-tests/test-scripts/kafka-common.sh",
    "raw_url": "https://github.com/flink-ci/flink-mirror/raw/f18a7de3388ed18cf9b763d891c16bc098667abb/flink-end-to-end-tests/test-scripts/kafka-common.sh",
    "contents_url": "https://api.github.com/repos/flink-ci/flink-mirror/contents/flink-end-to-end-tests/test-scripts/kafka-common.sh?ref=f18a7de3388ed18cf9b763d891c16bc098667abb",
    "patch": "@@ -22,6 +22,8 @@ if [[ -z $TEST_DATA_DIR ]]; then\n   exit 1\n fi\n \n+source \"${END_TO_END_DIR}/test-scripts/common_artifact_download_cacher.sh\"\n+\n KAFKA_VERSION=\"$1\"\n CONFLUENT_VERSION=\"$2\"\n CONFLUENT_MAJOR_VERSION=\"$3\"\n@@ -37,10 +39,15 @@ function setup_kafka_dist {\n   mkdir -p $TEST_DATA_DIR\n   KAFKA_URL=\"https://archive.apache.org/dist/kafka/$KAFKA_VERSION/kafka_2.12-$KAFKA_VERSION.tgz\"\n   echo \"Downloading Kafka from $KAFKA_URL\"\n-  curl ${KAFKA_URL} --retry 10 --retry-max-time 120 --output ${TEST_DATA_DIR}/kafka.tgz\n+  get_artifact cache_path $KAFKA_URL\n+  echo \"Kafka Downloaded to $cache_path\"\n+  ln \"$cache_path\" \"${TEST_DATA_DIR}/kafka.tgz\"\n+  echo \"Kafka ln'd to ${TEST_DATA_DIR}/kafka.tgz\"\n \n   tar xzf $TEST_DATA_DIR/kafka.tgz -C $TEST_DATA_DIR/\n \n+  echo \"Kafka untarred to $TEST_DATA_DIR\"\n+\n   # fix kafka config\n   sed -i -e \"s+^\\(dataDir\\s*=\\s*\\).*$+\\1$TEST_DATA_DIR/zookeeper+\" $KAFKA_DIR/config/zookeeper.properties\n   sed -i -e \"s+^\\(log\\.dirs\\s*=\\s*\\).*$+\\1$TEST_DATA_DIR/kafka+\" $KAFKA_DIR/config/server.properties\n@@ -51,7 +58,8 @@ function setup_confluent_dist {\n   mkdir -p $TEST_DATA_DIR\n   CONFLUENT_URL=\"http://packages.confluent.io/archive/$CONFLUENT_MAJOR_VERSION/confluent-oss-$CONFLUENT_VERSION-2.11.tar.gz\"\n   echo \"Downloading confluent from $CONFLUENT_URL\"\n-  curl ${CONFLUENT_URL} --retry 10 --retry-max-time 120 --output ${TEST_DATA_DIR}/confluent.tgz\n+  cache_path=$(get_artifact $CONFLUENT_URL)\n+  ln \"$cache_path\" \"${TEST_DATA_DIR}/confluent.tgz\"\n \n   tar xzf $TEST_DATA_DIR/confluent.tgz -C $TEST_DATA_DIR/\n "
  },
  {
    "sha": "608ebbf7a90a0feac6b482935d244494919717bf",
    "filename": "flink-end-to-end-tests/test-scripts/test_pyflink.sh",
    "status": "modified",
    "additions": 2,
    "deletions": 0,
    "changes": 2,
    "blob_url": "https://github.com/flink-ci/flink-mirror/blob/f18a7de3388ed18cf9b763d891c16bc098667abb/flink-end-to-end-tests/test-scripts/test_pyflink.sh",
    "raw_url": "https://github.com/flink-ci/flink-mirror/raw/f18a7de3388ed18cf9b763d891c16bc098667abb/flink-end-to-end-tests/test-scripts/test_pyflink.sh",
    "contents_url": "https://api.github.com/repos/flink-ci/flink-mirror/contents/flink-end-to-end-tests/test-scripts/test_pyflink.sh?ref=f18a7de3388ed18cf9b763d891c16bc098667abb",
    "patch": "@@ -101,6 +101,7 @@ cd \"${CURRENT_DIR}\"\n \n start_cluster\n \n+echo \"E2E Cache folder prior to Pyflink Table Job: ${E2E_TARBALL_CACHE}\"\n echo \"Test PyFlink Table job:\"\n \n FLINK_PYTHON_TEST_DIR=`cd \"${CURRENT_DIR}/../flink-python-test\" && pwd -P`\n@@ -208,6 +209,7 @@ JOB_ID=$($FLINK_DIR/bin/sql-client.sh embedded \\\n \n wait_job_terminal_state \"$JOB_ID\" \"FINISHED\"\n \n+echo \"E2E_TARBALL_CACHE prior to Pyflink DS job: ${E2E_TARBALL_CACHE}\"\n echo \"Test PyFlink DataStream job:\"\n \n # prepare Kafka"
  },
  {
    "sha": "a780fdb7affbde7e1d08e0b715bac5f22e38012d",
    "filename": "tools/azure-pipelines/jobs-template.yml",
    "status": "modified",
    "additions": 10,
    "deletions": 1,
    "changes": 11,
    "blob_url": "https://github.com/flink-ci/flink-mirror/blob/f18a7de3388ed18cf9b763d891c16bc098667abb/tools/azure-pipelines/jobs-template.yml",
    "raw_url": "https://github.com/flink-ci/flink-mirror/raw/f18a7de3388ed18cf9b763d891c16bc098667abb/tools/azure-pipelines/jobs-template.yml",
    "contents_url": "https://api.github.com/repos/flink-ci/flink-mirror/contents/tools/azure-pipelines/jobs-template.yml?ref=f18a7de3388ed18cf9b763d891c16bc098667abb",
    "patch": "@@ -136,7 +136,7 @@ jobs:\n       echo \"##vso[task.setvariable variable=JAVA_HOME]$JAVA_HOME_11_X64\"\n       echo \"##vso[task.setvariable variable=PATH]$JAVA_HOME_11_X64/bin:$PATH\"\n     displayName: \"Set to jdk11\"\n-    condition: eq('${{parameters.jdk}}', 'jdk11')  \n+    condition: eq('${{parameters.jdk}}', 'jdk11')\n \n   - script: sudo sysctl -w kernel.core_pattern=core.%p\n     displayName: Set coredump pattern\n@@ -206,6 +206,14 @@ jobs:\n       displayName: Cache E2E files\n       continueOnError: true\n       condition: not(eq(variables['SKIP'], '1'))\n+    - task: Cache@2\n+      inputs:\n+        key: e2e-artifact-cache | flink-end-to-end-tests/**/*.sh\n+        restoreKeys: e2e-artifact-cache\n+        path: $(E2E_TARBALL_CACHE)\n+      displayName: Cache E2E artifacts\n+      continueOnError: true\n+      condition: not(eq(variables['SKIP'], '1'))\n     - script: |\n         echo \"##vso[task.setvariable variable=JAVA_HOME]$JAVA_HOME_11_X64\"\n         echo \"##vso[task.setvariable variable=PATH]$JAVA_HOME_11_X64/bin:$PATH\"\n@@ -233,6 +241,7 @@ jobs:\n         IT_CASE_S3_BUCKET: $(SECRET_S3_BUCKET)\n         IT_CASE_S3_ACCESS_KEY: $(SECRET_S3_ACCESS_KEY)\n         IT_CASE_S3_SECRET_KEY: $(SECRET_S3_SECRET_KEY)\n+        E2E_TARBALL_CACHE: $(E2E_TARBALL_CACHE)\n       condition: and(succeeded(),not(eq(variables['SKIP'], '1')))\n       # upload debug artifacts\n     - task: PublishPipelineArtifact@1"
  }
]
