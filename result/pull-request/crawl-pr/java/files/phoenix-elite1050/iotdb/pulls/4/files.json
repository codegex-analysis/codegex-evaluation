[
  {
    "sha": "1cb13b9a6c454707d78cf476059367936661712f",
    "filename": "server/src/main/java/org/apache/iotdb/db/engine/flush/MemTableFlushTask.java",
    "status": "modified",
    "additions": 61,
    "deletions": 187,
    "changes": 248,
    "blob_url": "https://github.com/phoenix-elite1050/iotdb/blob/f94b072c31c88bea8500976b23770c241b6fb500/server/src/main/java/org/apache/iotdb/db/engine/flush/MemTableFlushTask.java",
    "raw_url": "https://github.com/phoenix-elite1050/iotdb/raw/f94b072c31c88bea8500976b23770c241b6fb500/server/src/main/java/org/apache/iotdb/db/engine/flush/MemTableFlushTask.java",
    "contents_url": "https://api.github.com/repos/phoenix-elite1050/iotdb/contents/server/src/main/java/org/apache/iotdb/db/engine/flush/MemTableFlushTask.java?ref=f94b072c31c88bea8500976b23770c241b6fb500",
    "patch": "@@ -19,21 +19,15 @@\n package org.apache.iotdb.db.engine.flush;\n \n import java.io.IOException;\n-import java.util.concurrent.LinkedBlockingQueue;\n import java.util.Map;\n import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.Future;\n-\n import org.apache.iotdb.db.conf.IoTDBConfig;\n import org.apache.iotdb.db.conf.IoTDBDescriptor;\n-import org.apache.iotdb.db.engine.flush.pool.FlushSubTaskPoolManager;\n import org.apache.iotdb.db.engine.memtable.IMemTable;\n import org.apache.iotdb.db.engine.memtable.IWritableMemChunk;\n-import org.apache.iotdb.db.exception.runtime.FlushRunTimeException;\n-import org.apache.iotdb.db.utils.datastructure.TVList;\n import org.apache.iotdb.db.rescon.SystemInfo;\n+import org.apache.iotdb.db.utils.datastructure.TVList;\n import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n-import org.apache.iotdb.tsfile.utils.Pair;\n import org.apache.iotdb.tsfile.write.chunk.ChunkWriterImpl;\n import org.apache.iotdb.tsfile.write.chunk.IChunkWriter;\n import org.apache.iotdb.tsfile.write.schema.MeasurementSchema;\n@@ -44,25 +38,12 @@\n public class MemTableFlushTask {\n \n   private static final Logger LOGGER = LoggerFactory.getLogger(MemTableFlushTask.class);\n-  private static final FlushSubTaskPoolManager SUB_TASK_POOL_MANAGER = FlushSubTaskPoolManager\n-      .getInstance();\n-  private static IoTDBConfig config = IoTDBDescriptor.getInstance().getConfig();\n-  private final Future<?> encodingTaskFuture;\n-  private final Future<?> ioTaskFuture;\n-  private RestorableTsFileIOWriter writer;\n-\n-  private final LinkedBlockingQueue<Object> encodingTaskQueue = new LinkedBlockingQueue<>();\n-  private final LinkedBlockingQueue<Object> ioTaskQueue = (config.isEnableMemControl()\n-      && SystemInfo.getInstance().isEncodingFasterThanIo())\n-          ? new LinkedBlockingQueue<>(config.getIoTaskQueueSizeForFlushing())\n-          : new LinkedBlockingQueue<>();\n-\n-  private String storageGroup;\n+  private static final IoTDBConfig config = IoTDBDescriptor.getInstance().getConfig();\n+  private final RestorableTsFileIOWriter writer;\n \n-  private IMemTable memTable;\n+  private final String storageGroup;\n \n-  private volatile long memSerializeTime = 0L;\n-  private volatile long ioTime = 0L;\n+  private final IMemTable memTable;\n \n   /**\n    * @param memTable the memTable to flush\n@@ -74,17 +55,14 @@ public MemTableFlushTask(IMemTable memTable, RestorableTsFileIOWriter writer, St\n     this.memTable = memTable;\n     this.writer = writer;\n     this.storageGroup = storageGroup;\n-    this.encodingTaskFuture = SUB_TASK_POOL_MANAGER.submit(encodingTask);\n-    this.ioTaskFuture = SUB_TASK_POOL_MANAGER.submit(ioTask);\n     LOGGER.debug(\"flush task of Storage group {} memtable is created, flushing to file {}.\",\n               storageGroup, writer.getFile().getName());\n   }\n \n   /**\n    * the function for flushing memtable.\n    */\n-  public void syncFlushMemTable()\n-      throws ExecutionException, InterruptedException {\n+  public void syncFlushMemTable() throws ExecutionException, IOException {\n     LOGGER.info(\"The memTable size of SG {} is {}, the avg series points num in chunk is {} \",\n         storageGroup,\n         memTable.memSize(),\n@@ -98,36 +76,49 @@ public void syncFlushMemTable()\n     }\n     long start = System.currentTimeMillis();\n     long sortTime = 0;\n+    long encodingTime = 0;\n+    long ioTime = 0;\n \n     //for map do not use get(key) to iteratate\n     for (Map.Entry<String, Map<String, IWritableMemChunk>> memTableEntry : memTable.getMemTableMap().entrySet()) {\n-      encodingTaskQueue.put(new StartFlushGroupIOTask(memTableEntry.getKey()));\n+      this.writer.startChunkGroup(memTableEntry.getKey());\n \n       final Map<String, IWritableMemChunk> value = memTableEntry.getValue();\n       for (Map.Entry<String, IWritableMemChunk> iWritableMemChunkEntry : value.entrySet()) {\n         long startTime = System.currentTimeMillis();\n         IWritableMemChunk series = iWritableMemChunkEntry.getValue();\n         MeasurementSchema desc = series.getSchema();\n         TVList tvList = series.getSortedTVListForFlush();\n-        sortTime += System.currentTimeMillis() - startTime;\n-        encodingTaskQueue.put(new Pair<>(tvList, desc));\n+        long encodingStartTime = System.currentTimeMillis();\n+        sortTime += encodingStartTime - startTime;\n+        IChunkWriter seriesWriter = new ChunkWriterImpl(desc);\n+        writeOneSeries(tvList, seriesWriter, desc.getType());\n+        seriesWriter.sealCurrentPage();\n+        seriesWriter.clearPageWriter();\n+        long ioStartTime = System.currentTimeMillis();\n+        encodingTime += ioStartTime - encodingStartTime;\n+        seriesWriter.writeToFileWriter(this.writer);\n+        ioTime += System.currentTimeMillis() - ioStartTime;\n       }\n-\n-      encodingTaskQueue.put(new EndChunkGroupIoTask());\n+      long ioStartTime = System.currentTimeMillis();\n+      this.writer.setMinPlanIndex(memTable.getMinPlanIndex());\n+      this.writer.setMaxPlanIndex(memTable.getMaxPlanIndex());\n+      this.writer.endChunkGroup();\n+      ioTime += System.currentTimeMillis() - ioStartTime;\n     }\n-    encodingTaskQueue.put(new TaskEnd());\n-    LOGGER.debug(\n+\n+    LOGGER.info(\n         \"Storage group {} memtable flushing into file {}: data sort time cost {} ms.\",\n         storageGroup, writer.getFile().getName(), sortTime);\n \n-    try {\n-      encodingTaskFuture.get();\n-    } catch (InterruptedException | ExecutionException e) {\n-      ioTaskFuture.cancel(true);\n-      throw e;\n-    }\n+    LOGGER.info(\n+        \"Storage group {} memtable flushing into file {}: data encoding time cost {} ms.\",\n+        storageGroup, writer.getFile().getName(), encodingTime);\n+\n+    LOGGER.info(\n+        \"Storage group {} memtable flushing into file {}: disk io time cost {} ms.\",\n+        storageGroup, writer.getFile().getName(), ioTime);\n \n-    ioTaskFuture.get();\n \n     try {\n       writer.writePlanIndices();\n@@ -139,165 +130,48 @@ public void syncFlushMemTable()\n       if (estimatedTemporaryMemSize != 0) {\n         SystemInfo.getInstance().releaseTemporaryMemoryForFlushing(estimatedTemporaryMemSize);\n       }\n-      SystemInfo.getInstance().setEncodingFasterThanIo(ioTime >= memSerializeTime);\n+      SystemInfo.getInstance().setEncodingFasterThanIo(ioTime >= encodingTime);\n     }\n \n     LOGGER.info(\n         \"Storage group {} memtable {} flushing a memtable has finished! Time consumption: {}ms\",\n         storageGroup, memTable, System.currentTimeMillis() - start);\n   }\n \n-  private Runnable encodingTask = new Runnable() {\n-    private void writeOneSeries(TVList tvPairs, IChunkWriter seriesWriterImpl,\n-        TSDataType dataType) {\n-      for (int i = 0; i < tvPairs.size(); i++) {\n-        long time = tvPairs.getTime(i);\n+  private void writeOneSeries(TVList tvPairs, IChunkWriter seriesWriterImpl,\n+      TSDataType dataType) {\n+    for (int i = 0; i < tvPairs.size(); i++) {\n+      long time = tvPairs.getTime(i);\n \n-        // skip duplicated data\n-        if ((i + 1 < tvPairs.size() && (time == tvPairs.getTime(i + 1)))) {\n-          continue;\n-        }\n-\n-        switch (dataType) {\n-          case BOOLEAN:\n-            seriesWriterImpl.write(time, tvPairs.getBoolean(i));\n-            break;\n-          case INT32:\n-            seriesWriterImpl.write(time, tvPairs.getInt(i));\n-            break;\n-          case INT64:\n-            seriesWriterImpl.write(time, tvPairs.getLong(i));\n-            break;\n-          case FLOAT:\n-            seriesWriterImpl.write(time, tvPairs.getFloat(i));\n-            break;\n-          case DOUBLE:\n-            seriesWriterImpl.write(time, tvPairs.getDouble(i));\n-            break;\n-          case TEXT:\n-            seriesWriterImpl.write(time, tvPairs.getBinary(i));\n-            break;\n-          default:\n-            LOGGER.error(\"Storage group {} does not support data type: {}\", storageGroup,\n-                dataType);\n-            break;\n-        }\n+      // skip duplicated data\n+      if ((i + 1 < tvPairs.size() && (time == tvPairs.getTime(i + 1)))) {\n+        continue;\n       }\n-    }\n \n-    @SuppressWarnings(\"squid:S135\")\n-    @Override\n-    public void run() {\n-      LOGGER.debug(\"Storage group {} memtable flushing to file {} starts to encoding data.\",\n-          storageGroup, writer.getFile().getName());\n-      while (true) {\n-\n-        Object task = null;\n-        try {\n-          task = encodingTaskQueue.take();\n-        } catch (InterruptedException e1) {\n-          LOGGER.error(\"Take task into ioTaskQueue Interrupted\");\n-          Thread.currentThread().interrupt();\n+      switch (dataType) {\n+        case BOOLEAN:\n+          seriesWriterImpl.write(time, tvPairs.getBoolean(i));\n           break;\n-        }\n-        if (task instanceof StartFlushGroupIOTask || task instanceof EndChunkGroupIoTask) {\n-          try {\n-            ioTaskQueue.put(task);\n-          } catch (@SuppressWarnings(\"squid:S2142\") InterruptedException e) {\n-            LOGGER.error(\"Storage group {} memtable flushing to file {}, encoding task is interrupted.\",\n-                storageGroup, writer.getFile().getName(), e);\n-            // generally it is because the thread pool is shutdown so the task should be aborted\n-            break;\n-          }\n-        } else if (task instanceof TaskEnd) {\n+        case INT32:\n+          seriesWriterImpl.write(time, tvPairs.getInt(i));\n           break;\n-        } else {\n-          long starTime = System.currentTimeMillis();\n-          Pair<TVList, MeasurementSchema> encodingMessage = (Pair<TVList, MeasurementSchema>) task;\n-          IChunkWriter seriesWriter = new ChunkWriterImpl(encodingMessage.right);\n-          writeOneSeries(encodingMessage.left, seriesWriter, encodingMessage.right.getType());\n-          seriesWriter.sealCurrentPage();\n-          seriesWriter.clearPageWriter();\n-          try {\n-            ioTaskQueue.put(seriesWriter);\n-          } catch (InterruptedException e) {\n-            LOGGER.error(\"Put task into ioTaskQueue Interrupted\");\n-            Thread.currentThread().interrupt();\n-          }\n-          memSerializeTime += System.currentTimeMillis() - starTime;\n-        }\n-      }\n-      try {\n-        ioTaskQueue.put(new TaskEnd());\n-      } catch (InterruptedException e) {\n-        LOGGER.error(\"Put task into ioTaskQueue Interrupted\");\n-        Thread.currentThread().interrupt();\n-      }\n-      \n-      LOGGER.debug(\"Storage group {}, flushing memtable {} into disk: Encoding data cost \"\n-          + \"{} ms.\",\n-          storageGroup, writer.getFile().getName(), memSerializeTime);\n-    }\n-  };\n-\n-  @SuppressWarnings(\"squid:S135\")\n-  private Runnable ioTask = () -> {\n-    LOGGER.debug(\"Storage group {} memtable flushing to file {} start io.\",\n-        storageGroup, writer.getFile().getName());\n-    while (true) {\n-      Object ioMessage = null;\n-      try {\n-        ioMessage = ioTaskQueue.take();\n-      } catch (InterruptedException e1) {\n-        LOGGER.error(\"take task from ioTaskQueue Interrupted\");\n-        Thread.currentThread().interrupt();\n-        break;\n-      }\n-      long starTime = System.currentTimeMillis();\n-      try {\n-        if (ioMessage instanceof StartFlushGroupIOTask) {\n-          this.writer.startChunkGroup(((StartFlushGroupIOTask) ioMessage).deviceId);\n-        } else if (ioMessage instanceof TaskEnd) {\n+        case INT64:\n+          seriesWriterImpl.write(time, tvPairs.getLong(i));\n+          break;\n+        case FLOAT:\n+          seriesWriterImpl.write(time, tvPairs.getFloat(i));\n+          break;\n+        case DOUBLE:\n+          seriesWriterImpl.write(time, tvPairs.getDouble(i));\n+          break;\n+        case TEXT:\n+          seriesWriterImpl.write(time, tvPairs.getBinary(i));\n+          break;\n+        default:\n+          LOGGER.error(\"Storage group {} does not support data type: {}\", storageGroup,\n+              dataType);\n           break;\n-        } else if (ioMessage instanceof IChunkWriter) {\n-          ChunkWriterImpl chunkWriter = (ChunkWriterImpl) ioMessage;\n-          chunkWriter.writeToFileWriter(this.writer);\n-        } else {\n-          this.writer.setMinPlanIndex(memTable.getMinPlanIndex());\n-          this.writer.setMaxPlanIndex(memTable.getMaxPlanIndex());\n-          this.writer.endChunkGroup();\n-        }\n-      } catch (IOException e) {\n-        LOGGER.error(\"Storage group {} memtable {}, io task meets error.\", storageGroup,\n-            memTable, e);\n-        throw new FlushRunTimeException(e);\n       }\n-      ioTime += System.currentTimeMillis() - starTime;\n-    }\n-    LOGGER.debug(\"flushing a memtable to file {} in storage group {}, io cost {}ms\",\n-            writer.getFile().getName(), storageGroup, ioTime);\n-  };\n-\n-  static class TaskEnd {\n-\n-    TaskEnd() {\n-\n-    }\n-  }\n-\n-  static class EndChunkGroupIoTask {\n-\n-    EndChunkGroupIoTask() {\n-\n-    }\n-  }\n-\n-  static class StartFlushGroupIOTask {\n-\n-    private final String deviceId;\n-\n-    StartFlushGroupIOTask(String deviceId) {\n-      this.deviceId = deviceId;\n     }\n   }\n }"
  },
  {
    "sha": "e46ddd167543bcbd77b8c07c24a2156cde70872d",
    "filename": "server/src/main/java/org/apache/iotdb/db/writelog/recover/TsFileRecoverPerformer.java",
    "status": "modified",
    "additions": 0,
    "deletions": 3,
    "changes": 3,
    "blob_url": "https://github.com/phoenix-elite1050/iotdb/blob/f94b072c31c88bea8500976b23770c241b6fb500/server/src/main/java/org/apache/iotdb/db/writelog/recover/TsFileRecoverPerformer.java",
    "raw_url": "https://github.com/phoenix-elite1050/iotdb/raw/f94b072c31c88bea8500976b23770c241b6fb500/server/src/main/java/org/apache/iotdb/db/writelog/recover/TsFileRecoverPerformer.java",
    "contents_url": "https://api.github.com/repos/phoenix-elite1050/iotdb/contents/server/src/main/java/org/apache/iotdb/db/writelog/recover/TsFileRecoverPerformer.java?ref=f94b072c31c88bea8500976b23770c241b6fb500",
    "patch": "@@ -222,9 +222,6 @@ private void redoLogs(RestorableTsFileIOWriter restorableTsFileIOWriter, Supplie\n       // into it\n     } catch (IOException | ExecutionException e) {\n       throw new StorageGroupProcessorException(e);\n-    } catch (InterruptedException e) {\n-      Thread.currentThread().interrupt();\n-      throw new StorageGroupProcessorException(e);\n     }\n   }\n "
  },
  {
    "sha": "de4c9851d69511e15f442aabac9e7d1ab8dbf483",
    "filename": "tsfile/src/main/java/org/apache/iotdb/tsfile/utils/PublicBAOS.java",
    "status": "modified",
    "additions": 33,
    "deletions": 0,
    "changes": 33,
    "blob_url": "https://github.com/phoenix-elite1050/iotdb/blob/f94b072c31c88bea8500976b23770c241b6fb500/tsfile/src/main/java/org/apache/iotdb/tsfile/utils/PublicBAOS.java",
    "raw_url": "https://github.com/phoenix-elite1050/iotdb/raw/f94b072c31c88bea8500976b23770c241b6fb500/tsfile/src/main/java/org/apache/iotdb/tsfile/utils/PublicBAOS.java",
    "contents_url": "https://api.github.com/repos/phoenix-elite1050/iotdb/contents/tsfile/src/main/java/org/apache/iotdb/tsfile/utils/PublicBAOS.java?ref=f94b072c31c88bea8500976b23770c241b6fb500",
    "patch": "@@ -19,6 +19,8 @@\n package org.apache.iotdb.tsfile.utils;\n \n import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n \n /**\n  * A subclass extending <code>ByteArrayOutputStream</code>. It's used to return\n@@ -46,4 +48,35 @@ public PublicBAOS(int size) {\n     return this.buf;\n   }\n \n+  /**\n+   * It's not a thread-safe method.\n+   * Override the super class's implementation.\n+   * Remove the synchronized key word, to save the synchronization overhead.\n+   *\n+   * Writes the complete contents of this byte array output stream to\n+   * the specified output stream argument, as if by calling the output\n+   * stream's write method using <code>out.write(buf, 0, count)</code>.\n+   *\n+   * @param      out   the output stream to which to write the data.\n+   * @exception  IOException  if an I/O error occurs.\n+   */\n+  @Override\n+  public void writeTo(OutputStream out) throws IOException {\n+    out.write(buf, 0, count);\n+  }\n+\n+  /**\n+   * It's not a thread-safe method.\n+   * Override the super class's implementation.\n+   * Remove the synchronized key word, to save the synchronization overhead.\n+   *\n+   * Resets the <code>count</code> field of this byte array output\n+   * stream to zero, so that all currently accumulated output in the\n+   * output stream is discarded. The output stream can be used again,\n+   * reusing the already allocated buffer space.\n+   */\n+  @Override\n+  public void reset() {\n+    count = 0;\n+  }\n }"
  },
  {
    "sha": "a1968c8228a53ab0eab932837ede589cff3e2a2c",
    "filename": "tsfile/src/main/java/org/apache/iotdb/tsfile/write/chunk/ChunkWriterImpl.java",
    "status": "modified",
    "additions": 10,
    "deletions": 4,
    "changes": 14,
    "blob_url": "https://github.com/phoenix-elite1050/iotdb/blob/f94b072c31c88bea8500976b23770c241b6fb500/tsfile/src/main/java/org/apache/iotdb/tsfile/write/chunk/ChunkWriterImpl.java",
    "raw_url": "https://github.com/phoenix-elite1050/iotdb/raw/f94b072c31c88bea8500976b23770c241b6fb500/tsfile/src/main/java/org/apache/iotdb/tsfile/write/chunk/ChunkWriterImpl.java",
    "contents_url": "https://api.github.com/repos/phoenix-elite1050/iotdb/contents/tsfile/src/main/java/org/apache/iotdb/tsfile/write/chunk/ChunkWriterImpl.java?ref=f94b072c31c88bea8500976b23770c241b6fb500",
    "patch": "@@ -52,6 +52,11 @@\n    */\n   private PublicBAOS pageBuffer;\n \n+  /**\n+   * current chunk data size, i.e the size of pageBuffer\n+   */\n+  private int chunkDataSize;\n+\n   private int numOfPages;\n \n   /**\n@@ -351,6 +356,7 @@ public void sealCurrentPage() {\n     if (pageWriter != null && pageWriter.getPointNumber() > 0) {\n       writePageToPageBuffer();\n     }\n+    chunkDataSize = pageBuffer.size();\n   }\n   \n   public void clearPageWriter() {\n@@ -418,18 +424,18 @@ public void writeAllPagesOfChunkToTsFile(TsFileIOWriter writer, Statistics<?> st\n \n     // start to write this column chunk\n     writer.startFlushChunk(measurementSchema, compressor.getType(), measurementSchema.getType(),\n-        measurementSchema.getEncodingType(), statistics, pageBuffer.size(), numOfPages);\n+        measurementSchema.getEncodingType(), statistics, chunkDataSize, numOfPages);\n \n     long dataOffset = writer.getPos();\n \n     // write all pages of this column\n     writer.writeBytesToStream(pageBuffer);\n \n-    long dataSize = writer.getPos() - dataOffset;\n-    if (dataSize != pageBuffer.size()) {\n+    int dataSize = (int) (writer.getPos() - dataOffset);\n+    if (dataSize != chunkDataSize) {\n       throw new IOException(\n           \"Bytes written is inconsistent with the size of data: \" + dataSize + \" !=\"\n-              + \" \" + pageBuffer.size());\n+              + \" \" + chunkDataSize);\n     }\n \n     writer.endCurrentChunk();"
  }
]
