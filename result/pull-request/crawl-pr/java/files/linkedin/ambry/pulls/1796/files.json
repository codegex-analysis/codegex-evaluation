[
  {
    "sha": "9a463bc840594a4c8e90cc4b4b7430618da3e489",
    "filename": "ambry-api/src/main/java/com/github/ambry/clustermap/VirtualReplicatorCluster.java",
    "status": "modified",
    "additions": 1,
    "deletions": 0,
    "changes": 1,
    "blob_url": "https://github.com/linkedin/ambry/blob/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-api/src/main/java/com/github/ambry/clustermap/VirtualReplicatorCluster.java",
    "raw_url": "https://github.com/linkedin/ambry/raw/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-api/src/main/java/com/github/ambry/clustermap/VirtualReplicatorCluster.java",
    "contents_url": "https://api.github.com/repos/linkedin/ambry/contents/ambry-api/src/main/java/com/github/ambry/clustermap/VirtualReplicatorCluster.java?ref=02d7236e4257ab44463ff42c6269d07713b051bd",
    "patch": "@@ -22,6 +22,7 @@\n  * In Virtual Replicator Cluster, {@link PartitionId}s are resources and they are assigned to participant node.\n  */\n public interface VirtualReplicatorCluster extends AutoCloseable {\n+  String Cloud_Replica_Keyword = \"vcr\";\n \n   /**\n    * Gets all nodes in the cluster."
  },
  {
    "sha": "60e5c594a786a63f2cab0ca8297f5244d28a735c",
    "filename": "ambry-api/src/main/java/com/github/ambry/config/StoreConfig.java",
    "status": "modified",
    "additions": 4,
    "deletions": 4,
    "changes": 8,
    "blob_url": "https://github.com/linkedin/ambry/blob/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-api/src/main/java/com/github/ambry/config/StoreConfig.java",
    "raw_url": "https://github.com/linkedin/ambry/raw/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-api/src/main/java/com/github/ambry/config/StoreConfig.java",
    "contents_url": "https://api.github.com/repos/linkedin/ambry/contents/ambry-api/src/main/java/com/github/ambry/config/StoreConfig.java?ref=02d7236e4257ab44463ff42c6269d07713b051bd",
    "patch": "@@ -134,9 +134,9 @@\n   /**\n    * Whether to purge expired delete tombstone in compaction.\n    */\n-  @Config(\"store.compaction.purge.expired.delete.tombstone\")\n+  @Config(\"store.compaction.purge.delete.tombstone\")\n   @Default(\"false\")\n-  public final boolean storeCompactionPurgeExpiredDeleteTombstone;\n+  public final boolean storeCompactionPurgeDeleteTombstone;\n \n   /**\n    * The minimum buffer size for compaction copy phase.\n@@ -403,8 +403,8 @@ public StoreConfig(VerifiableProperties verifiableProperties) {\n     storeCompactionThrottlerCheckIntervalMs =\n         verifiableProperties.getIntInRange(\"store.compaction.throttler.check.interval.ms\", -1, -1, Integer.MAX_VALUE);\n     storeCompactionEnableDirectIO = verifiableProperties.getBoolean(\"store.compaction.enable.direct.io\", false);\n-    storeCompactionPurgeExpiredDeleteTombstone =\n-        verifiableProperties.getBoolean(\"store.compaction.purge.expired.delete.tombstone\", false);\n+    storeCompactionPurgeDeleteTombstone =\n+        verifiableProperties.getBoolean(\"store.compaction.purge.delete.tombstone\", false);\n     storeCompactionMinBufferSize =\n         verifiableProperties.getIntInRange(\"store.compaction.min.buffer.size\", 10 * 1024 * 1024, 0, Integer.MAX_VALUE);\n     storeCompactionFilter ="
  },
  {
    "sha": "b4ea419152bb81db492b06118f701f90ef7ca066",
    "filename": "ambry-api/src/main/java/com/github/ambry/store/Store.java",
    "status": "modified",
    "additions": 4,
    "deletions": 1,
    "changes": 5,
    "blob_url": "https://github.com/linkedin/ambry/blob/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-api/src/main/java/com/github/ambry/store/Store.java",
    "raw_url": "https://github.com/linkedin/ambry/raw/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-api/src/main/java/com/github/ambry/store/Store.java",
    "contents_url": "https://api.github.com/repos/linkedin/ambry/contents/ambry-api/src/main/java/com/github/ambry/store/Store.java?ref=02d7236e4257ab44463ff42c6269d07713b051bd",
    "patch": "@@ -81,10 +81,13 @@\n    * @param token The token that acts as a bookmark to make subsequent searches\n    * @param maxTotalSizeOfEntries The maximum total size of entries that needs to be returned. The api will try to\n    *                              return a list of entries whose total size is close to this value.\n+   * @param hostname HostName of the datanode where the token belongs to.\n+   * @param remoteReplicaPath The path of remote replica.\n    * @return The FindInfo instance that contains the entries found and the new token for future searches\n    * @throws StoreException\n    */\n-  FindInfo findEntriesSince(FindToken token, long maxTotalSizeOfEntries) throws StoreException;\n+  FindInfo findEntriesSince(FindToken token, long maxTotalSizeOfEntries, String hostname, String remoteReplicaPath)\n+      throws StoreException;\n \n   /**\n    * Finds all the keys that are not present in the store from the input keys"
  },
  {
    "sha": "f7898a586975ec40b30ad77f6928ca753c291dee",
    "filename": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudBlobStore.java",
    "status": "modified",
    "additions": 2,
    "deletions": 1,
    "changes": 3,
    "blob_url": "https://github.com/linkedin/ambry/blob/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-cloud/src/main/java/com/github/ambry/cloud/CloudBlobStore.java",
    "raw_url": "https://github.com/linkedin/ambry/raw/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-cloud/src/main/java/com/github/ambry/cloud/CloudBlobStore.java",
    "contents_url": "https://api.github.com/repos/linkedin/ambry/contents/ambry-cloud/src/main/java/com/github/ambry/cloud/CloudBlobStore.java?ref=02d7236e4257ab44463ff42c6269d07713b051bd",
    "patch": "@@ -758,7 +758,8 @@ void removeFromCache(String blobKey) {\n   }\n \n   @Override\n-  public FindInfo findEntriesSince(FindToken token, long maxTotalSizeOfEntries) throws StoreException {\n+  public FindInfo findEntriesSince(FindToken token, long maxTotalSizeOfEntries, String hostname,\n+      String remoteReplicaPath) throws StoreException {\n     try {\n       FindResult findResult = requestAgent.doWithRetries(\n           () -> cloudDestination.findEntriesSince(partitionId.toPathString(), token, maxTotalSizeOfEntries),"
  },
  {
    "sha": "8c3bcac2fa19c515db2d104d77b04777336a2d46",
    "filename": "ambry-cloud/src/test/java/com/github/ambry/cloud/CloudBlobStoreTest.java",
    "status": "modified",
    "additions": 4,
    "deletions": 3,
    "changes": 7,
    "blob_url": "https://github.com/linkedin/ambry/blob/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-cloud/src/test/java/com/github/ambry/cloud/CloudBlobStoreTest.java",
    "raw_url": "https://github.com/linkedin/ambry/raw/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-cloud/src/test/java/com/github/ambry/cloud/CloudBlobStoreTest.java",
    "contents_url": "https://api.github.com/repos/linkedin/ambry/contents/ambry-cloud/src/test/java/com/github/ambry/cloud/CloudBlobStoreTest.java?ref=02d7236e4257ab44463ff42c6269d07713b051bd",
    "patch": "@@ -536,7 +536,8 @@ public void testFindEntriesSince() throws Exception {\n     when(dest.findEntriesSince(anyString(), any(CosmosChangeFeedFindToken.class), anyLong())).thenReturn(\n         new FindResult(Collections.emptyList(), cosmosChangeFeedFindToken));\n     CosmosChangeFeedFindToken startToken = new CosmosChangeFeedFindToken();\n-    FindInfo findInfo = store.findEntriesSince(startToken, maxTotalSize);\n+    // remote node host name and replica path are not really used by cloud store, it's fine to keep them null\n+    FindInfo findInfo = store.findEntriesSince(startToken, maxTotalSize, null, null);\n     CosmosChangeFeedFindToken outputToken = (CosmosChangeFeedFindToken) findInfo.getFindToken();\n     assertEquals(blobSize * numBlobsFound, outputToken.getBytesRead());\n     assertEquals(numBlobsFound, outputToken.getTotalItems());\n@@ -548,7 +549,7 @@ public void testFindEntriesSince() throws Exception {\n             UUID.randomUUID().toString());\n     when(dest.findEntriesSince(anyString(), any(CosmosChangeFeedFindToken.class), anyLong())).thenReturn(\n         new FindResult(Collections.emptyList(), cosmosChangeFeedFindToken));\n-    findInfo = store.findEntriesSince(outputToken, maxTotalSize);\n+    findInfo = store.findEntriesSince(outputToken, maxTotalSize, null, null);\n     outputToken = (CosmosChangeFeedFindToken) findInfo.getFindToken();\n     assertEquals(blobSize * 2 * numBlobsFound, outputToken.getBytesRead());\n     assertEquals(numBlobsFound, outputToken.getTotalItems());\n@@ -557,7 +558,7 @@ public void testFindEntriesSince() throws Exception {\n     // 3) call find with new token, no more data, verify token unchanged\n     when(dest.findEntriesSince(anyString(), any(CosmosChangeFeedFindToken.class), anyLong())).thenReturn(\n         new FindResult(Collections.emptyList(), outputToken));\n-    findInfo = store.findEntriesSince(outputToken, maxTotalSize);\n+    findInfo = store.findEntriesSince(outputToken, maxTotalSize, null, null);\n     assertTrue(findInfo.getMessageEntries().isEmpty());\n     FindToken finalToken = findInfo.getFindToken();\n     assertEquals(outputToken, finalToken);"
  },
  {
    "sha": "8dd6dd62e750b6836d040485cc7370296b255c0a",
    "filename": "ambry-clustermap/src/main/java/com/github/ambry/clustermap/CloudReplica.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/linkedin/ambry/blob/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-clustermap/src/main/java/com/github/ambry/clustermap/CloudReplica.java",
    "raw_url": "https://github.com/linkedin/ambry/raw/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-clustermap/src/main/java/com/github/ambry/clustermap/CloudReplica.java",
    "contents_url": "https://api.github.com/repos/linkedin/ambry/contents/ambry-clustermap/src/main/java/com/github/ambry/clustermap/CloudReplica.java?ref=02d7236e4257ab44463ff42c6269d07713b051bd",
    "patch": "@@ -19,6 +19,7 @@\n import org.json.JSONObject;\n \n import static com.github.ambry.clustermap.ClusterMapSnapshotConstants.*;\n+import static com.github.ambry.clustermap.VirtualReplicatorCluster.*;\n \n \n /**\n@@ -27,7 +28,6 @@\n public class CloudReplica implements ReplicaId {\n   private final PartitionId partitionId;\n   private final DataNodeId dataNodeId;\n-  public static final String Cloud_Replica_Keyword = \"vcr\";\n \n   /**\n    * Instantiate an CloudReplica instance."
  },
  {
    "sha": "2c02aa4be436f9dcb5ad7608c81b2669a7728abd",
    "filename": "ambry-clustermap/src/test/java/com/github/ambry/clustermap/CloudReplicaTest.java",
    "status": "modified",
    "additions": 2,
    "deletions": 1,
    "changes": 3,
    "blob_url": "https://github.com/linkedin/ambry/blob/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-clustermap/src/test/java/com/github/ambry/clustermap/CloudReplicaTest.java",
    "raw_url": "https://github.com/linkedin/ambry/raw/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-clustermap/src/test/java/com/github/ambry/clustermap/CloudReplicaTest.java",
    "contents_url": "https://api.github.com/repos/linkedin/ambry/contents/ambry-clustermap/src/test/java/com/github/ambry/clustermap/CloudReplicaTest.java?ref=02d7236e4257ab44463ff42c6269d07713b051bd",
    "patch": "@@ -20,6 +20,7 @@\n import java.util.Properties;\n import org.junit.Test;\n \n+import static com.github.ambry.clustermap.VirtualReplicatorCluster.*;\n import static org.junit.Assert.*;\n \n \n@@ -55,7 +56,7 @@ public void basicTest() {\n     CloudReplica cloudReplica = new CloudReplica(new MockPartitionId(), cloudDataNode);\n     assertEquals(\"Wrong mount path\", mockPartitionId.toPathString(), cloudReplica.getMountPath());\n     assertEquals(\"Wrong replica path\",\n-        CloudReplica.Cloud_Replica_Keyword + File.separator + mockPartitionId.toPathString() + File.separator\n+        Cloud_Replica_Keyword + File.separator + mockPartitionId.toPathString() + File.separator\n             + mockPartitionId.toPathString(), cloudReplica.getReplicaPath());\n     assertEquals(\"Wrong dataNodeId\", cloudDataNode, cloudReplica.getDataNodeId());\n     assertEquals(\"Wrong partitionId\", mockPartitionId, cloudReplica.getPartitionId());"
  },
  {
    "sha": "c1fabce3a073a6562447b546db32d86efee1044d",
    "filename": "ambry-protocol/src/main/java/com/github/ambry/protocol/AmbryRequests.java",
    "status": "modified",
    "additions": 2,
    "deletions": 1,
    "changes": 3,
    "blob_url": "https://github.com/linkedin/ambry/blob/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-protocol/src/main/java/com/github/ambry/protocol/AmbryRequests.java",
    "raw_url": "https://github.com/linkedin/ambry/raw/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-protocol/src/main/java/com/github/ambry/protocol/AmbryRequests.java",
    "contents_url": "https://api.github.com/repos/linkedin/ambry/contents/ambry-protocol/src/main/java/com/github/ambry/protocol/AmbryRequests.java?ref=02d7236e4257ab44463ff42c6269d07713b051bd",
    "patch": "@@ -576,7 +576,8 @@ public void handleReplicaMetadataRequest(NetworkRequest request) throws IOExcept\n \n             partitionStartTimeInMs = SystemTime.getInstance().milliseconds();\n             FindInfo findInfo =\n-                store.findEntriesSince(findToken, replicaMetadataRequest.getMaxTotalSizeOfEntriesInBytes());\n+                store.findEntriesSince(findToken, replicaMetadataRequest.getMaxTotalSizeOfEntriesInBytes(), hostName,\n+                    replicaPath);\n             logger.trace(\"{} Time used to find entry since: {}\", partitionId,\n                 (SystemTime.getInstance().milliseconds() - partitionStartTimeInMs));\n "
  },
  {
    "sha": "a312c3884f38d64fefb00b8a4bcc32956f10e509",
    "filename": "ambry-protocol/src/main/java/com/github/ambry/protocol/ReplicaMetadataRequestInfo.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/linkedin/ambry/blob/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-protocol/src/main/java/com/github/ambry/protocol/ReplicaMetadataRequestInfo.java",
    "raw_url": "https://github.com/linkedin/ambry/raw/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-protocol/src/main/java/com/github/ambry/protocol/ReplicaMetadataRequestInfo.java",
    "contents_url": "https://api.github.com/repos/linkedin/ambry/contents/ambry-protocol/src/main/java/com/github/ambry/protocol/ReplicaMetadataRequestInfo.java?ref=02d7236e4257ab44463ff42c6269d07713b051bd",
    "patch": "@@ -30,7 +30,7 @@\n \n \n /**\n- * Contains the token, hostname, replicapath for a local partition. This is used\n+ * Contains the token, hostname, replica path for a local partition. This is used\n  * by replica metadata request to specify token in a partition\n  */\n public class ReplicaMetadataRequestInfo {"
  },
  {
    "sha": "5ee41b62fec81f788fc4128bf90a4f6b98fef224",
    "filename": "ambry-replication/src/main/java/com/github/ambry/replication/ReplicationEngine.java",
    "status": "modified",
    "additions": 3,
    "deletions": 1,
    "changes": 4,
    "blob_url": "https://github.com/linkedin/ambry/blob/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-replication/src/main/java/com/github/ambry/replication/ReplicationEngine.java",
    "raw_url": "https://github.com/linkedin/ambry/raw/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-replication/src/main/java/com/github/ambry/replication/ReplicationEngine.java",
    "contents_url": "https://api.github.com/repos/linkedin/ambry/contents/ambry-replication/src/main/java/com/github/ambry/replication/ReplicationEngine.java?ref=02d7236e4257ab44463ff42c6269d07713b051bd",
    "patch": "@@ -56,6 +56,8 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import static com.github.ambry.clustermap.VirtualReplicatorCluster.*;\n+\n \n /**\n  * Abstract class to handle replication for given partitions. Responsible for the following\n@@ -231,7 +233,7 @@ protected RemoteReplicaInfo getRemoteReplicaInfo(PartitionId partitionId, String\n         break;\n       }\n     }\n-    if (foundRemoteReplicaInfo == null && !replicaPath.startsWith(CloudReplica.Cloud_Replica_Keyword)) {\n+    if (foundRemoteReplicaInfo == null && !replicaPath.startsWith(Cloud_Replica_Keyword)) {\n       replicationMetrics.unknownRemoteReplicaRequestCount.inc();\n       logger.error(\"ReplicaMetaDataRequest from unknown Replica {}, with path {}\", hostName, replicaPath);\n     }"
  },
  {
    "sha": "2bd0bed2e85d112ac21dedf38f75116f02079e4f",
    "filename": "ambry-replication/src/test/java/com/github/ambry/replication/CloudToStoreReplicationManagerTest.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/linkedin/ambry/blob/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-replication/src/test/java/com/github/ambry/replication/CloudToStoreReplicationManagerTest.java",
    "raw_url": "https://github.com/linkedin/ambry/raw/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-replication/src/test/java/com/github/ambry/replication/CloudToStoreReplicationManagerTest.java",
    "contents_url": "https://api.github.com/repos/linkedin/ambry/contents/ambry-replication/src/test/java/com/github/ambry/replication/CloudToStoreReplicationManagerTest.java?ref=02d7236e4257ab44463ff42c6269d07713b051bd",
    "patch": "@@ -55,9 +55,9 @@\n import org.junit.Test;\n import org.mockito.internal.util.reflection.FieldSetter;\n \n-import static com.github.ambry.clustermap.CloudReplica.*;\n import static com.github.ambry.clustermap.ClusterMapSnapshotConstants.*;\n import static com.github.ambry.clustermap.TestUtils.*;\n+import static com.github.ambry.clustermap.VirtualReplicatorCluster.*;\n import static org.junit.Assert.*;\n import static org.mockito.Mockito.*;\n "
  },
  {
    "sha": "18cf06fe043afd40a6cc1610d04144474a8ac4ad",
    "filename": "ambry-replication/src/test/java/com/github/ambry/replication/InMemoryStore.java",
    "status": "modified",
    "additions": 3,
    "deletions": 2,
    "changes": 5,
    "blob_url": "https://github.com/linkedin/ambry/blob/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-replication/src/test/java/com/github/ambry/replication/InMemoryStore.java",
    "raw_url": "https://github.com/linkedin/ambry/raw/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-replication/src/test/java/com/github/ambry/replication/InMemoryStore.java",
    "contents_url": "https://api.github.com/repos/linkedin/ambry/contents/ambry-replication/src/test/java/com/github/ambry/replication/InMemoryStore.java?ref=02d7236e4257ab44463ff42c6269d07713b051bd",
    "patch": "@@ -15,13 +15,13 @@\n \n import com.github.ambry.clustermap.PartitionId;\n import com.github.ambry.clustermap.ReplicaState;\n+import com.github.ambry.commons.Callback;\n import com.github.ambry.messageformat.DeleteMessageFormatInputStream;\n import com.github.ambry.messageformat.MessageFormatInputStream;\n import com.github.ambry.messageformat.MessageFormatWriteSet;\n import com.github.ambry.messageformat.TtlUpdateMessageFormatInputStream;\n import com.github.ambry.messageformat.UndeleteMessageFormatInputStream;\n import com.github.ambry.router.AsyncWritableChannel;\n-import com.github.ambry.commons.Callback;\n import com.github.ambry.store.FindInfo;\n import com.github.ambry.store.MessageInfo;\n import com.github.ambry.store.MessageReadSet;\n@@ -360,7 +360,8 @@ public short undelete(MessageInfo info) throws StoreException {\n   }\n \n   @Override\n-  public FindInfo findEntriesSince(FindToken token, long maxSizeOfEntries) throws StoreException {\n+  public FindInfo findEntriesSince(FindToken token, long maxSizeOfEntries, String hostname, String remoteReplicaPath)\n+      throws StoreException {\n     // unused function\n     MockFindToken mockToken = (MockFindToken) token;\n     List<MessageInfo> entriesToReturn = new ArrayList<>();"
  },
  {
    "sha": "d8a936f0a33a421e5640b29252f63d8c1f8fa354",
    "filename": "ambry-server/src/test/java/com/github/ambry/server/MockStorageManager.java",
    "status": "modified",
    "additions": 4,
    "deletions": 3,
    "changes": 7,
    "blob_url": "https://github.com/linkedin/ambry/blob/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-server/src/test/java/com/github/ambry/server/MockStorageManager.java",
    "raw_url": "https://github.com/linkedin/ambry/raw/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-server/src/test/java/com/github/ambry/server/MockStorageManager.java",
    "contents_url": "https://api.github.com/repos/linkedin/ambry/contents/ambry-server/src/test/java/com/github/ambry/server/MockStorageManager.java?ref=02d7236e4257ab44463ff42c6269d07713b051bd",
    "patch": "@@ -219,7 +219,7 @@ public short undelete(MessageInfo info) throws StoreException {\n       try {\n         MessageFormatInputStream stream =\n             new UndeleteMessageFormatInputStream(info.getStoreKey(), info.getAccountId(), info.getContainerId(),\n-                info.getOperationTimeMs(), (short) returnValueOfUndelete);\n+                info.getOperationTimeMs(), returnValueOfUndelete);\n         // Update info to add stream size;\n         info = new MessageInfo(info.getStoreKey(), stream.getSize(), false, false, true, Utils.Infinite_Time, null,\n             info.getAccountId(), info.getContainerId(), info.getOperationTimeMs(), returnValueOfUndelete);\n@@ -239,7 +239,8 @@ public short undelete(MessageInfo info) throws StoreException {\n     }\n \n     @Override\n-    public FindInfo findEntriesSince(FindToken token, long maxTotalSizeOfEntries) throws StoreException {\n+    public FindInfo findEntriesSince(FindToken token, long maxTotalSizeOfEntries, String hostname,\n+        String remoteReplicaPath) throws StoreException {\n       operationReceived = RequestOrResponseType.ReplicaMetadataRequest;\n       tokenReceived = token;\n       maxTotalSizeOfEntriesReceived = maxTotalSizeOfEntries;\n@@ -250,7 +251,7 @@ public FindInfo findEntriesSince(FindToken token, long maxTotalSizeOfEntries) th\n \n     @Override\n     public MessageInfo findKey(StoreKey key) throws StoreException {\n-      return new MessageInfo(key, 1, Utils.Infinite_Time, (short) 0, (short) 0, (long) 0);\n+      return new MessageInfo(key, 1, Utils.Infinite_Time, (short) 0, (short) 0, 0);\n     }\n \n     @Override"
  },
  {
    "sha": "269788f72091d4ee1826dffb167d7a31764c7b01",
    "filename": "ambry-server/src/test/java/com/github/ambry/server/StatsManagerTest.java",
    "status": "modified",
    "additions": 2,
    "deletions": 1,
    "changes": 3,
    "blob_url": "https://github.com/linkedin/ambry/blob/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-server/src/test/java/com/github/ambry/server/StatsManagerTest.java",
    "raw_url": "https://github.com/linkedin/ambry/raw/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-server/src/test/java/com/github/ambry/server/StatsManagerTest.java",
    "contents_url": "https://api.github.com/repos/linkedin/ambry/contents/ambry-server/src/test/java/com/github/ambry/server/StatsManagerTest.java?ref=02d7236e4257ab44463ff42c6269d07713b051bd",
    "patch": "@@ -760,7 +760,8 @@ public void updateTtl(List<MessageInfo> infos) throws StoreException {\n     }\n \n     @Override\n-    public FindInfo findEntriesSince(FindToken token, long maxTotalSizeOfEntries) throws StoreException {\n+    public FindInfo findEntriesSince(FindToken token, long maxTotalSizeOfEntries, String hostname,\n+        String remoteReplicaPath) throws StoreException {\n       throw new IllegalStateException(\"Not implemented\");\n     }\n "
  },
  {
    "sha": "9f368d8f00b165877c861143391fdbf618a10eca",
    "filename": "ambry-store/src/main/java/com/github/ambry/store/BlobStore.java",
    "status": "modified",
    "additions": 17,
    "deletions": 2,
    "changes": 19,
    "blob_url": "https://github.com/linkedin/ambry/blob/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-store/src/main/java/com/github/ambry/store/BlobStore.java",
    "raw_url": "https://github.com/linkedin/ambry/raw/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-store/src/main/java/com/github/ambry/store/BlobStore.java",
    "contents_url": "https://api.github.com/repos/linkedin/ambry/contents/ambry-store/src/main/java/com/github/ambry/store/BlobStore.java?ref=02d7236e4257ab44463ff42c6269d07713b051bd",
    "patch": "@@ -50,6 +50,8 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import static com.github.ambry.clustermap.VirtualReplicatorCluster.*;\n+\n \n /**\n  * The blob store that controls the log and index\n@@ -82,6 +84,7 @@\n   private final long thresholdBytesHigh;\n   private final long thresholdBytesLow;\n   private final long ttlUpdateBufferTimeMs;\n+  private final RemoteTokenTracker remoteTokenTracker;\n   private final AtomicInteger errorCount;\n   private final AccountService accountService;\n \n@@ -200,6 +203,7 @@ public BlobStore(ReplicaId replicaId, StoreConfig config, ScheduledExecutorServi\n     ttlUpdateBufferTimeMs = TimeUnit.SECONDS.toMillis(config.storeTtlUpdateBufferTimeSeconds);\n     errorCount = new AtomicInteger(0);\n     currentState = ReplicaState.OFFLINE;\n+    remoteTokenTracker = replicaId == null ? null : new RemoteTokenTracker(replicaId);\n     logger.debug(\n         \"The enable state of replicaStatusDelegate is {} on store {}. The high threshold is {} bytes and the low threshold is {} bytes\",\n         config.storeReplicaStatusDelegateEnable, storeId, this.thresholdBytesHigh, this.thresholdBytesLow);\n@@ -242,7 +246,7 @@ public void start() throws StoreException {\n         log = new Log(dataDir, capacityInBytes, diskSpaceAllocator, config, metrics);\n         compactor = new BlobStoreCompactor(dataDir, storeId, factory, config, metrics, storeUnderCompactionMetrics,\n             diskIOScheduler, diskSpaceAllocator, log, time, sessionId, storeDescriptor.getIncarnationId(),\n-            accountService);\n+            accountService, remoteTokenTracker);\n         index = new PersistentIndex(dataDir, storeId, taskScheduler, log, config, factory, recovery, hardDelete,\n             diskIOScheduler, metrics, time, sessionId, storeDescriptor.getIncarnationId());\n         compactor.initialize(index);\n@@ -892,10 +896,15 @@ private void maybeCallInDeleteBetweenGetEndOffsetAndFindKey() throws Exception {\n   }\n \n   @Override\n-  public FindInfo findEntriesSince(FindToken token, long maxTotalSizeOfEntries) throws StoreException {\n+  public FindInfo findEntriesSince(FindToken token, long maxTotalSizeOfEntries, String hostname,\n+      String remoteReplicaPath) throws StoreException {\n     checkStarted();\n     final Timer.Context context = metrics.findEntriesSinceResponse.time();\n     try {\n+      if (hostname != null && !hostname.startsWith(Cloud_Replica_Keyword) && remoteTokenTracker != null) {\n+        // only tokens from disk-backed replicas are tracked\n+        remoteTokenTracker.updateTokenFromPeerReplica(token, hostname, remoteReplicaPath);\n+      }\n       FindInfo findInfo = index.findEntriesSince(token, maxTotalSizeOfEntries);\n       onSuccess();\n       return findInfo;\n@@ -1210,6 +1219,9 @@ public boolean isStarted() {\n    */\n   void compact(CompactionDetails details, byte[] bundleReadBuffer) throws IOException, StoreException {\n     checkStarted();\n+    if (remoteTokenTracker != null) {\n+      remoteTokenTracker.refreshPeerReplicaTokens();\n+    }\n     compactor.compact(details, bundleReadBuffer);\n     checkCapacityAndUpdateReplicaStatusDelegate();\n     logger.info(\"One cycle of compaction is completed on the store {}\", storeId);\n@@ -1224,6 +1236,9 @@ void maybeResumeCompaction(byte[] bundleReadBuffer) throws StoreException {\n     checkStarted();\n     if (CompactionLog.isCompactionInProgress(dataDir, storeId)) {\n       logger.info(\"Resuming compaction of {}\", this);\n+      if(remoteTokenTracker != null) {\n+        remoteTokenTracker.refreshPeerReplicaTokens();\n+      }\n       compactor.resumeCompaction(bundleReadBuffer);\n       checkCapacityAndUpdateReplicaStatusDelegate();\n     }"
  },
  {
    "sha": "a5b9de3488cf0c9f2cab1a23c8b72c4976f223f2",
    "filename": "ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java",
    "status": "modified",
    "additions": 59,
    "deletions": 7,
    "changes": 66,
    "blob_url": "https://github.com/linkedin/ambry/blob/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java",
    "raw_url": "https://github.com/linkedin/ambry/raw/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java",
    "contents_url": "https://api.github.com/repos/linkedin/ambry/contents/ambry-store/src/main/java/com/github/ambry/store/BlobStoreCompactor.java?ref=02d7236e4257ab44463ff42c6269d07713b051bd",
    "patch": "@@ -17,6 +17,7 @@\n import com.github.ambry.account.AccountUtils;\n import com.github.ambry.account.Container;\n import com.github.ambry.config.StoreConfig;\n+import com.github.ambry.replication.FindToken;\n import com.github.ambry.utils.Pair;\n import com.github.ambry.utils.SystemTime;\n import com.github.ambry.utils.Time;\n@@ -88,6 +89,7 @@ public boolean accept(File dir, String name) {\n   private final IndexSegmentValidEntryFilter validEntryFilter;\n   private final AccountService accountService;\n   private final Set<Pair<Short, Short>> deprecatedContainers;\n+  private final RemoteTokenTracker remoteTokenTracker;\n   private final boolean useDirectIO;\n   private volatile boolean isActive = false;\n   private PersistentIndex srcIndex;\n@@ -113,13 +115,14 @@ public boolean accept(File dir, String name) {\n    * @param sessionId the sessionID of the store.\n    * @param incarnationId the incarnation ID of the store.\n    * @param accountService the {@link AccountService} instance to use.\n+   * @param remoteTokenTracker the {@link RemoteTokenTracker} that tracks tokens from all peer replicas.\n    * @throws IOException if the {@link CompactionLog} could not be created or if commit/cleanup failed during recovery.\n    * @throws StoreException if the commit failed during recovery.\n    */\n   BlobStoreCompactor(String dataDir, String storeId, StoreKeyFactory storeKeyFactory, StoreConfig config,\n       StoreMetrics srcMetrics, StoreMetrics tgtMetrics, DiskIOScheduler diskIOScheduler,\n       DiskSpaceAllocator diskSpaceAllocator, Log srcLog, Time time, UUID sessionId, UUID incarnationId,\n-      AccountService accountService) throws IOException, StoreException {\n+      AccountService accountService, RemoteTokenTracker remoteTokenTracker) throws IOException, StoreException {\n     this.dataDir = new File(dataDir);\n     this.storeId = storeId;\n     this.storeKeyFactory = storeKeyFactory;\n@@ -135,6 +138,7 @@ public boolean accept(File dir, String name) {\n     this.incarnationId = incarnationId;\n     this.useDirectIO = Utils.isLinux() && config.storeCompactionEnableDirectIO;\n     this.deprecatedContainers = new HashSet<>();\n+    this.remoteTokenTracker = remoteTokenTracker;\n     if (config.storeCompactionFilter.equals(IndexSegmentValidEntryFilterWithoutUndelete.class.getSimpleName())) {\n       validEntryFilter = new IndexSegmentValidEntryFilterWithoutUndelete();\n     } else {\n@@ -1134,12 +1138,59 @@ private IndexValue getPutValueFromDeleteEntry(StoreKey key, IndexValue deleteVal\n   /**\n    * Check if given delete tombstone is removable. There are two cases where delete tombstone can be safely removed:\n    * 1. delete record has finite expiration time and it has expired already;\n-   * 2. (TODO) all peer replica tokens have passed the offset that refers to this delete (That is, they have replicated this delete already)\n-   * @param deleteIndexValue the {@link IndexValue} associated with delete tombstone\n+   * 2. all peer replica tokens have passed position of this delete (That is, they have replicated this delete already)\n+   * @param deleteIndexEntry the {@link IndexEntry} associated with delete tombstone\n+   * @param currentIndexSegment the {@link IndexSegment} delete tombstone comes from.\n    * @return {@code true} if this delete tombstone can be safely removed. {@code false} otherwise.\n    */\n-  private boolean isDeleteTombstoneRemovable(IndexValue deleteIndexValue) {\n-    return srcIndex.isExpired(deleteIndexValue);\n+  private boolean isDeleteTombstoneRemovable(IndexEntry deleteIndexEntry, IndexSegment currentIndexSegment) {\n+    if (srcIndex.isExpired(deleteIndexEntry.getValue())) {\n+      return true;\n+    }\n+    if (remoteTokenTracker == null) {\n+      return false;\n+    }\n+    for (Map.Entry<String, FindToken> entry : remoteTokenTracker.getPeerReplicaAndToken().entrySet()) {\n+      FindToken token = srcIndex.resetTokenIfRequired((StoreFindToken) entry.getValue());\n+      if (!token.equals(entry.getValue())) {\n+        // incarnation id has changed or there is unclean shutdown\n+        return false;\n+      }\n+      token = srcIndex.revalidateFindToken(entry.getValue());\n+      if (!token.equals(entry.getValue())) {\n+        // the log segment (token refers to) has been compacted already\n+        return false;\n+      }\n+      switch (token.getType()) {\n+        case Uninitialized:\n+          return false;\n+        case JournalBased:\n+          // if code reaches here, it means the journal-based token is valid (didn't get reset). Since compaction always\n+          // performs on segments out of journal, this journal-based token must be past the delete tombstone.\n+          break;\n+        case IndexBased:\n+          // if code reaches here, the index-based token is valid (didn't get reset). We check two following rules:\n+          // 1. token's index segment is behind delete tombstone's index segment\n+          // 2. if they are in the same segment, compare the store key (sealed index segment is sorted based on key)\n+          StoreFindToken indexBasedToken = (StoreFindToken) token;\n+          if (indexBasedToken.getOffset().compareTo(currentIndexSegment.getStartOffset()) < 0) {\n+            // index-based token is ahead of current index segment (hasn't reached this delete tombstone)\n+            return false;\n+          }\n+          if (indexBasedToken.getOffset().compareTo(currentIndexSegment.getStartOffset()) == 0) {\n+            // index-based token refers to current index segment, we need to compare the key\n+            if (indexBasedToken.getStoreKey().compareTo(deleteIndexEntry.getKey()) <= 0) {\n+              return false;\n+            }\n+          }\n+          // if tokens start offset > current index segment start offset, then token is obviously past tombstone\n+          break;\n+        default:\n+          throw new IllegalArgumentException(\"Unsupported token type in compaction: \" + token.getType());\n+      }\n+    }\n+    srcMetrics.permanentDeleteTombstonePurgeCount.inc();\n+    return true;\n   }\n \n   /**\n@@ -1231,7 +1282,8 @@ private boolean isDeleteTombstoneRemovable(IndexValue deleteIndexValue) {\n           } else {\n             // DELETE entry without corresponding PUT (may be left by previous compaction). Check if this delete\n             // tombstone is removable.\n-            if (config.storeCompactionPurgeExpiredDeleteTombstone && isDeleteTombstoneRemovable(value)) {\n+            if (config.storeCompactionPurgeDeleteTombstone && isDeleteTombstoneRemovable(indexEntry,\n+                indexSegment)) {\n               logger.debug(\n                   \"Delete tombstone of {} (with expiration time {} ms) is removable and won't be copied to target log segment\",\n                   indexEntry.getKey(), value.getExpiresAtMs());\n@@ -1501,7 +1553,7 @@ public boolean alreadyExists(PersistentIndex idx, FileSpan searchSpan, StoreKey\n           }\n           if (currentLatestState.isDelete() && currentLatestState.getLifeVersion() == currentValue.getLifeVersion()) {\n             // check if this is a removable delete tombstone.\n-            if (config.storeCompactionPurgeExpiredDeleteTombstone && isDeleteTombstoneRemovable(currentValue)\n+            if (config.storeCompactionPurgeDeleteTombstone && isDeleteTombstoneRemovable(entry, indexSegment)\n                 && getPutValueFromSrc(currentKey, currentValue, indexSegment) == null) {\n               logger.debug(\n                   \"Delete tombstone of {} (with expiration time {} ms) is removable and won't be copied to target log segment\","
  },
  {
    "sha": "32703620a08ec8891c8e5115c0b1cd6ffdc92a41",
    "filename": "ambry-store/src/main/java/com/github/ambry/store/PersistentIndex.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/linkedin/ambry/blob/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-store/src/main/java/com/github/ambry/store/PersistentIndex.java",
    "raw_url": "https://github.com/linkedin/ambry/raw/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-store/src/main/java/com/github/ambry/store/PersistentIndex.java",
    "contents_url": "https://api.github.com/repos/linkedin/ambry/contents/ambry-store/src/main/java/com/github/ambry/store/PersistentIndex.java?ref=02d7236e4257ab44463ff42c6269d07713b051bd",
    "patch": "@@ -1310,7 +1310,7 @@ FindInfo findEntriesSince(FindToken token, long maxTotalSizeOfEntries) throws St\n    * @param storeToken the {@link StoreFindToken} that needs to be validated\n    * @return the new {@link StoreFindToken} after validating\n    */\n-  private StoreFindToken resetTokenIfRequired(StoreFindToken storeToken) {\n+   StoreFindToken resetTokenIfRequired(StoreFindToken storeToken) {\n     UUID remoteIncarnationId = storeToken.getIncarnationId();\n     // if incarnationId is null, for backwards compatibility purposes, the token is considered as good.\n     /// if not null, we check for a match"
  },
  {
    "sha": "27a422ffc61f86a5960dd5b17ce2c107ac73fbb9",
    "filename": "ambry-store/src/main/java/com/github/ambry/store/RemoteTokenTracker.java",
    "status": "added",
    "additions": 75,
    "deletions": 0,
    "changes": 75,
    "blob_url": "https://github.com/linkedin/ambry/blob/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-store/src/main/java/com/github/ambry/store/RemoteTokenTracker.java",
    "raw_url": "https://github.com/linkedin/ambry/raw/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-store/src/main/java/com/github/ambry/store/RemoteTokenTracker.java",
    "contents_url": "https://api.github.com/repos/linkedin/ambry/contents/ambry-store/src/main/java/com/github/ambry/store/RemoteTokenTracker.java?ref=02d7236e4257ab44463ff42c6269d07713b051bd",
    "patch": "@@ -0,0 +1,75 @@\n+/**\n+ * Copyright 2021 LinkedIn Corp. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ */\n+package com.github.ambry.store;\n+\n+import com.github.ambry.clustermap.ReplicaId;\n+import com.github.ambry.replication.FindToken;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+\n+\n+/**\n+ * {@link RemoteTokenTracker} tracks tokens from all peer replicas and updates them when handling metadata request from\n+ * peer node.\n+ */\n+public class RemoteTokenTracker {\n+  private static final String DELIMITER = \":\";\n+  private final ReplicaId localReplica;\n+  // The key of peerReplicaAndToken is a string containing hostname and path of peer replica.\n+  // For example: localhost:/mnt/u001/p1\n+  private ConcurrentMap<String, FindToken> peerReplicaAndToken = new ConcurrentHashMap<>();\n+\n+  public RemoteTokenTracker(ReplicaId localReplica) {\n+    this.localReplica = localReplica;\n+    localReplica.getPeerReplicaIds().forEach(r -> {\n+      String hostnameAndPath = r.getDataNodeId().getHostname() + DELIMITER + r.getReplicaPath();\n+      peerReplicaAndToken.put(hostnameAndPath, new StoreFindToken());\n+    });\n+  }\n+\n+  /**\n+   * Update peer replica token within this tracker.\n+   * @param token the most recent token from peer replica.\n+   * @param remoteHostName the hostname of peer node (where the peer replica resides).\n+   * @param remoteReplicaPath the path of peer replica on remote peer node.\n+   */\n+  void updateTokenFromPeerReplica(FindToken token, String remoteHostName, String remoteReplicaPath) {\n+    // this already handles newly added peer replica (i.e. move replica)\n+    peerReplicaAndToken.put(remoteHostName + DELIMITER + remoteReplicaPath, token);\n+  }\n+\n+  /**\n+   * Refresh the peerReplicaAndToken map in case peer replica has changed (i.e. new replica is added and old replica is removed)\n+   */\n+  void refreshPeerReplicaTokens() {\n+    ConcurrentMap<String, FindToken> newPeerReplicaAndToken = new ConcurrentHashMap<>();\n+    // this should remove peer replica that no longer exists (i.e original replica is moved to other node)\n+    localReplica.getPeerReplicaIds().forEach(r -> {\n+      String hostnameAndPath = r.getDataNodeId().getHostname() + DELIMITER + r.getReplicaPath();\n+      newPeerReplicaAndToken.put(hostnameAndPath,\n+          peerReplicaAndToken.getOrDefault(hostnameAndPath, new StoreFindToken()));\n+    });\n+    // atomic switch\n+    peerReplicaAndToken = newPeerReplicaAndToken;\n+  }\n+\n+  /**\n+   * @return a snapshot of peer replica to token map.\n+   */\n+  Map<String, FindToken> getPeerReplicaAndToken() {\n+    return new HashMap<>(peerReplicaAndToken);\n+  }\n+}"
  },
  {
    "sha": "05d7c5546d5f0c65b82c315ac615690b67d4ac63",
    "filename": "ambry-store/src/main/java/com/github/ambry/store/StoreMetrics.java",
    "status": "modified",
    "additions": 3,
    "deletions": 0,
    "changes": 3,
    "blob_url": "https://github.com/linkedin/ambry/blob/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-store/src/main/java/com/github/ambry/store/StoreMetrics.java",
    "raw_url": "https://github.com/linkedin/ambry/raw/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-store/src/main/java/com/github/ambry/store/StoreMetrics.java",
    "contents_url": "https://api.github.com/repos/linkedin/ambry/contents/ambry-store/src/main/java/com/github/ambry/store/StoreMetrics.java?ref=02d7236e4257ab44463ff42c6269d07713b051bd",
    "patch": "@@ -88,6 +88,7 @@\n   public final Counter compactionBundleReadBufferUsed;\n   public final Counter compactionBundleReadBufferIoCount;\n   public final Counter compactionTargetIndexDuplicateOnNonRecoveryCount;\n+  public final Counter permanentDeleteTombstonePurgeCount;\n   public final Timer compactionCopyRecordTimeInMs;\n   public final Timer compactionCopyDataByIndexSegmentTimeInMs;\n   public final Timer compactionCopyDataByLogSegmentTimeInMs;\n@@ -194,6 +195,8 @@ public StoreMetrics(String prefix, MetricRegistry registry) {\n         registry.counter(MetricRegistry.name(BlobStoreCompactor.class, name + \"CompactionBundleReadBufferIoCount\"));\n     compactionTargetIndexDuplicateOnNonRecoveryCount = registry.counter(\n         MetricRegistry.name(BlobStoreCompactor.class, name + \"CompactionTargetIndexDuplicateOnNonRecoveryCount\"));\n+    permanentDeleteTombstonePurgeCount = registry.counter(\n+        MetricRegistry.name(BlobStoreCompactor.class, name + \"PermanentDeleteTombstonePurgeCount\"));\n     compactionCopyRecordTimeInMs =\n         registry.timer(MetricRegistry.name(BlobStoreCompactor.class, name + \"CompactionCopyRecordTimeInMs\"));\n     compactionCopyDataByIndexSegmentTimeInMs = registry.timer("
  },
  {
    "sha": "33306f53c1333da980ea4f45272c20829bb5239e",
    "filename": "ambry-store/src/test/java/com/github/ambry/store/BlobStoreCompactorTest.java",
    "status": "modified",
    "additions": 208,
    "deletions": 50,
    "changes": 258,
    "blob_url": "https://github.com/linkedin/ambry/blob/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-store/src/test/java/com/github/ambry/store/BlobStoreCompactorTest.java",
    "raw_url": "https://github.com/linkedin/ambry/raw/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-store/src/test/java/com/github/ambry/store/BlobStoreCompactorTest.java",
    "contents_url": "https://api.github.com/repos/linkedin/ambry/contents/ambry-store/src/test/java/com/github/ambry/store/BlobStoreCompactorTest.java?ref=02d7236e4257ab44463ff42c6269d07713b051bd",
    "patch": "@@ -16,8 +16,14 @@\n import com.codahale.metrics.MetricRegistry;\n import com.github.ambry.account.AccountService;\n import com.github.ambry.account.Container;\n+import com.github.ambry.clustermap.MockClusterMap;\n+import com.github.ambry.clustermap.MockDataNodeId;\n+import com.github.ambry.clustermap.MockPartitionId;\n+import com.github.ambry.clustermap.MockReplicaId;\n import com.github.ambry.config.StoreConfig;\n import com.github.ambry.config.VerifiableProperties;\n+import com.github.ambry.network.Port;\n+import com.github.ambry.network.PortType;\n import com.github.ambry.utils.ByteBufferOutputStream;\n import com.github.ambry.utils.Pair;\n import com.github.ambry.utils.TestUtils;\n@@ -42,12 +48,12 @@\n import java.util.Set;\n import java.util.TreeMap;\n import java.util.TreeSet;\n+import java.util.UUID;\n import java.util.concurrent.ConcurrentNavigableMap;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicLong;\n import java.util.stream.Collectors;\n import org.junit.After;\n-import org.junit.Assume;\n import org.junit.Test;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n@@ -56,6 +62,7 @@\n import static com.github.ambry.store.CuratedLogIndexState.*;\n import static com.github.ambry.store.StoreTestUtils.*;\n import static org.junit.Assert.*;\n+import static org.junit.Assume.*;\n \n \n /**\n@@ -72,7 +79,7 @@\n   private final String tempDirStr;\n   private final boolean doDirectIO;\n   private final boolean withUndelete;\n-  private final boolean purgeExpiredDelete;\n+  private final boolean purgeDeleteTombstone;\n   private final StoreConfig config;\n   private boolean alwaysEnableTargetIndexDuplicateChecking = false;\n \n@@ -105,15 +112,16 @@\n    * Creates a temporary directory for the store.\n    * @throws Exception\n    */\n-  public BlobStoreCompactorTest(boolean doDirectIO, boolean withUndelete, boolean purgeExpiredDelete) throws Exception {\n+  public BlobStoreCompactorTest(boolean doDirectIO, boolean withUndelete, boolean purgeDeleteTombstone)\n+      throws Exception {\n     tempDir = StoreTestUtils.createTempDirectory(\"compactorDir-\" + TestUtils.getRandomString(10));\n     tempDirStr = tempDir.getAbsolutePath();\n     config = new StoreConfig(new VerifiableProperties(new Properties()));\n     this.doDirectIO = doDirectIO;\n     this.withUndelete = withUndelete;\n-    this.purgeExpiredDelete = purgeExpiredDelete;\n+    this.purgeDeleteTombstone = purgeDeleteTombstone;\n     if (doDirectIO) {\n-      Assume.assumeTrue(Utils.isLinux());\n+      assumeTrue(Utils.isLinux());\n     }\n     accountService = Mockito.mock(AccountService.class);\n   }\n@@ -137,7 +145,7 @@ public void cleanup() throws Exception {\n   @Test\n   public void initCloseTest() throws Exception {\n     refreshState(false, true);\n-    compactor = getCompactor(state.log, DISK_IO_SCHEDULER);\n+    compactor = getCompactor(state.log, DISK_IO_SCHEDULER, null);\n     compactor.initialize(state.index);\n     compactor.close(0);\n   }\n@@ -149,7 +157,7 @@ public void initCloseTest() throws Exception {\n   @Test\n   public void closeWithoutInitTest() throws Exception {\n     refreshState(false, true);\n-    compactor = getCompactor(state.log, DISK_IO_SCHEDULER);\n+    compactor = getCompactor(state.log, DISK_IO_SCHEDULER, null);\n     compactor.close(0);\n   }\n \n@@ -160,7 +168,7 @@ public void closeWithoutInitTest() throws Exception {\n   @Test\n   public void useServiceWithoutInitTest() throws Exception {\n     refreshState(false, true);\n-    compactor = getCompactor(state.log, DISK_IO_SCHEDULER);\n+    compactor = getCompactor(state.log, DISK_IO_SCHEDULER, null);\n     LogSegmentName firstSegmentName = state.log.getFirstSegment().getName();\n     CompactionDetails details =\n         new CompactionDetails(state.time.milliseconds(), Collections.singletonList(firstSegmentName));\n@@ -220,7 +228,7 @@ public void compactWithCompactionInProgressTest() throws Exception {\n \n     // create a compaction log in order to mimic a compaction being in progress\n     try (CompactionLog cLog = new CompactionLog(tempDirStr, STORE_ID, state.time, details, config)) {\n-      compactor = getCompactor(state.log, DISK_IO_SCHEDULER);\n+      compactor = getCompactor(state.log, DISK_IO_SCHEDULER, null);\n       compactor.initialize(state.index);\n       compactor.compact(details, bundleReadBuffer);\n       fail(\"compact() should have failed because a compaction is already in progress\");\n@@ -239,7 +247,7 @@ public void compactWithCompactionInProgressTest() throws Exception {\n   @Test\n   public void resumeCompactionWithoutAnyInProgressTest() throws Exception {\n     refreshState(false, true);\n-    compactor = getCompactor(state.log, DISK_IO_SCHEDULER);\n+    compactor = getCompactor(state.log, DISK_IO_SCHEDULER, null);\n     compactor.initialize(state.index);\n     assertFalse(\"Compaction should not be in progress\", CompactionLog.isCompactionInProgress(tempDirStr, STORE_ID));\n     assertEquals(\"Temp log segment should not be found\", 0, compactor.getSwapSegmentsInUse().length);\n@@ -266,6 +274,134 @@ public void basicTest() throws Exception {\n     compactAndVerify(segmentsUnderCompaction, deleteReferenceTimeMs, true);\n   }\n \n+  /**\n+   * Tests that permanent delete tombstone can be compacted once all peer tokens are past its position.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void deleteTombstoneCleanupTest() throws Exception {\n+    assumeTrue(purgeDeleteTombstone);\n+    refreshState(false, true);\n+    List<LogSegmentName> segmentsUnderCompaction = getLogSegments(0, 2);\n+    CompactionDetails details = new CompactionDetails(state.time.milliseconds(), segmentsUnderCompaction);\n+    List<MockReplicaId> localAndPeerReplicas = generateLocalAndPeerReplicas();\n+    RemoteTokenTracker tokenTracker = new RemoteTokenTracker(localAndPeerReplicas.get(0));\n+    // Generate tokens for peer replicas and make sure they are both past position of 1st delete tombstone and haven't\n+    // reached position of 2nd tombstone.\n+    MockId tombstone1 = state.permanentDeleteTombstones.get(0);\n+    MockId tombstone2 = state.permanentDeleteTombstones.get(1);\n+    IndexValue deleteIndexValue1 = state.index.findKey(tombstone1);\n+    IndexValue deleteIndexValue2 = state.index.findKey(tombstone2);\n+    IndexSegment indexSegment1 = state.index.getIndexSegments().floorEntry(deleteIndexValue1.getOffset()).getValue();\n+    IndexSegment indexSegment2 = state.index.getIndexSegments().floorEntry(deleteIndexValue2.getOffset()).getValue();\n+    // find a key that is behind tombstone1 and the other key that falls between tombstone1 and tombstone2\n+    Iterator<IndexEntry> iterator = indexSegment1.iterator();\n+    while(iterator.hasNext()){\n+      MockId key = (MockId) iterator.next().getKey();\n+      if(key.compareTo(tombstone1) == 0){\n+        break;\n+      }\n+    }\n+    MockId keyInToken1 = (MockId) iterator.next().getKey();\n+    MockId keyInToken2 = (MockId) indexSegment2.iterator().next().getKey();\n+    StoreFindToken peerToken1 =\n+        new StoreFindToken(keyInToken1, indexSegment1.getStartOffset(), state.sessionId, state.incarnationId,\n+            indexSegment1.getResetKey(), indexSegment1.getResetKeyType(), indexSegment1.getResetKeyLifeVersion());\n+    StoreFindToken peerToken2 =\n+        new StoreFindToken(keyInToken2, indexSegment2.getStartOffset(), state.sessionId, state.incarnationId,\n+            indexSegment2.getResetKey(), indexSegment2.getResetKeyType(), indexSegment2.getResetKeyLifeVersion());\n+    // update token associated with peer replica\n+    MockReplicaId peerReplica1 = localAndPeerReplicas.get(1);\n+    MockReplicaId peerReplica2 = localAndPeerReplicas.get(2);\n+    tokenTracker.updateTokenFromPeerReplica(peerToken1, peerReplica1.getDataNodeId().getHostname(),\n+        peerReplica1.getReplicaPath());\n+    tokenTracker.updateTokenFromPeerReplica(peerToken2, peerReplica2.getDataNodeId().getHostname(),\n+        peerReplica2.getReplicaPath());\n+    // initiate compaction\n+    compactor = getCompactor(state.log, DISK_IO_SCHEDULER, tokenTracker);\n+    compactor.initialize(state.index);\n+    try {\n+      compactor.compact(details, bundleReadBuffer);\n+    } finally {\n+      compactor.close(0);\n+    }\n+    // the first delete tombstone should be compacted\n+    assertNull(\"Delete tombstone should be compacted\", state.index.findKey(tombstone1));\n+    // the second delete tombstone should exist\n+    assertNotNull(\"Delete tombstone should be present as at least one token hasn't reached its position\",\n+        state.index.findKey(tombstone2));\n+  }\n+\n+  /**\n+   * Tests compacting delete tombstone with both invalid and journal based tokens.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void compactDeleteTombstoneTwice() throws Exception {\n+    assumeTrue(purgeDeleteTombstone);\n+    refreshState(false, true);\n+    List<LogSegmentName> segmentsUnderCompaction = getLogSegments(0, 2);\n+    CompactionDetails details = new CompactionDetails(state.time.milliseconds(), segmentsUnderCompaction);\n+    List<MockReplicaId> localAndPeerReplicas = generateLocalAndPeerReplicas();\n+    RemoteTokenTracker tokenTracker = new RemoteTokenTracker(localAndPeerReplicas.get(0));\n+    MockId tombstone1 = state.permanentDeleteTombstones.get(0);\n+    MockId tombstone2 = state.permanentDeleteTombstones.get(1);\n+    IndexValue deleteIndexValue2 = state.index.findKey(tombstone2);\n+    IndexSegment indexSegment2 = state.index.getIndexSegments().floorEntry(deleteIndexValue2.getOffset()).getValue();\n+    MockId keyInToken = (MockId) indexSegment2.iterator().next().getKey();\n+\n+    UUID invalidIncarnationId;\n+    do {\n+      invalidIncarnationId = UUID.randomUUID();\n+    } while (invalidIncarnationId.equals(state.incarnationId));\n+    // invalid token with other incarnation id\n+    StoreFindToken invalidToken =\n+        new StoreFindToken(keyInToken, indexSegment2.getStartOffset(), state.sessionId, invalidIncarnationId,\n+            indexSegment2.getResetKey(), indexSegment2.getResetKeyType(), indexSegment2.getResetKeyLifeVersion());\n+    StoreFindToken peerToken2 =\n+        new StoreFindToken(keyInToken, indexSegment2.getStartOffset(), state.sessionId, state.incarnationId,\n+            indexSegment2.getResetKey(), indexSegment2.getResetKeyType(), indexSegment2.getResetKeyLifeVersion());\n+    MockReplicaId peerReplica1 = localAndPeerReplicas.get(1);\n+    MockReplicaId peerReplica2 = localAndPeerReplicas.get(2);\n+    tokenTracker.updateTokenFromPeerReplica(invalidToken, peerReplica1.getDataNodeId().getHostname(),\n+        peerReplica1.getReplicaPath());\n+    tokenTracker.updateTokenFromPeerReplica(peerToken2, peerReplica2.getDataNodeId().getHostname(),\n+        peerReplica2.getReplicaPath());\n+    // initiate compaction\n+    compactor = getCompactor(state.log, DISK_IO_SCHEDULER, tokenTracker);\n+    compactor.initialize(state.index);\n+    try {\n+      compactor.compact(details, bundleReadBuffer);\n+    } finally {\n+      compactor.close(0);\n+    }\n+    // both tombstones should exist\n+    assertNotNull(\"Delete tombstone should be present\", state.index.findKey(tombstone1));\n+    assertNotNull(\"Delete tombstone should be present\", state.index.findKey(tombstone2));\n+\n+    // update remote token tracker with journal based tokens and compact again.\n+    IndexSegment lastIndexSegment = state.index.getIndexSegments().lastEntry().getValue();\n+    peerToken2 = new StoreFindToken(lastIndexSegment.getStartOffset(), state.sessionId, state.incarnationId, true,\n+        lastIndexSegment.getResetKey(), lastIndexSegment.getResetKeyType(), lastIndexSegment.getResetKeyLifeVersion());\n+    tokenTracker.updateTokenFromPeerReplica(peerToken2, peerReplica1.getDataNodeId().getHostname(),\n+        peerReplica1.getReplicaPath());\n+    tokenTracker.updateTokenFromPeerReplica(peerToken2, peerReplica2.getDataNodeId().getHostname(),\n+        peerReplica2.getReplicaPath());\n+    // initiate compaction\n+    segmentsUnderCompaction = getLogSegments(0, 2);\n+    details = new CompactionDetails(state.time.milliseconds(), segmentsUnderCompaction);\n+    compactor = getCompactor(state.log, DISK_IO_SCHEDULER, tokenTracker);\n+    compactor.initialize(state.index);\n+    try {\n+      compactor.compact(details, bundleReadBuffer);\n+    } finally {\n+      compactor.close(0);\n+    }\n+    // both tombstones should be compacted\n+    assertNull(\"Delete tombstone should be compacted\", state.index.findKey(tombstone1));\n+    assertNull(\"Delete tombstone should be compacted\", state.index.findKey(tombstone2));\n+  }\n+\n   /**\n    * A test similar to basicTest but doesn't use bundleReadBuffer.\n    * @throws Exception\n@@ -349,11 +485,11 @@ public void compactWholeLogMultipleTimesTest() throws Exception {\n         state.advanceTime(setTimeMs + Time.MsPerSec - state.time.milliseconds());\n       }\n       long deleteReferenceTimeMs = state.time.milliseconds();\n-      getCurrentBlobIdsFromWholeIndex(state.index, compactedDeletes, purgeExpiredDelete);\n+      getCurrentBlobIdsFromWholeIndex(state.index, compactedDeletes, purgeDeleteTombstone);\n       long logSegmentSizeSumBeforeCompaction = getSumOfLogSegmentEndOffsets();\n       CompactionDetails details = new CompactionDetails(deleteReferenceTimeMs, segmentsUnderCompaction);\n \n-      compactor = getCompactor(state.log, DISK_IO_SCHEDULER);\n+      compactor = getCompactor(state.log, DISK_IO_SCHEDULER, null);\n       compactor.initialize(state.index);\n       long logSegmentsBeforeCompaction = state.index.getLogSegmentCount();\n       try {\n@@ -426,7 +562,7 @@ public void dropAllSegmentsUnderCompactionTest() throws Exception {\n     long deleteReferenceTimeMs = state.time.milliseconds() + Time.MsPerSec;\n     state.advanceTime(deleteReferenceTimeMs - state.time.milliseconds());\n     assertEquals(\"Valid size in the segments under compaction should be 0\", 0,\n-        getValidDataSize(segmentsUnderCompaction, deleteReferenceTimeMs, purgeExpiredDelete));\n+        getValidDataSize(segmentsUnderCompaction, deleteReferenceTimeMs, purgeDeleteTombstone));\n     compactAndVerify(segmentsUnderCompaction, deleteReferenceTimeMs, true);\n   }\n \n@@ -599,7 +735,7 @@ public void interspersedDeletedAndExpiredBlobsTest() throws Exception {\n     List<LogSegmentName> segmentsUnderCompaction = Collections.singletonList(logSegmentName);\n     long endOffsetOfSegmentBeforeCompaction = state.log.getSegment(logSegmentName).getEndOffset();\n     CompactionDetails details = new CompactionDetails(deleteReferenceTimeMs, segmentsUnderCompaction);\n-    compactor = getCompactor(state.log, DISK_IO_SCHEDULER);\n+    compactor = getCompactor(state.log, DISK_IO_SCHEDULER, null);\n     compactor.initialize(state.index);\n     long logSegmentCountBeforeCompaction = state.index.getLogSegmentCount();\n     try {\n@@ -879,7 +1015,7 @@ public void allEntryTypesTest() throws Exception {\n     long logSegmentSizeSumBeforeCompaction = getSumOfLogSegmentEndOffsets();\n     CompactionDetails details = new CompactionDetails(deleteReferenceTimeMs, segmentsUnderCompaction);\n \n-    compactor = getCompactor(state.log, DISK_IO_SCHEDULER);\n+    compactor = getCompactor(state.log, DISK_IO_SCHEDULER, null);\n     compactor.initialize(state.index);\n     long logSegmentCountBeforeCompaction = state.index.getLogSegmentCount();\n     try {\n@@ -1035,7 +1171,7 @@ public void containerDeletionTest() throws Exception {\n     LogSegmentName compactedLogSegmentName = logSegmentName.getNextGenerationName();\n     long endOffsetOfSegmentBeforeCompaction = state.log.getSegment(logSegmentName).getEndOffset();\n \n-    compactor = getCompactor(state.log, DISK_IO_SCHEDULER);\n+    compactor = getCompactor(state.log, DISK_IO_SCHEDULER, null);\n     compactor.initialize(state.index);\n     int indexSegmentCountBeforeCompaction = state.index.getIndexSegments().size();\n \n@@ -1182,7 +1318,7 @@ public void ttlUpdateSpecificRecoveryTest() throws Exception {\n    */\n   @Test\n   public void undeleteSameIndexSegmentTest_NoTtlUpdate() throws Exception {\n-    Assume.assumeTrue(withUndelete);\n+    assumeTrue(withUndelete);\n     refreshState(false, false);\n     state.properties.put(\"store.index.max.number.of.inmem.elements\", Integer.toString(5));\n     state.initIndex(null);\n@@ -1273,7 +1409,7 @@ public void undeleteSameIndexSegmentTest_NoTtlUpdate() throws Exception {\n     long deleteReferenceTimeMs = state.time.milliseconds();\n     long endOffsetOfSegmentBeforeCompaction = state.log.getSegment(logSegmentName).getEndOffset();\n     CompactionDetails details = new CompactionDetails(deleteReferenceTimeMs, segmentsUnderCompaction);\n-    compactor = getCompactor(state.log, DISK_IO_SCHEDULER);\n+    compactor = getCompactor(state.log, DISK_IO_SCHEDULER, null);\n     compactor.initialize(state.index);\n \n     try {\n@@ -1379,7 +1515,7 @@ public void undeleteSameIndexSegmentTest_NoTtlUpdate() throws Exception {\n     endOffsetOfSegmentBeforeCompaction = state.log.getSegment(logSegmentName).getEndOffset();\n     details = new CompactionDetails(deleteReferenceTimeMs, segmentsUnderCompaction);\n \n-    compactor = getCompactor(state.log, DISK_IO_SCHEDULER);\n+    compactor = getCompactor(state.log, DISK_IO_SCHEDULER, null);\n     compactor.initialize(state.index);\n \n     try {\n@@ -1432,7 +1568,7 @@ public void undeleteSameIndexSegmentTest_NoTtlUpdate() throws Exception {\n    */\n   @Test\n   public void undeleteSameIndexSegmentTest_WithTtlUpdate() throws Exception {\n-    Assume.assumeTrue(withUndelete);\n+    assumeTrue(withUndelete);\n     refreshState(false, false);\n     state.properties.put(\"store.index.max.number.of.inmem.elements\", Integer.toString(5));\n     state.initIndex(null);\n@@ -1547,7 +1683,7 @@ public void undeleteSameIndexSegmentTest_WithTtlUpdate() throws Exception {\n     long deleteReferenceTimeMs = state.time.milliseconds();\n     long endOffsetOfSegmentBeforeCompaction = state.log.getSegment(logSegmentName).getEndOffset();\n     CompactionDetails details = new CompactionDetails(deleteReferenceTimeMs, segmentsUnderCompaction);\n-    compactor = getCompactor(state.log, DISK_IO_SCHEDULER);\n+    compactor = getCompactor(state.log, DISK_IO_SCHEDULER, null);\n     compactor.initialize(state.index);\n \n     try {\n@@ -1703,7 +1839,7 @@ public void undeleteSameIndexSegmentTest_WithTtlUpdate() throws Exception {\n     endOffsetOfSegmentBeforeCompaction = state.log.getSegment(logSegmentName).getEndOffset();\n     details = new CompactionDetails(deleteReferenceTimeMs, segmentsUnderCompaction);\n \n-    compactor = getCompactor(state.log, DISK_IO_SCHEDULER);\n+    compactor = getCompactor(state.log, DISK_IO_SCHEDULER, null);\n     compactor.initialize(state.index);\n \n     try {\n@@ -1796,7 +1932,7 @@ public void undeleteSameIndexSegmentTest_WithTtlUpdate() throws Exception {\n    */\n   @Test\n   public void undeleteSameIndexSegmentTest_DeleteNotInEffect() throws Exception {\n-    Assume.assumeTrue(withUndelete);\n+    assumeTrue(withUndelete);\n     refreshState(false, false);\n     state.properties.put(\"store.index.max.number.of.inmem.elements\", Integer.toString(5));\n     state.initIndex(null);\n@@ -1830,7 +1966,7 @@ public void undeleteSameIndexSegmentTest_DeleteNotInEffect() throws Exception {\n     long endOffsetOfSegmentBeforeCompaction = state.log.getSegment(logSegmentName).getEndOffset();\n     // Use 0 as delete reference time so all the deletes are still with in the retention date\n     CompactionDetails details = new CompactionDetails(0, segmentsUnderCompaction);\n-    compactor = getCompactor(state.log, DISK_IO_SCHEDULER);\n+    compactor = getCompactor(state.log, DISK_IO_SCHEDULER, null);\n     compactor.initialize(state.index);\n \n     try {\n@@ -1907,7 +2043,7 @@ public void undeleteSameIndexSegmentTest_DeleteNotInEffect() throws Exception {\n    */\n   @Test\n   public void undeleteSameIndexSegmentTest_NoPut() throws Exception {\n-    Assume.assumeTrue(withUndelete);\n+    assumeTrue(withUndelete);\n     refreshState(false, false);\n     state.properties.put(\"store.index.max.number.of.inmem.elements\", Integer.toString(5));\n     state.initIndex(null);\n@@ -1986,7 +2122,7 @@ public void undeleteSameIndexSegmentTest_NoPut() throws Exception {\n     LogSegmentName logSegmentName = segmentsUnderCompaction.get(0);\n     long endOffsetOfSegmentBeforeCompaction = state.log.getSegment(logSegmentName).getEndOffset();\n     CompactionDetails details = new CompactionDetails(state.time.milliseconds(), segmentsUnderCompaction);\n-    compactor = getCompactor(state.log, DISK_IO_SCHEDULER);\n+    compactor = getCompactor(state.log, DISK_IO_SCHEDULER, null);\n     compactor.initialize(state.index);\n \n     try {\n@@ -2117,7 +2253,7 @@ public void undeleteSameIndexSegmentTest_NoPut() throws Exception {\n    */\n   @Test\n   public void undeleteCrossLogSegmentTest_UndeleteAsFinal() throws Exception {\n-    Assume.assumeTrue(withUndelete);\n+    assumeTrue(withUndelete);\n     refreshState(false, false);\n     int numPuts = (int) Math.floorDiv(state.log.getSegmentCapacity() - LogSegment.HEADER_SIZE,\n         PUT_RECORD_SIZE + (DELETE_RECORD_SIZE) / 2) - 1;\n@@ -2164,7 +2300,7 @@ public void undeleteCrossLogSegmentTest_UndeleteAsFinal() throws Exception {\n    */\n   @Test\n   public void undeleteCrossLogSegmentTest_DeleteAsFinal() throws Exception {\n-    Assume.assumeTrue(withUndelete);\n+    assumeTrue(withUndelete);\n     for (long deleteReferenceTime : new long[]{-1, 0}) {\n       refreshState(false, false);\n       int numPuts = (int) Math.floorDiv(state.log.getSegmentCapacity() - LogSegment.HEADER_SIZE,\n@@ -2218,7 +2354,7 @@ public void undeleteCrossLogSegmentTest_DeleteAsFinal() throws Exception {\n    */\n   @Test\n   public void deleteWithOlderVersionTest() throws Exception {\n-    Assume.assumeTrue(withUndelete);\n+    assumeTrue(withUndelete);\n     // 1. Compact all the puts and deletes with delete reference time being 0\n     List<IndexEntry> entries = createDeleteWithOlderVersion();\n     int numPuts = entries.size();\n@@ -2285,7 +2421,7 @@ public void deleteWithOlderVersionTest() throws Exception {\n    */\n   @Test\n   public void undeleteTargetIndexOnlyHasTtlUpdateTest() throws Exception {\n-    Assume.assumeTrue(withUndelete);\n+    assumeTrue(withUndelete);\n     refreshState(false, false);\n     state.properties.put(\"store.index.max.number.of.inmem.elements\", Integer.toString(5));\n     state.initIndex(null);\n@@ -2329,7 +2465,7 @@ public void undeleteTargetIndexOnlyHasTtlUpdateTest() throws Exception {\n    */\n   @Test\n   public void undeletePutCompactedTest() throws Exception {\n-    Assume.assumeTrue(withUndelete);\n+    assumeTrue(withUndelete);\n     refreshState(false, false);\n     state.properties.put(\"store.index.max.number.of.inmem.elements\", Integer.toString(5));\n     state.initIndex(null);\n@@ -2367,7 +2503,7 @@ public void undeletePutCompactedTest() throws Exception {\n    */\n   @Test\n   public void undeleteSpecificRecoveryTest() throws Exception {\n-    Assume.assumeTrue(withUndelete);\n+    assumeTrue(withUndelete);\n     bundleReadBuffer = null;\n     // close testing\n     doUndeleteSrcDupTest();\n@@ -2381,6 +2517,26 @@ public void undeleteSpecificRecoveryTest() throws Exception {\n   // helpers\n \n   // general\n+\n+  /**\n+   * Generate local replica and two peer replicas.\n+   * @return a list of replicas (first one is local replica, others are remote peer replicas)\n+   */\n+  private List<MockReplicaId> generateLocalAndPeerReplicas() {\n+    Port port = new Port(6667, PortType.PLAINTEXT);\n+    List<String> mountPaths = Arrays.asList(\"/mnt/u001\", \"/mnt/u002\", \"/mnt/u003\");\n+    // generate two peer replicas\n+    MockDataNodeId peerNode1 = new MockDataNodeId(\"node1_host\", Collections.singletonList(port), mountPaths, null);\n+    MockDataNodeId peerNode2 = new MockDataNodeId(\"node2_host\", Collections.singletonList(port), mountPaths, null);\n+    MockDataNodeId localNode = new MockDataNodeId(\"local_host\", Collections.singletonList(port), mountPaths, null);\n+    MockPartitionId mockPartitionId = new MockPartitionId(101L, MockClusterMap.DEFAULT_PARTITION_CLASS);\n+    MockReplicaId peerReplica1 = new MockReplicaId(port.getPort(), mockPartitionId, peerNode1, 0);\n+    MockReplicaId peerReplica2 = new MockReplicaId(port.getPort(), mockPartitionId, peerNode2, 1);\n+    MockReplicaId localReplica = new MockReplicaId(port.getPort(), mockPartitionId, localNode, 2);\n+    localReplica.setPeerReplicas(Arrays.asList(peerReplica1, peerReplica2));\n+    return Arrays.asList(localReplica, peerReplica1, peerReplica2);\n+  }\n+\n   private void testDuplicatePutsHelper() throws Exception {\n     // Construct a blob store, where there is a duplicate PUT indexValue in different log segment.\n     refreshState(false, false);\n@@ -2403,7 +2559,7 @@ private void testDuplicatePutsHelper() throws Exception {\n     long deleteReferenceTimeMs = state.time.milliseconds();\n     List<LogSegmentName> segmentsUnderCompaction = getLogSegments(0, state.index.getLogSegmentCount() - 1);\n     CompactionDetails details = new CompactionDetails(deleteReferenceTimeMs, segmentsUnderCompaction);\n-    compactor = getCompactor(state.log, DISK_IO_SCHEDULER);\n+    compactor = getCompactor(state.log, DISK_IO_SCHEDULER, null);\n     compactor.initialize(state.index);\n \n     long duplicateCount = compactor.getTgtMetrics().compactionTargetIndexDuplicateOnNonRecoveryCount.getCount();\n@@ -2441,25 +2597,27 @@ private void refreshState(boolean hardDeleteEnabled, boolean initState) throws E\n    * Gets an instance of {@link BlobStoreCompactor}.\n    * @param log the {@link Log} instance to use.\n    * @param ioScheduler the {@link DiskIOScheduler} instance to use.\n+   * @param remoteTokenTracker the {@link RemoteTokenTracker} instance to use.\n    * @return an instance of {@link BlobStoreCompactor}.\n    * @throws IOException\n    * @throws StoreException\n    */\n-  private BlobStoreCompactor getCompactor(Log log, DiskIOScheduler ioScheduler) throws IOException, StoreException {\n+  private BlobStoreCompactor getCompactor(Log log, DiskIOScheduler ioScheduler, RemoteTokenTracker remoteTokenTracker)\n+      throws IOException, StoreException {\n     closeOrExceptionInduced = false;\n     state.properties.put(\"store.compaction.enable.direct.io\", Boolean.toString(doDirectIO));\n     if (withUndelete) {\n       state.properties.put(\"store.compaction.filter\", \"IndexSegmentValidEntryWithUndelete\");\n     }\n-    state.properties.put(\"store.compaction.purge.expired.delete.tombstone\", Boolean.toString(purgeExpiredDelete));\n+    state.properties.put(\"store.compaction.purge.delete.tombstone\", Boolean.toString(purgeDeleteTombstone));\n     state.properties.put(StoreConfig.storeAlwaysEnableTargetIndexDuplicateCheckingName,\n         Boolean.toString(alwaysEnableTargetIndexDuplicateChecking));\n     StoreConfig config = new StoreConfig(new VerifiableProperties(state.properties));\n     metricRegistry = new MetricRegistry();\n     StoreMetrics metrics = new StoreMetrics(metricRegistry);\n     return new BlobStoreCompactor(tempDirStr, STORE_ID, STORE_KEY_FACTORY, config, metrics, metrics, ioScheduler,\n         StoreTestUtils.DEFAULT_DISK_SPACE_ALLOCATOR, log, state.time, state.sessionId, state.incarnationId,\n-        accountService);\n+        accountService, remoteTokenTracker);\n   }\n \n   /**\n@@ -2550,7 +2708,7 @@ private void writeDataToMeetRequiredSegmentCount(long countRequired, List<Long>\n   private long reduceValidDataSizeInLogSegments(List<LogSegmentName> logSegmentsToReduceFrom, long ceilingSize)\n       throws StoreException {\n     List<LogSegmentName> logSegments = new ArrayList<>(logSegmentsToReduceFrom);\n-    long validDataSize = getValidDataSize(logSegmentsToReduceFrom, state.time.milliseconds(), purgeExpiredDelete);\n+    long validDataSize = getValidDataSize(logSegmentsToReduceFrom, state.time.milliseconds(), purgeDeleteTombstone);\n     while (validDataSize > ceilingSize) {\n       assertTrue(\"There are no more segments to delete data from\", logSegments.size() > 0);\n       int selectedIdx = TestUtils.RANDOM.nextInt(logSegments.size());\n@@ -2560,7 +2718,7 @@ private long reduceValidDataSizeInLogSegments(List<LogSegmentName> logSegmentsTo\n         logSegments.remove(selectedIdx);\n       } else {\n         state.addDeleteEntry(idToDelete);\n-        validDataSize = getValidDataSize(logSegmentsToReduceFrom, state.time.milliseconds() + 1, purgeExpiredDelete);\n+        validDataSize = getValidDataSize(logSegmentsToReduceFrom, state.time.milliseconds() + 1, purgeDeleteTombstone);\n       }\n     }\n     state.advanceTime(Time.MsPerSec);\n@@ -2583,26 +2741,26 @@ private void compactAndVerify(List<LogSegmentName> segmentsUnderCompaction, long\n     long indexSegmentCountBeforeCompaction = state.index.getIndexSegments().size();\n \n     CompactionDetails details = new CompactionDetails(deleteReferenceTimeMs, segmentsUnderCompaction);\n-    long expectedValidDataSize = getValidDataSize(segmentsUnderCompaction, deleteReferenceTimeMs, purgeExpiredDelete);\n+    long expectedValidDataSize = getValidDataSize(segmentsUnderCompaction, deleteReferenceTimeMs, purgeDeleteTombstone);\n     List<LogSegmentName> unaffectedSegments = getUnaffectedSegments(segmentsUnderCompaction);\n     Pair<Set<MockId>, Set<MockId>> expiredDeletes = new Pair<>(new HashSet<>(), new HashSet<>());\n     List<LogEntry> validLogEntriesInOrder =\n-        getValidLogEntriesInOrder(segmentsUnderCompaction, deleteReferenceTimeMs, expiredDeletes, purgeExpiredDelete);\n+        getValidLogEntriesInOrder(segmentsUnderCompaction, deleteReferenceTimeMs, expiredDeletes, purgeDeleteTombstone);\n     Set<MockId> idsInCompactedLogSegments = getIdsWithPutInSegments(segmentsUnderCompaction);\n     // \"compactedDeletes\" are those tombstones that should be compacted in single run (if no exception occurs);\n     // \"deletesWithPuts\" are those tombstones temporarily with PUTs but may be eligible to be compacted in subsequent cycle\n     Set<MockId> compactedDeletes = expiredDeletes.getFirst();\n     Set<MockId> deletesWithPuts = expiredDeletes.getSecond();\n \n-    compactor = getCompactor(state.log, DISK_IO_SCHEDULER);\n+    compactor = getCompactor(state.log, DISK_IO_SCHEDULER, null);\n     compactor.initialize(state.index);\n \n     try {\n       compactor.compact(details, bundleReadBuffer);\n     } finally {\n       compactor.close(0);\n     }\n-    Set<MockId> remainingBlobIds = getCurrentBlobIdsFromWholeIndex(state.index, null, purgeExpiredDelete);\n+    Set<MockId> remainingBlobIds = getCurrentBlobIdsFromWholeIndex(state.index, null, purgeDeleteTombstone);\n     // since this method aims to verify success compaction case, we only need to account for some deletes with PUTs are\n     // compacted in the multi-cycle compaction (i.e. PUT is 1st log segment and gets compacted in 1st cycle. DELETE is\n     // in 2nd log segment and the 2nd cycle compaction may compact the DELETE as well because source index is updated\n@@ -2660,17 +2818,17 @@ private void compactWithRecoveryAndVerify(Log log, DiskIOScheduler diskIOSchedul\n     long indexSegmentCountBeforeCompaction = state.index.getIndexSegments().size();\n \n     CompactionDetails details = new CompactionDetails(deleteReferenceTimeMs, segmentsUnderCompaction);\n-    long expectedValidDataSize = getValidDataSize(segmentsUnderCompaction, deleteReferenceTimeMs, purgeExpiredDelete);\n+    long expectedValidDataSize = getValidDataSize(segmentsUnderCompaction, deleteReferenceTimeMs, purgeDeleteTombstone);\n     List<LogSegmentName> unaffectedSegments = getUnaffectedSegments(segmentsUnderCompaction);\n     // \"expiredDeletes\" has two parts: 1. expired delete without PUT; 2. expired delete with PUT.\n     Pair<Set<MockId>, Set<MockId>> expiredDeletes = new Pair<>(new HashSet<>(), new HashSet<>());\n     // get valid log entries including deletes as a backup (in case exception occurred in the middle of compaction)\n     List<LogEntry> validLogEntriesInOrder =\n-        getValidLogEntriesInOrder(segmentsUnderCompaction, deleteReferenceTimeMs, expiredDeletes, purgeExpiredDelete);\n+        getValidLogEntriesInOrder(segmentsUnderCompaction, deleteReferenceTimeMs, expiredDeletes, purgeDeleteTombstone);\n     Set<MockId> compactedDeletes = expiredDeletes.getFirst();\n     Set<MockId> deletesWithPuts = expiredDeletes.getSecond();\n     Set<MockId> idsInCompactedLogSegments = getIdsWithPutInSegments(segmentsUnderCompaction);\n-    compactor = getCompactor(log, diskIOScheduler);\n+    compactor = getCompactor(log, diskIOScheduler, null);\n     compactor.initialize(index);\n \n     try {\n@@ -2694,7 +2852,7 @@ private void compactWithRecoveryAndVerify(Log log, DiskIOScheduler diskIOSchedul\n     // have to reload log since the instance changed by the old compactor is different.\n     state.reloadLog(false);\n     // use the \"real\" log, index and disk IO schedulers this time.\n-    compactor = getCompactor(state.log, DISK_IO_SCHEDULER);\n+    compactor = getCompactor(state.log, DISK_IO_SCHEDULER, null);\n     state.initIndex(null);\n     compactor.initialize(state.index);\n     assertEquals(\"Wrong number of swap segments in use\",\n@@ -2706,7 +2864,7 @@ private void compactWithRecoveryAndVerify(Log log, DiskIOScheduler diskIOSchedul\n     } finally {\n       compactor.close(0);\n     }\n-    Set<MockId> remainingBlobIds = getCurrentBlobIdsFromWholeIndex(state.index, null, purgeExpiredDelete);\n+    Set<MockId> remainingBlobIds = getCurrentBlobIdsFromWholeIndex(state.index, null, purgeDeleteTombstone);\n     // find those deletes initially had PUTs but were compacted in the subsequent compaction cycle.\n     deletesWithPuts.removeAll(remainingBlobIds);\n \n@@ -3151,7 +3309,7 @@ private void verifySavedBytesCount(long logSegmentCountBeforeCompaction, long al\n    * @throws Exception\n    */\n   private void ensureArgumentFailure(CompactionDetails details, String msg) throws Exception {\n-    compactor = getCompactor(state.log, DISK_IO_SCHEDULER);\n+    compactor = getCompactor(state.log, DISK_IO_SCHEDULER, null);\n     compactor.initialize(state.index);\n     try {\n       compactor.compact(details, bundleReadBuffer);\n@@ -3429,7 +3587,7 @@ private void doInterruptionDuringRecordCopyTest() throws Exception {\n         // while copying each index segment, the bytes copied for the last record is not reported to the diskIOScheduler\n         long unreported = getIndexSegmentStartOffsetsForLogSegments(segmentsUnderCompaction).size() * PUT_RECORD_SIZE;\n         countToInterruptAt +=\n-            getValidDataSize(segmentsUnderCompaction, state.time.milliseconds(), purgeExpiredDelete) - unreported;\n+            getValidDataSize(segmentsUnderCompaction, state.time.milliseconds(), purgeDeleteTombstone) - unreported;\n       }\n       // create a DiskIOScheduler\n       DiskIOScheduler diskIOScheduler = new InterruptionInducingDiskIOScheduler(Integer.MAX_VALUE, countToInterruptAt);\n@@ -3446,7 +3604,7 @@ private void doInterruptionDuringRecordCopyTest() throws Exception {\n         // while copying each index segment, the bytes copied for the last record is not reported to the diskIOScheduler\n         long unreported = getIndexSegmentStartOffsetsForLogSegments(segmentsUnderCompaction).size() * PUT_RECORD_SIZE;\n         countToInterruptAt +=\n-            getValidDataSize(segmentsUnderCompaction, state.time.milliseconds(), purgeExpiredDelete) - unreported;\n+            getValidDataSize(segmentsUnderCompaction, state.time.milliseconds(), purgeDeleteTombstone) - unreported;\n       }\n       diskIOScheduler = new InterruptionInducingDiskIOScheduler(Integer.MAX_VALUE, countToInterruptAt);\n       compactWithRecoveryAndVerify(state.log, diskIOScheduler, state.index, segmentsUnderCompaction,"
  },
  {
    "sha": "1abe7d1d7f75f4c4ea0c168938be3315c2166b35",
    "filename": "ambry-store/src/test/java/com/github/ambry/store/BlobStoreTest.java",
    "status": "modified",
    "additions": 4,
    "deletions": 4,
    "changes": 8,
    "blob_url": "https://github.com/linkedin/ambry/blob/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-store/src/test/java/com/github/ambry/store/BlobStoreTest.java",
    "raw_url": "https://github.com/linkedin/ambry/raw/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-store/src/test/java/com/github/ambry/store/BlobStoreTest.java",
    "contents_url": "https://api.github.com/repos/linkedin/ambry/contents/ambry-store/src/test/java/com/github/ambry/store/BlobStoreTest.java?ref=02d7236e4257ab44463ff42c6269d07713b051bd",
    "patch": "@@ -1687,7 +1687,7 @@ public void putIdCollisionTest() throws Exception {\n   }\n \n   /**\n-   * Tests {@link BlobStore#findEntriesSince(FindToken, long)}.\n+   * Tests {@link Store#findEntriesSince(FindToken, long, String, String)}.\n    * <p/>\n    * This test is minimal for two reasons\n    * 1. The BlobStore simply calls into the index for this function and the index has extensive tests for this.\n@@ -1697,7 +1697,7 @@ public void putIdCollisionTest() throws Exception {\n    */\n   @Test\n   public void findEntriesSinceTest() throws StoreException {\n-    FindInfo findInfo = store.findEntriesSince(new StoreFindToken(), Long.MAX_VALUE);\n+    FindInfo findInfo = store.findEntriesSince(new StoreFindToken(), Long.MAX_VALUE, null, null);\n     Set<StoreKey> keysPresent = new HashSet<>();\n     for (MessageInfo info : findInfo.getMessageEntries()) {\n       keysPresent.add(info.getStoreKey());\n@@ -1707,7 +1707,7 @@ public void findEntriesSinceTest() throws StoreException {\n     // Extra Test: findEntriesSince method can correctly capture disk related IO error and shutdown store if needed.\n     store.shutdown();\n     catchStoreExceptionAndVerifyErrorCode(\n-        (blobStore) -> blobStore.findEntriesSince(new StoreFindToken(), Long.MAX_VALUE));\n+        (blobStore) -> blobStore.findEntriesSince(new StoreFindToken(), Long.MAX_VALUE, null, null));\n     reloadStore();\n   }\n \n@@ -3232,7 +3232,7 @@ private void verifyOperationFailuresOnInactiveStore(BlobStore blobStore) {\n       assertEquals(\"Unexpected StoreErrorCode\", StoreErrorCodes.Store_Not_Started, e.getErrorCode());\n     }\n     try {\n-      blobStore.findEntriesSince(new StoreFindToken(), Long.MAX_VALUE);\n+      blobStore.findEntriesSince(new StoreFindToken(), Long.MAX_VALUE, null, null);\n       fail(\"Operation should have failed because store is inactive\");\n     } catch (StoreException e) {\n       assertEquals(\"Unexpected StoreErrorCode\", StoreErrorCodes.Store_Not_Started, e.getErrorCode());"
  },
  {
    "sha": "9fb2b554ba6258fc3698a6f2d98173c7ee73e75d",
    "filename": "ambry-store/src/test/java/com/github/ambry/store/CuratedLogIndexState.java",
    "status": "modified",
    "additions": 2,
    "deletions": 0,
    "changes": 2,
    "blob_url": "https://github.com/linkedin/ambry/blob/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-store/src/test/java/com/github/ambry/store/CuratedLogIndexState.java",
    "raw_url": "https://github.com/linkedin/ambry/raw/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-store/src/test/java/com/github/ambry/store/CuratedLogIndexState.java",
    "contents_url": "https://api.github.com/repos/linkedin/ambry/contents/ambry-store/src/test/java/com/github/ambry/store/CuratedLogIndexState.java?ref=02d7236e4257ab44463ff42c6269d07713b051bd",
    "patch": "@@ -136,6 +136,7 @@\n   MetricRegistry metricRegistry = new MetricRegistry();\n   // The deleted key with associated PUT record in the same log segment\n   MockId deletedKeyWithPutInSameSegment = null;\n+  List<MockId> permanentDeleteTombstones = new ArrayList<>();\n \n   // Variables that represent the folder where the data resides\n   private final File tempDir;\n@@ -1412,6 +1413,7 @@ private void addCuratedIndexEntriesToLogSegment(long sizeToMakeIndexEntriesFor,\n       }\n       // 1 DELETE that has the TTL update flag set but has no corresponding TTL update or PUT entries\n       uniqueId = getUniqueId();\n+      permanentDeleteTombstones.add(uniqueId);\n       addDeleteEntry(uniqueId,\n           new MessageInfo(uniqueId, Integer.MAX_VALUE, true, true, Utils.Infinite_Time, uniqueId.getAccountId(),\n               uniqueId.getContainerId(), time.milliseconds()));"
  },
  {
    "sha": "fd79f34bb7fbb390b5815c2438fc16aaf581011c",
    "filename": "ambry-store/src/test/java/com/github/ambry/store/StoreTestUtils.java",
    "status": "modified",
    "additions": 2,
    "deletions": 1,
    "changes": 3,
    "blob_url": "https://github.com/linkedin/ambry/blob/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-store/src/test/java/com/github/ambry/store/StoreTestUtils.java",
    "raw_url": "https://github.com/linkedin/ambry/raw/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-store/src/test/java/com/github/ambry/store/StoreTestUtils.java",
    "contents_url": "https://api.github.com/repos/linkedin/ambry/contents/ambry-store/src/test/java/com/github/ambry/store/StoreTestUtils.java?ref=02d7236e4257ab44463ff42c6269d07713b051bd",
    "patch": "@@ -29,6 +29,7 @@\n import java.io.IOException;\n import java.nio.file.Files;\n import java.util.Collection;\n+import java.util.Collections;\n import java.util.List;\n import java.util.Properties;\n import org.json.JSONObject;\n@@ -110,7 +111,7 @@ public String getReplicaPath() {\n \n     @Override\n     public List<? extends ReplicaId> getPeerReplicaIds() {\n-      return null;\n+      return Collections.emptyList();\n     }\n \n     @Override"
  },
  {
    "sha": "c3821377f073c2488bc6d4884f7f9b4741f47868",
    "filename": "ambry-test-utils/src/main/java/com/github/ambry/clustermap/MockReplicaId.java",
    "status": "modified",
    "additions": 2,
    "deletions": 2,
    "changes": 4,
    "blob_url": "https://github.com/linkedin/ambry/blob/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-test-utils/src/main/java/com/github/ambry/clustermap/MockReplicaId.java",
    "raw_url": "https://github.com/linkedin/ambry/raw/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-test-utils/src/main/java/com/github/ambry/clustermap/MockReplicaId.java",
    "contents_url": "https://api.github.com/repos/linkedin/ambry/contents/ambry-test-utils/src/main/java/com/github/ambry/clustermap/MockReplicaId.java?ref=02d7236e4257ab44463ff42c6269d07713b051bd",
    "patch": "@@ -27,7 +27,7 @@\n   private static final String REPLICA_FILE_PREFIX = \"replica\";\n   private String mountPath;\n   private String replicaPath;\n-  private List<ReplicaId> peerReplicas;\n+  private List<ReplicaId> peerReplicas = new ArrayList<>();\n   private MockPartitionId partitionId;\n   private MockDataNodeId dataNodeId;\n   private MockDiskId diskId;\n@@ -90,7 +90,7 @@ public String getReplicaPath() {\n   }\n \n   public void setPeerReplicas(List<ReplicaId> peerReplicas) {\n-    this.peerReplicas = new ArrayList<>();\n+    this.peerReplicas.clear();\n     for (ReplicaId replicaId : peerReplicas) {\n       if (!Objects.equals(mountPath, replicaId.getMountPath())) {\n         this.peerReplicas.add(replicaId);"
  },
  {
    "sha": "4f2632c9aa2358c7f05698d8b272dec3b3686939",
    "filename": "ambry-tools/src/main/java/com/github/ambry/store/StoreCopier.java",
    "status": "modified",
    "additions": 3,
    "deletions": 3,
    "changes": 6,
    "blob_url": "https://github.com/linkedin/ambry/blob/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-tools/src/main/java/com/github/ambry/store/StoreCopier.java",
    "raw_url": "https://github.com/linkedin/ambry/raw/02d7236e4257ab44463ff42c6269d07713b051bd/ambry-tools/src/main/java/com/github/ambry/store/StoreCopier.java",
    "contents_url": "https://api.github.com/repos/linkedin/ambry/contents/ambry-tools/src/main/java/com/github/ambry/store/StoreCopier.java?ref=02d7236e4257ab44463ff42c6269d07713b051bd",
    "patch": "@@ -53,8 +53,8 @@\n  * made before copying\n  * <p/>\n  * This tool requires the source store to *not* return blobs that have already been deleted when\n- * {@link Store#findEntriesSince(FindToken, long)} is called. It is also expected to be run when both locations (src and\n- * tgt) are offline.\n+ * {@link Store#findEntriesSince(FindToken, long, String, String)} is called. It is also expected to be run when both\n+ * locations (src and tgt) are offline.\n  */\n public class StoreCopier implements Closeable {\n \n@@ -204,7 +204,7 @@ public void close() throws IOException {\n     FindToken token = startToken;\n     do {\n       lastToken = token;\n-      FindInfo findInfo = src.findEntriesSince(lastToken, fetchSizeInBytes);\n+      FindInfo findInfo = src.findEntriesSince(lastToken, fetchSizeInBytes, null, null);\n       List<MessageInfo> messageInfos = findInfo.getMessageEntries();\n       for (Transformer transformer : transformers) {\n         transformer.warmup(messageInfos);"
  }
]
