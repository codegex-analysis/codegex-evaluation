[
  {
    "sha": "ef60462fbec019f11c3195c5b6aaa3f7c0bae6b6",
    "filename": "zookeeper-server/pom.xml",
    "status": "modified",
    "additions": 48,
    "deletions": 1,
    "changes": 49,
    "blob_url": "https://github.com/SJSU-CS-systems-group/zookeeper/blob/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/pom.xml",
    "raw_url": "https://github.com/SJSU-CS-systems-group/zookeeper/raw/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/pom.xml",
    "contents_url": "https://api.github.com/repos/SJSU-CS-systems-group/zookeeper/contents/zookeeper-server/pom.xml?ref=ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7",
    "patch": "@@ -30,7 +30,9 @@\n   <packaging>jar</packaging>\n   <name>Apache ZooKeeper - Server</name>\n   <description>ZooKeeper server</description>\n-\n+  <properties>\n+    <skipTests>true</skipTests>\n+  </properties>\n   <dependencies>\n     <dependency>\n       <groupId>com.github.spotbugs</groupId>\n@@ -172,9 +174,26 @@\n       <artifactId>snappy-java</artifactId>\n       <scope>provided</scope>\n     </dependency>\n+    <dependency>\n+      <groupId>io.grpc</groupId>\n+      <artifactId>grpc-all</artifactId>\n+      <version>1.23.0</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>javax.annotation</groupId>\n+      <artifactId>javax.annotation-api</artifactId>\n+      <version>1.3.2</version>\n+    </dependency>\n   </dependencies>\n \n   <build>\n+    <extensions>\n+      <extension>\n+        <groupId>kr.motd.maven</groupId>\n+        <artifactId>os-maven-plugin</artifactId>\n+        <version>1.6.2</version>\n+      </extension>\n+    </extensions>\n     <plugins>\n       <plugin>\n         <groupId>org.codehaus.mojo</groupId>\n@@ -284,6 +303,34 @@\n           </execution>\n         </executions>\n       </plugin>\n+      <plugin>\n+        <groupId>org.xolstice.maven.plugins</groupId>\n+        <artifactId>protobuf-maven-plugin</artifactId>\n+        <version>0.6.1</version>\n+        <configuration>\n+\n+          <protocArtifact>com.google.protobuf:protoc:3.9.0:exe:${os.detected.classifier}</protocArtifact>\n+          <pluginId>grpc-java</pluginId>\n+\n+          <pluginArtifact>io.grpc:protoc-gen-grpc-java:1.23.0:exe:${os.detected.classifier}</pluginArtifact>\n+        </configuration>\n+        <executions>\n+          <execution>\n+            <phase>generate-sources</phase>\n+            <goals>\n+              <goal>compile</goal>\n+              <goal>compile-custom</goal>\n+            </goals>\n+          </execution>\n+          <!--execution>\n+            <phase>generate-sources</phase>\n+            <id>gen-witness-service</id>\n+            <goals>\n+              <goal>gen-witness-service</goal>\n+            </goals>\n+          </execution-->\n+        </executions>\n+      </plugin>\n      </plugins>\n   </build>\n "
  },
  {
    "sha": "95c8dea81ee75d917846c62d99f6561e876675e4",
    "filename": "zookeeper-server/src/main/java/org/apache/zookeeper/server/SyncRequestProcessor.java",
    "status": "modified",
    "additions": 64,
    "deletions": 4,
    "changes": 68,
    "blob_url": "https://github.com/SJSU-CS-systems-group/zookeeper/blob/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/SyncRequestProcessor.java",
    "raw_url": "https://github.com/SJSU-CS-systems-group/zookeeper/raw/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/SyncRequestProcessor.java",
    "contents_url": "https://api.github.com/repos/SJSU-CS-systems-group/zookeeper/contents/zookeeper-server/src/main/java/org/apache/zookeeper/server/SyncRequestProcessor.java?ref=ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7",
    "patch": "@@ -20,15 +20,15 @@\n \n import java.io.Flushable;\n import java.io.IOException;\n-import java.util.ArrayDeque;\n-import java.util.Objects;\n-import java.util.Queue;\n+import java.util.*;\n import java.util.concurrent.BlockingQueue;\n import java.util.concurrent.LinkedBlockingQueue;\n import java.util.concurrent.Semaphore;\n import java.util.concurrent.ThreadLocalRandom;\n import java.util.concurrent.TimeUnit;\n import org.apache.zookeeper.common.Time;\n+import org.apache.zookeeper.server.quorum.LeaderZooKeeperServer;\n+import org.apache.zookeeper.server.quorum.WitnessHandler;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -81,8 +81,10 @@\n      * Transactions that have been written and are waiting to be flushed to\n      * disk. Basically this is the list of SyncItems whose callbacks will be\n      * invoked after flush returns successfully.\n+     * Changed declaration of toFlush from Queue to Deque, since we are initializing it as an ArrayDequeue. And we need the ability to get\n+     * the first and last element of the queue for send the request to the witness.\n      */\n-    private final Queue<Request> toFlush;\n+    private final Deque<Request> toFlush;\n     private long lastFlushTime;\n \n     public SyncRequestProcessor(ZooKeeperServer zks, RequestProcessor nextProcessor) {\n@@ -152,6 +154,27 @@ private void resetSnapshotStats() {\n         randSize = Math.abs(ThreadLocalRandom.current().nextLong() % (snapSizeInBytes / 2));\n     }\n \n+    /***\n+     * Priyatham Notes: When proccessRequest() is invoked on this SyncRequestProcessor, the request added to the queuedRequests queue.\n+     * In this run method loop, queuedRequests is polled and the Requests are appended to the Log. But, the log is not written to the disk immediately.\n+     * Multiple requests are batched together either based on elapsed time or batchSize and committed at once. Committed means writing the log to the disk.\n+     * This happens in the flush() method call. Once committed, all the requests in the current batch are iterated over and sent to the nextProcessor one by one.\n+     * In case of Leader, the nextRequestProcessor is ACKProcessor. Where the leader self ACKs each request.\n+     *\n+     * So related to Witness's Implementation:\n+     * We send a Request to the witness in SyncRequestProcessor (i.e In proposal stage), only when the witness is active\n+     * And We guarantee that, we send a proposal to a witness at least after it is written to the log of the leader.\n+     * So, I can queue a Request to the WitnessHandler any time after the zks.getZKDatabase().commit(); call in flush() method.\n+     *\n+     *Choice or Decision Required for the following:\n+     * Option 1: Send each request/proposal in the batch to the witness.\n+     * Option 2: Send only the last request in the batch to the witness and use the ACK sent by witness for the last request as an indirect ACK for requests in that batch.\n+     *   Op2 Impl Approach1: Augment WitnessRequestObject with batchStartZxid field. So when we create WitnessRequest, populate both batchStartZxid and Zxid of last request. Once\n+     *   ACK is recieved from witness for that request, WH will invoke processACK() on request from batchStartZxid to Zxid.\n+     *   Op2 Total Time Taken for Leader to ACK a request would be: timeWaitedToFormBatch + TimeToWriteToDisk + TimeTakenByWitHandlerToWriteToWitness(assuming there are other requests in the WH request queue, before this request)\n+     *   Op2 Analysis on time taken: When witness is not used, the total time taken would still be (timeWaitedToFormBatch + TimeToWriteToDisk). By introducing witness, we\n+     *   additionally add TimeTakenByWitHandlerToWriteToWitness. We should take this additional time into consideration when setting any related time limits.\n+     * */\n     @Override\n     public void run() {\n         try {\n@@ -238,6 +261,7 @@ private void flush() throws IOException, RequestProcessorException {\n         if (this.nextProcessor == null) {\n             this.toFlush.clear();\n         } else {\n+            sendRequestToWitness();\n             while (!this.toFlush.isEmpty()) {\n                 final Request i = this.toFlush.remove();\n                 long latency = Time.currentElapsedTime() - i.syncQueueStartTime;\n@@ -278,4 +302,40 @@ public void processRequest(final Request request) {\n         ServerMetrics.getMetrics().SYNC_PROCESSOR_QUEUED.add(1);\n     }\n \n+    /**\n+     * We attempt to send a request to a witness only when\n+     *  - the peer is a leader,\n+     *  - there are some writes that needs to be sent\n+     *  - there is a witness configured\n+     *  - and the witness is active.\n+     *\n+     *  Note: the code behaves as if there are multiple witnesses, and queues requests only to active witnesses.\n+     *  In actual, we will have only a single witness..\n+     *  I still have to investigate the Potential Complexities in having multiple witnesses.\n+     * */\n+    private void sendRequestToWitness() {\n+        if(this.zks instanceof LeaderZooKeeperServer && !this.toFlush.isEmpty()) {\n+            LeaderZooKeeperServer lzks = (LeaderZooKeeperServer)this.zks;\n+            List<WitnessHandler> witnessHandlerList = lzks.getLeader().getWitnesses();\n+            //If there are witnesses\n+            if(witnessHandlerList.size() != 0) {\n+                LOG.info(\"We have {} witnesses \", witnessHandlerList.size());\n+                long firstRequestZxid = this.toFlush.peekFirst().zxid;\n+                long lastRequestZxid = this.toFlush.peekLast().zxid;\n+                //WitnessHandler.WitnessRequest witnessRequest = new WitnessHandler.WitnessRequest()\n+                for(WitnessHandler wh : witnessHandlerList) {\n+                    if(wh.isActive()) {\n+                        LOG.info(\"Witness {} is active, queueing zxid batch {}(firstZxid) - {}(lastZxid)\", wh.getSid(), Long.toHexString(firstRequestZxid), Long.toHexString(lastRequestZxid));\n+                        WitnessHandler.WitnessRequest wr = new WitnessHandler.WitnessRequest(lastRequestZxid, firstRequestZxid, true);\n+                        wh.queueRequest(wr);\n+                    }\n+                    else {\n+                        LOG.info(\"Witness {} is passive, so not sending proposal at this stage.\");\n+                    }\n+                }\n+            }\n+\n+        }\n+    }\n+\n }"
  },
  {
    "sha": "a106a9813c5ca5476f0a0bb511da654f83c5e2a2",
    "filename": "zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/FastLeaderElection.java",
    "status": "modified",
    "additions": 341,
    "deletions": 40,
    "changes": 381,
    "blob_url": "https://github.com/SJSU-CS-systems-group/zookeeper/blob/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/FastLeaderElection.java",
    "raw_url": "https://github.com/SJSU-CS-systems-group/zookeeper/raw/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/FastLeaderElection.java",
    "contents_url": "https://api.github.com/repos/SJSU-CS-systems-group/zookeeper/contents/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/FastLeaderElection.java?ref=ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7",
    "patch": "@@ -22,11 +22,12 @@\n import java.io.IOException;\n import java.nio.BufferUnderflowException;\n import java.nio.ByteBuffer;\n-import java.util.HashMap;\n-import java.util.Map;\n+import java.util.*;\n import java.util.concurrent.LinkedBlockingQueue;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicLong;\n+\n+import org.apache.zookeeper.Environment;\n import org.apache.zookeeper.common.Time;\n import org.apache.zookeeper.jmx.MBeanRegistry;\n import org.apache.zookeeper.server.ZooKeeperThread;\n@@ -200,6 +201,75 @@\n \n     }\n \n+    /**\n+     * This class provides constructs to cache witness's vote and retrieve a\n+     * cached vote safely.\n+     * I assumed, that there can be multiple witnesses in an ensemble..\n+     * */\n+    public static class WitnessNotificationCache {\n+        //Map<epoch, Map<witnessId, Vote>>\n+        private Map<Long, Map<Long, Notification>> actualIndex;\n+        //Map<witnessId, epoch>\n+        private Map<Long, Long> reverseIndex;\n+\n+        public WitnessNotificationCache() {\n+            actualIndex = new TreeMap<>();\n+            reverseIndex = new HashMap<>();\n+        }\n+\n+        public void cacheWitnessNotification(Notification notification) {\n+            //assuming that a witness always sends notifications with epoch values which only increase or stay the same.\n+            //first, clean up any previously cached vote for this witness.\n+            if(reverseIndex.containsKey(notification.sid)) {\n+                long cachedEpoch = reverseIndex.get(notification.sid);\n+                Map<Long, Notification> notMap = actualIndex.get(cachedEpoch);\n+                notMap.remove(notification.sid);\n+                reverseIndex.remove(notification.sid);\n+            }\n+            if(!actualIndex.containsKey(notification.electionEpoch)) {\n+                actualIndex.put(notification.electionEpoch, new TreeMap<>());\n+            }\n+            //now update the actualIndex and reverse index.\n+            Map<Long, Notification> widNotMap = actualIndex.get(notification.electionEpoch);\n+            widNotMap.put(notification.sid, notification);\n+            reverseIndex.put(notification.sid, notification.electionEpoch);\n+        }\n+\n+        private List<Notification> getCachedNotifications(long epoch, ServerState state) {\n+            if(!actualIndex.containsKey(epoch) || actualIndex.get(epoch).size() == 0) {\n+                return new ArrayList<Notification>();\n+            }\n+            Map<Long, Notification> witNotMap = actualIndex.get(epoch);\n+            List<Notification> notifications = new ArrayList<>();\n+            List<Long> toRemove = new ArrayList<>();\n+            for(Map.Entry<Long, Notification> entry :witNotMap.entrySet()) {\n+                if(entry.getValue().state == state){\n+                    notifications.add(entry.getValue());\n+                    toRemove.add(entry.getKey());\n+                }\n+            }\n+            for(Long wid: toRemove) {\n+                witNotMap.remove(wid);\n+            }\n+\n+            List<Long> epochs = new ArrayList<>(actualIndex.keySet());\n+            for(int i=0; epochs.get(i) < epoch; i++) {\n+                Map<Long, Notification> map = actualIndex.get(epochs.get(i));\n+                for(Long wid: map.keySet()) {\n+                    reverseIndex.remove(wid);\n+                }\n+                actualIndex.remove(epochs.get(i));\n+            }\n+            return notifications;\n+        }\n+\n+        public void getCachedNotifications(long epoch, ServerState state, Map<Long, Vote> targetSet) {\n+            for(Notification n: getCachedNotifications(epoch, state)) {\n+                targetSet.put(n.sid, new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch));\n+            }\n+        }\n+    }\n+\n     LinkedBlockingQueue<ToSend> sendqueue;\n     LinkedBlockingQueue<Notification> recvqueue;\n \n@@ -339,7 +409,7 @@ public void run() {\n                          * If it is from a non-voting server (such as an observer or\n                          * a non-voting follower), respond right away.\n                          */\n-                        if (!validVoter(response.sid)) {\n+                        if (!validVoter(response.sid) && !self.isWitness(response.sid)) {\n                             Vote current = self.getCurrentVote();\n                             QuorumVerifier qv = self.getQuorumVerifier();\n                             ToSend notmsg = new ToSend(\n@@ -353,37 +423,62 @@ public void run() {\n                                 qv.toString().getBytes(UTF_8));\n \n                             sendqueue.offer(notmsg);\n-                        } else {\n+                        } else if(self.isWitness(response.sid) && isInitialVote(rleader, rzxid, rpeerepoch)) {\n+                            LOG.info(\"Received initial empty vote from witness {}\", response.sid);\n+                            QuorumVerifier qv = self.getQuorumVerifier();\n+                            ToSend notmsg = null;\n+                            //send the current proposal if the peer is in Looking..\n+                            if(self.getPeerState() == ServerState.LOOKING) {\n+                                LOG.info(\"I am also LOOKING, sending my current proposal\");\n+                                if(relectionEpoch > logicalclock.get()) {\n+                                    LOG.info(\"Witness's election epoch {} > my electionEpoch {}. So updating my logical clock and resending the notifications to all the peers\");\n+                                    QuorumPeer.ServerState ackstate = determineServerState(rstate);\n+                                    if(ackstate == null) {\n+                                        continue;\n+                                    }\n+                                    populateNotification(n, rleader, rzxid, relectionEpoch, ackstate, response.sid, rpeerepoch, version, rqv);\n+                                    recvqueue.offer(n);\n+                                }\n+                                else {\n+                                    notmsg = new ToSend(\n+                                            ToSend.mType.notification,\n+                                            proposedLeader,\n+                                            proposedZxid,\n+                                            logicalclock.get(),\n+                                            QuorumPeer.ServerState.LOOKING,\n+                                            response.sid, //sid of the target server.\n+                                            proposedEpoch,\n+                                            qv.toString().getBytes());\n+                                }\n+                            } else {\n+                                //send the current vote, just like the above peiece of code\n+                                LOG.info(\"I am {}, so sending my current vote\", self.getPeerState());\n+                                Vote current = self.getCurrentVote();\n+                                notmsg = new ToSend(\n+                                        ToSend.mType.notification,\n+                                        current.getId(),\n+                                        current.getZxid(),\n+                                        logicalclock.get(),\n+                                        self.getPeerState(),\n+                                        response.sid,\n+                                        current.getPeerEpoch(),\n+                                        qv.toString().getBytes());\n+                            }\n+                            if(notmsg != null)\n+                            sendqueue.offer(notmsg);\n+                        }\n+                        else {\n                             // Receive new message\n-                            LOG.debug(\"Receive new notification message. My id = {}\", self.getId());\n+                            // Note: In the reciever, a non-empty notification message sent by a witness, is treated in the same manner\n+                            // as a notification sent by a normal zookeeper server.\n+                            LOG.info(\"Receive new notification message. My id = {}\", self.getId());\n \n                             // State of peer that sent this message\n-                            QuorumPeer.ServerState ackstate = QuorumPeer.ServerState.LOOKING;\n-                            switch (rstate) {\n-                            case 0:\n-                                ackstate = QuorumPeer.ServerState.LOOKING;\n-                                break;\n-                            case 1:\n-                                ackstate = QuorumPeer.ServerState.FOLLOWING;\n-                                break;\n-                            case 2:\n-                                ackstate = QuorumPeer.ServerState.LEADING;\n-                                break;\n-                            case 3:\n-                                ackstate = QuorumPeer.ServerState.OBSERVING;\n-                                break;\n-                            default:\n+                            QuorumPeer.ServerState ackstate = determineServerState(rstate);\n+                            if(ackstate == null) {\n                                 continue;\n                             }\n-\n-                            n.leader = rleader;\n-                            n.zxid = rzxid;\n-                            n.electionEpoch = relectionEpoch;\n-                            n.state = ackstate;\n-                            n.sid = response.sid;\n-                            n.peerEpoch = rpeerepoch;\n-                            n.version = version;\n-                            n.qv = rqv;\n+                            populateNotification(n, rleader, rzxid, relectionEpoch, ackstate, response.sid, rpeerepoch, version, rqv);\n                             /*\n                              * Print notification info\n                              */\n@@ -463,6 +558,25 @@ public void run() {\n                                         qv.toString().getBytes());\n                                     sendqueue.offer(notmsg);\n                                 }\n+\n+                                /**\n+                                 * If you are leading and the witness sends you a notification saying that it voted for you (this is witness's way of\n+                                 * indirectly notifying the leader that its leader election has concluded and it has reached a quorum that you are the\n+                                 * leader for this epoch),\n+                                 * then instruct the witness handler manager to create witness handler to communicate with this witness.\n+                                 * */\n+                                if(self.getPeerState() == ServerState.LEADING &&\n+                                        self.isWitness(response.sid) && ackstate == ServerState.FOLLOWING) {\n+                                    Vote currentVote = self.getCurrentVote();\n+                                    if(rleader == self.getId()\n+                                            && currentVote.getElectionEpoch() == relectionEpoch\n+                                            && currentVote.getZxid() == rzxid) {\n+                                        //Tell the witness handler manager to create a wintess handler for this witness, if it is not already created\n+                                        LOG.info(\"Requesting the witness handler manager to create a wintess handler for this witness: {}, if it is not already created\", response.sid);\n+                                        self.requestWitnessHandlerCreation(response.sid);\n+                                    }\n+                                }\n+\n                             }\n                         }\n                     } catch (InterruptedException e) {\n@@ -472,6 +586,31 @@ public void run() {\n                 LOG.info(\"WorkerReceiver is down\");\n             }\n \n+            QuorumPeer.ServerState determineServerState(int rstate){\n+                switch (rstate) {\n+                    case 0:\n+                        return QuorumPeer.ServerState.LOOKING;\n+                    case 1:\n+                        return QuorumPeer.ServerState.FOLLOWING;\n+                    case 2:\n+                        return QuorumPeer.ServerState.LEADING;\n+                    case 3:\n+                        return QuorumPeer.ServerState.OBSERVING;\n+                    default:\n+                        return null;\n+                }\n+            }\n+\n+            void populateNotification(FastLeaderElection.Notification n, long rleader, long rzxid, long relectionEpoch, QuorumPeer.ServerState ackstate, long sid, long rpeerepoch, int version, QuorumVerifier rqv) {\n+                n.leader = rleader;\n+                n.zxid = rzxid;\n+                n.electionEpoch = relectionEpoch;\n+                n.state = ackstate;\n+                n.sid = sid;\n+                n.peerEpoch = rpeerepoch;\n+                n.version = version;\n+                n.qv = rqv;\n+            }\n         }\n \n         /**\n@@ -615,6 +754,10 @@ static ByteBuffer buildMsg(int state, long leader, long zxid, long electionEpoch\n         return requestBuffer;\n     }\n \n+    boolean isInitialVote(long proposedLeader, long proposedZxid, long peerEpoch) {\n+        return  proposedLeader == Long.MIN_VALUE && proposedZxid == Long.MIN_VALUE && peerEpoch == Long.MIN_VALUE;\n+    }\n+\n     /**\n      * Constructor of FastLeaderElection. It takes two parameters, one\n      * is the QuorumPeer object that instantiated this object, and the other\n@@ -688,7 +831,10 @@ public void shutdown() {\n      * Send notifications to all peers upon a change in our vote\n      */\n     private void sendNotifications() {\n-        for (long sid : self.getCurrentAndNextConfigVoters()) {\n+        Set<Long> peers = new HashSet<>(self.getCurrentAndNextConfigVoters());\n+        peers.addAll(self.getCurrentAndNextConfigWitnesses());\n+\n+        for (long sid : peers) {\n             QuorumVerifier qv = self.getQuorumVerifier();\n             ToSend notmsg = new ToSend(\n                 ToSend.mType.notification,\n@@ -935,6 +1081,13 @@ public Vote lookForLeader() throws InterruptedException {\n              */\n             Map<Long, Vote> outofelection = new HashMap<Long, Vote>();\n \n+            //WitnessNotificationCache witNotCache = null;\n+            boolean witnessPresent = false;\n+            if(self.isWitnessPresent()) {\n+                witnessPresent = true;\n+                //witNotCache = new WitnessNotificationCache();\n+            }\n+\n             int notTimeout = minNotificationInterval;\n \n             synchronized (this) {\n@@ -977,12 +1130,14 @@ public Vote lookForLeader() throws InterruptedException {\n                      */\n                     int tmpTimeOut = notTimeout * 2;\n                     notTimeout = Math.min(tmpTimeOut, maxNotificationInterval);\n-                    LOG.info(\"Notification time out: {} ms\", notTimeout);\n-                } else if (validVoter(n.sid) && validVoter(n.leader)) {\n+\n+                    LOG.info(\"Notification time out: {}\", notTimeout);\n+                } else if ((validVoter(n.sid) || self.isWitness(n.sid) )&& validVoter(n.leader)) {\n                     /*\n                      * Only proceed if the vote comes from a replica in the current or next\n                      * voting view for a replica in the current or next voting view.\n                      */\n+                    boolean isWitness = self.isWitness(n.sid);\n                     switch (n.state) {\n                     case LOOKING:\n                         if (getInitLastLoggedZxid() == -1) {\n@@ -995,21 +1150,32 @@ public Vote lookForLeader() throws InterruptedException {\n                         }\n                         // If notification > current, replace and send messages out\n                         if (n.electionEpoch > logicalclock.get()) {\n+                            /**\n+                             * If the witness is at a higher election epoch,\n+                             * you update your election epoch and restart the election by\n+                             * becoming a candidate again and telling your peers about it.\n+                             * Note that, you just updated your election epoch, you DID NOT do a predicate check with the witness's vote.\n+                             * This is consistent with our claim that we just count a witness's vote, but don't change our vote based on vote from the witness.\n+                             * */\n                             logicalclock.set(n.electionEpoch);\n                             recvset.clear();\n-                            if (totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, getInitId(), getInitLastLoggedZxid(), getPeerEpoch())) {\n+                            if (!isWitness && totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, getInitId(), getInitLastLoggedZxid(), getPeerEpoch())) {\n                                 updateProposal(n.leader, n.zxid, n.peerEpoch);\n                             } else {\n                                 updateProposal(getInitId(), getInitLastLoggedZxid(), getPeerEpoch());\n                             }\n                             sendNotifications();\n+                            if(isWitness && isInitialVote(n.leader, n.zxid, n.peerEpoch)) {\n+                                //If this is an initial/empty vote notification from the witness, you dont count it.\n+                                continue;\n+                            }\n                         } else if (n.electionEpoch < logicalclock.get()) {\n                                 LOG.debug(\n                                     \"Notification election epoch is smaller than logicalclock. n.electionEpoch = 0x{}, logicalclock=0x{}\",\n                                     Long.toHexString(n.electionEpoch),\n                                     Long.toHexString(logicalclock.get()));\n                             break;\n-                        } else if (totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, proposedLeader, proposedZxid, proposedEpoch)) {\n+                        } else if (!isWitness && totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, proposedLeader, proposedZxid, proposedEpoch)) {\n                             updateProposal(n.leader, n.zxid, n.peerEpoch);\n                             sendNotifications();\n                         }\n@@ -1023,11 +1189,17 @@ public Vote lookForLeader() throws InterruptedException {\n \n                         // don't care about the version if it's in LOOKING state\n                         recvset.put(n.sid, new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch));\n-\n+                        /*if(witnessPresent) {\n+                            witNotCache.getCachedNotifications(logicalclock.get(), ServerState.LOOKING, recvset);\n+                        }*/\n+                        //Priyatham - the new vote we are creating in the call basically represents, the peer(myself/or any other node) I am currently supporting\n+                        //When in looking state; I will count (addACK) a vote(recieved) only if it is also supporting the same leader as I am -- however I\n+                        //will still retain the vote...\n                         voteSet = getVoteTracker(recvset, new Vote(proposedLeader, proposedZxid, logicalclock.get(), proposedEpoch));\n \n-                        if (voteSet.hasAllQuorums()) {\n-\n+                        //Logically, calling hasAllQuorumsWithWitness() alone would suffice, doing the below check explicitly to increase the understandbility of code.\n+                        boolean hasAllQuorums = witnessPresent ? voteSet.hasAllQuorumsWithWitness() : voteSet.hasAllQuorums();\n+                        if (hasAllQuorums) {\n                             // Verify if there is any change in the proposed leader\n                             while ((n = recvqueue.poll(finalizeWait, TimeUnit.MILLISECONDS)) != null) {\n                                 if (totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, proposedLeader, proposedZxid, proposedEpoch)) {\n@@ -1044,6 +1216,7 @@ public Vote lookForLeader() throws InterruptedException {\n                                 setPeerState(proposedLeader, voteSet);\n                                 Vote endVote = new Vote(proposedLeader, proposedZxid, logicalclock.get(), proposedEpoch);\n                                 leaveInstance(endVote);\n+                                self.requestWitnessHandlerCreation(recvset);\n                                 return endVote;\n                             }\n                         }\n@@ -1053,14 +1226,26 @@ public Vote lookForLeader() throws InterruptedException {\n                         break;\n                     case FOLLOWING:\n                     case LEADING:\n+                        if(n.state == ServerState.LEADING && isWitness) {\n+                            LOG.warn(\"Server {} is a witness, it cannot be a leader\",n.sid);\n+                            //TODO: This will never happen, but just in case, find a way to shutdown the witness...and notify other servers that the witness is byzantine.\n+                        }\n+\n                         /*\n                          * Consider all notifications from the same epoch\n                          * together.\n                          */\n                         if (n.electionEpoch == logicalclock.get()) {\n                             recvset.put(n.sid, new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch, n.state));\n+                            /*//A witness in Looking state could have voted for the same epoch. and its vote would have been\n+                            // cached earlier. We get it now and use it.\n+                            if(witnessPresent) {\n+                               witNotCache.getCachedNotifications(logicalclock.get(), ServerState.LOOKING, recvset);\n+                            }*/\n                             voteSet = getVoteTracker(recvset, new Vote(n.version, n.leader, n.zxid, n.electionEpoch, n.peerEpoch, n.state));\n-                            if (voteSet.hasAllQuorums() && checkLeader(recvset, n.leader, n.electionEpoch)) {\n+\n+                            hasAllQuorums = witnessPresent ? voteSet.hasAllQuorumsWithWitness() : voteSet.hasAllQuorums();\n+                            if (hasAllQuorums && checkLeader(recvset, n.leader, n.electionEpoch)) {\n                                 setPeerState(n.leader, voteSet);\n                                 Vote endVote = new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch);\n                                 leaveInstance(endVote);\n@@ -1076,9 +1261,11 @@ public Vote lookForLeader() throws InterruptedException {\n                          * See ZOOKEEPER-1732 for more information.\n                          */\n                         outofelection.put(n.sid, new Vote(n.version, n.leader, n.zxid, n.electionEpoch, n.peerEpoch, n.state));\n+\n                         voteSet = getVoteTracker(outofelection, new Vote(n.version, n.leader, n.zxid, n.electionEpoch, n.peerEpoch, n.state));\n \n-                        if (voteSet.hasAllQuorums() && checkLeader(outofelection, n.leader, n.electionEpoch)) {\n+                        hasAllQuorums = witnessPresent ? voteSet.hasAllQuorumsWithWitness() : voteSet.hasAllQuorums();\n+                        if (hasAllQuorums && checkLeader(outofelection, n.leader, n.electionEpoch)) {\n                             synchronized (this) {\n                                 logicalclock.set(n.electionEpoch);\n                                 setPeerState(n.leader, voteSet);\n@@ -1096,7 +1283,7 @@ public Vote lookForLeader() throws InterruptedException {\n                     if (!validVoter(n.leader)) {\n                         LOG.warn(\"Ignoring notification for non-cluster member sid {} from sid {}\", n.leader, n.sid);\n                     }\n-                    if (!validVoter(n.sid)) {\n+                    if (!validVoter(n.sid) && !self.isWitness(n.sid)) {\n                         LOG.warn(\"Ignoring notification for sid {} from non-quorum member sid {}\", n.leader, n.sid);\n                     }\n                 }\n@@ -1118,6 +1305,7 @@ public Vote lookForLeader() throws InterruptedException {\n     /**\n      * Check if a given sid is represented in either the current or\n      * the next voting view\n+     * //TODO: Modify, validVoter to return true even for a witness..because witness is also a voting server during leader election.\n      *\n      * @param sid     Server identifier\n      * @return boolean\n@@ -1126,4 +1314,117 @@ private boolean validVoter(long sid) {\n         return self.getCurrentAndNextConfigVoters().contains(sid);\n     }\n \n+    /*private void handleWitnessNotification(Notification wn, Map<Long, Vote> recvset, Map<Long, Vote> outofelection, WitnessNotificationCache witNotCache,\n+                                           SyncedLearnerTracker voteSet) throws InterruptedException {\n+        *//*\n+         * Only proceed if the vote comes from a replica in the current or next\n+         * voting view for a replica in the current or next voting view.\n+         *//*\n+        switch (wn.state) {\n+            case LOOKING:\n+                if (getInitLastLoggedZxid() == -1) {\n+                    LOG.debug(\"Ignoring notification as our zxid is -1\");\n+                    return;\n+                }\n+                if (wn.zxid == -1) {\n+                    LOG.debug(\"Ignoring notification from member with -1 zxid {}\", wn.sid);\n+                    return;\n+                }\n+                // If notification > current, replace and send messages out\n+                if (wn.electionEpoch > logicalclock.get()) {\n+                   //You can't update your proposal/vote based on a vote from witness, So just cache it for now and use it\n+                   //when a another zookeeper server sends notification with the same vote.\n+                    witNotCache.cacheWitnessNotification(wn);\n+                    return;\n+                } else if (wn.electionEpoch < logicalclock.get()) {\n+                    LOG.debug(\n+                            \"Notification election epoch is smaller than logicalclock. n.electionEpoch = 0x{}, logicalclock=0x{}\",\n+                            Long.toHexString(wn.electionEpoch),\n+                            Long.toHexString(logicalclock.get()));\n+                    return;\n+                }\n+\n+                LOG.debug(\n+                        \"Adding vote: from={}, proposed leader={}, proposed zxid=0x{}, proposed election epoch=0x{}\",\n+                        wn.sid,\n+                        wn.leader,\n+                        Long.toHexString(wn.zxid),\n+                        Long.toHexString(wn.electionEpoch));\n+\n+\n+                Vote witnessVote = new Vote(wn.leader, wn.zxid, wn.electionEpoch, wn.peerEpoch);\n+                // don't care about the version if it's in LOOKING state\n+                recvset.put(wn.sid, witnessVote);\n+\n+                Vote peerVote = new Vote(proposedLeader, proposedZxid, logicalclock.get(), proposedEpoch);\n+\n+                if(peerVote.equals(witnessVote)) {\n+                    //if the witness and I are supporting the same candidate, then I will check if we have quorum\n+                    voteSet = getVoteTracker(recvset, new Vote(proposedLeader, proposedZxid, logicalclock.get(), proposedEpoch));\n+\n+                    if (voteSet.hasAllQuorums()) {\n+\n+                        // Verify if there is any change in the proposed leader\n+                        while ((wn = recvqueue.poll(finalizeWait, TimeUnit.MILLISECONDS)) != null) {\n+                            if (totalOrderPredicate(wn.leader, wn.zxid, wn.peerEpoch, proposedLeader, proposedZxid, proposedEpoch)) {\n+                                recvqueue.put(wn);\n+                                break;\n+                            }\n+                        }\n+\n+                        *//*\n+                         * This predicate is true once we don't read any new\n+                         * relevant message from the reception queue\n+                         *//*\n+                        if (wn == null) {\n+                            setPeerState(proposedLeader, voteSet);\n+                            Vote endVote = new Vote(proposedLeader, proposedZxid, logicalclock.get(), proposedEpoch);\n+                            leaveInstance(endVote);\n+                            return endVote;\n+                        }\n+                    }\n+                }\n+                break;\n+            case FOLLOWING:\n+                *//*\n+                 * Consider all notifications from the same epoch\n+                 * together.\n+                 *//*\n+                if (wn.electionEpoch == logicalclock.get()) {\n+                    recvset.put(wn.sid, new Vote(wn.leader, wn.zxid, wn.electionEpoch, wn.peerEpoch, wn.state));\n+                    voteSet = getVoteTracker(recvset, new Vote(wn.version, wn.leader, wn.zxid, wn.electionEpoch, wn.peerEpoch, wn.state));\n+                    if (voteSet.hasAllQuorums() && checkLeader(recvset, wn.leader, wn.electionEpoch)) {\n+                        setPeerState(wn.leader, voteSet);\n+                        Vote endVote = new Vote(wn.leader, wn.zxid, wn.electionEpoch, wn.peerEpoch);\n+                        leaveInstance(endVote);\n+                        return endVote;\n+                    }\n+                }\n+\n+                *//*\n+                 * Before joining an established ensemble, verify that\n+                 * a majority are following the same leader.\n+                 *\n+                 * Note that the outofelection map also stores votes from the current leader election.\n+                 * See ZOOKEEPER-1732 for more information.\n+                 *//*\n+                outofelection.put(wn.sid, new Vote(wn.version, wn.leader, wn.zxid, wn.electionEpoch, wn.peerEpoch, wn.state));\n+                voteSet = getVoteTracker(outofelection, new Vote(wn.version, wn.leader, wn.zxid, wn.electionEpoch, wn.peerEpoch, wn.state));\n+\n+                if (voteSet.hasAllQuorums() && checkLeader(outofelection, wn.leader, wn.electionEpoch)) {\n+                    synchronized (this) {\n+                        logicalclock.set(wn.electionEpoch);\n+                        setPeerState(wn.leader, voteSet);\n+                    }\n+                    Vote endVote = new Vote(wn.leader, wn.zxid, wn.electionEpoch, wn.peerEpoch);\n+                    leaveInstance(endVote);\n+                    return endVote;\n+                }\n+                break;\n+            default:\n+                LOG.warn(\"Notification state unrecoginized: {} (n.state), {}(n.sid)\", wn.state, wn.sid);\n+                break;\n+        }\n+    }*/\n+\n }"
  },
  {
    "sha": "6e639993376dba80142fa3303f727aee62e4b7ba",
    "filename": "zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/Follower.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/SJSU-CS-systems-group/zookeeper/blob/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/Follower.java",
    "raw_url": "https://github.com/SJSU-CS-systems-group/zookeeper/raw/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/Follower.java",
    "contents_url": "https://api.github.com/repos/SJSU-CS-systems-group/zookeeper/contents/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/Follower.java?ref=ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7",
    "patch": "@@ -179,7 +179,7 @@ protected void processPacket(QuorumPacket qp) throws Exception {\n             if (hdr.getType() == OpCode.reconfig) {\n                 SetDataTxn setDataTxn = (SetDataTxn) txn;\n                 QuorumVerifier qv = self.configFromString(new String(setDataTxn.getData(), UTF_8));\n-                self.setLastSeenQuorumVerifier(qv, true);\n+                self.setLastSeenQuorumVerifier(qv, true);//TODO:Priyatham: set each time..a reconfig type packet is recieved..?\n             }\n \n             fzk.logRequest(hdr, txn, digest);"
  },
  {
    "sha": "d4b7de4405fb37286b65d91da590610018c6723a",
    "filename": "zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/Leader.java",
    "status": "modified",
    "additions": 355,
    "deletions": 29,
    "changes": 384,
    "blob_url": "https://github.com/SJSU-CS-systems-group/zookeeper/blob/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/Leader.java",
    "raw_url": "https://github.com/SJSU-CS-systems-group/zookeeper/raw/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/Leader.java",
    "contents_url": "https://api.github.com/repos/SJSU-CS-systems-group/zookeeper/contents/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/Leader.java?ref=ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7",
    "patch": "@@ -42,29 +42,17 @@\n import java.util.Objects;\n import java.util.Optional;\n import java.util.Set;\n-import java.util.concurrent.ConcurrentHashMap;\n-import java.util.concurrent.ConcurrentLinkedQueue;\n-import java.util.concurrent.ConcurrentMap;\n-import java.util.concurrent.CountDownLatch;\n-import java.util.concurrent.ExecutorService;\n-import java.util.concurrent.Executors;\n-import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.*;\n import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.concurrent.atomic.AtomicLong;\n import java.util.stream.Collectors;\n import javax.security.sasl.SaslException;\n+\n import org.apache.zookeeper.KeeperException;\n import org.apache.zookeeper.ZooDefs.OpCode;\n import org.apache.zookeeper.common.Time;\n import org.apache.zookeeper.jmx.MBeanRegistry;\n-import org.apache.zookeeper.server.ExitCode;\n-import org.apache.zookeeper.server.FinalRequestProcessor;\n-import org.apache.zookeeper.server.Request;\n-import org.apache.zookeeper.server.RequestProcessor;\n-import org.apache.zookeeper.server.ServerMetrics;\n-import org.apache.zookeeper.server.ZKDatabase;\n-import org.apache.zookeeper.server.ZooKeeperCriticalThread;\n-import org.apache.zookeeper.server.ZooTrace;\n+import org.apache.zookeeper.server.*;\n import org.apache.zookeeper.server.quorum.QuorumPeer.LearnerType;\n import org.apache.zookeeper.server.quorum.auth.QuorumAuthServer;\n import org.apache.zookeeper.server.quorum.flexible.QuorumVerifier;\n@@ -99,6 +87,19 @@ public String toString() {\n \n     }\n \n+    public static class WitnessProposal extends Proposal {\n+        public boolean ignoreWitnessAck;\n+\n+        @Override\n+        public String toString() {\n+            return packet.getType() + \", \" + packet.getZxid() + \", \" + request + \", ignoreWitnessAck = \" + ignoreWitnessAck;\n+        }\n+\n+        public boolean shouldIgnoreWitnessAck() {\n+            return ignoreWitnessAck;\n+        }\n+    }\n+\n     // log ack latency if zxid is a multiple of ackLoggingFrequency. If <=0, disable logging.\n     private static final String ACK_LOGGING_FREQUENCY = \"zookeeper.leader.ackLoggingFrequency\";\n     private static int ackLoggingFrequency;\n@@ -126,6 +127,19 @@ public static int getAckLoggingFrequency() {\n     // the follower acceptor thread\n     volatile LearnerCnxAcceptor cnxAcceptor = null;\n \n+    // the witness manager thread\n+    volatile WitnessHandlerManager witnessHandlerManager = null;\n+\n+    /**\n+     * Returns a copy of the current Witness snapshot\n+     */\n+    public List<WitnessHandler> getWitnesses() {\n+        if(witnessHandlerManager != null) {\n+            return witnessHandlerManager.getWitnessHandlers();\n+        }\n+        return new ArrayList<>();\n+    }\n+\n     // list of all the learners, including followers and observers\n     private final HashSet<LearnerHandler> learners = new HashSet<LearnerHandler>();\n \n@@ -549,6 +563,191 @@ private void acceptConnections() throws IOException {\n \n     }\n \n+    class WitnessHandlerManager extends ZooKeeperCriticalThread{\n+        /**\n+         * Responsibilities:\n+         * 1. Create WitnessHandler objects\n+         * 2. Stop WitnessHandlers\n+         * 3. Track WitnessHandlers.\n+         *\n+         * Behavior:\n+         * 1. This object is created when this peer becomes a leader\n+         * 2. WH creation and stopping/deletion are handled in different threads.\n+         * 3. Requests to start a WH will be placed in createQueue..The CreationThread will poll this queue and creates a WH if its not already created.\n+         * 4. Requests to stop a witness, will be placed in the Stop queue...The stop thread will call shutdown() method associated with that witness.\n+         * 5. When a peer, stops being leader a global shutdown will be called, which will first\n+         *     - empty the creationQueue\n+         *     - stop the creation thread\n+         *     - enqueue stop requests for all the witness handlers,\n+         *     - then enqueue a halt packet to the deletion thread.\n+         * */\n+\n+        ConcurrentHashMap<Long, WitnessHandler> witnessHandlers = new ConcurrentHashMap<>();\n+        LinkedBlockingQueue<Long> startQueue = new LinkedBlockingQueue<>();\n+        Set<Long> startInProgress = (new ConcurrentHashMap<>()).newKeySet();\n+        LinkedBlockingQueue<Long> stopQueue = new LinkedBlockingQueue<>();\n+\n+        final long stopMarker = -3;\n+        private final AtomicBoolean stop = new AtomicBoolean(false);\n+        private final AtomicBoolean fail = new AtomicBoolean(false);\n+\n+        //TODO: change the parameters later, similar to what is done in LearnerCnxAcceptor\n+        public WitnessHandlerManager() {\n+            super(\"WitnessHandlerManager\", zk.getZooKeeperServerListener());\n+        }\n+\n+        void startWitnessHandler(long sid) {\n+            if(!stop.get() && !witnessHandlers.containsKey(sid)) {\n+                startQueue.add(sid);\n+            }\n+        }\n+\n+        void stopWitnessHandler(long sid) {\n+            if(witnessHandlers.containsKey(sid)) {\n+                stopQueue.add(sid);\n+            }\n+        }\n+\n+        @Override\n+        public void run() {\n+            if(!stop.get() && self.isWitnessPresent()) {\n+                Thread whStarter = new Thread(new WitnessHandlerStarter());\n+                Thread whStopper = new Thread(new WitnessHandlerStopper());\n+                whStarter.start();\n+                whStopper.start();\n+                try {\n+                    whStarter.join();\n+                    whStopper.join();\n+                } catch (InterruptedException e) {\n+                    LOG.error(\"Interrupted while waiting for the WitnessHandler Starter and Stopper threads to exit\", e);\n+                } finally {\n+                    shutdown();\n+                }\n+\n+            }\n+        }\n+\n+        class WitnessHandlerStarter implements Runnable {\n+            @Override\n+            public void run() {\n+                while (true) {\n+                    try {\n+                        long sid = startQueue.take();\n+                        if(sid == stopMarker) {\n+                            //stop the starter thread.\n+                            break;\n+                        }\n+                        if(!startInProgress.contains(sid)) {\n+                            WitnessHandler wh = new WitnessHandler(sid, getWitnessAddress(sid), Leader.this, WitnessHandlerManager.this);\n+                            startInProgress.add(sid); //This sid will be removed from the start in progress from the WH thread itself.\n+                            wh.start();\n+                        }\n+                    } catch (InterruptedException e) {\n+                        if(stop.get()) {\n+                            LOG.error(\"WitnessHandlerStarter has been interrupted because the WitnessHandlerManager thread has been stopped.\");\n+                        } else {\n+                            LOG.error(\"Interrupted while waiting for requests to start WitnessHandlers\", e);\n+                            fail.set(true);\n+                            shutdown();\n+                        }\n+                    } catch (Exception e) {\n+                        LOG.error(e.getMessage());\n+                        fail.set(true);\n+                        shutdown();\n+                    }\n+                }\n+            }\n+\n+            InetSocketAddress getWitnessAddress(long sid) throws Exception {\n+                QuorumPeer.QuorumServer temp = self.getQuorumVerifier().getWitnessingMembers().get(sid);\n+                if(temp == null) {\n+                    temp =  self.getLastSeenQuorumVerifier().getWitnessingMembers().get(sid);\n+                    if(temp == null) {\n+                        throw new Exception(String.format(\"Requested witness {} is not present\", sid));\n+                    }\n+                }\n+                Set<InetSocketAddress> set = temp.grpcAddr.getAllAddresses();\n+                InetSocketAddress[] array = new InetSocketAddress[set.size()];\n+                set.toArray(array);\n+                return array[0];\n+            }\n+        }\n+\n+        class WitnessHandlerStopper implements Runnable {\n+            @Override\n+            public void run() {\n+                while (true) {\n+                    try {\n+                        long sid = stopQueue.take();\n+                        if(sid == stopMarker) {\n+                            break;\n+                        }\n+                        if(witnessHandlers.containsKey(sid)) {\n+                            WitnessHandler wh = witnessHandlers.get(sid);\n+                            wh.shutdown();\n+                        }\n+                    } catch (InterruptedException e) {\n+                        if(stop.get()) {\n+                            LOG.error(\"WitnessHandlerStopper has been interrupted because the WitnessHandlerManager thread has been stopped.\");\n+                        } else {\n+                            LOG.error(\"Interrupted while waiting for requests to stop WitnessHandlers\", e);\n+                            fail.set(true);\n+                            shutdown();\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+\n+        List<WitnessHandler> getWitnessHandlers() {\n+            synchronized (witnessHandlers) {\n+                return new ArrayList<>(witnessHandlers.values());\n+            }\n+        }\n+\n+        boolean makeWitnessesActive() {\n+            int activatedCount = 0;\n+            for(WitnessHandler wh : witnessHandlers.values()) {\n+                if(wh.makeActive()) {\n+                    activatedCount++;\n+                }\n+            }\n+            if(activatedCount > 0) {\n+                return true;\n+            }\n+            return false;\n+        }\n+\n+        boolean makeWitnessesPassive() {\n+            int deactivatedCount = 0;\n+            for(WitnessHandler wh : witnessHandlers.values()) {\n+                if(wh.makePassive()) {\n+                    deactivatedCount++;\n+                }\n+            }\n+            if(deactivatedCount > 0) {\n+                return true;\n+            }\n+            return false;\n+        }\n+\n+        void shutdown() {\n+            /**\n+             * -empty the creationQueue\n+             * -stop the creation thread\n+             * -enqueue stop requests for all the witness handlers,\n+             * -then enqueue a halt packet to the deletion thread.\n+             * */\n+            stop.set(true);\n+            startQueue.clear();\n+            startQueue.add(stopMarker);\n+            for(long sid: witnessHandlers.keySet()) {\n+                stopQueue.add(sid);\n+            }\n+            stopQueue.add(stopMarker);\n+            //TODO: Wait here for a finite time, for the witnessHandler map tp become empty.\n+        }\n+    }\n     StateSummary leaderStateSummary;\n \n     long epoch = -1;\n@@ -599,14 +798,22 @@ void lead() throws IOException, InterruptedException {\n             cnxAcceptor = new LearnerCnxAcceptor();\n             cnxAcceptor.start();\n \n+            //if witnesses are present in the configuration, then create the witness handler manager.\n+            if(self.isWitnessPresent()) {\n+                witnessHandlerManager = new WitnessHandlerManager();\n+                witnessHandlerManager.start();\n+                //enqueue any readily available witness start requests.\n+                witnessHandlerManager.startQueue.addAll(self.getWitnessesTobeConnected());\n+            }\n+\n             long epoch = getEpochToPropose(self.getId(), self.getAcceptedEpoch());\n \n             zk.setZxid(ZxidUtils.makeZxid(epoch, 0));\n \n             synchronized (this) {\n                 lastProposed = zk.getZxid();\n             }\n-\n+            //TODO: The zxid here is basically the the newEpoch\n             newLeaderProposal.packet = new QuorumPacket(NEWLEADER, zk.getZxid(), null, null);\n \n             if ((newLeaderProposal.packet.getZxid() & 0xffffffffL) != 0) {\n@@ -750,13 +957,22 @@ void lead() throws IOException, InterruptedException {\n                         }\n                     }\n \n-                    // check leader running status\n+                    if(witnessHandlerManager!=null) {\n+                        for(WitnessHandler w: witnessHandlerManager.getWitnessHandlers()) {\n+                            if(w.synced()) {\n+                                syncedAckSet.addAck(w.getSid());\n+                            }\n+                        }\n+                    }\n+\n+                    // check   running status\n                     if (!this.isRunning()) {\n                         // set shutdown flag\n                         shutdownMessage = \"Unexpected internal error\";\n                         break;\n                     }\n \n+                    /*\n                     if (!tickSkip && !syncedAckSet.hasAllQuorums()) {\n                         // Lost quorum of last committed and/or last proposed\n                         // config, set shutdown flag\n@@ -765,11 +981,41 @@ void lead() throws IOException, InterruptedException {\n                                           + \" ]\";\n                         break;\n                     }\n+                    */\n+                    if(!tickSkip) {\n+                        if(syncedAckSet.hasAllQuorums()) {\n+                            //has a natural quorum, if witnesses are present, mark all the witnesses passive if they were previously active.\n+                            if(witnessHandlerManager != null && witnessHandlerManager.makeWitnessesPassive()) {\n+                                LOG.info(\"Natural quorum is formed, made witnesses passive if they were currently active\");\n+                            }\n+                        } else if(syncedAckSet.hasAllQuorumsWithWitness()) {\n+                            //we did not have a natural quorum, but we are able to form quorum with help of witness. So make all the witnesses active\n+                            if(witnessHandlerManager != null && witnessHandlerManager.makeWitnessesActive()) {\n+                                //The null check here is redundant because, we can't reach a quorum with the help of witnesses..without witnessHandlerManager that maintains them.\n+                                //But still keeping it just for testing.\n+                                LOG.info(\"Quorum formed with help of witness, made witnesses active if they were currently passive\");\n+                            }\n+                        } else {\n+                            // Lost quorum of last committed and/or last proposed\n+                            // config, set shutdown flag\n+                            shutdownMessage = \"Not sufficient followers synced, only synced with sids: [ \"\n+                                    + syncedAckSet.ackSetsToString()\n+                                    + \" ]\";\n+                            break;\n+                        }\n+                    }\n+\n                     tickSkip = !tickSkip;\n                 }\n                 for (LearnerHandler f : getLearners()) {\n                     f.ping();\n                 }\n+\n+                if(witnessHandlerManager != null) {\n+                    for(WitnessHandler w : witnessHandlerManager.getWitnessHandlers()) {\n+                        w.ping();\n+                    }\n+                }\n             }\n             if (shutdownMessage != null) {\n                 shutdown(shutdownMessage);\n@@ -800,6 +1046,10 @@ void shutdown(String reason) {\n             closeSockets();\n         }\n \n+        if(witnessHandlerManager != null) {\n+            witnessHandlerManager.shutdown();\n+        }\n+\n         // NIO should not accept conenctions\n         self.setZooKeeperServer(null);\n         self.adminServer.setZooKeeperServer(null);\n@@ -902,8 +1152,15 @@ public synchronized boolean tryToCommit(Proposal p, long zxid, SocketAddress fol\n         // in order to be committed, a proposal must be accepted by a quorum.\n         //\n         // getting a quorum from all necessary configurations.\n-        if (!p.hasAllQuorums()) {\n+        if (!p.hasAllQuorums() && !p.hasAllQuorumsWithWitness()) {\n             return false;\n+        } else {\n+            /** TODO: When the witness is passive\n+             *Here, if a natural quorum could not be formed after a certain period, try to use the witness's vote even though witness is passive...\n+             * Since, witness will not invoke processAck() when isActive=false for a particular witness request. We can look at the latestMetadata object\n+             * to get this info.\n+             *  I don't think, we wait for a certain timeout to reach quorum...So find a way to determine, when to use witness's vote...if its available.\n+             */\n         }\n \n         // commit proposals in order\n@@ -932,6 +1189,7 @@ public synchronized boolean tryToCommit(Proposal p, long zxid, SocketAddress fol\n             //leader election time, unless the designated leader fails\n             Long designatedLeader = getDesignatedLeader(p, zxid);\n \n+            //TODO: Priyatham: Be aware of this..\n             QuorumVerifier newQV = p.qvAcksetPairs.get(p.qvAcksetPairs.size() - 1).getQuorumVerifier();\n \n             self.processReconfig(newQV, designatedLeader, zk.getZxid(), true);\n@@ -950,6 +1208,7 @@ public synchronized boolean tryToCommit(Proposal p, long zxid, SocketAddress fol\n         } else {\n             p.request.logLatency(ServerMetrics.getMetrics().QUORUM_ACK_LATENCY);\n             commit(zxid);\n+            informWitness(zxid);\n             inform(p);\n         }\n         zk.commitProcessor.commit(p.request);\n@@ -962,6 +1221,7 @@ public synchronized boolean tryToCommit(Proposal p, long zxid, SocketAddress fol\n         return true;\n     }\n \n+\n     /**\n      * Keep a count of acks that are received by the leader for a particular\n      * proposal\n@@ -1018,6 +1278,7 @@ public synchronized void processAck(long sid, long zxid, SocketAddress followerA\n \n         p.addAck(sid);\n \n+        isWitness(sid);\n         boolean hasCommitted = tryToCommit(p, zxid, followerAddr);\n \n         // If p is a reconfiguration, multiple other operations may be ready to be committed,\n@@ -1176,6 +1437,23 @@ public static QuorumPacket buildInformAndActivePacket(long zxid, long designated\n         return new QuorumPacket(Leader.INFORMANDACTIVATE, zxid, data, null);\n     }\n \n+    public void informWitness(long zxid) {\n+        for(WitnessHandler wh: getWitnesses()) {\n+            /**\n+             * It may so happen that, the witness was active while sending the proposal, but, could have become passive later or the witness\n+             * could be passive at proposal stage so we dont queue it, but become active now - so we have to queue it.\n+             * Making this check, takes care of both the conditions\n+             * */\n+            if(zxid > wh.lastQueuedZxid.get()) {\n+                LOG.info(\"Queued {} to witness {}\", Long.toHexString(zxid), wh.getSid());\n+                wh.queueRequest(zxid, false);\n+            }\n+            else {\n+                LOG.info(\"Witness {}'s last queued zxid {} >  zxid{}. So not queuing it\", wh.getSid(), Long.toHexString(wh.lastQueuedZxid.get()), Long.toHexString(zxid));\n+            }\n+        }\n+    }\n+\n     /**\n      * Create an inform and activate packet and send it to all observers.\n      */\n@@ -1387,20 +1665,27 @@ public void reportLookingSid(long sid) {\n         }\n     }\n \n+    //This effectively creates a concurrent hashset.\n+    protected final Set<Long> connectingWitnesses = new ConcurrentHashMap<>().newKeySet();\n+\n     @Override\n     public long getEpochToPropose(long sid, long lastAcceptedEpoch) throws InterruptedException, IOException {\n         synchronized (connectingFollowers) {\n             if (!waitingForNewEpoch) {\n+                LOG.info(\"Not waiting of new epoch, returning epoch: {}\", epoch);\n                 return epoch;\n             }\n             if (lastAcceptedEpoch >= epoch) {\n                 epoch = lastAcceptedEpoch + 1;\n             }\n             if (isParticipant(sid)) {\n                 connectingFollowers.add(sid);\n+            } else if (isWitness(sid)) {\n+                connectingWitnesses.add(sid);\n             }\n+\n             QuorumVerifier verifier = self.getQuorumVerifier();\n-            if (connectingFollowers.contains(self.getId()) && verifier.containsQuorum(connectingFollowers)) {\n+            if (connectingFollowers.contains(self.getId()) && verifier.containsQuorumWithWitness(connectingFollowers, new HashSet<>(connectingWitnesses))) {\n                 waitingForNewEpoch = false;\n                 self.setAcceptedEpoch(epoch);\n                 connectingFollowers.notifyAll();\n@@ -1430,6 +1715,9 @@ public ZKDatabase getZKDatabase() {\n \n     // VisibleForTesting\n     protected final Set<Long> electingFollowers = new HashSet<Long>();\n+\n+    //This effectively creates a concurrent hashset.\n+    protected final Set<Long> electingWitnesses = new ConcurrentHashMap<>().newKeySet();\n     // VisibleForTesting\n     protected boolean electionFinished = false;\n \n@@ -1440,18 +1728,18 @@ public void waitForEpochAck(long id, StateSummary ss) throws IOException, Interr\n                 return;\n             }\n             if (ss.getCurrentEpoch() != -1) {\n-                if (ss.isMoreRecentThan(leaderStateSummary)) {\n-                    throw new IOException(\"Follower is ahead of the leader, leader summary: \"\n-                                          + leaderStateSummary.getCurrentEpoch()\n-                                          + \" (current epoch), \"\n-                                          + leaderStateSummary.getLastZxid()\n-                                          + \" (last zxid)\");\n-                }\n-                if (ss.getLastZxid() != -1 && isParticipant(id)) {\n-                    electingFollowers.add(id);\n+                if (ss.getLastZxid() != -1) {\n+                    if(isParticipant(id)) {\n+                        isPeerMoreRecentThanLeader(ss, \"Follower\");\n+                        electingFollowers.add(id);\n+                    } else if(isWitness(id)) {\n+                        isPeerMoreRecentThanLeader(ss, \"Witness\");\n+                        electingWitnesses.add(id);\n+                    }\n                 }\n             }\n             QuorumVerifier verifier = self.getQuorumVerifier();\n+            //This if block, checks for a the presence of a natural quorum.\n             if (electingFollowers.contains(self.getId()) && verifier.containsQuorum(electingFollowers)) {\n                 electionFinished = true;\n                 electingFollowers.notifyAll();\n@@ -1460,16 +1748,38 @@ public void waitForEpochAck(long id, StateSummary ss) throws IOException, Interr\n                 long cur = start;\n                 long end = start + self.getInitLimit() * self.getTickTime();\n                 while (!electionFinished && cur < end) {\n+                    //Note: Both witness and follower handler threads wait on the same object.\n                     electingFollowers.wait(end - cur);\n                     cur = Time.currentElapsedTime();\n                 }\n                 if (!electionFinished) {\n+                    //Since the election could not be completed only by using followers, now check if a quorum could be reached with the help of witness.s\n+                    synchronized (electingWitnesses) {\n+                        if (electingFollowers.contains(self.getId()) && verifier.containsQuorumWithWitness(electingFollowers, electingWitnesses)) {\n+                            //quorum reached with the help of witness.\n+                            electionFinished = true;\n+                            electingFollowers.notifyAll();\n+                            return;\n+                        }\n+                    }\n+                    //At the end of timeout, a quorum could not be formed even with the help of witness.\n                     throw new InterruptedException(\"Timeout while waiting for epoch to be acked by quorum\");\n                 }\n             }\n         }\n     }\n \n+    void isPeerMoreRecentThanLeader(StateSummary peerStateSummary, String peerType) throws IOException {\n+        if (peerStateSummary.isMoreRecentThan(leaderStateSummary)) {\n+            //This condition should never satisfy for a witness because of our guarantees.\n+            throw new IOException(peerType + \" is ahead of the leader, leader summary: \"\n+                    + leaderStateSummary.getCurrentEpoch()\n+                    + \" (current epoch), \"\n+                    + leaderStateSummary.getLastZxid()\n+                    + \" (last zxid)\");\n+        }\n+    }\n+\n     /**\n      * Return a list of sid in set as string\n      */\n@@ -1546,7 +1856,7 @@ public void waitForNewLeaderAck(long sid, long zxid) throws InterruptedException\n             if (quorumFormed) {\n                 return;\n             }\n-\n+            //This zxid is basically, currentEpoch + 0 (counter)\n             long currentZxid = newLeaderProposal.packet.getZxid();\n             if (zxid != currentZxid) {\n                 LOG.error(\n@@ -1575,6 +1885,17 @@ public void waitForNewLeaderAck(long sid, long zxid) throws InterruptedException\n                     cur = Time.currentElapsedTime();\n                 }\n                 if (!quorumFormed) {\n+                    if(newLeaderProposal.hasAllQuorumsWithWitness()) {\n+                        quorumFormed = true;\n+                        newLeaderProposal.qvAcksetPairs.notifyAll();\n+                        //Mark witness active.\n+                        //Note: When using witness, we assume that there will only be a single witness..Since I initially wrote code that\n+                        //can maintain multiple witness handlers, I am iterating through all WH objects and marking them active.\n+                        for(WitnessHandler wh: witnessHandlerManager.getWitnessHandlers()) {\n+                            wh.makeActive();\n+                        }\n+                        return;\n+                    }\n                     throw new InterruptedException(\"Timeout while waiting for NEWLEADER to be acked by quorum\");\n                 }\n             }\n@@ -1639,6 +1960,11 @@ private boolean isParticipant(long sid) {\n         return self.getQuorumVerifier().getVotingMembers().containsKey(sid);\n     }\n \n+    private boolean isWitness(long sid) {\n+        return self.getQuorumVerifier().getWitnessingMembers().containsKey(sid);\n+    }\n+\n+\n     @Override\n     public int getCurrentTick() {\n         return self.tick.get();"
  },
  {
    "sha": "d0706aa4e9a35fc64019f5729cfdb384b5148871",
    "filename": "zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/Learner.java",
    "status": "modified",
    "additions": 4,
    "deletions": 1,
    "changes": 5,
    "blob_url": "https://github.com/SJSU-CS-systems-group/zookeeper/blob/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/Learner.java",
    "raw_url": "https://github.com/SJSU-CS-systems-group/zookeeper/raw/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/Learner.java",
    "contents_url": "https://api.github.com/repos/SJSU-CS-systems-group/zookeeper/contents/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/Learner.java?ref=ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7",
    "patch": "@@ -282,6 +282,7 @@ protected QuorumServer findLeader() {\n         }\n         if (leaderServer == null) {\n             LOG.warn(\"Couldn't find the leader with id = {}\", current.getId());\n+            //TODO:Wouldn't just returning null here, throw a null pointer exception..But I guess.. this wont happen..\n         }\n         return leaderServer;\n     }\n@@ -523,6 +524,7 @@ protected long registerWithLeader(int pktType) throws IOException {\n             }\n             QuorumPacket ackNewEpoch = new QuorumPacket(Leader.ACKEPOCH, lastLoggedZxid, epochBytes, null);\n             writePacket(ackNewEpoch, true);\n+            //TODO: Here why are we not directly returning the zxid returned by the the leader??..why are we initializing ...- i think we just need the epoch..\n             return ZxidUtils.makeZxid(newEpoch, 0);\n         } else {\n             if (newEpoch > self.getAcceptedEpoch()) {\n@@ -554,7 +556,7 @@ protected void syncWithLeader(long newLeaderZxid) throws Exception {\n         // For SNAP and TRUNC the snapshot is needed to save that history\n         boolean snapshotNeeded = true;\n         boolean syncSnapshot = false;\n-        readPacket(qp);\n+        readPacket(qp); //TODO: Priyatham...The first packet we read in the synch process is the OP packet?\n         Deque<Long> packetsCommitted = new ArrayDeque<>();\n         Deque<PacketInFlight> packetsNotCommitted = new ArrayDeque<>();\n         synchronized (zk) {\n@@ -653,6 +655,7 @@ protected void syncWithLeader(long newLeaderZxid) throws Exception {\n                             throw new Exception(\"changes proposed in reconfig\");\n                         }\n                     }\n+                    //TODO: Priyatham: Understand the reasoning behind this..\n                     if (!writeToTxnLog) {\n                         if (pif.hdr.getZxid() != qp.getZxid()) {\n                             LOG.warn("
  },
  {
    "sha": "d4d2d21089eba16b1e98bfd54ae62a9a66581b33",
    "filename": "zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/LearnerHandler.java",
    "status": "modified",
    "additions": 13,
    "deletions": 5,
    "changes": 18,
    "blob_url": "https://github.com/SJSU-CS-systems-group/zookeeper/blob/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/LearnerHandler.java",
    "raw_url": "https://github.com/SJSU-CS-systems-group/zookeeper/raw/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/LearnerHandler.java",
    "contents_url": "https://api.github.com/repos/SJSU-CS-systems-group/zookeeper/contents/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/LearnerHandler.java?ref=ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7",
    "patch": "@@ -170,7 +170,7 @@ public synchronized void updateProposal(long zxid, long time) {\n                 nextZxid = zxid;\n             }\n         }\n-\n+        //TODO: Priyatham: currentTime and currentZxid will become 0, when no other zxid is proposed after the currentZxid\n         public synchronized void updateAck(long zxid) {\n             if (currentZxid == zxid) {\n                 currentTime = nextTime;\n@@ -187,6 +187,7 @@ public synchronized void updateAck(long zxid) {\n             }\n         }\n \n+        //TODO: Priyatham: So check will always return true, when the LearnerHandler thread is not waiting for any ACK..i.e currentTime == 0\n         public synchronized boolean check(long time) {\n             if (currentTime == 0) {\n                 return true;\n@@ -512,7 +513,10 @@ public void run() {\n             }\n \n             learnerMaster.registerLearnerHandlerBean(this, sock);\n-\n+            //TODO: Priyatham..look at which zxid the follower is sending during the registration phase....epoch+0\n+            /**\n+             * Priyatham: In the Learner.registerWithLeader() it is actually fetching the lastAcceptedEpoch and appending it with counter 0.\n+             * */\n             long lastAcceptedEpoch = ZxidUtils.getEpochFromZxid(qp.getZxid());\n \n             long peerLastZxid;\n@@ -602,6 +606,7 @@ public void run() {\n             }\n             bufferedOutput.flush();\n \n+            //TODO: Priyatham: Till this point, packets are only added to the queue...only now we will start sending them..\n             // Start thread that blast packets in the queue to learner\n             startSendingPackets();\n \n@@ -620,7 +625,7 @@ public void run() {\n             }\n \n             LOG.debug(\"Received NEWLEADER-ACK message from {}\", sid);\n-\n+            //TODO: Priyatham: Since NEWLEADER is the last message that was queued..recieving an ACK would mean that SYNC is completed from the leader's perspective.\n             learnerMaster.waitForNewLeaderAck(getSid(), qp.getZxid());\n \n             syncLimitCheck.start();\n@@ -634,6 +639,7 @@ public void run() {\n             syncThrottler = null;\n \n             // now that the ack has been processed expect the syncLimit\n+            //TODO: Priyatham: I think, a similar line in follower, plays a role in deciding if a leader did not respond with in a timep it.\n             sock.setSoTimeout(learnerMaster.syncTimeout());\n \n             /*\n@@ -647,7 +653,7 @@ public void run() {\n             //\n             LOG.debug(\"Sending UPTODATE message to {}\", sid);\n             queuedPackets.add(new QuorumPacket(Leader.UPTODATE, -1, null, null));\n-\n+            // I think...this ends the synching phase...\n             while (true) {\n                 qp = new QuorumPacket();\n                 ia.readRecord(qp, \"packet\");\n@@ -852,6 +858,7 @@ boolean syncFollower(long peerLastZxid, LearnerMaster learnerMaster) {\n                 needOpPacket = false;\n                 needSnap = false;\n             } else if (peerLastZxid > maxCommittedLog && !isPeerNewEpochZxid) {\n+                //TODO: Priyatham: means the follower has a laterZxid than the current leader...but belongs to the same epoch..\n                 // Newer than committedLog, send trunc and done\n                 LOG.debug(\n                     \"Sending TRUNC to follower zxidToSend=0x{} for peer sid:{}\",\n@@ -979,7 +986,7 @@ protected long queueCommittedProposals(Iterator<Proposal> itr, long peerLastZxid\n                     needOpPacket = false;\n                     continue;\n                 }\n-\n+                //TODO: Will this ever be successfull, because..before we call queueCommittedProposals...we check if the peer's last zxid..is within the range.\n                 if (isPeerNewEpochZxid) {\n                     // Send diff and fall through if zxid is of a new-epoch\n                     LOG.info(\n@@ -991,6 +998,7 @@ protected long queueCommittedProposals(Iterator<Proposal> itr, long peerLastZxid\n                 } else if (packetZxid > peerLastZxid) {\n                     // Peer have some proposals that the learnerMaster hasn't seen yet\n                     // it may used to be a leader\n+                    //TODO: Priyatham: If the follower has a more recent zxid, then it means..peerLastZxid > packetZxid.. the above statement says teh epposite..\n                     if (ZxidUtils.getEpochFromZxid(packetZxid) != ZxidUtils.getEpochFromZxid(peerLastZxid)) {\n                         // We cannot send TRUNC that cross epoch boundary.\n                         // The learner will crash if it is asked to do so."
  },
  {
    "sha": "e065936cc9da9ddb4ed74b4818a0b936d0db9932",
    "filename": "zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/QuorumPeer.java",
    "status": "modified",
    "additions": 133,
    "deletions": 18,
    "changes": 151,
    "blob_url": "https://github.com/SJSU-CS-systems-group/zookeeper/blob/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/QuorumPeer.java",
    "raw_url": "https://github.com/SJSU-CS-systems-group/zookeeper/raw/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/QuorumPeer.java",
    "contents_url": "https://api.github.com/repos/SJSU-CS-systems-group/zookeeper/contents/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/QuorumPeer.java?ref=ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7",
    "patch": "@@ -208,6 +208,9 @@ public void setMultiAddressReachabilityCheckEnabled(boolean multiAddressReachabi\n \n         public MultipleAddresses addr = new MultipleAddresses();\n \n+        //TODO: Note: For now, witness starts grpc service on only one address\n+        public MultipleAddresses grpcAddr = new MultipleAddresses();\n+\n         public MultipleAddresses electionAddr = new MultipleAddresses();\n \n         public InetSocketAddress clientAddr = null;\n@@ -264,6 +267,8 @@ private LearnerType getType(String s) throws ConfigException {\n                     return LearnerType.OBSERVER;\n                 case \"participant\":\n                     return LearnerType.PARTICIPANT;\n+                case \"witness\":\n+                    return LearnerType.WITNESS;\n                 default:\n                     throw new ConfigException(\"Unrecognised peertype: \" + s);\n             }\n@@ -340,13 +345,31 @@ private void initializeWithAddressString(String addressStr, Function<InetSocketA\n                 }\n \n                 String serverHostName = serverParts[0];\n+                if (serverParts.length == 4) {\n+                    LearnerType tempType = getType(serverParts[3]);\n+                    if (newType == null) {\n+                        newType = tempType;\n+                    }\n+\n+                    if (newType != tempType) {\n+                        throw new ConfigException(\"Multiple addresses should have similar roles: \" + type + \" vs \" + tempType);\n+                    }\n+                }\n \n                 // server_config should be either host:port:port or host:port:port:type\n                 InetSocketAddress tempAddress;\n                 InetSocketAddress tempElectionAddress;\n                 try {\n                     tempAddress = new InetSocketAddress(serverHostName, Integer.parseInt(serverParts[1]));\n                     addr.addAddress(tempAddress);\n+                    tempAddress = new InetSocketAddress(serverParts[0], Integer.parseInt(serverParts[1]));\n+                    if(isWitness(newType)) {\n+                        LOG.debug(\"Witness GrpcAddress Resolved: \" + tempAddress.getHostName() + tempAddress.getPort());\n+                        grpcAddr.addAddress(tempAddress);\n+                    }\n+                    else {\n+                        addr.addAddress(tempAddress);\n+                    }\n                 } catch (NumberFormatException e) {\n                     throw new ConfigException(\"Address unresolved: \" + serverHostName + \":\" + serverParts[1]);\n                 }\n@@ -358,8 +381,14 @@ private void initializeWithAddressString(String addressStr, Function<InetSocketA\n                 }\n \n                 if (tempAddress.getPort() == tempElectionAddress.getPort()) {\n-                    throw new ConfigException(\"Client and election port must be different! Please update the \"\n-                            + \"configuration file on server.\" + this.id);\n+                    if(isWitness(newType)) {\n+                        LOG.debug(\"Witness GrpcAddress and Election address are the same\");\n+                        throw new ConfigException(\"Witness grpc service and election port must be different! Please update the \"\n+                                + \"configuration file on server.\" + this.id);\n+                    } else {\n+                        throw new ConfigException(\"Client and election port must be different! Please update the \"\n+                                + \"configuration file on server.\" + this.id);\n+                    }\n                 }\n \n                 if (canonicalize) {\n@@ -381,17 +410,6 @@ private void initializeWithAddressString(String addressStr, Function<InetSocketA\n                     }\n                 }\n \n-                if (serverParts.length == 4) {\n-                    LearnerType tempType = getType(serverParts[3]);\n-                    if (newType == null) {\n-                        newType = tempType;\n-                    }\n-\n-                    if (newType != tempType) {\n-                        throw new ConfigException(\"Multiple addresses should have similar roles: \" + type + \" vs \" + tempType);\n-                    }\n-                }\n-\n                 this.hostname = serverHostName;\n             }\n \n@@ -406,6 +424,32 @@ private static InetAddress getInetAddress(InetSocketAddress addr) {\n             return addr.getAddress();\n         }\n \n+        private boolean isWitness(LearnerType type) {\n+            if(type != null && type == LearnerType.WITNESS) {\n+                return  true;\n+            }\n+            return  false;\n+        }\n+\n+        /*public QuorumServer(long id, InetSocketAddress addr, InetSocketAddress electionAddr, LearnerType type) {\n+            this(id, addr, electionAddr, null, type);\n+        }\n+\n+        public QuorumServer(long id, InetSocketAddress addr, InetSocketAddress electionAddr, InetSocketAddress clientAddr, LearnerType type) {\n+            this.id = id;\n+            if (addr != null) {\n+                this.addr.addAddress(addr);\n+            }\n+            if (electionAddr != null) {\n+                this.electionAddr.addAddress(electionAddr);\n+            }\n+            this.type = type;\n+            this.clientAddr = clientAddr;\n+\n+            setMyAddrs();\n+>>>>>>> Stashed changes\n+        }*/\n+\n         private void setMyAddrs() {\n             this.myAddrs = new ArrayList<>();\n             this.myAddrs.addAll(this.addr.getAllAddresses());\n@@ -428,19 +472,29 @@ public String toString() {\n \n             List<InetSocketAddress> addrList = new LinkedList<>(addr.getAllAddresses());\n             List<InetSocketAddress> electionAddrList = new LinkedList<>(electionAddr.getAllAddresses());\n+            List<InetSocketAddress> grpcAddrList = new LinkedList<>(grpcAddr.getAllAddresses());\n \n-            if (addrList.size() > 0 && electionAddrList.size() > 0) {\n+            if ((addrList.size() > 0 || grpcAddrList.size() > 0)  && electionAddrList.size() > 0) {\n                 addrList.sort(Comparator.comparing(InetSocketAddress::getHostString));\n                 electionAddrList.sort(Comparator.comparing(InetSocketAddress::getHostString));\n-                sw.append(IntStream.range(0, addrList.size()).mapToObj(i -> String.format(\"%s:%d:%d\",\n-                        delimitedHostString(addrList.get(i)), addrList.get(i).getPort(), electionAddrList.get(i).getPort()))\n-                        .collect(Collectors.joining(\"|\")));\n+                if(type == LearnerType.WITNESS) {\n+                    sw.append(IntStream.range(0, grpcAddrList.size()).mapToObj(i -> String.format(\"%s:%d:%d\",\n+                            delimitedHostString(grpcAddrList.get(i)), grpcAddrList.get(i).getPort(), electionAddrList.get(i).getPort()))\n+                            .collect(Collectors.joining(\"|\")));\n+                }\n+                else {\n+                    sw.append(IntStream.range(0, addrList.size()).mapToObj(i -> String.format(\"%s:%d:%d\",\n+                            delimitedHostString(addrList.get(i)), addrList.get(i).getPort(), electionAddrList.get(i).getPort()))\n+                            .collect(Collectors.joining(\"|\")));\n+                }\n             }\n \n             if (type == LearnerType.OBSERVER) {\n                 sw.append(\":observer\");\n             } else if (type == LearnerType.PARTICIPANT) {\n                 sw.append(\":participant\");\n+            } else if (type == LearnerType.WITNESS) {\n+                sw.append(\":witness\");\n             }\n \n             if (clientAddr != null && !isClientAddrFromStatic) {\n@@ -557,7 +611,8 @@ public void checkAddressDuplicate(QuorumServer s) throws BadArgumentsException {\n      */\n     public enum LearnerType {\n         PARTICIPANT,\n-        OBSERVER\n+        OBSERVER,\n+        WITNESS\n     }\n \n     /*\n@@ -2660,4 +2715,64 @@ public static QuorumPeer createFromConfig(QuorumPeerConfig config) throws IOExce\n         return quorumPeer;\n     }\n \n+    public synchronized Set<Long> getCurrentAndNextConfigWitnesses() {\n+        Set<Long> voterIds = new HashSet<Long>(getQuorumVerifier().getWitnessingMembers().keySet());\n+        if (getLastSeenQuorumVerifier() != null) {\n+            voterIds.addAll(getLastSeenQuorumVerifier().getWitnessingMembers().keySet());\n+        }\n+        return voterIds;\n+    }\n+\n+    /**\n+     * Returns true if a witness is present in the current configuration\n+     * Note: I am only relying on quorumVerifier object and NOT on lastSeenQuorumVerfier.\n+     * //TODO: use something similar to currentAndNext config voters. Dont just rely on the uc\n+     * */\n+    boolean isWitnessPresent() {\n+      return (getCurrentAndNextConfigWitnesses().size() > 0);\n+    }\n+\n+    /**\n+     * Returns true if a server with given id is a witness.\n+     * */\n+    boolean isWitness(long id) {\n+        Set<Long> witnesses = getCurrentAndNextConfigWitnesses();\n+        return (witnesses.size() > 0 && witnesses.contains(id));\n+    }\n+\n+    /**\n+     * Set of witnesses that have voted for this Peer during the previous leader election and helped it become the leader.\n+     * THe leader will create witness handlers to talk to these witnesses.\n+     * */\n+    Set<Long> witnessesTobeConnected = new HashSet<>();\n+\n+    Set<Long> getWitnessesTobeConnected() {\n+        return witnessesTobeConnected;\n+    }\n+\n+    /**\n+     * Tell the witness handler manager to create a wintess(s) handler for this witness, if it is not already created\n+     * */\n+    void requestWitnessHandlerCreation(Map<Long, Vote> recvset) {\n+        if(isWitnessPresent()) {\n+            //TODO: Check if fetching currentAndNextConfigWitnesses is CORRECT. Should I only rely on current config.\n+            Set<Long> witnesses = getCurrentAndNextConfigWitnesses();\n+            for(Long id : recvset.keySet()) {\n+                if(witnesses.contains(id)) {\n+                    witnessesTobeConnected.add(id);\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Tell the witness handler manager to create a witness handler for this witness, if it is not already created\n+     * */\n+    void requestWitnessHandlerCreation(long witnessId) {\n+        //The required checks for verifying if this is a witness, are made by the caller.\n+        if(leader != null && leader.witnessHandlerManager != null) {\n+            leader.witnessHandlerManager.startWitnessHandler(witnessId);\n+        }\n+    }\n+\n }"
  },
  {
    "sha": "1cc478592bd89dbde0d432482109b6cfa98432e9",
    "filename": "zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/QuorumPeerConfig.java",
    "status": "modified",
    "additions": 3,
    "deletions": 2,
    "changes": 5,
    "blob_url": "https://github.com/SJSU-CS-systems-group/zookeeper/blob/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/QuorumPeerConfig.java",
    "raw_url": "https://github.com/SJSU-CS-systems-group/zookeeper/raw/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/QuorumPeerConfig.java",
    "contents_url": "https://api.github.com/repos/SJSU-CS-systems-group/zookeeper/contents/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/QuorumPeerConfig.java?ref=ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7",
    "patch": "@@ -670,7 +670,7 @@ public static QuorumVerifier parseDynamicConfig(Properties dynamicConfigProp, in\n \n         QuorumVerifier qv = createQuorumVerifier(dynamicConfigProp, isHierarchical);\n \n-        int numParticipators = qv.getVotingMembers().size();\n+        int numParticipators = qv.getVotingMembers().size() + qv.getWitnessingMembers().size();\n         int numObservers = qv.getObservingMembers().size();\n         if (numParticipators == 0) {\n             if (!standaloneEnabled) {\n@@ -691,13 +691,14 @@ public static QuorumVerifier parseDynamicConfig(Properties dynamicConfigProp, in\n             }\n         } else {\n             if (warnings) {\n+                //TODO: Here the 2 participant + 1 witness concept check will be added.\n                 if (numParticipators <= 2) {\n                     LOG.warn(\"No server failure will be tolerated. You need at least 3 servers.\");\n                 } else if (numParticipators % 2 == 0) {\n                     LOG.warn(\"Non-optimal configuration, consider an odd number of servers.\");\n                 }\n             }\n-\n+            //TODO: Here add a check for witness's election port too.\n             for (QuorumServer s : qv.getVotingMembers().values()) {\n                 if (s.electionAddr == null) {\n                     throw new IllegalArgumentException(\"Missing election port for server: \" + s.id);"
  },
  {
    "sha": "76a866c12a703258ef1fce5e92c3e0fde460d5f6",
    "filename": "zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/SyncedLearnerTracker.java",
    "status": "modified",
    "additions": 38,
    "deletions": 3,
    "changes": 41,
    "blob_url": "https://github.com/SJSU-CS-systems-group/zookeeper/blob/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/SyncedLearnerTracker.java",
    "raw_url": "https://github.com/SJSU-CS-systems-group/zookeeper/raw/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/SyncedLearnerTracker.java",
    "contents_url": "https://api.github.com/repos/SJSU-CS-systems-group/zookeeper/contents/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/SyncedLearnerTracker.java?ref=ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7",
    "patch": "@@ -27,7 +27,7 @@\n     protected ArrayList<QuorumVerifierAcksetPair> qvAcksetPairs = new ArrayList<QuorumVerifierAcksetPair>();\n \n     public void addQuorumVerifier(QuorumVerifier qv) {\n-        qvAcksetPairs.add(new QuorumVerifierAcksetPair(qv, new HashSet<Long>(qv.getVotingMembers().size())));\n+        qvAcksetPairs.add(new QuorumVerifierAcksetPair(qv, new HashSet<Long>(qv.getVotingMembers().size()), new HashSet<Long>(qv.getWitnessingMembers().size())));\n     }\n \n     public boolean addAck(Long sid) {\n@@ -36,6 +36,9 @@ public boolean addAck(Long sid) {\n             if (qvAckset.getQuorumVerifier().getVotingMembers().containsKey(sid)) {\n                 qvAckset.getAckset().add(sid);\n                 change = true;\n+            } else if(qvAckset.getQuorumVerifier().getWitnessingMembers().containsKey(sid)) {\n+                qvAckset.getWitnessAckset().add(sid);\n+                change = true;\n             }\n         }\n         return change;\n@@ -59,24 +62,53 @@ public boolean hasAllQuorums() {\n         return true;\n     }\n \n+    public boolean hasAllQuorumsWithWitness() {\n+        for (QuorumVerifierAcksetPair qvAcksetPair : qvAcksetPairs) {\n+            if (!qvAcksetPair.getQuorumVerifier().containsQuorumWithWitness(qvAcksetPair.getAckset(), qvAcksetPair.getWitnessAckset())) {\n+                return false;\n+            }\n+        }\n+        return true;\n+    }\n+\n     public String ackSetsToString() {\n         StringBuilder sb = new StringBuilder();\n-\n+        sb.append(\"Followers : [\");\n         for (QuorumVerifierAcksetPair qvAckset : qvAcksetPairs) {\n             sb.append(qvAckset.getAckset().toString()).append(\",\");\n         }\n+        sb.append(\"],\");\n \n+        boolean isWitnessPresent = false;\n+        for (QuorumVerifierAcksetPair qvAckset : qvAcksetPairs) {\n+            if(qvAckset.getQuorumVerifier().getWitnessingMembers().size() > 0) {\n+                isWitnessPresent = true;\n+                break;\n+            }\n+        }\n+        if(isWitnessPresent) {\n+            sb.append(\"Witnesses: [\");\n+            for (QuorumVerifierAcksetPair qvAckset : qvAcksetPairs) {\n+                sb.append(qvAckset.getWitnessAckset().toString()).append(\",\");\n+            }\n+            sb.append(\"]\");\n+        }\n         return sb.substring(0, sb.length() - 1);\n     }\n \n+    /**\n+     * Out of the servers in the QV, the ackset tracks, how many of them have voted in the elction.\n+     * */\n     public static class QuorumVerifierAcksetPair {\n \n         private final QuorumVerifier qv;\n         private final HashSet<Long> ackset;\n+        private final HashSet<Long> witnessAckset;\n \n-        public QuorumVerifierAcksetPair(QuorumVerifier qv, HashSet<Long> ackset) {\n+        public QuorumVerifierAcksetPair(QuorumVerifier qv, HashSet<Long> ackset, HashSet<Long> witnessAckset) {\n             this.qv = qv;\n             this.ackset = ackset;\n+            this.witnessAckset = witnessAckset;\n         }\n \n         public QuorumVerifier getQuorumVerifier() {\n@@ -87,6 +119,9 @@ public QuorumVerifier getQuorumVerifier() {\n             return this.ackset;\n         }\n \n+        public HashSet<Long> getWitnessAckset() {\n+            return this.witnessAckset;\n+        }\n     }\n \n }"
  },
  {
    "sha": "399df294671d7b562151f8ed17a2444f7ddd9cc0",
    "filename": "zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/WitnessHandler.java",
    "status": "added",
    "additions": 691,
    "deletions": 0,
    "changes": 691,
    "blob_url": "https://github.com/SJSU-CS-systems-group/zookeeper/blob/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/WitnessHandler.java",
    "raw_url": "https://github.com/SJSU-CS-systems-group/zookeeper/raw/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/WitnessHandler.java",
    "contents_url": "https://api.github.com/repos/SJSU-CS-systems-group/zookeeper/contents/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/WitnessHandler.java?ref=ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7",
    "patch": "@@ -0,0 +1,691 @@\n+package org.apache.zookeeper.server.quorum;\n+\n+import com.google.protobuf.ByteString;\n+import io.grpc.ManagedChannel;\n+import io.grpc.ManagedChannelBuilder;\n+import io.grpc.Status;\n+import io.grpc.StatusRuntimeException;\n+import org.apache.zookeeper.server.ZKDatabase;\n+import org.apache.zookeeper.server.ZooKeeperThread;\n+import org.apache.zookeeper.server.quorum.witness.generated.ReadRequest;\n+import org.apache.zookeeper.server.quorum.witness.generated.ReadResponse;\n+import org.apache.zookeeper.server.quorum.witness.generated.WitnessGrpc;\n+import org.apache.zookeeper.server.quorum.witness.generated.WriteRequest;\n+import org.apache.zookeeper.server.quorum.witness.generated.WriteResponse;\n+import org.apache.zookeeper.server.util.ZxidUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.*;\n+import java.net.InetSocketAddress;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+\n+public class WitnessHandler extends ZooKeeperThread {\n+    /**\n+     * Primary Capabilities:\n+     * 1. Read() from witness.\n+     * 2. write() from witness\n+     * 3. Validate write() operations.\n+     * 5. Convert Proposals into write ops and make writes\n+     * 6. Convert responses returned by witness into metadata and use them as ACKs.\n+     * What does it need:\n+     * 0. A ping thread that should start when the Witness handler is started, so that is asynchronously pings..thw witness\n+     * 1. A sendQueue - the leader thread will add Proposals to this queue.. - Proposals have to be wrapped as\n+     * WitnessProposal as we need the context of whether a witness is active or not decide on how to handle the response.\n+     * 2. A recieveQueue. - Update the prposal with the recieved response...so that the response can be processed appropriately.\n+     * 3. WintessService Synch and async stubs.\n+     * 4.\n+     * */\n+    private static final Logger LOG = LoggerFactory.getLogger(WitnessHandler.class);\n+    ManagedChannel managedChannel;\n+    WitnessGrpc.WitnessBlockingStub stub;\n+    WitnessGrpc.WitnessStub asyncStub;\n+    InetSocketAddress address;\n+\n+    final LearnerMaster learnerMaster;\n+    final QuorumPeer self = null;\n+    final Leader.WitnessHandlerManager witnessHandlerManager;\n+    final AtomicBoolean isActive = new AtomicBoolean(false);\n+\n+    boolean makeActive() {\n+        return isActive.compareAndSet(false, true);\n+    }\n+\n+    boolean makePassive() {\n+        return isActive.compareAndSet(true, false);\n+    }\n+\n+    public boolean isActive() {\n+        return isActive.get();\n+    }\n+\n+    /** Deadline for receiving the next ack. If we are bootstrapping then\n+     * it's based on the initLimit, if we are done bootstrapping it's based\n+     * on the syncLimit. Once the deadline is past this learner should\n+     * be considered no longer \"sync'd\" with the leader. */\n+    volatile long tickOfNextAckDeadline;\n+\n+    /**\n+     * ZooKeeper server identifier of this witness\n+     */\n+    protected long sid = 0;\n+\n+    public long getSid() {\n+        return sid;\n+    }\n+\n+    String getRemoteAddress() {\n+        //TODO: Return appropriate information from the service object that would have been created.\n+        return \"<null>\";\n+    }\n+\n+    public WitnessHandler(long sid, InetSocketAddress address, LearnerMaster learnerMaster, Leader.WitnessHandlerManager witnessHandlerManager) {\n+        //TODO: pass the exact witnessIp+grpcPort\n+        super(\"WitnessHandler-\");\n+        this.sid = sid;\n+        this.address = address;\n+        this.learnerMaster = learnerMaster;\n+        this.witnessHandlerManager = witnessHandlerManager;\n+    }\n+\n+    private void createStubs() {\n+        managedChannel = ManagedChannelBuilder.forAddress(address.getHostString(), address.getPort()).usePlaintext().build();\n+        stub = WitnessGrpc.newBlockingStub(managedChannel);\n+        asyncStub = WitnessGrpc.newStub(managedChannel);\n+    }\n+\n+    private void destroyStubs() {\n+        stub = null;\n+        asyncStub = null;\n+        if(managedChannel!=null) {\n+            managedChannel.shutdownNow();\n+            managedChannel = null;\n+        }\n+    }\n+\n+    final WitnessRequest proposalOfDeath = new WitnessRequest();\n+\n+    public static class WitnessRequest {\n+        public long zxid = -1;\n+        public long batchStartZxid = -1;\n+        public boolean isActive = false;\n+        public Type type;\n+\n+        public enum Type {\n+            READ,\n+            WRITE\n+        }\n+\n+        //proposal of death\n+        public WitnessRequest() {\n+        }\n+\n+        public WitnessRequest(long zxid, boolean isActive) {\n+            this.zxid = zxid;\n+            this.isActive = isActive;\n+            this.type = Type.WRITE;\n+        }\n+\n+        public WitnessRequest(long zxid, long batchStartZxid, boolean isActive) {\n+            this.zxid = zxid;\n+            this.batchStartZxid = batchStartZxid;\n+            this.isActive = isActive;\n+            this.type = Type.WRITE;\n+        }\n+\n+        public WitnessRequest(Type type) {\n+            this.type = type;\n+        }\n+\n+        public long getZxid() {\n+            return zxid;\n+        }\n+\n+        public long getBatchStartZxid() {\n+            return batchStartZxid;\n+        }\n+\n+        public boolean isActive() {\n+            return isActive;\n+        }\n+\n+    }\n+    /**\n+     * The requests to be sent to the Witness\n+     */\n+    final LinkedBlockingQueue<WitnessRequest> witnessRequests = new LinkedBlockingQueue<>();\n+\n+    /**\n+     * Holds requests which are successfully written to the witness.\n+     * */\n+    final LinkedBlockingQueue<WitnessRequest> witnessAcks = new LinkedBlockingQueue<>();\n+\n+    /**\n+     * These two witness metadata fields will be updated and used for cross referencing when ever\n+     * we read or write from a witness.\n+     * */\n+    protected long latestMetadataVersion = -1;\n+    protected WitnessMetadata latestMetadata = new WitnessMetadata(-1, -1, -1);\n+\n+    /**\n+     * Keep track of whether we have started send packets thread\n+     */\n+    private volatile boolean sendingThreadStarted = false;\n+\n+    /**\n+     * This class controls the time that the Leader has been\n+     * waiting for acknowledgement of a proposal from this Learner.\n+     * If the time is above syncLimit, the connection will be closed.\n+     * It keeps track of only one proposal at a time, when the ACK for\n+     * that proposal arrives, it switches to the last proposal received\n+     * or clears the value if there is no pending proposal.\n+     */\n+    private class SyncLimitCheck {\n+\n+        private boolean started = false;\n+        private long currentZxid = 0;\n+        private long currentTime = 0;\n+        private long nextZxid = 0;\n+        private long nextTime = 0;\n+\n+        public synchronized void start() {\n+            started = true;\n+        }\n+\n+        public synchronized void updateProposal(long zxid, long time) {\n+            if (!started) {\n+                return;\n+            }\n+            if (currentTime == 0) {\n+                currentTime = time;\n+                currentZxid = zxid;\n+            } else {\n+                nextTime = time;\n+                nextZxid = zxid;\n+            }\n+        }\n+        //currentTime and currentZxid will become 0 when no other zxid is proposed after the currentZxid\n+        public synchronized void updateAck(long zxid) {\n+            if (currentZxid == zxid) {\n+                currentTime = nextTime;\n+                currentZxid = nextZxid;\n+                nextTime = 0;\n+                nextZxid = 0;\n+            } else if (nextZxid == zxid) {\n+                LOG.warn(\n+                        \"ACK for 0x{} received before ACK for 0x{}\",\n+                        Long.toHexString(zxid),\n+                        Long.toHexString(currentZxid));\n+                nextTime = 0;\n+                nextZxid = 0;\n+            }\n+        }\n+\n+        //This will always return true, when the LearnerHandler thread is not waiting for any ACK..i.e currentTime == 0\n+        public synchronized boolean check(long time) {\n+            if (currentTime == 0) {\n+                return true;\n+            } else {\n+                long msDelay = (time - currentTime) / 1000000;\n+                return (msDelay < learnerMaster.syncTimeout());\n+            }\n+        }\n+\n+    }\n+\n+    private SyncLimitCheck syncLimitCheck = new SyncLimitCheck();\n+\n+    @Override\n+    public void run() {\n+        try {\n+            //1. add this witness handler object to a leader's data structure\n+            //learnerMaster.addLearnerHandler(this);\n+            witnessHandlerManager.witnessHandlers.put(getSid(), this);\n+            witnessHandlerManager.startInProgress.remove(getSid());\n+\n+            //2. Any stub initialization logic goes here\n+            tickOfNextAckDeadline = learnerMaster.getTickOfInitialAckDeadline();\n+            createStubs();\n+\n+            //3. Discovery phase\n+            performDiscovery();\n+\n+            /*4. synchronize witness\n+            TODO: Address the problem, where the witness could get ahead of the leader and other servers..\n+            Refer to the comments in my notes.*/\n+            synchronizeWitness();\n+\n+            //prepare for taking part in the broadcast phase\n+            startSendingPackets();\n+            syncLimitCheck.start();\n+            /*\n+             * Wait until learnerMaster starts up\n+             */\n+            learnerMaster.waitForStartup();\n+\n+\n+            //5. Process responses returned by witness.\n+            while(true) {\n+                WitnessRequest ackedRequest = witnessAcks.take();\n+                if(ackedRequest == proposalOfDeath) {\n+                    //stop processing..you are done\n+                    break;\n+                }\n+                /*\n+                tickOfNextAckDeadline can also be updated when we are adding a response to the\n+                witnessACKs queue\n+                */\n+                tickOfNextAckDeadline = learnerMaster.getTickOfNextAckDeadline();\n+\n+                if(ackedRequest.type.equals(WitnessRequest.Type.WRITE)) {\n+                    syncLimitCheck.updateAck(ackedRequest.getZxid());\n+                    if(ackedRequest.isActive()) {\n+                        //help them reach quorum\n+                        //TODO: For now just passing null for localSocketAddress param. Its just being used for logging.\n+                        /**\n+                         * Send only the last request in the batch to the witness and use the ACK sent by witness for the last request as an indirect ACK for all the requests\n+                         * in that batch.\n+                         * Op2 Impl Approach1: Augment WitnessRequestObject with batchStartZxid field. So when we create WitnessRequest, populate both batchStartZxid and Zxid of last request.\n+                         * Once ACK is received from witness for the last request, WH will invoke processACK() on request from batchStartZxid to Zxid.\n+                         * */\n+                        if(ackedRequest.getBatchStartZxid() != -1) {\n+                            long batchStartZxid = ackedRequest.getBatchStartZxid();\n+                            long batchEndZxid = ackedRequest.getZxid();\n+                            if(batchStartZxid != batchEndZxid) {\n+                                LOG.info(\"Processing ACKs returned by witness {} for the request batch {} to {}\", getSid(), Long.toHexString(batchStartZxid), Long.toHexString(batchEndZxid));\n+                            }\n+                            else\n+                            {\n+                                LOG.info(\"Processing ACK returned by witness {} for request {} \", getSid(), Long.toHexString(batchStartZxid));\n+                            }\n+                            //I am assuming that there will not be any gaps in zxids\n+                            while (batchStartZxid <= batchEndZxid) {\n+                                //processACK is a non blocking call\n+                                learnerMaster.processAck(this.getSid(), batchStartZxid++, null);\n+                            }\n+                        }\n+                        else {\n+                            LOG.info(\"Processing ACK returned by witness {} for request {} \", getSid(), Long.toHexString(ackedRequest.getZxid()));\n+                            learnerMaster.processAck(this.getSid(), ackedRequest.getZxid(), null);\n+                        }\n+                    } else {\n+                        //else just ignore the ACK.\n+                        LOG.info(\"Witness {} was passive at the time this request {}(zxid) was queued, hence ignoring the ACK \", getSid(), Long.toHexString(ackedRequest.getZxid()));\n+                    }\n+                }\n+            }\n+\n+\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        } catch (ClassNotFoundException e) {\n+            e.printStackTrace();\n+        } catch (InterruptedException e) {\n+            e.printStackTrace();\n+        } catch (RuntimeException e) {\n+            if(e instanceof StatusRuntimeException) {\n+                StatusRuntimeException sre = (StatusRuntimeException)e;\n+                if (sre.getStatus() == Status.UNAVAILABLE) {\n+                    LOG.warn(\"Witness {} is unavailable. So shutting down its witness handler\", getSid());\n+                } else {\n+                    LOG.warn(\"Witness {} returned {} status. So shutting down its witness handler\", getSid(), sre.getStatus().toString());\n+                }\n+            }\n+            else {\n+                LOG.error(\"Runtime exception occurred: \", e);\n+            }\n+        }\n+        finally {\n+            shutdown();\n+        }\n+    }\n+\n+    void performDiscovery() throws IOException, ClassNotFoundException, InterruptedException {\n+        //Read current contents of witness.\n+        WitnessResponseWrapper readResp = syncReadFromWitness();\n+        if(readResp.getVersion() == -1) {\n+            //The witness is not in following state, so shutting down the witness handler\n+            shutdown();\n+            return;\n+        }\n+        //Begin Discovery\n+        LOG.info(\"Begin Discovery phase\");\n+        //4. Read the witness's current metadata - this is equivalent to a LearnerHandler receiving FOLLOWER-INFO\n+        latestMetadataVersion = readResp.getVersion();\n+        latestMetadata = readResp.getMetadata();\n+        LOG.info(\"Witness's current info : \\n version = {} \\n {}\", latestMetadataVersion, latestMetadata.toString());\n+        //Use the acceptedEpoch returned by the witness to generate new epoch.\n+        long newEpoch = learnerMaster.getEpochToPropose(this.getSid(), latestMetadata.getAcceptedEpoch());\n+\n+        WitnessMetadata discoveryMetadata = new WitnessMetadata(newEpoch, latestMetadata.getCurrentEpoch(), latestMetadata.getZxid());\n+        WitnessResponseWrapper writeResponse = WitnessResponseWrapper.buildFromWriteResponse(writeMetadata(discoveryMetadata, latestMetadataVersion+1));\n+        if(writeResponse.getVersion() != latestMetadataVersion+1) {\n+            //TODO: Make a more comprehensive write success check.\n+            //Write was unsuccessful.\n+            //TODO: Findout why the write failed and shutdown the witness handler accordingly and return from here.\n+            LOG.info(\"Discovery: Writing newEpoch : {} to witness : {} failed. \\n Expected Version: {} , Returned Version : {}\",\n+                    newEpoch, getSid(), latestMetadataVersion+1, writeResponse.getVersion());\n+            shutdown();\n+            return;\n+        }\n+\n+        LOG.info(\"Received ACKEPOCH from witness : {}, acceptedEpoch is {}\", getSid(), newEpoch);\n+        latestMetadataVersion++;\n+        latestMetadata.setAcceptedEpoch(newEpoch);\n+        StateSummary ss = new StateSummary(latestMetadata.getCurrentEpoch(), latestMetadata.getZxid());\n+        learnerMaster.waitForEpochAck(this.getSid(), ss);\n+        LOG.info(\"END discovery phase. Its acceptedEpoch = {}\", latestMetadata.getAcceptedEpoch());\n+        }\n+\n+    void synchronizeWitness() throws IOException, InterruptedException {\n+        LOG.info(\"SYNC Begin\");\n+        ZKDatabase db = learnerMaster.getZKDatabase();\n+        ReentrantReadWriteLock lock = db.getLogLock();\n+        ReentrantReadWriteLock.ReadLock rl = lock.readLock();\n+        try {\n+            rl.lock();\n+            long maxCommittedLog = db.getmaxCommittedLog();\n+            long lastProcessedZxid = db.getDataTreeLastProcessedZxid();\n+            if(db.getCommittedLog().isEmpty()) {\n+                maxCommittedLog = lastProcessedZxid;\n+            }\n+            LOG.info(\"Witness's current info : \\n version = {} \\n {}\", latestMetadataVersion, latestMetadata.toString());\n+            WitnessMetadata syncMetadata = new WitnessMetadata(latestMetadata.getAcceptedEpoch()\n+                    , latestMetadata.getAcceptedEpoch()\n+                    , maxCommittedLog);\n+            LOG.info(\"Sync info : \\n version = {} \\n {}\", latestMetadataVersion+1, syncMetadata.toString());\n+            WitnessResponseWrapper writeResponse = WitnessResponseWrapper.buildFromWriteResponse(writeMetadata(syncMetadata, latestMetadataVersion+1));\n+            if (writeResponse.getVersion() != latestMetadataVersion+1) {\n+                //Write was unsuccessful\n+                //TODO: Determine why the write has failed. LOG the reason and shutdown the WitnessHandler thread and return\n+                LOG.info(\"Synch: Writing metadata to witness : {} failed. \\n Expected Version: {} , Returned Version : {}\",\n+                        getSid(), latestMetadataVersion+1, writeResponse.getVersion());\n+                shutdown();\n+            }\n+            latestMetadataVersion++;\n+            latestMetadata.setCurrentEpoch(syncMetadata.getCurrentEpoch());\n+            latestMetadata.setZxid(maxCommittedLog);\n+            learnerMaster.waitForNewLeaderAck(getSid(), ZxidUtils.makeZxid(latestMetadata.getCurrentEpoch(), 0));\n+        }\n+        finally {\n+            rl.unlock();\n+        }\n+        LOG.info(\"SYNC END\");\n+        LOG.info(\"Post SYNC: Latest Metadata info : version = {}, \\n {}\", latestMetadataVersion, latestMetadata.toString());\n+    }\n+\n+    WitnessResponseWrapper syncReadFromWitness() throws IOException, ClassNotFoundException {\n+        ReadResponse readResponse = stub.read(ReadRequest.newBuilder().build());\n+        return WitnessResponseWrapper.buildFromReadResponse(readResponse);\n+    }\n+\n+    public static class WitnessResponseWrapper {\n+        long version;\n+        //Currently metadata is null for write response, because it returns only version\n+        WitnessMetadata metadata;\n+        WitnessRequest.Type type;\n+\n+        public static WitnessResponseWrapper buildFromWriteResponse(WriteResponse wResponse) {\n+            return new WitnessResponseWrapper(wResponse.getVersion(), WitnessRequest.Type.WRITE);\n+        }\n+\n+        public static WitnessResponseWrapper buildFromReadResponse(ReadResponse readResponse) throws IOException, ClassNotFoundException {\n+            WitnessMetadata returnedMetadata = createMetadata(readResponse.getMetadata().toByteArray());\n+            return new WitnessResponseWrapper(readResponse.getVersion(), returnedMetadata, WitnessRequest.Type.READ);\n+        }\n+\n+        public WitnessResponseWrapper(long version, WitnessRequest.Type type) {\n+            this.version = version;\n+            this.type = type;\n+        }\n+\n+        public WitnessResponseWrapper(long version, WitnessMetadata metadata, WitnessRequest.Type type) {\n+            this.version = version;\n+            this.metadata = metadata;\n+            this.type = type;\n+        }\n+\n+        public long getVersion() {\n+            return version;\n+        }\n+\n+        public WitnessMetadata getMetadata() {\n+            return metadata;\n+        }\n+\n+        public WitnessRequest.Type getType() {\n+            return type;\n+        }\n+    }\n+\n+    AtomicLong lastQueuedZxid = new AtomicLong(-1);\n+    public void queueRequest(long zxid, boolean isWitnessActive) {\n+        WitnessRequest witnessRequest = new WitnessRequest(zxid, isWitnessActive);\n+        lastQueuedZxid.set(zxid);\n+        witnessRequests.add(witnessRequest);\n+    }\n+\n+    public void queueRequest(WitnessRequest witnessRequest) {\n+        lastQueuedZxid.set(witnessRequest.zxid);\n+        witnessRequests.add(witnessRequest);\n+    }\n+\n+    /**\n+     * ping calls from the learnerMaster to the peers\n+     */\n+    public void ping() {\n+        // If learner hasn't sync properly yet, don't send ping packet\n+        // otherwise, the learner will crash\n+        if (!sendingThreadStarted) {\n+            return;\n+        }\n+        /*  SynclimitCheck may not be required for witness because,\n+            writes to witness happen synchronously\n+         */\n+        if (syncLimitCheck.check(System.nanoTime())) {\n+            witnessRequests.add(new WitnessRequest(WitnessRequest.Type.READ));\n+        } else {\n+            LOG.warn(\"Closing connection to witness due to transaction timeout.\");\n+            shutdown();\n+        }\n+    }\n+\n+    /**\n+     * Start thread that will forward any packet in the queue to the follower\n+     */\n+    protected void startSendingPackets() {\n+        if (!sendingThreadStarted) {\n+            // Start sending packets\n+            new Thread() {\n+                public void run() {\n+                    //TODO: Replace getSid() with the ip+grpcPort string of the witness.\n+                    Thread.currentThread().setName(\"Sender-\" + getSid());\n+                    try {\n+                        sendRequests();\n+                    } catch (InterruptedException e) {\n+                        LOG.warn(\"Unexpected interruption\", e);\n+                    }\n+                }\n+            }.start();\n+            sendingThreadStarted = true;\n+        } else {\n+            LOG.error(\"Attempting to start sending thread after it already started\");\n+        }\n+    }\n+\n+    private void sendRequests() throws InterruptedException {\n+        WitnessMetadata metadata = new WitnessMetadata(latestMetadata.getAcceptedEpoch(), latestMetadata.getCurrentEpoch(), latestMetadata.getZxid());\n+        while (true) {\n+            try {\n+                WitnessRequest request = witnessRequests.take();\n+                if(request == proposalOfDeath) {\n+                    //stop sending requests to the witness\n+                    break;\n+                }\n+                switch (request.type) {\n+                    case WRITE:\n+                        /**\n+                         * 3. Call writeMetadat() function\n+                         * 4. In the write response check,\n+                         *          *          if the sentVersion == returnedVersion,\n+                         *          *              write is succesfull. Add the associated WitnessRequest to the response queue.\n+                         *          *          else\n+                         *          *              //could be because 2 reasons.\n+                         *          *              1.returnedVersion = -1 (witness no longer following)\n+                         *          *              2. Witness has a higher version, this means the witness has moved on to following another server\n+                         *          *             In both these scenario, we consider that the leader has lost the support of witness and shutdown the\n+                         *          *             witness handler\n+                         *          *          else (Some error occurred while invoking the RPC)\n+                         *          *              Based on error, if its retryable, invoke the rpc again.\n+                         *          *              Else, we shutdown the witness handler.\n+                         * */\n+                        syncLimitCheck.updateProposal(request.getZxid(), System.nanoTime());\n+\n+                        long newVersion = latestMetadataVersion + 1;\n+                        //metadata.updateMetadata(self.getAcceptedEpoch(), self.getCurrentEpoch(), request.zxid);\n+                        metadata.setZxid(request.zxid);\n+                        metadata.setAcceptedEpoch(latestMetadata.getAcceptedEpoch());\n+                        metadata.setCurrentEpoch(latestMetadata.getCurrentEpoch());\n+                        WriteResponse response = writeMetadata(metadata, newVersion);\n+                        if(newVersion == response.getVersion()) {\n+                            //The write is successful.\n+                            //TODO: Simple equals check on version, would not suffice, we may have to check the content as well. Refer to the comment\n+                            //on WitnessService.write() function implementation.\n+                            latestMetadata.readWriteLock.writeLock().lock();\n+                            latestMetadataVersion = newVersion;\n+                            latestMetadata.setZxid(metadata.getZxid());\n+                            latestMetadata.setAcceptedEpoch(metadata.getAcceptedEpoch());\n+                            latestMetadata.setCurrentEpoch(metadata.getCurrentEpoch());\n+                            latestMetadata.readWriteLock.writeLock().unlock();\n+                            witnessAcks.add(request);\n+                        }\n+                        else {\n+                            //Shutdown the witness handler.\n+                            shutdown();\n+                        }\n+                        break;\n+                    case READ:\n+                        //TODO: Reads can be performed asynchronously.\n+                        LOG.info(\"Pinging the witness\");\n+                        ReadResponse readResponse = stub.read(ReadRequest.newBuilder().build());\n+                        WitnessMetadata returnedMetadata = createMetadata(readResponse.getMetadata().toByteArray());\n+                        if(latestMetadataVersion == readResponse.getVersion() && latestMetadata.equals(returnedMetadata)) {\n+                            witnessAcks.add(request);\n+                        }\n+                        else {\n+                            //Shutdown the witness handler, witness is not in synch with the leader.\n+                            LOG.info(\"Comparing Read response: localMetadataVersion = {} , returnedVersion = {} \\n localMetadata : {} \\n , returnedMetadata : {}\",\n+                                    latestMetadataVersion, readResponse.getVersion(), latestMetadata.toString(), returnedMetadata.toString());\n+                            LOG.info(\"Shutdown the witness handler, witness is not in synch with the leader\");\n+                            shutdown();\n+                        }\n+                        break;\n+                }\n+\n+            }\n+            catch (IOException | ClassNotFoundException e) {\n+\n+            }\n+            catch (RuntimeException exception) {\n+                if (exception instanceof StatusRuntimeException) {\n+                    StatusRuntimeException sre = (StatusRuntimeException) exception;\n+                    if (sre.getStatus() == Status.UNAVAILABLE) {\n+                        LOG.warn(\"Witness {} is unavailable. So shutting down its witness handler\", getSid());\n+                    } else {\n+                        LOG.warn(\"Witness {} returned {} status. So shutting down its witness handler\", getSid(), sre.getStatus().toString());\n+                    }\n+                    //currently shutting down if the witness returns any sort of exception\n+                }\n+                else {\n+                    LOG.error(\"Run time exception occurred :\" + exception);\n+                }\n+                shutdown();\n+                break;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Constructs a WriteRequest from the given metadata and version.\n+     * Perform a write operation on the witness and returns the response.\n+     * */\n+    WriteResponse writeMetadata(WitnessMetadata metadata, long version) throws IOException {\n+        /**\n+         * 1. Construct the WriteRequest.\n+         * 2. Then perform the write opeartion and get the writeResponse\n+         * */\n+        byte[] metadataByteArr = null;\n+        try {\n+            metadataByteArr = convertToByteArray(metadata);\n+        }\n+        catch (IOException ioe) {\n+            LOG.warn(\"Error while converting Metadata to byte array\", ioe);\n+            throw ioe;\n+        }\n+\n+        ByteString metadataBS = ByteString.copyFrom(metadataByteArr);\n+        WriteRequest writeRequest = WriteRequest.newBuilder()\n+                .setMetadata(metadataBS)\n+                .setVersion(version)\n+                .build();\n+        WriteResponse writeResponse = stub.write(writeRequest);\n+        return writeResponse;\n+    }\n+\n+    /**\n+     * Note: Duplicate Method: The same method exists in witness as well\n+     * */\n+    public byte[] convertToByteArray(WitnessMetadata metadata) throws IOException {\n+        try {\n+            ByteArrayOutputStream bos = new ByteArrayOutputStream();\n+            ObjectOutputStream oos = new ObjectOutputStream(bos);\n+            oos.writeObject(metadata);\n+            oos.flush();\n+            return bos.toByteArray();\n+        } catch (IOException e) {\n+            //TODO: Handle Exception\n+            e.printStackTrace();\n+            throw e;\n+        }\n+    }\n+\n+    /**\n+     * This method takes in a metadata byte array and returns an NEW metadata object\n+     * TODO: Future: Accept, a metadata object as an argument, read the metadatabytearray and populate the passed object\n+     * with information in the array, instead of creating a new object. This reduces the stress on garbage collection.\n+     * */\n+    public static WitnessMetadata createMetadata(byte[] metadataByteArray) throws IOException, ClassNotFoundException {\n+        try {\n+            ObjectInputStream ois = new ObjectInputStream(new ByteArrayInputStream(metadataByteArray));\n+            return (WitnessMetadata) ois.readObject();\n+        } catch (Exception e) {\n+            //TODO: handle execption\n+            e.printStackTrace();\n+            throw e;\n+        }\n+    }\n+\n+    public void shutdown() {\n+        //Send packet of death\n+        try {\n+            witnessRequests.clear();\n+            witnessRequests.put(proposalOfDeath);\n+            witnessAcks.clear();\n+            witnessAcks.put(proposalOfDeath);\n+        } catch (InterruptedException e) {\n+            LOG.warn(\"Ignoring unexpected exception\", e);\n+        }\n+        //Just interrupting would suffice, but also queuing proposal of death to the witnessAcksQueue just in case\n+        this.interrupt();\n+        //TODO: Close any channel or stub related stuff..\n+        destroyStubs();\n+        witnessHandlerManager.witnessHandlers.remove(getSid());\n+    }\n+\n+    public boolean synced() {\n+        return isAlive() && learnerMaster.getCurrentTick() <= tickOfNextAckDeadline;\n+    }\n+}"
  },
  {
    "sha": "8b7ed9b691afeea07c49eda66ea10cdc6a23d2ae",
    "filename": "zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/WitnessMetadata.java",
    "status": "added",
    "additions": 118,
    "deletions": 0,
    "changes": 118,
    "blob_url": "https://github.com/SJSU-CS-systems-group/zookeeper/blob/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/WitnessMetadata.java",
    "raw_url": "https://github.com/SJSU-CS-systems-group/zookeeper/raw/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/WitnessMetadata.java",
    "contents_url": "https://api.github.com/repos/SJSU-CS-systems-group/zookeeper/contents/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/WitnessMetadata.java?ref=ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7",
    "patch": "@@ -0,0 +1,118 @@\n+package org.apache.zookeeper.server.quorum;\n+\n+import java.io.Serializable;\n+import java.util.Objects;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+\n+/**\n+ * This class will be used by both Witness (separate project) and WitnessHandler.\n+ * */\n+public class WitnessMetadata implements Serializable {\n+    /**\n+     * These values are required by the leader election thread.\n+     * So, whenever, a write operation is performed these values should also be updated.,\n+     * */\n+    private AtomicLong acceptedEpoch;\n+    private AtomicLong currentEpoch;\n+    private AtomicLong zxid;\n+    /**\n+     * Callers have explicitly acquire read and write locks before reading or writing metadata.\n+     * I am not adding the acquire and release logic in getters and setters, because the calling functions many times get these values together.\n+     * So they can acquire and release locks at once.\n+     * */\n+    ReadWriteLock readWriteLock = new ReentrantReadWriteLock();\n+\n+    public WitnessMetadata(long acceptedEpoch, long currentEpoch, long zxid) {\n+        this.acceptedEpoch = new AtomicLong(acceptedEpoch);\n+        this.currentEpoch = new AtomicLong(currentEpoch);\n+        this.zxid = new AtomicLong(zxid);\n+    }\n+\n+    /*\n+     * This will be called when you are updating the in memory copy of metadata once you have successfully persisted it.\n+     * This will synchronized on the @WitnessData object\n+     * */\n+    public void updateMetadata(WitnessMetadata newMetadata) {\n+        setAcceptedEpoch(newMetadata.getAcceptedEpoch());\n+        setCurrentEpoch(newMetadata.getCurrentEpoch());\n+        setZxid(newMetadata.getZxid());\n+    }\n+\n+    public void updateMetadata(long acceptedEpoch, long currentEpoch, long zxid) {\n+        setAcceptedEpoch(acceptedEpoch);\n+        setCurrentEpoch(currentEpoch);\n+        setZxid(zxid);\n+    }\n+\n+    public long getAcceptedEpoch() {\n+        return acceptedEpoch.get();\n+    }\n+\n+    public void setAcceptedEpoch(long acceptedEpoch) {\n+        this.acceptedEpoch.set(acceptedEpoch);\n+    }\n+\n+    public long getCurrentEpoch() {\n+        return currentEpoch.get();\n+    }\n+\n+    public void setCurrentEpoch(long currentEpoch) {\n+        this.currentEpoch.set(currentEpoch);\n+    }\n+\n+    public long getZxid() {\n+        return zxid.get();\n+    }\n+\n+    public void setZxid(long zxid) {\n+        this.zxid.set(zxid);\n+    }\n+\n+    public void acquireWriteLock(){\n+        readWriteLock.writeLock().lock();\n+    }\n+\n+    public void releaseWriteLock(){\n+        readWriteLock.writeLock().unlock();\n+    }\n+\n+    public void acquireReadLock() {\n+        readWriteLock.readLock().lock();\n+    }\n+\n+    public void releaseReadLock() {\n+        readWriteLock.readLock().unlock();\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+        if (this == o) return true;\n+        if (o == null || getClass() != o.getClass()) return false;\n+        WitnessMetadata metadata = (WitnessMetadata) o;\n+        return  acceptedEpoch.get() == metadata.acceptedEpoch.get() &&\n+        currentEpoch.get() == metadata.currentEpoch.get() &&\n+        zxid.get() == metadata.zxid.get();\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+        return Objects.hash(acceptedEpoch, currentEpoch, zxid);\n+    }\n+\n+    @Override\n+    public String toString() {\n+        try {\n+            readWriteLock.readLock().lock();\n+            return \"WitnessMetadata{\" +\n+                    \"acceptedEpoch=\" + acceptedEpoch +\n+                    \", currentEpoch=\" + currentEpoch +\n+                    \", zxid=\" + zxid +\n+                    '}';\n+        }\n+        finally {\n+            readWriteLock.readLock().unlock();\n+        }\n+    }\n+}"
  },
  {
    "sha": "edcd31ca0566e32ea09f8bc8701520808d4ae98a",
    "filename": "zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/flexible/QuorumHierarchical.java",
    "status": "modified",
    "additions": 15,
    "deletions": 0,
    "changes": 15,
    "blob_url": "https://github.com/SJSU-CS-systems-group/zookeeper/blob/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/flexible/QuorumHierarchical.java",
    "raw_url": "https://github.com/SJSU-CS-systems-group/zookeeper/raw/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/flexible/QuorumHierarchical.java",
    "contents_url": "https://api.github.com/repos/SJSU-CS-systems-group/zookeeper/contents/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/flexible/QuorumHierarchical.java?ref=ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7",
    "patch": "@@ -369,6 +369,14 @@ public boolean containsQuorum(Set<Long> set) {\n             return false;\n         }\n     }\n+\n+    /*\n+    * Dummy implementation\n+    * */\n+    public boolean containsQuorumWithWitness(Set<Long> ackSet, Set<Long> witnessAckSet) {\n+        return false;\n+    }\n+\n     public Map<Long, QuorumServer> getVotingMembers() {\n         return participatingMembers;\n     }\n@@ -377,6 +385,13 @@ public boolean containsQuorum(Set<Long> set) {\n         return observingMembers;\n     }\n \n+    /*\n+     * Dummy implementation\n+     * */\n+    public Map<Long, QuorumServer> getWitnessingMembers() {\n+        return null;\n+    }\n+\n     public long getVersion() {\n         return version;\n     }"
  },
  {
    "sha": "599b97ba380d645742cc0fe1d9ca614da480c605",
    "filename": "zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/flexible/QuorumMaj.java",
    "status": "modified",
    "additions": 45,
    "deletions": 6,
    "changes": 51,
    "blob_url": "https://github.com/SJSU-CS-systems-group/zookeeper/blob/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/flexible/QuorumMaj.java",
    "raw_url": "https://github.com/SJSU-CS-systems-group/zookeeper/raw/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/flexible/QuorumMaj.java",
    "contents_url": "https://api.github.com/repos/SJSU-CS-systems-group/zookeeper/contents/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/flexible/QuorumMaj.java?ref=ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7",
    "patch": "@@ -37,6 +37,12 @@\n     private Map<Long, QuorumServer> allMembers = new HashMap<Long, QuorumServer>();\n     private Map<Long, QuorumServer> votingMembers = new HashMap<Long, QuorumServer>();\n     private Map<Long, QuorumServer> observingMembers = new HashMap<Long, QuorumServer>();\n+    /*\n+      TODO: Ideally, witness should be a voting member when it is active and a non-voting member when passive.\n+      During election, witness should be voting member..\n+      So, I think it would be better to put witnesses in a separate map and use it based on our need.\n+     */\n+    private Map<Long, QuorumServer> witnessingMembers = new HashMap<Long, QuorumServer>();\n     private long version = 0;\n     private int half;\n \n@@ -78,6 +84,7 @@ public QuorumMaj(Map<Long, QuorumServer> allMembers) {\n                 observingMembers.put(Long.valueOf(qs.id), qs);\n             }\n         }\n+        //TODO: If required include the witness logic also here, just like the below constructor.\n         half = votingMembers.size() / 2;\n     }\n \n@@ -91,16 +98,27 @@ public QuorumMaj(Properties props) throws ConfigException {\n                 long sid = Long.parseLong(key.substring(dot + 1));\n                 QuorumServer qs = new QuorumServer(sid, value);\n                 allMembers.put(Long.valueOf(sid), qs);\n-                if (qs.type == LearnerType.PARTICIPANT) {\n-                    votingMembers.put(Long.valueOf(sid), qs);\n-                } else {\n-                    observingMembers.put(Long.valueOf(sid), qs);\n+                switch (qs.type) {\n+                    case PARTICIPANT:\n+                        votingMembers.put(Long.valueOf(sid), qs);\n+                        break;\n+                    case OBSERVER:\n+                        observingMembers.put(Long.valueOf(sid), qs);\n+                        break;\n+                    case WITNESS:\n+                        witnessingMembers.put(Long.valueOf(sid), qs);\n                 }\n             } else if (key.equals(\"version\")) {\n                 version = Long.parseLong(value, 16);\n             }\n         }\n-        half = votingMembers.size() / 2;\n+        if(witnessingMembers.size() > 0) {\n+            //witness is being used in the config.\n+            half = (votingMembers.size() + witnessingMembers.size()) / 2;\n+        }\n+        else {\n+            half = votingMembers.size() / 2;\n+        }\n     }\n \n     /**\n@@ -129,14 +147,31 @@ public String toString() {\n         return sw.toString();\n     }\n \n+    //TODO: Priyatham: This can be modified to use witness's vote incase a Natural Quorum cannot be formed,\n+    /**\n+     * if(does not contain natural quorum && (witnessActive and witnessAcked)) {n\n+     *     return ackset.size() + 1 > half.\n+     * }\n+     * */\n     /**\n      * Verifies if a set is a majority. Assumes that ackSet contains acks only\n-     * from votingMembers\n+     * from votingMembers.\n+     * Note: Leader election uses the same method to verify quorum with witness, FLE stores acks from\n+     * full replicas and Witnesses in the same ackset. So calling this method directly should work correctly.\n      */\n     public boolean containsQuorum(Set<Long> ackSet) {\n         return (ackSet.size() > half);\n     }\n \n+    /**\n+     * Verifies if the given sets of acks from full replicas and witnesses together secure a majority.\n+     * Assumes that ackSet contains acks only\n+     * from votingMembers and witnessAckset contains votes from witnessingMembers\n+     */\n+    public boolean containsQuorumWithWitness(Set<Long> ackSet, Set<Long> witnessAckSet) {\n+        return ((ackSet.size()+ witnessAckSet.size()) > half);\n+    }\n+\n     public Map<Long, QuorumServer> getAllMembers() {\n         return allMembers;\n     }\n@@ -149,6 +184,10 @@ public boolean containsQuorum(Set<Long> ackSet) {\n         return observingMembers;\n     }\n \n+    public Map<Long, QuorumServer> getWitnessingMembers() {\n+        return witnessingMembers;\n+    }\n+\n     public long getVersion() {\n         return version;\n     }"
  },
  {
    "sha": "b3f984bb2d36d452ca9084ceeb5ef0f86d58bf9d",
    "filename": "zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/flexible/QuorumVerifier.java",
    "status": "modified",
    "additions": 3,
    "deletions": 0,
    "changes": 3,
    "blob_url": "https://github.com/SJSU-CS-systems-group/zookeeper/blob/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/flexible/QuorumVerifier.java",
    "raw_url": "https://github.com/SJSU-CS-systems-group/zookeeper/raw/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/flexible/QuorumVerifier.java",
    "contents_url": "https://api.github.com/repos/SJSU-CS-systems-group/zookeeper/contents/zookeeper-server/src/main/java/org/apache/zookeeper/server/quorum/flexible/QuorumVerifier.java?ref=ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7",
    "patch": "@@ -33,11 +33,14 @@\n \n     long getWeight(long id);\n     boolean containsQuorum(Set<Long> set);\n+    boolean containsQuorumWithWitness(Set<Long> ackSet, Set<Long> witnessAckSet);\n+    //TODO: Priyatham: I think, each time the servers in the quorum changes because of a reconfig, the QV version is incremented.\n     long getVersion();\n     void setVersion(long ver);\n     Map<Long, QuorumServer> getAllMembers();\n     Map<Long, QuorumServer> getVotingMembers();\n     Map<Long, QuorumServer> getObservingMembers();\n+    Map<Long, QuorumServer> getWitnessingMembers();\n     boolean equals(Object o);\n     String toString();\n "
  },
  {
    "sha": "b2e68e462267e50e08ea105628f69b8e46573dc2",
    "filename": "zookeeper-server/src/main/proto/witness_v1.proto",
    "status": "added",
    "additions": 37,
    "deletions": 0,
    "changes": 37,
    "blob_url": "https://github.com/SJSU-CS-systems-group/zookeeper/blob/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/proto/witness_v1.proto",
    "raw_url": "https://github.com/SJSU-CS-systems-group/zookeeper/raw/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/main/proto/witness_v1.proto",
    "contents_url": "https://api.github.com/repos/SJSU-CS-systems-group/zookeeper/contents/zookeeper-server/src/main/proto/witness_v1.proto?ref=ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7",
    "patch": "@@ -0,0 +1,37 @@\n+syntax = \"proto3\";\n+\n+option java_multiple_files = true;\n+option java_package = \"org.apache.zookeeper.server.quorum.witness.generated\";\n+option java_outer_classname = \"WitnessRegister\";\n+\n+service Witness {\n+  rpc read(ReadRequest) returns (ReadResponse);\n+  rpc write(WriteRequest) returns (WriteResponse);\n+}\n+/**\n+//TODO:\n+   1. Only respond to requests when you are in follower mode..\n+        i.e respond to read with version = -1..which informs that witness is not yet ready\n+   2. Add nodeId to the read and write requests so that the witness only responds to the leader.\n+      This I feel prevents information leak, as the witness has to serve only the leader..\n+*/\n+\n+message ReadRequest {\n+}\n+\n+message ReadResponse {\n+  uint64 version = 1;\n+  bytes metadata = 2;\n+}\n+\n+message WriteRequest {\n+  uint64 version = 1;\n+  bytes metadata = 2;\n+}\n+\n+message WriteResponse {\n+  // request.version != response.version when, witness has a more latest version than the one in the response..\n+  // the caller(witnessHandler) can take necessary action.\n+\n+  uint64 version = 1;\n+}"
  },
  {
    "sha": "1b563b6ed6dcf472f83f2716efbeb9f3130869f2",
    "filename": "zookeeper-server/src/test/resources/data/invalidsnap/version-2/snapshot.83f",
    "status": "modified",
    "additions": 0,
    "deletions": 0,
    "changes": 0,
    "blob_url": "https://github.com/SJSU-CS-systems-group/zookeeper/blob/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/test/resources/data/invalidsnap/version-2/snapshot.83f",
    "raw_url": "https://github.com/SJSU-CS-systems-group/zookeeper/raw/ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7/zookeeper-server/src/test/resources/data/invalidsnap/version-2/snapshot.83f",
    "contents_url": "https://api.github.com/repos/SJSU-CS-systems-group/zookeeper/contents/zookeeper-server/src/test/resources/data/invalidsnap/version-2/snapshot.83f?ref=ee22ef7c956f57c8dcb9c5bdb3ed5f61cc7234a7"
  }
]
