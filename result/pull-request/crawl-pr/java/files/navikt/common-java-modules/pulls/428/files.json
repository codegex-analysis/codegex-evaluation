[
  {
    "sha": "0ef02d58ff3640c7477e0e940d88ca3524eb0456",
    "filename": "bom/pom.xml",
    "status": "modified",
    "additions": 12,
    "deletions": 1,
    "changes": 13,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/bom/pom.xml",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/bom/pom.xml",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/bom/pom.xml?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -43,8 +43,9 @@\n         <commons-io.version>2.5</commons-io.version>\n         <httpclient.version>4.5.6</httpclient.version>\n         <httpcore.version>4.4</httpcore.version>\n-        <shedlock.version>0.18.2</shedlock.version>\n+        <shedlock.version>4.21.0</shedlock.version>\n         <handlebars.version>4.1.2</handlebars.version>\n+        <kafka.version>2.4.1</kafka.version>\n \n         <!-- Test -->\n         <assertj.version>3.9.1</assertj.version>\n@@ -121,6 +122,11 @@\n                 <artifactId>json</artifactId>\n                 <version>${project.version}</version>\n             </dependency>\n+            <dependency>\n+                <groupId>no.nav.common</groupId>\n+                <artifactId>kafka</artifactId>\n+                <version>${project.version}</version>\n+            </dependency>\n             <dependency>\n                 <groupId>no.nav.common</groupId>\n                 <artifactId>json</artifactId>\n@@ -447,6 +453,11 @@\n                 <artifactId>shedlock-core</artifactId>\n                 <version>${shedlock.version}</version>\n             </dependency>\n+            <dependency>\n+                <groupId>org.apache.kafka</groupId>\n+                <artifactId>kafka-clients</artifactId>\n+                <version>${kafka.version}</version>\n+            </dependency>\n \n             <!-- jetty -->\n             <dependency>"
  },
  {
    "sha": "1f047892759523eb50f2cc89987ae8a3493ba201",
    "filename": "kafka/README.md",
    "status": "added",
    "additions": 193,
    "deletions": 0,
    "changes": 193,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/README.md",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/README.md",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/README.md?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,193 @@\n+# NAV common kafka\n+\n+Formålet for denne modulen er å gjøre det lettere å integrere mot Kafka med sane defaults.\n+Det er også lagt til funksjonalitet for standard feilhåndtering samt forslag til konfigurering av kafka for konsumenter og produsenter.\n+Modulen er ikke avhengig av rammeverk som f.eks Spring og kan brukes uavhengig.\n+\n+## Installering\n+\n+Legg til følgende i pom.xml. Hvis man allerede har avhengigheter som trekker inn kafka (som f.eks spring-kafka), så anbefales det å fjerne disse.\n+```xml\n+<dependency>\n+    <groupId>no.nav.common</groupId>\n+    <artifactId>kafka</artifactId>\n+</dependency>\n+```\n+\n+## Consumer\n+\n+### Basic\n+\n+```java\n+Credentials credentials = new Credentials(\"username\", \"password\");\n+\n+KafkaConsumerClient<String, String> consumerClient = KafkaConsumerClientBuilder.<String, String>builder()\n+        .withProps(KafkaProperties.defaultConsumerProperties(\"group_id\", \"broker_url\", credentials))\n+        .withTopic(\"topic1\", (record) -> {\n+            System.out.println(\"Record from topic 1: \" + record.value());\n+            return ConsumeStatus.OK;\n+        })\n+        .withTopic(\"topic2\", (record) -> {\n+            System.out.println(\"Record from topic 2: \" + record.value());\n+            return ConsumeStatus.OK;\n+        })\n+        .build();\n+\n+consumerClient.start();\n+\n+// Records will be consumed from topic1 and topic2\n+```\n+\n+### Feilhåndtering\n+\n+Feilhåndtering for consumer settes opp pr topic ved bruk av `StoreOnFailureTopicConsumer`. \n+Feilhåndteringen vil sørge for at meldinger blir lagret hvis det feiler under konsumeringen, f.eks hvis en ekstern tjeneste feiler for en bestemt bruker.\n+Hvis feilhåndtering ikke brukes så vil konsumeringen stå stille helt til konsumeringen går igjennom.\n+\n+Hvordan sette opp topic med feilhåndtering:\n+```java\n+KafkaConsumerRepository<String, String> consumerRepository = new OracleConsumerRepository<>(\n+        dataSource,\n+        new StringSerializer(),\n+        new StringDeserializer(),\n+        new StringSerializer(),\n+        new StringDeserializer()\n+);\n+\n+TopicConsumer<String, String> consumer = TopicConsumerBuilder.<String, String>builder()\n+        .withConsumer((record -> ConsumeStatus.OK))\n+        .withStoreOnFailure(consumerRepository)\n+        .build();\n+\n+KafkaConsumerClient<String, String> consumerClient = KafkaConsumerClientBuilder.<String, String>builder()\n+        .withProps(KafkaProperties.defaultConsumerProperties(\"group_id\", \"broker_url\", credentials))\n+        .withTopic(\"topic1\", consumer)\n+        .build();\n+```\n+\n+Hvordan sette opp retry for feilede konsumerte meldinger:\n+```java\n+Map<String, TopicConsumer<String, String>> topics = new HashMap<>();\n+// Add topics to the map\n+\n+KafkaRetryConsumerRecordHandler<String, String> retryConsumerHandler = new KafkaRetryConsumerRecordHandler<>(topics, consumerRepository);\n+\n+// This should be used periodically in a schedule\n+retryConsumerHandler.consumeFailedMessages();\n+```\n+\n+#### NB\n+\n+Siden meldingene lagres til databasen og vi går videre til å konsumere neste melding så må det også lages en periodisk jobb for å rekonsumere feilede meldinger.\n+\n+En ting som er viktig å være obs på er at når man lagrer unna feilede meldinger, så mister man garantien for at meldinger blir lest inn i riktig rekkefølge.\n+\n+F.eks \n+```\n+    Melding 1 for bruker med fnr: 123 -> Konsumering feiler, lagrer i databasen\n+    Melding 2 for bruker med fnr: 456 -> Konsumering fullført, trenger ikke å lagre\n+    Periodisk jobb -> Prøver å konsumere melding 1 på nytt, feiler fortsatt\n+    Melding 3 for bruker med fnr: 123 -> Konsumering fullført, trenger ikke å lagre\n+    Periodisk jobb -> Prøver å konsumere melding 1 på nytt, melding blir sendt\n+```\n+\n+I dette tilfellet så blir melding 1 publisert etter melding 3, som kan føre til problemer hvis konsumenten ikke håndterer dette riktig.\n+\n+### Metrikker\n+\n+Det er lagt til støtte for et sett med default prometheus metrikker for consumer/producer.\n+\n+Metrikker for consumer settes opp pr topic:\n+```java\n+MeterRegistry registry = /* ... */;\n+\n+TopicConsumer<String, String> consumer1 = TopicConsumerBuilder.<String, String>builder()\n+        .withConsumer((record -> ConsumeStatus.OK))\n+        .withMetrics(registry)\n+        .withLogging() // Kan også legge på optional logging til stdout\n+        .build();\n+```\n+\n+\n+## ===============================\n+\n+\n+## Producer\n+\n+### Basic\n+\n+```java\n+Credentials credentials = new Credentials(\"username\", \"password\");\n+\n+KafkaProducerClient<String, String> producerClient = KafkaProducerClientBuilder.<String, String>builder()\n+        .withProps(KafkaProperties.defaultProducerProperties(\"group_id\", \"broker_url\", credentials))\n+        .build();\n+\n+// Send synchronously. Will block until sent or throw an exception\n+producerClient.sendSync(new ProducerRecord<>(\"topic\", \"key\", \"value\"));\n+\n+// Send asynchronously. Will batch up records and send after a short time has passed. Callback is triggered for both failure and success\n+producerClient.send(new ProducerRecord<>(\"topic\", \"key\", \"value\"), ((metadata, exception) -> { /* ... */ }));\n+```\n+\n+### Feilhåntering\n+\n+Basic produceren er relativt enkel og vil i flere tilfeller være den foretrukkede måten og sende meldinger på.\n+F.eks hvis man ikke sender store mengder med meldinger og kan tåle at Kafka er nede, så kan man bruke `sendSync()`\n+som vil blocke eller kaste et exception hvis meldingen ikke ble sendt.\n+\n+Hvis man sender asynkront så vil man ikke være garantert at meldignen er sendt med mindre man lagrer meldingen før man prøver å sende den ut.\n+Kafka-modulen inneholder en producer som implementerer store-and-forward patternet, som lagrer meldingen før den sender, og fjerner meldingen hvis sendingen gikk greit.\n+\n+For å bruke store-and-forward så må det til en del mer konfigurasjon. Først så må tabellene settes opp slik at vi kan lagre meldingene når ting feiler.\n+Nedenfor så ligger det lenker til ferdig SQL som kan brukes for Oracle og PostgreSQL.\n+\n+[Oracle](src/test/resources/kafka-producer-record-oracle.sql)\n+\n+[Postgres](src/test/resources/kafka-producer-record-postgres.sql)\n+\n+Deretter så må det settes opp med builderen når man lager produceren at man ønsker å bruke store-and-forward.\n+\n+```java\n+DataSource dataSource = null; // Must be retrieved from somewhere\n+        \n+KafkaProducerRepository<String, String> producerRepository = new OracleProducerRepository<>(\n+        dataSource,\n+        new StringSerializer(),\n+        new StringDeserializer(),\n+        new StringSerializer(),\n+        new StringDeserializer()\n+);\n+\n+KafkaProducerClient<String, String> producerClient = KafkaProducerClientBuilder.<String, String>builder()\n+        .withProps(KafkaProperties.defaultProducerProperties(\"group_id\", \"broker_url\", credentials))\n+        .withStoreAndForward(producerRepository)\n+        .build();\n+```\n+\n+Dette vil sørge for at meldingene blir lagret i databasen først før man sender og at de blir fjernet hvis meldingen ble sendt.\n+Men hvis kafka er nede så trengs det også logikk for å periodisk sjekke om det ligger meldinger i databasen som ikke har blitt lagt ut på kafka.\n+\n+```java\n+// It is important to not use the store-and-forward producer with KafkaRetryProducerRecordHandler, \n+//  or else a new record will be stored in the database if the retry handler fails to send a record\n+KafkaProducerClient<String, String> producerClient = KafkaProducerClientBuilder.<String, String>builder()\n+        .withProps(KafkaProperties.defaultProducerProperties(\"group_id\", \"broker_url\", credentials))\n+        .build();\n+\n+KafkaRetryProducerRecordHandler<String, String> retryProducerRecordHandler = new KafkaRetryProducerRecordHandler<>(List.of(\"topic1\"), producerRepository, producerClient);\n+\n+// This should be used periodically in a schedule\n+retryProducerRecordHandler.sendFailedMessages();\n+```\n+### Metrikker\n+\n+Metrikker for producer kan settes opp gjennom builder:\n+```java\n+MeterRegistry registry = /* ... */;\n+\n+KafkaProducerClient<String, String> producerClient = KafkaProducerClientBuilder.<String, String>builder()\n+        .withProps(KafkaProperties.defaultProducerProperties(\"group_id\", \"broker_url\", credentials))\n+        .withMetrics(registry)\n+        .build();\n+```"
  },
  {
    "sha": "5065c0b33dd29849d4fc5e9507bd01a608d89c11",
    "filename": "kafka/pom.xml",
    "status": "added",
    "additions": 94,
    "deletions": 0,
    "changes": 94,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/pom.xml",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/pom.xml",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/pom.xml?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,94 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n+         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n+         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+    <parent>\n+        <artifactId>mor-pom</artifactId>\n+        <groupId>no.nav.common</groupId>\n+        <version>1-SNAPSHOT</version>\n+    </parent>\n+    <modelVersion>4.0.0</modelVersion>\n+\n+    <artifactId>kafka</artifactId>\n+\n+    <dependencyManagement>\n+        <dependencies>\n+            <dependency>\n+                <groupId>no.nav.common</groupId>\n+                <artifactId>bom</artifactId>\n+                <version>${project.parent.version}</version>\n+                <type>pom</type>\n+                <scope>import</scope>\n+            </dependency>\n+        </dependencies>\n+    </dependencyManagement>\n+\n+    <dependencies>\n+        <dependency>\n+            <groupId>org.slf4j</groupId>\n+            <artifactId>slf4j-api</artifactId>\n+        </dependency>\n+        <dependency>\n+            <groupId>org.apache.kafka</groupId>\n+            <artifactId>kafka-clients</artifactId>\n+        </dependency>\n+        <dependency>\n+            <groupId>net.javacrumbs.shedlock</groupId>\n+            <artifactId>shedlock-core</artifactId>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.projectlombok</groupId>\n+            <artifactId>lombok</artifactId>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>io.micrometer</groupId>\n+            <artifactId>micrometer-core</artifactId>\n+            <optional>true</optional>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>no.nav.common</groupId>\n+            <artifactId>json</artifactId>\n+        </dependency>\n+        <dependency>\n+            <groupId>no.nav.common</groupId>\n+            <artifactId>job</artifactId>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>no.nav.common</groupId>\n+            <artifactId>log</artifactId>\n+            <scope>test</scope>\n+        </dependency>\n+        <dependency>\n+            <groupId>com.h2database</groupId>\n+            <artifactId>h2</artifactId>\n+            <version>1.4.200</version>\n+            <scope>test</scope>\n+        </dependency>\n+        <dependency>\n+            <groupId>org.testcontainers</groupId>\n+            <artifactId>kafka</artifactId>\n+            <version>1.15.2</version>\n+            <scope>test</scope>\n+        </dependency>\n+        <dependency>\n+            <groupId>org.junit.vintage</groupId>\n+            <artifactId>junit-vintage-engine</artifactId>\n+            <scope>test</scope>\n+        </dependency>\n+        <dependency>\n+            <groupId>junit</groupId>\n+            <artifactId>junit</artifactId>\n+            <scope>test</scope>\n+        </dependency>\n+        <dependency>\n+            <groupId>org.mockito</groupId>\n+            <artifactId>mockito-core</artifactId>\n+            <scope>test</scope>\n+        </dependency>\n+    </dependencies>\n+\n+</project>"
  },
  {
    "sha": "8bc2aa6138368f4fd0f55afce3312dcd39223673",
    "filename": "kafka/src/main/java/no/nav/common/kafka/consumer/ConsumeStatus.java",
    "status": "added",
    "additions": 8,
    "deletions": 0,
    "changes": 8,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/ConsumeStatus.java",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/ConsumeStatus.java",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/src/main/java/no/nav/common/kafka/consumer/ConsumeStatus.java?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,8 @@\n+package no.nav.common.kafka.consumer;\n+\n+/**\n+ * Specifies the status of a consumed record.\n+ */\n+public enum ConsumeStatus {\n+    OK, FAILED\n+}"
  },
  {
    "sha": "a7de4b9f77c9a90c8cb415ef20cc62a99a81edce",
    "filename": "kafka/src/main/java/no/nav/common/kafka/consumer/KafkaConsumerClient.java",
    "status": "added",
    "additions": 228,
    "deletions": 0,
    "changes": 228,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/KafkaConsumerClient.java",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/KafkaConsumerClient.java",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/src/main/java/no/nav/common/kafka/consumer/KafkaConsumerClient.java?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,228 @@\n+package no.nav.common.kafka.consumer;\n+\n+import no.nav.common.kafka.consumer.util.ConsumerUtils;\n+import org.apache.kafka.clients.consumer.*;\n+import org.apache.kafka.common.TopicPartition;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Duration;\n+import java.util.*;\n+import java.util.concurrent.*;\n+\n+import static java.lang.String.format;\n+\n+public class KafkaConsumerClient<K, V> implements ConsumerRebalanceListener {\n+\n+    public final static long DEFAULT_POLL_DURATION_MS = 1000;\n+\n+    private final static long POLL_ERROR_TIMEOUT_MS = 5000;\n+\n+    private enum ClientState {\n+       RUNNING, NOT_RUNNING\n+    }\n+\n+    private final Logger log = LoggerFactory.getLogger(KafkaConsumerClient.class);\n+\n+    private final ExecutorService pollExecutor = Executors.newSingleThreadExecutor();\n+\n+    private final Map<TopicPartition, OffsetAndMetadata> currentOffsets = new ConcurrentHashMap<>();\n+\n+    private final Set<TopicPartition> revokedOrFailedPartitions = ConcurrentHashMap.newKeySet();\n+\n+    private final KafkaConsumerClientConfig<K, V> config;\n+\n+    private volatile ClientState clientState = ClientState.NOT_RUNNING;\n+\n+    private volatile CountDownLatch processedRecordsLatch;\n+\n+    private volatile CountDownLatch shutdownLatch;\n+\n+    private KafkaConsumer<K, V> consumer;\n+\n+    public KafkaConsumerClient(KafkaConsumerClientConfig<K, V> config) {\n+        validateConfig(config);\n+\n+        this.config = config;\n+\n+        Runtime.getRuntime().addShutdownHook(new Thread(this::stop));\n+    }\n+\n+    public void start() {\n+        if (clientState == ClientState.RUNNING) {\n+            return;\n+        }\n+\n+        clientState = ClientState.RUNNING;\n+\n+        log.info(\"Starting kafka consumer client...\");\n+\n+        pollExecutor.submit(this::consumeTopics);\n+    }\n+\n+    public void stop() {\n+        if (clientState != ClientState.RUNNING) {\n+            return;\n+        }\n+\n+        clientState = ClientState.NOT_RUNNING;\n+\n+        log.info(\"Stopping kafka consumer client...\");\n+\n+        try {\n+            shutdownLatch.await(15, TimeUnit.SECONDS);\n+            log.info(\"Kafka client stopped\");\n+        } catch (InterruptedException e) {\n+            log.error(\"Failed to stop gracefully\", e);\n+        }\n+    }\n+\n+    public boolean isRunning() {\n+        return clientState == ClientState.RUNNING;\n+    }\n+\n+    @Override\n+    public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+        log.info(\"Partitions has been revoked from consumer: \" + Arrays.toString(partitions.toArray()));\n+        revokedOrFailedPartitions.addAll(partitions);\n+        try {\n+            commitCurrentOffsets();\n+        } catch (Exception e) {\n+            log.error(\"Failed to commit offsets when partitions were revoked: \" + currentOffsets.toString(), e);\n+        }\n+    }\n+\n+    @Override\n+    public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+        log.info(\"New partitions has been assigned to the consumer: \" + Arrays.toString(partitions.toArray()));\n+    }\n+\n+    private void consumeTopics() {\n+        try {\n+            final List<String> topicNames = new ArrayList<>(config.topics.keySet());\n+            final Map<String, ExecutorService> topicConsumptionExecutors = createTopicExecutors(topicNames);\n+\n+            shutdownLatch = new CountDownLatch(1);\n+            consumer = new KafkaConsumer<>(config.properties);\n+            consumer.subscribe(topicNames, this);\n+\n+            while (clientState == ClientState.RUNNING) {\n+                ConsumerRecords<K, V> records;\n+\n+                revokedOrFailedPartitions.clear();\n+                currentOffsets.clear();\n+\n+                try {\n+                    records = consumer.poll(Duration.ofMillis(config.pollDurationMs));\n+                } catch (Exception e) {\n+                    log.error(\"Exception occurred during polling of records. Waiting before trying again.\", e);\n+                    Thread.sleep(POLL_ERROR_TIMEOUT_MS);\n+                    continue;\n+                }\n+\n+                if (records.isEmpty()) {\n+                    continue;\n+                }\n+\n+                int totalRecords = records.count();\n+\n+                processedRecordsLatch = new CountDownLatch(totalRecords);\n+\n+                for (ConsumerRecord<K, V> record : records) {\n+                    String topic = record.topic();\n+                    TopicConsumer<K, V> topicConsumer = config.topics.get(topic);\n+                    ExecutorService executor = topicConsumptionExecutors.get(topic);\n+                    TopicPartition topicPartition = new TopicPartition(record.topic(), record.partition());\n+\n+                    executor.submit(() -> {\n+                        try {\n+                            /*\n+                                If the client is stopped then fast-track through the remaining records so that\n+                                 we can wait for those that are already being consumed after the while-loop.\n+\n+                                If some previous record on the same topic + partition has failed to be consumed or has been revoked during rebalancing,\n+                                 then we cannot consume any more records for this topic + partition for the duration of this poll.\n+\n+                                 The finally-clause will make sure that the processed records counter is incremented.\n+                            */\n+                            if (clientState == ClientState.NOT_RUNNING || revokedOrFailedPartitions.contains(topicPartition)) {\n+                                return;\n+                            }\n+\n+                            ConsumeStatus status = ConsumerUtils.safeConsume(topicConsumer, record);\n+\n+                            if (status == ConsumeStatus.OK) {\n+                                OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(record.offset());\n+                                currentOffsets.put(topicPartition, offsetAndMetadata);\n+                            } else {\n+                                revokedOrFailedPartitions.add(topicPartition);\n+                            }\n+                        } catch (Exception e) {\n+                            String msg = format(\n+                                    \"Unexpected error occurred wile processing record. topic=%s partition=%d offset=%d\",\n+                                    topic, record.partition(), record.offset()\n+                            );\n+                            log.error(msg, e);\n+                        } finally {\n+                            processedRecordsLatch.countDown();\n+                        }\n+                    });\n+                }\n+\n+                processedRecordsLatch.await();\n+\n+                try {\n+                    commitCurrentOffsets();\n+                } catch (Exception e) {\n+                    // If we fail to commit offsets then continue polling records\n+                    log.error(\"Failed to commit offsets: \" + currentOffsets.toString(), e);\n+                }\n+            }\n+        } catch (Exception e) {\n+            log.error(\"Unexpected exception caught from main loop. Shutting down...\", e);\n+        } finally {\n+            try {\n+                commitCurrentOffsets();\n+                consumer.close(Duration.ofSeconds(3));\n+            } catch (Exception e) {\n+                log.error(\"Failed to shutdown properly\", e);\n+            } finally {\n+                shutdownLatch.countDown();\n+\n+                if (clientState == ClientState.RUNNING) {\n+                    log.warn(\"Unexpected failure while client was running. Restarting...\");\n+                    pollExecutor.submit(this::consumeTopics);\n+                }\n+            }\n+        }\n+    }\n+\n+    private void commitCurrentOffsets() {\n+        if (!currentOffsets.isEmpty()) {\n+            consumer.commitSync(currentOffsets, Duration.ofSeconds(3));\n+            log.info(\"Offsets commited: \" + currentOffsets.toString());\n+            currentOffsets.clear();\n+        }\n+    }\n+\n+    private static void validateConfig(KafkaConsumerClientConfig<?, ?> config) {\n+        if (config.topics.isEmpty()) {\n+            throw new IllegalArgumentException(\"\\\"topics\\\" must contain at least 1 topic\");\n+        }\n+\n+        if (config.pollDurationMs <= 0) {\n+            throw new IllegalArgumentException(\"\\\"pollDurationMs\\\" must be larger than 0\");\n+        }\n+\n+        if (!Boolean.FALSE.equals(config.properties.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG))) {\n+            throw new IllegalArgumentException(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG + \" must be false!\");\n+        }\n+    }\n+\n+    private static Map<String, ExecutorService> createTopicExecutors(Iterable<String> topics) {\n+        Map<String, ExecutorService> executorsMap = new HashMap<>();\n+        topics.forEach(topic -> executorsMap.put(topic, Executors.newSingleThreadExecutor()));\n+        return executorsMap;\n+    }\n+\n+}"
  },
  {
    "sha": "36b8d0a73d4a245d76fb1f113d1d31fd0515817e",
    "filename": "kafka/src/main/java/no/nav/common/kafka/consumer/KafkaConsumerClientConfig.java",
    "status": "added",
    "additions": 25,
    "deletions": 0,
    "changes": 25,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/KafkaConsumerClientConfig.java",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/KafkaConsumerClientConfig.java",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/src/main/java/no/nav/common/kafka/consumer/KafkaConsumerClientConfig.java?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,25 @@\n+package no.nav.common.kafka.consumer;\n+\n+import lombok.Data;\n+\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static no.nav.common.kafka.consumer.KafkaConsumerClient.DEFAULT_POLL_DURATION_MS;\n+\n+@Data\n+public class KafkaConsumerClientConfig<K, V> {\n+\n+    Properties properties;\n+\n+    Map<String, TopicConsumer<K, V>> topics;\n+\n+    long pollDurationMs;\n+\n+    public KafkaConsumerClientConfig(Properties properties, Map<String, TopicConsumer<K, V>> topics) {\n+        this.properties = properties;\n+        this.topics = topics;\n+        this.pollDurationMs = DEFAULT_POLL_DURATION_MS;\n+    }\n+\n+}"
  },
  {
    "sha": "7a803aeaf0168d5d9e859d97d298dea2e544b029",
    "filename": "kafka/src/main/java/no/nav/common/kafka/consumer/TopicConsumer.java",
    "status": "added",
    "additions": 9,
    "deletions": 0,
    "changes": 9,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/TopicConsumer.java",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/TopicConsumer.java",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/src/main/java/no/nav/common/kafka/consumer/TopicConsumer.java?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,9 @@\n+package no.nav.common.kafka.consumer;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+\n+public interface TopicConsumer<K, V> {\n+\n+    ConsumeStatus consume(ConsumerRecord<K, V> record);\n+\n+}"
  },
  {
    "sha": "573c00c4a96931a716eff194c3519fbcef7d1790",
    "filename": "kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/KafkaConsumerRecordProcessor.java",
    "status": "added",
    "additions": 166,
    "deletions": 0,
    "changes": 166,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/KafkaConsumerRecordProcessor.java",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/KafkaConsumerRecordProcessor.java",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/KafkaConsumerRecordProcessor.java?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,166 @@\n+package no.nav.common.kafka.consumer.feilhandtering;\n+\n+import net.javacrumbs.shedlock.core.LockConfiguration;\n+import net.javacrumbs.shedlock.core.LockProvider;\n+import net.javacrumbs.shedlock.core.SimpleLock;\n+import no.nav.common.kafka.consumer.ConsumeStatus;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.*;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+\n+public class KafkaConsumerRecordProcessor {\n+\n+    private final static long ERROR_TIMEOUT_MS = 5000;\n+\n+    private final static long POLL_TIMEOUT_MS = 3000;\n+\n+    private final static int RECORDS_BATCH_SIZE = 100;\n+\n+    private final Logger log = LoggerFactory.getLogger(this.getClass());\n+\n+    private final ExecutorService executorService = Executors.newSingleThreadExecutor();\n+\n+    private final LockProvider lockProvider;\n+\n+    private final KafkaConsumerRepository kafkaConsumerRepository;\n+\n+    private final Map<String, StoredRecordConsumer> recordConsumers;\n+\n+    private volatile boolean isRunning;\n+\n+    private volatile boolean isClosed;\n+\n+    public KafkaConsumerRecordProcessor(\n+            LockProvider lockProvider,\n+            KafkaConsumerRepository kafkaRepository,\n+            Map<String, StoredRecordConsumer> recordConsumers\n+    ) {\n+        this.lockProvider = lockProvider;\n+        this.kafkaConsumerRepository = kafkaRepository;\n+        this.recordConsumers = recordConsumers;\n+        Runtime.getRuntime().addShutdownHook(new Thread(this::close));\n+    }\n+\n+    public void start() {\n+        if (isClosed) {\n+            throw new IllegalStateException(\"Cannot start closed consumer record forwarder\");\n+        }\n+\n+        if (!isRunning) {\n+            executorService.submit(this::recordHandlerLoop);\n+        }\n+    }\n+\n+    public void close() {\n+        isRunning = false;\n+        isClosed = true;\n+    }\n+\n+    private void recordHandlerLoop() {\n+        isRunning = true;\n+\n+        List<String> topics = new ArrayList<>(recordConsumers.keySet());\n+\n+        try {\n+            while (isRunning) {\n+                try {\n+                    List<TopicPartition> uniquePartitions = kafkaConsumerRepository.getTopicPartitions(topics);\n+\n+                    if (uniquePartitions.isEmpty()) {\n+                        Thread.sleep(POLL_TIMEOUT_MS);\n+                    } else {\n+                        consumeFromTopicPartitions(uniquePartitions);\n+                    }\n+\n+                } catch (Exception e) {\n+                    log.error(\"Failed to forward kafka records\", e);\n+                    Thread.sleep(ERROR_TIMEOUT_MS);\n+                }\n+            }\n+        } catch (Exception e) {\n+            log.error(\"Unexpected exception caught in record handler loop\", e);\n+        } finally {\n+            log.info(\"Closing kafka producer record forwarder...\");\n+        }\n+    }\n+\n+    private void consumeFromTopicPartitions(List<TopicPartition> uniquePartitions) {\n+        uniquePartitions.forEach(topicPartition -> {\n+            if (!isRunning) {\n+                return;\n+            }\n+\n+            Optional<SimpleLock> lock = Optional.empty();\n+            try {\n+                lock = acquireLock(topicPartition);\n+\n+                // Lock has already been acquired by someone else\n+                if (lock.isEmpty()) {\n+                    return;\n+                }\n+\n+                List<StoredConsumerRecord> records = kafkaConsumerRepository\n+                        .getRecords(topicPartition.topic(), topicPartition.partition(), RECORDS_BATCH_SIZE);\n+\n+                StoredRecordConsumer recordConsumer = recordConsumers.get(topicPartition.topic());\n+\n+                List<Long> recordsToDelete = new ArrayList<>();\n+                Map<TopicPartition, Set<Bytes>> failedKeys = new HashMap<>();\n+\n+                records.forEach(r -> {\n+                    Set<Bytes> keySet = failedKeys.get(topicPartition);\n+\n+                    // We cannot process records where a previous record with the same key (and topic+partition) has failed to be consumed\n+                    if (keySet != null && keySet.contains(Bytes.wrap(r.getKey()))) {\n+                        return;\n+                    }\n+\n+                    // TODO: Can implement exponential backoff if necessary\n+\n+                    ConsumeStatus status;\n+\n+                    try {\n+                        status = recordConsumer.consume(r);\n+                    } catch (Exception e) {\n+                        status = ConsumeStatus.FAILED;\n+                    }\n+\n+                    if (status == ConsumeStatus.OK) {\n+                        recordsToDelete.add(r.getId());\n+                    } else {\n+                        log.error(\n+                                \"Failed to process consumer record topic={} partition={} offset={} dbId={}\",\n+                                r.getTopic(), r.getPartition(), r.getOffset(), r.getId()\n+                        );\n+                        kafkaConsumerRepository.incrementRetries(r.getId());\n+                        failedKeys.computeIfAbsent(topicPartition, (_ignored) -> new HashSet<>()).add(Bytes.wrap(r.getKey()));\n+                    }\n+                });\n+\n+                if (!recordsToDelete.isEmpty()) {\n+                    kafkaConsumerRepository.deleteRecords(recordsToDelete);\n+                    log.info(\"Consumed records deleted \" + Arrays.toString(recordsToDelete.toArray()));\n+                }\n+\n+            } catch (Exception e) {\n+                log.error(\"Unexpected exception caught while processing consumer records\", e);\n+            } finally {\n+                lock.ifPresent(SimpleLock::unlock);\n+            }\n+        });\n+    }\n+\n+    private Optional<SimpleLock> acquireLock(TopicPartition topicPartition) {\n+        String name = \"kcrp-\" + topicPartition.topic() + \"-\" + topicPartition.partition();\n+        LockConfiguration configuration = new LockConfiguration(Instant.now(), name, Duration.ofMinutes(5), Duration.ofSeconds(5));\n+        return lockProvider.lock(configuration);\n+    }\n+\n+}"
  },
  {
    "sha": "6359c89345643cc152b3b122dc3d24391590dd3b",
    "filename": "kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/KafkaConsumerRepository.java",
    "status": "added",
    "additions": 21,
    "deletions": 0,
    "changes": 21,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/KafkaConsumerRepository.java",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/KafkaConsumerRepository.java",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/KafkaConsumerRepository.java?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,21 @@\n+package no.nav.common.kafka.consumer.feilhandtering;\n+\n+import org.apache.kafka.common.TopicPartition;\n+\n+import java.util.List;\n+\n+public interface KafkaConsumerRepository {\n+\n+    long storeRecord(StoredConsumerRecord record);\n+\n+    void deleteRecords(List<Long> ids);\n+\n+    boolean hasRecordWithKey(String topic, int partition, byte[] key);\n+\n+    List<StoredConsumerRecord> getRecords(String topic, int partition, int maxRecords);\n+\n+    void incrementRetries(long id);\n+\n+    List<TopicPartition> getTopicPartitions(List<String> topics);\n+\n+}"
  },
  {
    "sha": "d4dfe8634bae591aba80eeab919664deae84216f",
    "filename": "kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/OracleConsumerRepository.java",
    "status": "added",
    "additions": 130,
    "deletions": 0,
    "changes": 130,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/OracleConsumerRepository.java",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/OracleConsumerRepository.java",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/OracleConsumerRepository.java?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,130 @@\n+package no.nav.common.kafka.consumer.feilhandtering;\n+\n+import lombok.SneakyThrows;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import javax.sql.DataSource;\n+import java.sql.Array;\n+import java.sql.PreparedStatement;\n+import java.sql.SQLIntegrityConstraintViolationException;\n+import java.util.List;\n+\n+import static java.lang.String.format;\n+import static no.nav.common.kafka.util.DatabaseConstants.*;\n+import static no.nav.common.kafka.util.DatabaseUtils.*;\n+\n+public class OracleConsumerRepository implements KafkaConsumerRepository {\n+\n+    private final DataSource dataSource;\n+\n+    private final String consumerRecordTable;\n+\n+    public OracleConsumerRepository(DataSource dataSource, String consumerRecordTable) {\n+        this.dataSource = dataSource;\n+        this.consumerRecordTable = consumerRecordTable;\n+    }\n+\n+    public OracleConsumerRepository(DataSource dataSource) {\n+        this(dataSource, CONSUMER_RECORD_TABLE);\n+    }\n+\n+    @SneakyThrows\n+    @Override\n+    public long storeRecord(StoredConsumerRecord record) {\n+        String sql = format(\n+                \"INSERT INTO %s (%s, %s, %s, %s, %s, %s, %s, %s) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\",\n+                consumerRecordTable, ID, TOPIC, PARTITION, RECORD_OFFSET, KEY, VALUE, HEADERS_JSON, RECORD_TIMESTAMP\n+        );\n+\n+        long id = incrementAndGetOracleSequence(dataSource, CONSUMER_RECORD_ID_SEQ);\n+\n+        try (PreparedStatement statement = createPreparedStatement(dataSource, sql)) {\n+            statement.setLong(1, id);\n+            statement.setString(2, record.getTopic());\n+            statement.setInt(3, record.getPartition());\n+            statement.setLong(4, record.getOffset());\n+            statement.setBytes(5, record.getKey());\n+            statement.setBytes(6, record.getValue());\n+            statement.setString(7, record.getHeadersJson());\n+            statement.setLong(8, record.getTimestamp());\n+            statement.executeUpdate();\n+\n+            return id;\n+        } catch (SQLIntegrityConstraintViolationException e) {\n+            return -1;\n+        }\n+    }\n+\n+    @SneakyThrows\n+    @Override\n+    public void deleteRecords(List<Long> ids) {\n+        String sql = format(\"DELETE FROM %s WHERE %s = ANY(?)\", consumerRecordTable, ID);\n+        try (PreparedStatement statement = createPreparedStatement(dataSource, sql)) {\n+            Array array = dataSource.getConnection().createArrayOf(\"INTEGER\", ids.toArray());\n+            statement.setArray(1, array);\n+            statement.executeUpdate();\n+        }\n+    }\n+\n+    @SneakyThrows\n+    @Override\n+    public boolean hasRecordWithKey(String topic, int partition, byte[] key) {\n+        String sql = format(\n+                \"SELECT %s FROM %s WHERE %s = ? AND %s = ? AND %s = ? FETCH NEXT 1 ROWS ONLY\",\n+                ID, consumerRecordTable, TOPIC, PARTITION, KEY\n+        );\n+\n+        try (PreparedStatement statement = createPreparedStatement(dataSource, sql)) {\n+            statement.setString(1, topic);\n+            statement.setInt(2, partition);\n+            statement.setBytes(3, key);\n+\n+            return statement.executeQuery().next();\n+        }\n+    }\n+\n+    @SneakyThrows\n+    @Override\n+    public List<StoredConsumerRecord> getRecords(String topic, int partition, int maxRecords) {\n+        String sql = format(\n+                \"SELECT * FROM %s WHERE %s = ? AND %s = ? ORDER BY %s FETCH NEXT %d ROWS ONLY\",\n+                consumerRecordTable, TOPIC, PARTITION, RECORD_OFFSET, maxRecords\n+        );\n+\n+        try (PreparedStatement statement = createPreparedStatement(dataSource, sql)) {\n+            statement.setString(1, topic);\n+            statement.setInt(2, partition);\n+            return fetchConsumerRecords(statement.executeQuery());\n+        }\n+    }\n+\n+    @SneakyThrows\n+    @Override\n+    public void incrementRetries(long id) {\n+        String sql = format(\n+                \"UPDATE %s SET %s = %s + 1, %s = CURRENT_TIMESTAMP WHERE %s = ?\",\n+                consumerRecordTable, RETRIES, RETRIES, LAST_RETRY, ID\n+        );\n+\n+        try (PreparedStatement statement = createPreparedStatement(dataSource, sql)) {\n+            statement.setLong(1, id);\n+            statement.execute();\n+        }\n+    }\n+\n+    @SneakyThrows\n+    @Override\n+    public List<TopicPartition> getTopicPartitions(List<String> topics) {\n+        String sql = format(\n+                \"SELECT DISTINCT %s, %s FROM %s WHERE %s = ANY(?)\",\n+                TOPIC, PARTITION, consumerRecordTable, TOPIC\n+        );\n+\n+        try (PreparedStatement statement = createPreparedStatement(dataSource, sql)) {\n+            Array array = dataSource.getConnection().createArrayOf(\"VARCHAR\", topics.toArray());\n+            statement.setArray(1, array);\n+            return fetchTopicPartitions(statement.executeQuery());\n+        }\n+    }\n+\n+}"
  },
  {
    "sha": "5f6a9be2a94b81027be98ae4eb2b79e3d7197783",
    "filename": "kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/PostgresConsumerRepository.java",
    "status": "added",
    "additions": 130,
    "deletions": 0,
    "changes": 130,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/PostgresConsumerRepository.java",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/PostgresConsumerRepository.java",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/PostgresConsumerRepository.java?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,130 @@\n+package no.nav.common.kafka.consumer.feilhandtering;\n+\n+import lombok.SneakyThrows;\n+import org.apache.kafka.common.TopicPartition;\n+\n+import javax.sql.DataSource;\n+import java.sql.Array;\n+import java.sql.PreparedStatement;\n+import java.sql.SQLIntegrityConstraintViolationException;\n+import java.util.List;\n+\n+import static java.lang.String.format;\n+import static no.nav.common.kafka.util.DatabaseConstants.*;\n+import static no.nav.common.kafka.util.DatabaseUtils.*;\n+\n+public class PostgresConsumerRepository implements KafkaConsumerRepository {\n+\n+    private final DataSource dataSource;\n+\n+    private final String consumerRecordTable;\n+\n+    public PostgresConsumerRepository(DataSource dataSource, String consumerRecordTable) {\n+        this.dataSource = dataSource;\n+        this.consumerRecordTable = consumerRecordTable;\n+    }\n+\n+    public PostgresConsumerRepository(DataSource dataSource) {\n+        this(dataSource, CONSUMER_RECORD_TABLE);\n+    }\n+\n+    @SneakyThrows\n+    @Override\n+    public long storeRecord(StoredConsumerRecord record) {\n+        String sql = format(\n+                \"INSERT INTO %s (%s, %s, %s, %s, %s, %s, %s, %s) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\",\n+                consumerRecordTable, ID, TOPIC, PARTITION, RECORD_OFFSET, KEY, VALUE, HEADERS_JSON, RECORD_TIMESTAMP\n+        );\n+\n+        long id = incrementAndGetPostgresSequence(dataSource, CONSUMER_RECORD_ID_SEQ);\n+\n+        try (PreparedStatement statement = createPreparedStatement(dataSource, sql)) {\n+            statement.setLong(1, id);\n+            statement.setString(2, record.getTopic());\n+            statement.setInt(3, record.getPartition());\n+            statement.setLong(4, record.getOffset());\n+            statement.setBytes(5, record.getKey());\n+            statement.setBytes(6, record.getValue());\n+            statement.setString(7, record.getHeadersJson());\n+            statement.setLong(8, record.getTimestamp());\n+            statement.executeUpdate();\n+\n+            return id;\n+        } catch (SQLIntegrityConstraintViolationException e) {\n+            return -1;\n+        }\n+    }\n+\n+    @SneakyThrows\n+    @Override\n+    public void deleteRecords(List<Long> ids) {\n+        String sql = format(\"DELETE FROM %s WHERE %s = ANY(?)\", consumerRecordTable, ID);\n+        try (PreparedStatement statement = createPreparedStatement(dataSource, sql)) {\n+            Array array = dataSource.getConnection().createArrayOf(\"INTEGER\", ids.toArray());\n+            statement.setArray(1, array);\n+            statement.executeUpdate();\n+        }\n+    }\n+\n+    @SneakyThrows\n+    @Override\n+    public boolean hasRecordWithKey(String topic, int partition, byte[] key) {\n+        String sql = format(\n+                \"SELECT %s FROM %s WHERE %s = ? AND %s = ? AND %s = ? LIMIT 1\",\n+                ID, consumerRecordTable, TOPIC, PARTITION, KEY\n+        );\n+\n+        try (PreparedStatement statement = createPreparedStatement(dataSource, sql)) {\n+            statement.setString(1, topic);\n+            statement.setInt(2, partition);\n+            statement.setBytes(3, key);\n+\n+            return statement.executeQuery().next();\n+        }\n+    }\n+\n+    @SneakyThrows\n+    @Override\n+    public List<StoredConsumerRecord> getRecords(String topic, int partition, int maxRecords) {\n+        String sql = format(\n+                \"SELECT * FROM %s WHERE %s = ? AND %s = ? ORDER BY %s LIMIT %d\",\n+                consumerRecordTable, TOPIC, PARTITION, RECORD_OFFSET, maxRecords\n+        );\n+\n+        try (PreparedStatement statement = createPreparedStatement(dataSource, sql)) {\n+            statement.setString(1, topic);\n+            statement.setInt(2, partition);\n+            return fetchConsumerRecords(statement.executeQuery());\n+        }\n+    }\n+\n+    @SneakyThrows\n+    @Override\n+    public void incrementRetries(long id) {\n+        String sql = format(\n+                \"UPDATE %s SET %s = %s + 1, %s = CURRENT_TIMESTAMP WHERE %s = ?\",\n+                consumerRecordTable, RETRIES, RETRIES, LAST_RETRY, ID\n+        );\n+\n+        try (PreparedStatement statement = createPreparedStatement(dataSource, sql)) {\n+            statement.setLong(1, id);\n+            statement.execute();\n+        }\n+    }\n+\n+    @SneakyThrows\n+    @Override\n+    public List<TopicPartition> getTopicPartitions(List<String> topics) {\n+        String sql = format(\n+                \"SELECT DISTINCT %s, %s FROM %s WHERE %s = ANY(?)\",\n+                TOPIC, PARTITION, consumerRecordTable, TOPIC\n+        );\n+\n+        try (PreparedStatement statement = createPreparedStatement(dataSource, sql)) {\n+            Array array = dataSource.getConnection().createArrayOf(\"VARCHAR\", topics.toArray());\n+            statement.setArray(1, array);\n+            return fetchTopicPartitions(statement.executeQuery());\n+        }\n+    }\n+\n+}"
  },
  {
    "sha": "c03d847752124ed7b72e2e73e42d998326049090",
    "filename": "kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/StoreOnFailureTopicConsumer.java",
    "status": "added",
    "additions": 59,
    "deletions": 0,
    "changes": 59,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/StoreOnFailureTopicConsumer.java",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/StoreOnFailureTopicConsumer.java",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/StoreOnFailureTopicConsumer.java?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,59 @@\n+package no.nav.common.kafka.consumer.feilhandtering;\n+\n+import no.nav.common.kafka.consumer.ConsumeStatus;\n+import no.nav.common.kafka.consumer.TopicConsumer;\n+import no.nav.common.kafka.consumer.util.ConsumerUtils;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.Serializer;\n+\n+/**\n+ * Wraps a TopicConsumer and stores the record into a database if the underlying consumer fails to process the record.\n+ * To ensure that records are processed in the proper order, if there is a record with the same topic+partition+key already stored,\n+ * then the underlying consumer will be skipped and the record will be stored.\n+ * @param <K> topic key\n+ * @param <V> topic value\n+ */\n+public class StoreOnFailureTopicConsumer<K, V> implements TopicConsumer<K, V> {\n+\n+    private final TopicConsumer<K, V> consumer;\n+\n+    private final KafkaConsumerRepository consumerRepository;\n+\n+    private final Serializer<K> keySerializer;\n+\n+    private final Serializer<V> valueSerializer;\n+\n+    public StoreOnFailureTopicConsumer(\n+            TopicConsumer<K, V> consumer,\n+            KafkaConsumerRepository consumerRepository,\n+            Serializer<K> keySerializer,\n+            Serializer<V> valueSerializer\n+    ) {\n+        this.keySerializer = keySerializer;\n+        this.valueSerializer = valueSerializer;\n+        this.consumer = consumer;\n+        this.consumerRepository = consumerRepository;\n+    }\n+\n+    @Override\n+    public ConsumeStatus consume(ConsumerRecord<K, V> record) {\n+        byte[] key = keySerializer.serialize(record.topic(), record.key());\n+        boolean hasRecord = consumerRepository.hasRecordWithKey(record.topic(), record.partition(), key);\n+\n+        /*\n+            If we consumed the record while there already was a record with the same key in the database,\n+            then the consumption would be out-of-order.\n+         */\n+        if (!hasRecord) {\n+            ConsumeStatus status = ConsumerUtils.safeConsume(consumer, record);\n+\n+            if (status == ConsumeStatus.OK) {\n+                return ConsumeStatus.OK;\n+            }\n+        }\n+\n+        consumerRepository.storeRecord(ConsumerUtils.mapToStoredRecord(record, keySerializer, valueSerializer));\n+        return ConsumeStatus.OK;\n+    }\n+\n+}"
  },
  {
    "sha": "494943fc0e14538d8e0e364831efcf15a78aa58c",
    "filename": "kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/StoreRecordTopicConsumer.java",
    "status": "added",
    "additions": 41,
    "deletions": 0,
    "changes": 41,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/StoreRecordTopicConsumer.java",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/StoreRecordTopicConsumer.java",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/StoreRecordTopicConsumer.java?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,41 @@\n+package no.nav.common.kafka.consumer.feilhandtering;\n+\n+import no.nav.common.kafka.consumer.ConsumeStatus;\n+import no.nav.common.kafka.consumer.TopicConsumer;\n+import no.nav.common.kafka.consumer.util.ConsumerUtils;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static java.lang.String.format;\n+\n+/**\n+ * Stores all consumer records into a database without further processing.\n+ */\n+public class StoreRecordTopicConsumer implements TopicConsumer<byte[], byte[]> {\n+\n+    private final Logger log = LoggerFactory.getLogger(this.getClass());\n+\n+    private final KafkaConsumerRepository consumerRepository;\n+\n+    public StoreRecordTopicConsumer(KafkaConsumerRepository consumerRepository) {\n+        this.consumerRepository = consumerRepository;\n+    }\n+\n+    @Override\n+    public ConsumeStatus consume(ConsumerRecord<byte[], byte[]> record) {\n+        try {\n+            consumerRepository.storeRecord(ConsumerUtils.mapToStoredRecord(record));\n+            log.info(\"Stored consumer record topic={} partition={} offset={}\", record.topic(), record.partition(), record.offset());\n+            return ConsumeStatus.OK;\n+        } catch (Exception e) {\n+            String msg = format(\n+                    \"Failed to store consumer record topic=%s, partition=%d offset=%d\",\n+                    record.topic(), record.partition(), record.offset()\n+            );\n+\n+            log.error(msg, e);\n+            return ConsumeStatus.FAILED;\n+        }\n+    }\n+}"
  },
  {
    "sha": "309135cc40a72220a201314bfaffc35e351e4ea4",
    "filename": "kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/StoredConsumerRecord.java",
    "status": "added",
    "additions": 34,
    "deletions": 0,
    "changes": 34,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/StoredConsumerRecord.java",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/StoredConsumerRecord.java",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/StoredConsumerRecord.java?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,34 @@\n+package no.nav.common.kafka.consumer.feilhandtering;\n+\n+import lombok.AllArgsConstructor;\n+import lombok.Data;\n+\n+import java.sql.Timestamp;\n+\n+@AllArgsConstructor\n+@Data\n+public class StoredConsumerRecord {\n+    private final long id;\n+    private final String topic;\n+    private final int partition;\n+    private final long offset;\n+    private final byte[] key;\n+    private final byte[] value;\n+    private final String headersJson;\n+    private final int retries;\n+    private final Timestamp lastRetry;\n+    private final long timestamp;\n+\n+    public StoredConsumerRecord(String topic, int partition, long offset, byte[] key, byte[] value, String headersJson, long timestamp) {\n+        this.id = -1;\n+        this.topic = topic;\n+        this.partition = partition;\n+        this.offset = offset;\n+        this.key = key;\n+        this.value = value;\n+        this.headersJson = headersJson;\n+        this.timestamp = timestamp;\n+        this.retries = 0;\n+        this.lastRetry = null;\n+    }\n+}"
  },
  {
    "sha": "deb1aa9b2fab72293f92fcde2edf6facd7dae977",
    "filename": "kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/StoredRecordConsumer.java",
    "status": "added",
    "additions": 9,
    "deletions": 0,
    "changes": 9,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/StoredRecordConsumer.java",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/StoredRecordConsumer.java",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/src/main/java/no/nav/common/kafka/consumer/feilhandtering/StoredRecordConsumer.java?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,9 @@\n+package no.nav.common.kafka.consumer.feilhandtering;\n+\n+import no.nav.common.kafka.consumer.ConsumeStatus;\n+\n+public interface StoredRecordConsumer {\n+\n+    ConsumeStatus consume(StoredConsumerRecord consumerRecord);\n+\n+}"
  },
  {
    "sha": "c7f0fd798e9a48ce01db717a0fa68862df299cc9",
    "filename": "kafka/src/main/java/no/nav/common/kafka/consumer/util/ConsumerUtils.java",
    "status": "added",
    "additions": 140,
    "deletions": 0,
    "changes": 140,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/util/ConsumerUtils.java",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/util/ConsumerUtils.java",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/src/main/java/no/nav/common/kafka/consumer/util/ConsumerUtils.java?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,140 @@\n+package no.nav.common.kafka.consumer.util;\n+\n+import no.nav.common.kafka.consumer.ConsumeStatus;\n+import no.nav.common.kafka.consumer.TopicConsumer;\n+import no.nav.common.kafka.consumer.feilhandtering.StoredConsumerRecord;\n+import no.nav.common.kafka.consumer.feilhandtering.StoredRecordConsumer;\n+import no.nav.common.kafka.util.KafkaUtils;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.header.Headers;\n+import org.apache.kafka.common.record.TimestampType;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.serialization.Serializer;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.lang.String.format;\n+\n+public class ConsumerUtils {\n+\n+    private final static Logger log = LoggerFactory.getLogger(ConsumerUtils.class);\n+\n+    public static <K, V> StoredConsumerRecord mapToStoredRecord(\n+            ConsumerRecord<K, V> record,\n+            Serializer<K> keySerializer,\n+            Serializer<V> valueSerializer\n+    ) {\n+        byte[] key = keySerializer.serialize(record.topic(), record.key());\n+        byte[] value = valueSerializer.serialize(record.topic(), record.value());\n+        String headersJson = KafkaUtils.headersToJson(record.headers());\n+\n+        return new StoredConsumerRecord(\n+                record.topic(),\n+                record.partition(),\n+                record.offset(),\n+                key,\n+                value,\n+                headersJson,\n+                record.timestamp()\n+        );\n+    }\n+\n+    public static StoredConsumerRecord mapToStoredRecord(ConsumerRecord<byte[], byte[]> record) {\n+        String headersJson = KafkaUtils.headersToJson(record.headers());\n+        return new StoredConsumerRecord(\n+                record.topic(),\n+                record.partition(),\n+                record.offset(),\n+                record.key(),\n+                record.value(),\n+                headersJson,\n+                record.timestamp()\n+        );\n+    }\n+\n+    public static <K, V> ConsumerRecord<K, V> mapFromStoredRecord(\n+            StoredConsumerRecord record,\n+            Deserializer<K> keyDeserializer,\n+            Deserializer<V> valueDeserializer\n+    ) {\n+        K key = keyDeserializer.deserialize(record.getTopic(), record.getKey());\n+        V value = valueDeserializer.deserialize(record.getTopic(), record.getValue());\n+        Headers headers = KafkaUtils.jsonToHeaders(record.getHeadersJson());\n+\n+        ConsumerRecord<K, V> consumerRecord = new ConsumerRecord<>(\n+                record.getTopic(),\n+                record.getPartition(),\n+                record.getOffset(),\n+                record.getTimestamp(),\n+                TimestampType.CREATE_TIME,\n+                ConsumerRecord.NULL_CHECKSUM,\n+                ConsumerRecord.NULL_SIZE,\n+                ConsumerRecord.NULL_SIZE,\n+                key,\n+                value\n+        );\n+\n+        headers.forEach(header -> consumerRecord.headers().add(header));\n+\n+        return consumerRecord;\n+    }\n+\n+    public static <K, V> Map<String, StoredRecordConsumer> toStoredRecordConsumerMap(\n+            Map<String, TopicConsumer<K, V>> consumerMap,\n+            Deserializer<K> keyDeserializer,\n+            Deserializer<V> valueDeserializer\n+    ) {\n+        Map<String, StoredRecordConsumer> storedRecordConsumerMap = new HashMap<>();\n+\n+        consumerMap.forEach((topic, topicConsumer) -> {\n+            storedRecordConsumerMap.put(topic, toStoredRecordConsumer(topicConsumer, keyDeserializer, valueDeserializer));\n+        });\n+\n+        return storedRecordConsumerMap;\n+    }\n+\n+    public static <K, V> StoredRecordConsumer toStoredRecordConsumer(\n+            TopicConsumer<K, V> topicConsumer,\n+            Deserializer<K> keyDeserializer,\n+            Deserializer<V> valueDeserializer\n+    ) {\n+        return storedRecord -> topicConsumer.consume(mapFromStoredRecord(storedRecord, keyDeserializer, valueDeserializer));\n+    }\n+\n+    public static <K, V> TopicConsumer<K, V> aggregateConsumer(final List<TopicConsumer<K, V>> consumers) {\n+        return record -> {\n+            ConsumeStatus aggregatedStatus = ConsumeStatus.OK;\n+\n+            for (TopicConsumer<K, V> consumer : consumers) {\n+                ConsumeStatus status = consumer.consume(record);\n+\n+                if (status == ConsumeStatus.FAILED) {\n+                    aggregatedStatus = ConsumeStatus.FAILED;\n+                }\n+            }\n+\n+            return aggregatedStatus;\n+        };\n+    }\n+\n+    public static <K, V> ConsumeStatus safeConsume(TopicConsumer<K, V> topicConsumer, ConsumerRecord<K, V> consumerRecord) {\n+        try {\n+            return topicConsumer.consume(consumerRecord);\n+        } catch (Exception e) {\n+            String msg = format(\n+                    \"Consumer failed to process record from topic=%s partition=%d offset=%d\",\n+                    consumerRecord.topic(),\n+                    consumerRecord.partition(),\n+                    consumerRecord.offset()\n+            );\n+\n+            log.error(msg, e);\n+            return ConsumeStatus.FAILED;\n+        }\n+    }\n+\n+}"
  },
  {
    "sha": "e2da7bb544a09d74a102fa300be960bcc4e2a115",
    "filename": "kafka/src/main/java/no/nav/common/kafka/consumer/util/JsonTopicConsumer.java",
    "status": "added",
    "additions": 47,
    "deletions": 0,
    "changes": 47,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/util/JsonTopicConsumer.java",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/util/JsonTopicConsumer.java",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/src/main/java/no/nav/common/kafka/consumer/util/JsonTopicConsumer.java?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,47 @@\n+package no.nav.common.kafka.consumer.util;\n+\n+import no.nav.common.json.JsonUtils;\n+import no.nav.common.kafka.consumer.ConsumeStatus;\n+import no.nav.common.kafka.consumer.TopicConsumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+/**\n+ * Topic consumer which deserializes JSON messages from topics.\n+ * @param <K> topic key\n+ * @param <V> topic value (NB: This must always be a String! A generic type is used to make JsonTopicConsumer compatible with the other generic kafka components)\n+ * @param <T> deserialized JSON\n+ */\n+public class JsonTopicConsumer<K, V, T> implements TopicConsumer<K, V> {\n+\n+    private final BiFunction<ConsumerRecord<K, V>, T, ConsumeStatus> consumer;\n+\n+    private final Class<T> dataClass;\n+\n+    public JsonTopicConsumer(Class<T> dataClass, Function<T, ConsumeStatus> consumer) {\n+        this.dataClass = dataClass;\n+        this.consumer = (k, t) -> consumer.apply(t);\n+    }\n+\n+    public JsonTopicConsumer(Class<T> dataClass, BiFunction<ConsumerRecord<K, V>, T, ConsumeStatus> consumer) {\n+        this.dataClass = dataClass;\n+        this.consumer = consumer;\n+    }\n+\n+    @Override\n+    public ConsumeStatus consume(ConsumerRecord<K, V> record) {\n+        String stringValue = assertedString(record.value());\n+        return consumer.apply(record, JsonUtils.fromJson(stringValue, dataClass));\n+    }\n+\n+    private static String assertedString(Object obj) {\n+        if (!(obj instanceof String)) {\n+            throw new IllegalStateException(\"JsonTopicConsumer can only consume topics where the value is a String\");\n+        }\n+\n+        return (String) obj;\n+    }\n+\n+}"
  },
  {
    "sha": "07148e2c804edff9875c604094f9975e7053d3d9",
    "filename": "kafka/src/main/java/no/nav/common/kafka/consumer/util/KafkaConsumerClientBuilder.java",
    "status": "added",
    "additions": 136,
    "deletions": 0,
    "changes": 136,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/util/KafkaConsumerClientBuilder.java",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/util/KafkaConsumerClientBuilder.java",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/src/main/java/no/nav/common/kafka/consumer/util/KafkaConsumerClientBuilder.java?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,136 @@\n+package no.nav.common.kafka.consumer.util;\n+\n+import io.micrometer.core.instrument.MeterRegistry;\n+import no.nav.common.kafka.consumer.KafkaConsumerClient;\n+import no.nav.common.kafka.consumer.KafkaConsumerClientConfig;\n+import no.nav.common.kafka.consumer.TopicConsumer;\n+import no.nav.common.kafka.consumer.feilhandtering.KafkaConsumerRepository;\n+import no.nav.common.kafka.consumer.feilhandtering.StoreOnFailureTopicConsumer;\n+import org.apache.kafka.common.serialization.Serializer;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+public class KafkaConsumerClientBuilder<K, V> {\n+\n+    private final Map<String, TopicConsumer<K, V>> consumerMap = new HashMap<>();\n+\n+    private final Map<String, TopicConsumer<K, V>> consumersWithErrorHandlingMap = new HashMap<>();\n+\n+    private Properties properties;\n+\n+    private long pollDurationMs = -1;\n+\n+    private KafkaConsumerRepository consumerRepository;\n+\n+    private Serializer<K> keySerializer;\n+\n+    private Serializer<V> valueSerializer;\n+\n+    private boolean enableLogging;\n+\n+    private MeterRegistry meterRegistry;\n+\n+    private KafkaConsumerClientBuilder() {}\n+\n+    public static <K, V> KafkaConsumerClientBuilder<K, V> builder() {\n+        return new KafkaConsumerClientBuilder<K, V>();\n+    }\n+\n+    public KafkaConsumerClientBuilder<K, V> withProps(Properties properties) {\n+        this.properties = properties;\n+        return this;\n+    }\n+\n+    public KafkaConsumerClientBuilder<K, V> withTopic(String topic, TopicConsumer<K, V> consumer) {\n+        consumerMap.put(topic, consumer);\n+        return this;\n+    }\n+\n+    public KafkaConsumerClientBuilder<K, V> withTopics(Map<String, TopicConsumer<K, V>> topicConsumers) {\n+        consumerMap.putAll(topicConsumers);\n+        return this;\n+    }\n+\n+    public KafkaConsumerClientBuilder<K, V> withStoreOnFailureConsumer(String topic, TopicConsumer<K, V> topicConsumer) {\n+        consumersWithErrorHandlingMap.put(topic, topicConsumer);\n+        return this;\n+    }\n+\n+    public KafkaConsumerClientBuilder<K, V> withStoreOnFailureConsumers(Map<String, TopicConsumer<K, V>> topicConsumers) {\n+        consumersWithErrorHandlingMap.putAll(topicConsumers);\n+        return this;\n+    }\n+\n+    public KafkaConsumerClientBuilder<K, V> withLogging() {\n+        this.enableLogging = true;\n+        return this;\n+    }\n+\n+    public KafkaConsumerClientBuilder<K, V> withMetrics(MeterRegistry meterRegistry) {\n+        this.meterRegistry = meterRegistry;\n+        return this;\n+    }\n+\n+    public KafkaConsumerClientBuilder<K, V> withRepository(KafkaConsumerRepository consumerRepository) {\n+        this.consumerRepository = consumerRepository;\n+        return this;\n+    }\n+\n+    public KafkaConsumerClientBuilder<K, V> withSerializers(Serializer<K> keySerializer, Serializer<V> valueSerializer) {\n+        this.keySerializer = keySerializer;\n+        this.valueSerializer = valueSerializer;\n+        return this;\n+    }\n+\n+    public KafkaConsumerClientBuilder<K, V> withPollDuration(long pollDurationMs) {\n+        this.pollDurationMs = pollDurationMs;\n+        return this;\n+    }\n+\n+    public KafkaConsumerClient<K, V> build() {\n+        if (properties == null) {\n+            throw new IllegalStateException(\"Cannot build kafka consumer without properties\");\n+        }\n+\n+        if (!consumersWithErrorHandlingMap.isEmpty()) {\n+            if (consumerRepository == null) {\n+                throw new IllegalStateException(\"Consumer repository is required when using error handling\");\n+            }\n+\n+            if (keySerializer == null || valueSerializer == null) {\n+                throw new IllegalStateException(\"Key serializer and value serializer is required when using error handling\");\n+            }\n+        }\n+\n+        consumersWithErrorHandlingMap.forEach((topic, consumer) -> {\n+            consumerMap.put(topic, new StoreOnFailureTopicConsumer<>(consumer, consumerRepository, keySerializer, valueSerializer));\n+        });\n+\n+        Map<String, TopicConsumer<K, V>> extendedConsumers = new HashMap<>();\n+\n+        consumerMap.forEach((topic, consumer) -> {\n+            TopicConsumerBuilder<K, V> builder = TopicConsumerBuilder.builder();\n+\n+            if (enableLogging) {\n+                builder.withLogging();\n+            }\n+\n+            if (meterRegistry != null) {\n+                builder.withMetrics(meterRegistry);\n+            }\n+\n+            extendedConsumers.put(topic, builder.build());\n+        });\n+\n+        KafkaConsumerClientConfig<K, V> config = new KafkaConsumerClientConfig<>(properties, extendedConsumers);\n+\n+        if (pollDurationMs >= 0) {\n+            config.setPollDurationMs(pollDurationMs);\n+        }\n+\n+        return new KafkaConsumerClient<>(config);\n+    }\n+\n+}"
  },
  {
    "sha": "3145f98c559955e6379c6d65ed6cbc1cb57b3aa2",
    "filename": "kafka/src/main/java/no/nav/common/kafka/consumer/util/TopicConsumerBuilder.java",
    "status": "added",
    "additions": 85,
    "deletions": 0,
    "changes": 85,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/util/TopicConsumerBuilder.java",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/util/TopicConsumerBuilder.java",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/src/main/java/no/nav/common/kafka/consumer/util/TopicConsumerBuilder.java?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,85 @@\n+package no.nav.common.kafka.consumer.util;\n+\n+import io.micrometer.core.instrument.MeterRegistry;\n+import no.nav.common.kafka.consumer.ConsumeStatus;\n+import no.nav.common.kafka.consumer.TopicConsumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+\n+/**\n+ * Builder which makes it easier to compose a {@link no.nav.common.kafka.consumer.TopicConsumer} with additional functionality\n+ * such as logging and metrics\n+ * @param <K> topic key\n+ * @param <V> topic value\n+ */\n+public class TopicConsumerBuilder<K, V> {\n+\n+    private final Logger log = LoggerFactory.getLogger(this.getClass());\n+\n+    private final List<TopicConsumerListener<K, V>> listeners = new ArrayList<>();\n+\n+    private TopicConsumer<K, V> consumer;\n+\n+    private TopicConsumerBuilder() {}\n+\n+    public static <K, V> TopicConsumerBuilder<K, V> builder() {\n+        return new TopicConsumerBuilder<>();\n+    }\n+\n+    public TopicConsumerBuilder<K, V> withConsumer(TopicConsumer<K, V> consumer) {\n+        this.consumer = consumer;\n+        return this;\n+    }\n+\n+    public <T> TopicConsumerBuilder<K, V> withJsonConsumer(Class<T> jsonType, Function<T, ConsumeStatus> consumer) {\n+        this.consumer = new JsonTopicConsumer<>(jsonType, consumer);\n+        return this;\n+    }\n+\n+    public <T> TopicConsumerBuilder<K, V> withJsonConsumer(Class<T> jsonType, BiFunction<ConsumerRecord<K, V>, T, ConsumeStatus> consumer) {\n+        this.consumer = new JsonTopicConsumer<>(jsonType, consumer);\n+        return this;\n+    }\n+\n+    public TopicConsumerBuilder<K, V> withLogging() {\n+        listeners.add(new TopicConsumerLogger<>());\n+        return this;\n+    }\n+\n+    public TopicConsumerBuilder<K, V> withMetrics(MeterRegistry meterRegistry) {\n+        listeners.add(new TopicConsumerMetrics<>(meterRegistry));\n+        return this;\n+    }\n+\n+    public TopicConsumerBuilder<K, V> withListener(TopicConsumerListener<K, V> consumerListener) {\n+        listeners.add(consumerListener);\n+        return this;\n+    }\n+\n+    public TopicConsumer<K, V> build() {\n+        if (consumer == null) {\n+            throw new IllegalStateException(\"Cannot build TopicConsumer without consumer\");\n+        }\n+\n+        TopicConsumer<K, V> topicConsumer = (record) -> {\n+            ConsumeStatus status = ConsumerUtils.safeConsume(consumer, record);\n+            listeners.forEach(listener -> {\n+                try {\n+                    listener.onConsumed(record, status);\n+                } catch (Exception e) {\n+                    log.error(\"Caught exception from consumer listener\", e);\n+                }\n+            });\n+            return status;\n+        };\n+\n+        return topicConsumer;\n+    }\n+\n+}"
  },
  {
    "sha": "4beceaf27f875430bd1635dcb8cb89fd8fc99c4c",
    "filename": "kafka/src/main/java/no/nav/common/kafka/consumer/util/TopicConsumerListener.java",
    "status": "added",
    "additions": 15,
    "deletions": 0,
    "changes": 15,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/util/TopicConsumerListener.java",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/util/TopicConsumerListener.java",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/src/main/java/no/nav/common/kafka/consumer/util/TopicConsumerListener.java?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,15 @@\n+package no.nav.common.kafka.consumer.util;\n+\n+import no.nav.common.kafka.consumer.ConsumeStatus;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+\n+/**\n+ * Listener which receives a consumed record and status from a {@link no.nav.common.kafka.consumer.TopicConsumer}\n+ * @param <K> topic key\n+ * @param <V> topic value\n+ */\n+public interface TopicConsumerListener<K, V> {\n+\n+    void onConsumed(ConsumerRecord<K, V> record, ConsumeStatus status);\n+\n+}"
  },
  {
    "sha": "0235f29e91ce15892082af69d4dca1a1085b88ea",
    "filename": "kafka/src/main/java/no/nav/common/kafka/consumer/util/TopicConsumerLogger.java",
    "status": "added",
    "additions": 26,
    "deletions": 0,
    "changes": 26,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/util/TopicConsumerLogger.java",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/util/TopicConsumerLogger.java",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/src/main/java/no/nav/common/kafka/consumer/util/TopicConsumerLogger.java?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,26 @@\n+package no.nav.common.kafka.consumer.util;\n+\n+import no.nav.common.kafka.consumer.ConsumeStatus;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Listener which receives a consumed record and status from a {@link no.nav.common.kafka.consumer.TopicConsumer}\n+ * @param <K> topic key\n+ * @param <V> topic value\n+ */\n+public class TopicConsumerLogger<K, V> implements TopicConsumerListener<K, V> {\n+\n+    private final Logger log = LoggerFactory.getLogger(this.getClass());\n+\n+    @Override\n+    public void onConsumed(ConsumerRecord<K, V> record, ConsumeStatus status) {\n+        if (status == ConsumeStatus.OK) {\n+            log.info(\"Consumed record topic={} partition={} offset={}\", record.topic(), record.partition(), record.offset());\n+        } else {\n+            log.error(\"Failed to consume record topic={} partition={} offset={}\", record.topic(), record.partition(), record.offset());\n+        }\n+    }\n+\n+}"
  },
  {
    "sha": "11c728e691f81b8974e6d45097b0f0f4d34ef458",
    "filename": "kafka/src/main/java/no/nav/common/kafka/consumer/util/TopicConsumerMetrics.java",
    "status": "added",
    "additions": 64,
    "deletions": 0,
    "changes": 64,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/util/TopicConsumerMetrics.java",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/consumer/util/TopicConsumerMetrics.java",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/src/main/java/no/nav/common/kafka/consumer/util/TopicConsumerMetrics.java?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,64 @@\n+package no.nav.common.kafka.consumer.util;\n+\n+import io.micrometer.core.instrument.Counter;\n+import io.micrometer.core.instrument.Gauge;\n+import io.micrometer.core.instrument.MeterRegistry;\n+import no.nav.common.kafka.consumer.ConsumeStatus;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+/**\n+ * Consumer listener which adds a consumption status metric for each topic + partition\n+ * @param <K> topic key\n+ * @param <V> topic value\n+ */\n+public class TopicConsumerMetrics<K, V> implements TopicConsumerListener<K, V> {\n+\n+    public final static String KAFKA_CONSUMER_STATUS_COUNTER = \"kafka.consumer.status\";\n+\n+    public final static String KAFKA_CONSUMER_CONSUMED_OFFSET_GAUGE = \"kafka.consumer.consumed-offset\";\n+\n+    private final MeterRegistry meterRegistry;\n+\n+    private final Map<String, Counter> statusCounterMap = new HashMap<>();\n+\n+    private final Map<String, Gauge> consumedOffsetGaugeMap = new HashMap<>();\n+\n+    private final Map<String, Long> consumedOffsetMap = new HashMap<>();\n+\n+    public TopicConsumerMetrics(MeterRegistry meterRegistry) {\n+        this.meterRegistry = meterRegistry;\n+    }\n+\n+    @Override\n+    public void onConsumed(ConsumerRecord<K, V> record, ConsumeStatus status) {\n+        String statusMapKey = String.format(\"%s-%d-%s\", record.topic(), record.partition(), status);\n+\n+        statusCounterMap.computeIfAbsent(statusMapKey, (k) ->\n+                Counter.builder(KAFKA_CONSUMER_STATUS_COUNTER)\n+                        .tag(\"topic\", record.topic())\n+                        .tag(\"partition\", String.valueOf(record.partition()))\n+                        .tag(\"status\", status.name().toLowerCase())\n+                        .register(meterRegistry))\n+                .increment();\n+\n+        if (status == ConsumeStatus.OK) {\n+            String offsetMapKey = String.format(\"%s-%d\", record.topic(), record.partition());\n+\n+            consumedOffsetMap.put(offsetMapKey, record.offset());\n+\n+            consumedOffsetGaugeMap.computeIfAbsent(statusMapKey, (k) ->\n+                    Gauge.builder(KAFKA_CONSUMER_CONSUMED_OFFSET_GAUGE, () -> {\n+                        Long offset = consumedOffsetMap.get(offsetMapKey);\n+                        return offset != null ? offset : 0;\n+                    })\n+                    .description(\"The latest consumed offset. The offset is not guaranteed to have been committed.\")\n+                    .tag(\"topic\", record.topic())\n+                    .tag(\"partition\", String.valueOf(record.partition()))\n+                    .register(meterRegistry));\n+        }\n+    }\n+\n+}"
  },
  {
    "sha": "654e67dd65e30505e67791d4d6e10eab9ca280c6",
    "filename": "kafka/src/main/java/no/nav/common/kafka/producer/GracefulKafkaProducer.java",
    "status": "added",
    "additions": 35,
    "deletions": 0,
    "changes": 35,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/producer/GracefulKafkaProducer.java",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/producer/GracefulKafkaProducer.java",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/src/main/java/no/nav/common/kafka/producer/GracefulKafkaProducer.java?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,35 @@\n+package no.nav.common.kafka.producer;\n+\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Duration;\n+import java.util.Properties;\n+\n+/**\n+ * Adds graceful shutdown to {@link KafkaProducer}}\n+ * @param <K> topic key\n+ * @param <V> topic value\n+ */\n+public class GracefulKafkaProducer<K, V> extends KafkaProducer<K, V> {\n+\n+    private final Logger log = LoggerFactory.getLogger(this.getClass());\n+\n+    public GracefulKafkaProducer(Properties properties) {\n+        super(properties);\n+        Runtime.getRuntime().addShutdownHook(new Thread(this::shutdown));\n+    }\n+\n+    private void shutdown() {\n+        log.info(\"Shutting down kafka producer...\");\n+\n+        try {\n+            close(Duration.ofSeconds(3));\n+            log.info(\"Kafka producer was shut down successfully\");\n+        } catch (Exception e) {\n+            log.error(\"Failed to shut down kafka producer gracefully\", e);\n+        }\n+    }\n+\n+}"
  },
  {
    "sha": "33025e9e214ae16a67e101f62f93223dee5026e6",
    "filename": "kafka/src/main/java/no/nav/common/kafka/producer/KafkaProducerClient.java",
    "status": "added",
    "additions": 19,
    "deletions": 0,
    "changes": 19,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/producer/KafkaProducerClient.java",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/producer/KafkaProducerClient.java",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/src/main/java/no/nav/common/kafka/producer/KafkaProducerClient.java?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,19 @@\n+package no.nav.common.kafka.producer;\n+\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+\n+import java.util.concurrent.Future;\n+\n+public interface KafkaProducerClient<K, V> {\n+\n+    void close();\n+\n+    RecordMetadata sendSync(ProducerRecord<K, V> record);\n+\n+    Future<RecordMetadata> send(ProducerRecord<K, V> record);\n+\n+    Future<RecordMetadata> send(ProducerRecord<K, V> record, Callback callback);\n+\n+}"
  },
  {
    "sha": "a871107366aa2e34b02de09e03feff968ec95646",
    "filename": "kafka/src/main/java/no/nav/common/kafka/producer/KafkaProducerClientImpl.java",
    "status": "added",
    "additions": 60,
    "deletions": 0,
    "changes": 60,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/producer/KafkaProducerClientImpl.java",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/producer/KafkaProducerClientImpl.java",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/src/main/java/no/nav/common/kafka/producer/KafkaProducerClientImpl.java?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,60 @@\n+package no.nav.common.kafka.producer;\n+\n+import lombok.SneakyThrows;\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Properties;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.Future;\n+\n+public class KafkaProducerClientImpl<K, V> implements KafkaProducerClient<K, V> {\n+\n+    private final Logger log = LoggerFactory.getLogger(KafkaProducerClientImpl.class);\n+\n+    private final Producer<K, V> producer;\n+\n+    public KafkaProducerClientImpl(Producer<K, V> producer) {\n+        this.producer = producer;\n+    }\n+\n+    public KafkaProducerClientImpl(Properties properties) {\n+        this(new GracefulKafkaProducer<>(properties));\n+    }\n+\n+    @Override\n+    public void close() {\n+        log.info(\"Closing kafka producer...\");\n+        producer.close();\n+    }\n+\n+    @SneakyThrows\n+    @Override\n+    public RecordMetadata sendSync(ProducerRecord<K, V> record) {\n+        Future<RecordMetadata> future = send(record, null);\n+        producer.flush(); // This will block until all buffered records are sent\n+        return future.get();\n+    }\n+\n+    @Override\n+    public Future<RecordMetadata> send(ProducerRecord<K, V> record) {\n+       return send(record, null);\n+    }\n+\n+    @Override\n+    public Future<RecordMetadata> send(final ProducerRecord<K, V> record, Callback callback) {\n+        try {\n+            return producer.send(record, callback);\n+        } catch (Exception e) {\n+            if (callback != null) {\n+                callback.onCompletion(null, e);\n+            }\n+            return CompletableFuture.failedFuture(e);\n+        }\n+    }\n+\n+}"
  },
  {
    "sha": "10d0131378bcc14e6fa54a332d4adf427666cc8a",
    "filename": "kafka/src/main/java/no/nav/common/kafka/producer/feilhandtering/KafkaProducerRecordProcessor.java",
    "status": "added",
    "additions": 128,
    "deletions": 0,
    "changes": 128,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/producer/feilhandtering/KafkaProducerRecordProcessor.java",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/producer/feilhandtering/KafkaProducerRecordProcessor.java",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/src/main/java/no/nav/common/kafka/producer/feilhandtering/KafkaProducerRecordProcessor.java?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,128 @@\n+package no.nav.common.kafka.producer.feilhandtering;\n+\n+import no.nav.common.job.leader_election.LeaderElectionClient;\n+import no.nav.common.kafka.producer.util.ProducerUtils;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.List;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+\n+import static java.lang.String.format;\n+\n+public class KafkaProducerRecordProcessor {\n+\n+    private final static long ERROR_TIMEOUT_MS = 5000;\n+\n+    private final static long POLL_TIMEOUT_MS = 3000;\n+\n+    private final static long WAITING_FOR_LEADER_TIMEOUT_MS = 10_000;\n+\n+    private final static int RECORDS_BATCH_SIZE = 100;\n+\n+    private final Logger log = LoggerFactory.getLogger(this.getClass());\n+\n+    private final ExecutorService executorService = Executors.newSingleThreadExecutor();\n+\n+    private final KafkaProducerRepository producerRepository;\n+\n+    private final Producer<byte[], byte[]> producer;\n+\n+    private final LeaderElectionClient leaderElectionClient;\n+\n+    private volatile boolean isRunning;\n+\n+    private volatile boolean isClosed;\n+\n+    public KafkaProducerRecordProcessor(\n+            KafkaProducerRepository producerRepository,\n+            Producer<byte[], byte[]> producerClient,\n+            LeaderElectionClient leaderElectionClient\n+    ) {\n+        this.producerRepository = producerRepository;\n+        this.producer = producerClient;\n+        this.leaderElectionClient = leaderElectionClient;\n+\n+        Runtime.getRuntime().addShutdownHook(new Thread(this::close));\n+    }\n+\n+    public void start() {\n+        if (isClosed) {\n+            throw new IllegalStateException(\"Cannot start closed producer record processor\");\n+        }\n+\n+        if (!isRunning) {\n+            executorService.submit(this::recordHandlerLoop);\n+        }\n+    }\n+\n+    public void close() {\n+        isRunning = false;\n+        isClosed = true;\n+    }\n+\n+    private void recordHandlerLoop() {\n+        isRunning = true;\n+\n+        try {\n+           while (isRunning) {\n+               try {\n+                   if (!leaderElectionClient.isLeader()) {\n+                       Thread.sleep(WAITING_FOR_LEADER_TIMEOUT_MS);\n+                       continue;\n+                   }\n+\n+                   List<StoredProducerRecord> records = producerRepository.getRecords(RECORDS_BATCH_SIZE);\n+\n+                   if (!records.isEmpty()) {\n+                       publishStoredRecordsBatch(records);\n+                   }\n+\n+                   // If the number of records are less than the max batch size,\n+                   //   then most likely there are not many messages to process and we can wait a bit\n+                   if (records.size() < RECORDS_BATCH_SIZE) {\n+                       Thread.sleep(POLL_TIMEOUT_MS);\n+                   }\n+               } catch (Exception e) {\n+                   log.error(\"Failed to process kafka producer records\", e);\n+                   Thread.sleep(ERROR_TIMEOUT_MS);\n+               }\n+           }\n+       } catch (Exception e) {\n+           log.error(\"Unexpected exception caught in record handler loop\", e);\n+       } finally {\n+           log.info(\"Closing kafka producer record processor...\");\n+           producer.close();\n+       }\n+    }\n+\n+    private void publishStoredRecordsBatch(List<StoredProducerRecord> records) throws InterruptedException {\n+        // TODO: could be done inside a kafka transaction\n+\n+        CountDownLatch latch = new CountDownLatch(records.size());\n+\n+        records.forEach(record -> {\n+            producer.send(ProducerUtils.mapFromStoredRecord(record), (metadata, exception) -> {\n+                try {\n+                    if (exception != null) {\n+                        log.warn(format(\"Failed to resend failed message to topic %s\", record.getTopic()), exception);\n+                    } else {\n+                        producerRepository.deleteRecord(record.getId());\n+                    }\n+                } catch (Exception e) {\n+                    log.error(\"Failed to send message to kafka\", e);\n+                } finally {\n+                    latch.countDown();\n+                }\n+            });\n+        });\n+\n+        producer.flush();\n+\n+        latch.await();\n+    }\n+\n+}"
  },
  {
    "sha": "552c5ba9063f7121db28ecbc715814d82699fe9c",
    "filename": "kafka/src/main/java/no/nav/common/kafka/producer/feilhandtering/KafkaProducerRecordStorage.java",
    "status": "added",
    "additions": 39,
    "deletions": 0,
    "changes": 39,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/producer/feilhandtering/KafkaProducerRecordStorage.java",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/producer/feilhandtering/KafkaProducerRecordStorage.java",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/src/main/java/no/nav/common/kafka/producer/feilhandtering/KafkaProducerRecordStorage.java?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,39 @@\n+package no.nav.common.kafka.producer.feilhandtering;\n+\n+import no.nav.common.kafka.producer.util.ProducerUtils;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.serialization.Serializer;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class KafkaProducerRecordStorage<K, V> {\n+\n+    private final Logger log = LoggerFactory.getLogger(KafkaProducerRecordStorage.class);\n+\n+    private final KafkaProducerRepository producerRepository;\n+\n+    private final Serializer<K> keySerializer;\n+\n+    private final Serializer<V> valueSerializer;\n+\n+    public KafkaProducerRecordStorage(\n+            KafkaProducerRepository producerRepository,\n+            Serializer<K> keySerializer,\n+            Serializer<V> valueSerializer\n+    ) {\n+        this.producerRepository = producerRepository;\n+        this.keySerializer = keySerializer;\n+        this.valueSerializer = valueSerializer;\n+    }\n+\n+    public void store(ProducerRecord<K, V> record) {\n+        try {\n+            producerRepository.storeRecord(ProducerUtils.mapToStoredRecord(record, keySerializer, valueSerializer));\n+            log.info(\"Stored record for topic \" + record.topic());\n+        } catch (Exception e) {\n+            log.error(\"Failed to store record for topic \" + record.topic(), e);\n+            throw e;\n+        }\n+    }\n+\n+}"
  },
  {
    "sha": "eaae1ddef8d3186815f4082592f95ca27b8ec5fc",
    "filename": "kafka/src/main/java/no/nav/common/kafka/producer/feilhandtering/KafkaProducerRepository.java",
    "status": "added",
    "additions": 13,
    "deletions": 0,
    "changes": 13,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/producer/feilhandtering/KafkaProducerRepository.java",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/producer/feilhandtering/KafkaProducerRepository.java",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/src/main/java/no/nav/common/kafka/producer/feilhandtering/KafkaProducerRepository.java?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,13 @@\n+package no.nav.common.kafka.producer.feilhandtering;\n+\n+import java.util.List;\n+\n+public interface KafkaProducerRepository {\n+\n+    long storeRecord(StoredProducerRecord record);\n+\n+    void deleteRecord(long id);\n+\n+    List<StoredProducerRecord> getRecords(int maxMessages);\n+\n+}"
  },
  {
    "sha": "26848edee325e769a18f480b93280782d5aeee8e",
    "filename": "kafka/src/main/java/no/nav/common/kafka/producer/feilhandtering/OracleProducerRepository.java",
    "status": "added",
    "additions": 73,
    "deletions": 0,
    "changes": 73,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/producer/feilhandtering/OracleProducerRepository.java",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/producer/feilhandtering/OracleProducerRepository.java",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/src/main/java/no/nav/common/kafka/producer/feilhandtering/OracleProducerRepository.java?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,73 @@\n+package no.nav.common.kafka.producer.feilhandtering;\n+\n+import lombok.SneakyThrows;\n+\n+import javax.sql.DataSource;\n+import java.sql.PreparedStatement;\n+import java.util.List;\n+\n+import static java.lang.String.format;\n+import static no.nav.common.kafka.util.DatabaseConstants.*;\n+import static no.nav.common.kafka.util.DatabaseUtils.*;\n+\n+public class OracleProducerRepository implements KafkaProducerRepository {\n+\n+    private final DataSource dataSource;\n+\n+    private final String producerRecordTable;\n+\n+    public OracleProducerRepository(DataSource dataSource, String producerRecordTableName) {\n+        this.dataSource = dataSource;\n+        this.producerRecordTable = producerRecordTableName;\n+    }\n+\n+    public OracleProducerRepository(DataSource dataSource) {\n+        this(dataSource, PRODUCER_RECORD_TABLE);\n+    }\n+\n+    @SneakyThrows\n+    @Override\n+    public long storeRecord(StoredProducerRecord record) {\n+        String sql = format(\n+                \"INSERT INTO %s (%s, %s, %s, %s, %s) VALUES (?, ?, ?, ?, ?)\",\n+                producerRecordTable, ID, TOPIC, KEY, VALUE, HEADERS_JSON\n+        );\n+\n+        long id = incrementAndGetOracleSequence(dataSource, PRODUCER_RECORD_ID_SEQ);\n+\n+        try(PreparedStatement statement = createPreparedStatement(dataSource, sql)) {\n+            statement.setLong(1, id);\n+            statement.setString(2, record.getTopic());\n+            statement.setBytes(3, record.getKey());\n+            statement.setBytes(4, record.getValue());\n+            statement.setString(5, record.getHeadersJson());\n+            statement.executeUpdate();\n+        }\n+\n+        return id;\n+    }\n+\n+    @SneakyThrows\n+    @Override\n+    public void deleteRecord(long id) {\n+        String sql = format(\"DELETE FROM %s WHERE %s = ?\", producerRecordTable, ID);\n+        try(PreparedStatement statement = createPreparedStatement(dataSource, sql)) {\n+            statement.setLong(1, id);\n+            statement.executeUpdate();\n+        }\n+    }\n+\n+    @SneakyThrows\n+    @Override\n+    public List<StoredProducerRecord> getRecords(int maxMessages) {\n+        String sql = format(\n+                \"SELECT * FROM %s ORDER BY %s FETCH NEXT %d ROWS ONLY\",\n+                producerRecordTable, ID, maxMessages\n+        );\n+\n+        try(PreparedStatement statement = createPreparedStatement(dataSource, sql)) {\n+            return fetchProducerRecords(statement.executeQuery());\n+        }\n+    }\n+\n+}"
  },
  {
    "sha": "231eb0ae23feef332297be5f7f0a60126fa7de91",
    "filename": "kafka/src/main/java/no/nav/common/kafka/producer/feilhandtering/PostgresProducerRepository.java",
    "status": "added",
    "additions": 73,
    "deletions": 0,
    "changes": 73,
    "blob_url": "https://github.com/navikt/common-java-modules/blob/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/producer/feilhandtering/PostgresProducerRepository.java",
    "raw_url": "https://github.com/navikt/common-java-modules/raw/047ca14d65e9d9099c9c32ab03188616beb66ce5/kafka/src/main/java/no/nav/common/kafka/producer/feilhandtering/PostgresProducerRepository.java",
    "contents_url": "https://api.github.com/repos/navikt/common-java-modules/contents/kafka/src/main/java/no/nav/common/kafka/producer/feilhandtering/PostgresProducerRepository.java?ref=047ca14d65e9d9099c9c32ab03188616beb66ce5",
    "patch": "@@ -0,0 +1,73 @@\n+package no.nav.common.kafka.producer.feilhandtering;\n+\n+import lombok.SneakyThrows;\n+\n+import javax.sql.DataSource;\n+import java.sql.PreparedStatement;\n+import java.util.List;\n+\n+import static java.lang.String.format;\n+import static no.nav.common.kafka.util.DatabaseConstants.*;\n+import static no.nav.common.kafka.util.DatabaseUtils.*;\n+\n+public class PostgresProducerRepository implements KafkaProducerRepository {\n+\n+    private final DataSource dataSource;\n+\n+    private final String producerRecordTable;\n+\n+    public PostgresProducerRepository(DataSource dataSource, String producerRecordTableName) {\n+        this.dataSource = dataSource;\n+        this.producerRecordTable = producerRecordTableName;\n+    }\n+\n+    public PostgresProducerRepository(DataSource dataSource) {\n+        this(dataSource, PRODUCER_RECORD_TABLE);\n+    }\n+\n+    @SneakyThrows\n+    @Override\n+    public long storeRecord(StoredProducerRecord record) {\n+        String sql = format(\n+                \"INSERT INTO %s (%s, %s, %s, %s, %s) VALUES (?, ?, ?, ?, ?)\",\n+                producerRecordTable, ID, TOPIC, KEY, VALUE, HEADERS_JSON\n+        );\n+\n+        long id = incrementAndGetPostgresSequence(dataSource, PRODUCER_RECORD_ID_SEQ);\n+\n+        try(PreparedStatement statement = createPreparedStatement(dataSource, sql)) {\n+            statement.setLong(1, id);\n+            statement.setString(2, record.getTopic());\n+            statement.setBytes(3, record.getKey());\n+            statement.setBytes(4, record.getValue());\n+            statement.setString(5, record.getHeadersJson());\n+            statement.executeUpdate();\n+        }\n+\n+        return id;\n+    }\n+\n+    @SneakyThrows\n+    @Override\n+    public void deleteRecord(long id) {\n+        String sql = format(\"DELETE FROM %s WHERE %s = ?\", producerRecordTable, ID);\n+        try(PreparedStatement statement = createPreparedStatement(dataSource, sql)) {\n+            statement.setLong(1, id);\n+            statement.executeUpdate();\n+        }\n+    }\n+\n+    @SneakyThrows\n+    @Override\n+    public List<StoredProducerRecord> getRecords(int maxMessages) {\n+        String sql = format(\n+                \"SELECT * FROM %s ORDER BY %s LIMIT %d\",\n+                producerRecordTable, ID, maxMessages\n+        );\n+\n+        try(PreparedStatement statement = createPreparedStatement(dataSource, sql)) {\n+            return fetchProducerRecords(statement.executeQuery());\n+        }\n+    }\n+\n+}"
  }
]
