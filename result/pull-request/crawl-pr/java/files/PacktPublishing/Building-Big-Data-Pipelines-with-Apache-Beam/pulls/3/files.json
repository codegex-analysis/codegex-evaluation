[
  {
    "sha": "dcd3409500b2be03a3358f662f8a6e5c1a4fddb0",
    "filename": ".github/workflows/maven.yaml",
    "status": "added",
    "additions": 21,
    "deletions": 0,
    "changes": 21,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/.github/workflows/maven.yaml",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/.github/workflows/maven.yaml",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/.github/workflows/maven.yaml?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,21 @@\n+\n+name: Compile\n+\n+on:\n+  push:\n+    branches:\n+    - master\n+  pull_request:\n+\n+jobs:\n+  build:\n+    runs-on: ubuntu-latest\n+    steps:\n+    - uses: actions/checkout@v1\n+    - name: Set up JDK 11\n+      uses: actions/setup-java@v1\n+      with:\n+        java-version: 11\n+    - name: Generate and upload javadocs\n+      run: ./mvnw clean package\n+"
  },
  {
    "sha": "2cc7d4a55c0cd0092912bf49ae38b3a9e3fd0054",
    "filename": ".mvn/wrapper/maven-wrapper.jar",
    "status": "removed",
    "additions": 0,
    "deletions": 0,
    "changes": 0,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/da9986cfd844dbb4ed09edc174da57b59e830ea8/.mvn/wrapper/maven-wrapper.jar",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/da9986cfd844dbb4ed09edc174da57b59e830ea8/.mvn/wrapper/maven-wrapper.jar",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/.mvn/wrapper/maven-wrapper.jar?ref=da9986cfd844dbb4ed09edc174da57b59e830ea8"
  },
  {
    "sha": "63c60cee9f29c7ee60edd409cdb1031a39e1414b",
    "filename": "bin/run-class.sh",
    "status": "added",
    "additions": 11,
    "deletions": 0,
    "changes": 11,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/bin/run-class.sh",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/bin/run-class.sh",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/bin/run-class.sh?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,11 @@\n+#!/bin/bash\n+\n+CLASSPATH=\"\"\n+for jar in /usr/local/lib/*.jar; do\n+  if [[ ! -z $CLASSPATH ]]; then\n+    CLASSPATH=\"${CLASSPATH}:\"\n+  fi\n+  CLASSPATH=\"${CLASSPATH}${jar}\"\n+done\n+\n+${JAVA_HOME}/bin/java -cp ${CLASSPATH} \"$@\""
  },
  {
    "sha": "4897a5cf4a52db9840fb0d5a37ad75f03cba0a3e",
    "filename": "bin/sleep-forever.sh",
    "status": "added",
    "additions": 8,
    "deletions": 0,
    "changes": 8,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/bin/sleep-forever.sh",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/bin/sleep-forever.sh",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/bin/sleep-forever.sh?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,8 @@\n+#!/bin/bash\n+\n+set -e\n+while true; do\n+  if ! sleep 10; then\n+    break;\n+  fi\n+done"
  },
  {
    "sha": "077ffe5c537481f9a41d22262c0b870f3efef2ff",
    "filename": "chapter2/license-header-spotless.txt",
    "status": "added",
    "additions": 1,
    "deletions": 0,
    "changes": 1,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/license-header-spotless.txt",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/license-header-spotless.txt",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/chapter2/license-header-spotless.txt?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1 @@\n+../license-header-spotless.txt\n\\ No newline at end of file"
  },
  {
    "sha": "19c75803b55be4f54ac2937c298f8ceaf74cddff",
    "filename": "chapter2/pom.xml",
    "status": "added",
    "additions": 107,
    "deletions": 0,
    "changes": 107,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/pom.xml",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/pom.xml",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/chapter2/pom.xml?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,107 @@\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n+  xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\">\n+\n+  <modelVersion>4.0.0</modelVersion>\n+  <groupId>com.packtpub.beam</groupId>\n+  <artifactId>beam-chapter2</artifactId>\n+  <packaging>jar</packaging>\n+  <parent>\n+    <groupId>com.packtpub.beam</groupId>\n+    <artifactId>beam-parent</artifactId>\n+    <version>1.0-SNAPSHOT</version>\n+  </parent>\n+\n+  <name>Apache Beam Practical Guide Chapter 2</name>\n+\n+  <profiles>\n+    <profile>\n+      <id>docker</id>\n+      <build>\n+        <plugins>\n+          <plugin>\n+            <groupId>org.apache.maven.plugins</groupId>\n+            <artifactId>maven-dependency-plugin</artifactId>\n+            <version>3.1.2</version>\n+            <executions>\n+              <execution>\n+                <id>copy-dependencies</id>\n+                <phase>package</phase>\n+                <goals>\n+                  <goal>copy-dependencies</goal>\n+                </goals>\n+                <configuration>\n+                </configuration>\n+              </execution>\n+            </executions>\n+          </plugin>\n+        </plugins>\n+      </build>\n+    </profile>\n+  </profiles>\n+\n+  <dependencies>\n+\n+    <dependency>\n+      <groupId>com.packtpub.beam</groupId>\n+      <artifactId>beam-util</artifactId>\n+    </dependency>\n+\n+    <dependency>\n+      <groupId>org.apache.beam</groupId>\n+      <artifactId>beam-sdks-java-core</artifactId>\n+    </dependency>\n+\n+    <dependency>\n+      <groupId>org.apache.beam</groupId>\n+      <artifactId>beam-sdks-java-io-kafka</artifactId>\n+    </dependency>\n+\n+    <dependency>\n+      <groupId>org.apache.beam</groupId>\n+      <artifactId>beam-runners-direct-java</artifactId>\n+    </dependency>\n+\n+    <dependency>\n+      <groupId>org.apache.beam</groupId>\n+      <artifactId>beam-runners-flink-1.12</artifactId>\n+    </dependency>\n+\n+    <dependency>\n+      <groupId>org.apache.kafka</groupId>\n+      <artifactId>kafka-clients</artifactId>\n+    </dependency>\n+\n+    <dependency>\n+      <groupId>org.apache.flink</groupId>\n+      <artifactId>flink-streaming-java_2.12</artifactId>\n+    </dependency>\n+\n+    <dependency>\n+      <groupId>org.junit.jupiter</groupId>\n+      <artifactId>junit-jupiter-api</artifactId>\n+    </dependency>\n+\n+    <!-- junit and hamcrest is needed for PAssert -->\n+    <dependency>\n+      <groupId>junit</groupId>\n+      <artifactId>junit</artifactId>\n+    </dependency>\n+\n+    <dependency>\n+      <groupId>org.hamcrest</groupId>\n+      <artifactId>hamcrest-core</artifactId>\n+    </dependency>\n+\n+    <dependency>\n+      <groupId>org.slf4j</groupId>\n+      <artifactId>slf4j-log4j12</artifactId>\n+    </dependency>\n+\n+    <dependency>\n+      <groupId>org.projectlombok</groupId>\n+      <artifactId>lombok</artifactId>\n+    </dependency>\n+\n+  </dependencies>\n+\n+</project>"
  },
  {
    "sha": "7375717bce6577f77c248dd04abcf0ed397a8c5a",
    "filename": "chapter2/src/main/java/com/packtpub/beam/chapter2/AverageWordLength.java",
    "status": "added",
    "additions": 169,
    "deletions": 0,
    "changes": 169,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/main/java/com/packtpub/beam/chapter2/AverageWordLength.java",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/main/java/com/packtpub/beam/chapter2/AverageWordLength.java",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/chapter2/src/main/java/com/packtpub/beam/chapter2/AverageWordLength.java?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,169 @@\n+/**\n+ * Copyright 2021-2021 Packt Publishing Limited\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.packtpub.beam.chapter2;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.packtpub.beam.chapter2.MaxWordLength.Params;\n+import com.packtpub.beam.chapter2.utils.MapToLines;\n+import com.packtpub.beam.util.Tokenize;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.util.Arrays;\n+import lombok.Value;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.coders.CannotProvideCoderException;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderException;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.coders.CustomCoder;\n+import org.apache.beam.sdk.coders.VarLongCoder;\n+import org.apache.beam.sdk.io.kafka.KafkaIO;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.Combine;\n+import org.apache.beam.sdk.transforms.Combine.CombineFn;\n+import org.apache.beam.sdk.transforms.WithKeys;\n+import org.apache.beam.sdk.transforms.windowing.AfterPane;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindows;\n+import org.apache.beam.sdk.transforms.windowing.Repeatedly;\n+import org.apache.beam.sdk.transforms.windowing.Window;\n+import org.apache.beam.sdk.transforms.windowing.Window.OnTimeBehavior;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.beam.sdk.values.TypeDescriptors;\n+import org.apache.kafka.common.serialization.DoubleSerializer;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+public class AverageWordLength {\n+\n+  public static void main(String[] args) {\n+    Params params = parseArgs(args);\n+    Pipeline pipeline =\n+        Pipeline.create(PipelineOptionsFactory.fromArgs(params.getRemainingArgs()).create());\n+    PCollection<String> words =\n+        pipeline\n+            .apply(\n+                KafkaIO.<String, String>read()\n+                    .withBootstrapServers(params.getBootstrapServer())\n+                    .withKeyDeserializer(StringDeserializer.class)\n+                    .withValueDeserializer(StringDeserializer.class)\n+                    .withTopic(params.getInputTopic()))\n+            .apply(MapToLines.of())\n+            .apply(Tokenize.of());\n+\n+    calculateAverageWordLength(words)\n+        .apply(WithKeys.of(\"\"))\n+        .apply(\n+            KafkaIO.<String, Double>write()\n+                .withBootstrapServers(params.getBootstrapServer())\n+                .withTopic(params.getOutputTopic())\n+                .withKeySerializer(StringSerializer.class)\n+                .withValueSerializer(DoubleSerializer.class));\n+\n+    pipeline.run().waitUntilFinish();\n+  }\n+\n+  @VisibleForTesting\n+  static PCollection<Double> calculateAverageWordLength(PCollection<String> words) {\n+    return words\n+        .apply(\n+            Window.<String>into(new GlobalWindows())\n+                .triggering(Repeatedly.forever(AfterPane.elementCountAtLeast(1)))\n+                .accumulatingFiredPanes()\n+                .withOnTimeBehavior(OnTimeBehavior.FIRE_IF_NON_EMPTY))\n+        .apply(Combine.globally(new AverageFn()));\n+  }\n+\n+  static Params parseArgs(String[] args) {\n+    if (args.length < 3) {\n+      throw new IllegalArgumentException(\n+          \"Expected at least 3 arguments: <bootstrapServer> <inputTopic> <outputTopic>\");\n+    }\n+    return new Params(args[0], args[1], args[2], Arrays.copyOfRange(args, 3, args.length));\n+  }\n+\n+  @Value\n+  static class Params {\n+    String bootstrapServer;\n+    String inputTopic;\n+    String outputTopic;\n+    String[] remainingArgs;\n+  }\n+\n+  private static class AverageFn extends CombineFn<String, AverageAccumulator, Double> {\n+    @Override\n+    public AverageAccumulator createAccumulator() {\n+      return new AverageAccumulator(0, 0);\n+    }\n+\n+    @Override\n+    public AverageAccumulator addInput(AverageAccumulator accumulator, String input) {\n+      return new AverageAccumulator(\n+          accumulator.getSumLength() + input.length(), accumulator.getCount() + 1);\n+    }\n+\n+    @Override\n+    public AverageAccumulator mergeAccumulators(Iterable<AverageAccumulator> accumulators) {\n+      long sumLength = 0L;\n+      long count = 0L;\n+      for (AverageAccumulator acc : accumulators) {\n+        sumLength += acc.getSumLength();\n+        count += acc.getCount();\n+      }\n+      return new AverageAccumulator(sumLength, count);\n+    }\n+\n+    @Override\n+    public Double extractOutput(AverageAccumulator accumulator) {\n+      return accumulator.getSumLength() / (double) accumulator.getCount();\n+    }\n+\n+    @Override\n+    public TypeDescriptor<Double> getOutputType() {\n+      return TypeDescriptors.doubles();\n+    }\n+\n+    @Override\n+    public Coder<AverageAccumulator> getAccumulatorCoder(\n+        CoderRegistry registry, Coder<String> inputCoder) throws CannotProvideCoderException {\n+      return new AverageAccumulatorCoder();\n+    }\n+  }\n+\n+  @Value\n+  private static class AverageAccumulator {\n+    long sumLength;\n+    long count;\n+  }\n+\n+  private static class AverageAccumulatorCoder extends CustomCoder<AverageAccumulator> {\n+\n+    private final Coder<Long> coder = VarLongCoder.of();\n+\n+    @Override\n+    public void encode(AverageAccumulator value, OutputStream outStream)\n+        throws CoderException, IOException {\n+      coder.encode(value.getSumLength(), outStream);\n+      coder.encode(value.getCount(), outStream);\n+    }\n+\n+    @Override\n+    public AverageAccumulator decode(InputStream inStream) throws CoderException, IOException {\n+      return new AverageAccumulator(coder.decode(inStream), coder.decode(inStream));\n+    }\n+  }\n+}"
  },
  {
    "sha": "c356af959a924207579765189dd11f4397f571d5",
    "filename": "chapter2/src/main/java/com/packtpub/beam/chapter2/MaxWordLength.java",
    "status": "added",
    "additions": 100,
    "deletions": 0,
    "changes": 100,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/main/java/com/packtpub/beam/chapter2/MaxWordLength.java",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/main/java/com/packtpub/beam/chapter2/MaxWordLength.java",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/chapter2/src/main/java/com/packtpub/beam/chapter2/MaxWordLength.java?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,100 @@\n+/**\n+ * Copyright 2021-2021 Packt Publishing Limited\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.packtpub.beam.chapter2;\n+\n+import com.packtpub.beam.chapter2.utils.MapToLines;\n+import com.packtpub.beam.util.Tokenize;\n+import java.io.Serializable;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import lombok.Value;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.io.kafka.KafkaIO;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.transforms.Max;\n+import org.apache.beam.sdk.transforms.windowing.AfterPane;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindows;\n+import org.apache.beam.sdk.transforms.windowing.Repeatedly;\n+import org.apache.beam.sdk.transforms.windowing.Window;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.TypeDescriptors;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.joda.time.Duration;\n+\n+public class MaxWordLength {\n+\n+  public static void main(String[] args) {\n+    Params params = parseArgs(args);\n+    PipelineOptions options = PipelineOptionsFactory.fromArgs(params.getRemainingArgs()).create();\n+    Pipeline pipeline = Pipeline.create(options);\n+    PCollection<String> lines =\n+        pipeline\n+            .apply(\n+                KafkaIO.<String, String>read()\n+                    .withBootstrapServers(params.getBootstrapServer())\n+                    .withKeyDeserializer(StringDeserializer.class)\n+                    .withValueDeserializer(StringDeserializer.class)\n+                    .withTopic(params.getInputTopic()))\n+            .apply(MapToLines.of());\n+    PCollection<String> output = computeLongestWord(lines);\n+    output\n+        .apply(\n+            MapElements.into(\n+                    TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.strings()))\n+                .via(e -> KV.of(\"\", e)))\n+        .apply(\n+            KafkaIO.<String, String>write()\n+                .withBootstrapServers(params.getBootstrapServer())\n+                .withTopic(params.getOutputTopic())\n+                .withKeySerializer(StringSerializer.class)\n+                .withValueSerializer(StringSerializer.class));\n+    pipeline.run().waitUntilFinish();\n+  }\n+\n+  static PCollection<String> computeLongestWord(PCollection<String> lines) {\n+    return lines\n+        .apply(Tokenize.of())\n+        .apply(\n+            Window.<String>into(new GlobalWindows())\n+                .triggering(Repeatedly.forever(AfterPane.elementCountAtLeast(1)))\n+                .withAllowedLateness(Duration.ZERO)\n+                .accumulatingFiredPanes())\n+        .apply(\n+            Max.globally(\n+                (Comparator<String> & Serializable)\n+                    (a, b) -> Long.compare(a.length(), b.length())));\n+  }\n+\n+  static Params parseArgs(String[] args) {\n+    if (args.length < 3) {\n+      throw new IllegalArgumentException(\n+          \"Expected at least 3 arguments: <bootstrapServer> <inputTopic> <outputTopic>\");\n+    }\n+    return new Params(args[0], args[1], args[2], Arrays.copyOfRange(args, 3, args.length));\n+  }\n+\n+  @Value\n+  static class Params {\n+    String bootstrapServer;\n+    String inputTopic;\n+    String outputTopic;\n+    String[] remainingArgs;\n+  }\n+}"
  },
  {
    "sha": "1e5781bee3f3bb0ad656a685d732677330a8f65a",
    "filename": "chapter2/src/main/java/com/packtpub/beam/chapter2/MaxWordLengthWithTimestamp.java",
    "status": "added",
    "additions": 98,
    "deletions": 0,
    "changes": 98,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/main/java/com/packtpub/beam/chapter2/MaxWordLengthWithTimestamp.java",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/main/java/com/packtpub/beam/chapter2/MaxWordLengthWithTimestamp.java",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/chapter2/src/main/java/com/packtpub/beam/chapter2/MaxWordLengthWithTimestamp.java?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,98 @@\n+/**\n+ * Copyright 2021-2021 Packt Publishing Limited\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.packtpub.beam.chapter2;\n+\n+import static com.packtpub.beam.chapter2.MaxWordLength.parseArgs;\n+\n+import com.packtpub.beam.chapter2.MaxWordLength.Params;\n+import com.packtpub.beam.chapter2.utils.MapToLines;\n+import com.packtpub.beam.util.Tokenize;\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.io.kafka.KafkaIO;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.transforms.Max;\n+import org.apache.beam.sdk.transforms.Reify;\n+import org.apache.beam.sdk.transforms.windowing.AfterPane;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindows;\n+import org.apache.beam.sdk.transforms.windowing.Repeatedly;\n+import org.apache.beam.sdk.transforms.windowing.TimestampCombiner;\n+import org.apache.beam.sdk.transforms.windowing.Window;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.TypeDescriptors;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.joda.time.Duration;\n+\n+public class MaxWordLengthWithTimestamp {\n+\n+  public static void main(String[] args) {\n+    runWithArgsAndCombiner(args, TimestampCombiner.END_OF_WINDOW);\n+  }\n+\n+  static void runWithArgsAndCombiner(String[] args, TimestampCombiner combiner) {\n+    Params params = parseArgs(args);\n+    PipelineOptions options = PipelineOptionsFactory.fromArgs(params.getRemainingArgs()).create();\n+    Pipeline pipeline = Pipeline.create(options);\n+    PCollection<String> lines =\n+        pipeline\n+            .apply(\n+                KafkaIO.<String, String>read()\n+                    .withBootstrapServers(params.getBootstrapServer())\n+                    .withKeyDeserializer(StringDeserializer.class)\n+                    .withValueDeserializer(StringDeserializer.class)\n+                    .withTopic(params.getInputTopic()))\n+            .apply(MapToLines.of());\n+    PCollection<KV<String, String>> output = computeLongestWordWithTimestamp(lines, combiner);\n+    output.apply(\n+        KafkaIO.<String, String>write()\n+            .withBootstrapServers(params.getBootstrapServer())\n+            .withTopic(params.getOutputTopic())\n+            .withKeySerializer(StringSerializer.class)\n+            .withValueSerializer(StringSerializer.class));\n+    pipeline.run().waitUntilFinish();\n+  }\n+\n+  static PCollection<KV<String, String>> computeLongestWordWithTimestamp(\n+      PCollection<String> lines) {\n+\n+    return computeLongestWordWithTimestamp(lines, TimestampCombiner.LATEST);\n+  }\n+\n+  static PCollection<KV<String, String>> computeLongestWordWithTimestamp(\n+      PCollection<String> lines, TimestampCombiner combiner) {\n+    return lines\n+        .apply(Tokenize.of())\n+        .apply(\n+            Window.<String>into(new GlobalWindows())\n+                .triggering(Repeatedly.forever(AfterPane.elementCountAtLeast(1)))\n+                .withAllowedLateness(Duration.ZERO)\n+                .withTimestampCombiner(combiner)\n+                .accumulatingFiredPanes())\n+        .apply(\n+            Max.globally(\n+                (Comparator<String> & Serializable) (a, b) -> Long.compare(a.length(), b.length())))\n+        .apply(Reify.timestamps())\n+        .apply(\n+            MapElements.into(\n+                    TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.strings()))\n+                .via(tv -> KV.of(tv.getTimestamp().toString(), tv.getValue())));\n+  }\n+}"
  },
  {
    "sha": "f886cb8aa1e5f68787bcb1ed7420bae51349a69a",
    "filename": "chapter2/src/main/java/com/packtpub/beam/chapter2/MaxWordLengthWithTimestampWithLatestCombiner.java",
    "status": "added",
    "additions": 24,
    "deletions": 0,
    "changes": 24,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/main/java/com/packtpub/beam/chapter2/MaxWordLengthWithTimestampWithLatestCombiner.java",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/main/java/com/packtpub/beam/chapter2/MaxWordLengthWithTimestampWithLatestCombiner.java",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/chapter2/src/main/java/com/packtpub/beam/chapter2/MaxWordLengthWithTimestampWithLatestCombiner.java?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,24 @@\n+/**\n+ * Copyright 2021-2021 Packt Publishing Limited\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.packtpub.beam.chapter2;\n+\n+import org.apache.beam.sdk.transforms.windowing.TimestampCombiner;\n+\n+public class MaxWordLengthWithTimestampWithLatestCombiner {\n+  public static void main(String[] args) {\n+    MaxWordLengthWithTimestamp.runWithArgsAndCombiner(args, TimestampCombiner.LATEST);\n+  }\n+}"
  },
  {
    "sha": "cb9e89a3d84b22d5af8aa15fe926f30c2cb58236",
    "filename": "chapter2/src/main/java/com/packtpub/beam/chapter2/MyStringCoder.java",
    "status": "added",
    "additions": 41,
    "deletions": 0,
    "changes": 41,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/main/java/com/packtpub/beam/chapter2/MyStringCoder.java",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/main/java/com/packtpub/beam/chapter2/MyStringCoder.java",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/chapter2/src/main/java/com/packtpub/beam/chapter2/MyStringCoder.java?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,41 @@\n+/**\n+ * Copyright 2021-2021 Packt Publishing Limited\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.packtpub.beam.chapter2;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.nio.charset.StandardCharsets;\n+import org.apache.beam.sdk.coders.CoderException;\n+import org.apache.beam.sdk.coders.CustomCoder;\n+import org.apache.beam.sdk.util.VarInt;\n+\n+public class MyStringCoder extends CustomCoder<String> {\n+\n+  @Override\n+  public void encode(String value, OutputStream outStream) throws CoderException, IOException {\n+    byte[] bytes = value.getBytes(StandardCharsets.UTF_8);\n+    VarInt.encode(bytes.length, outStream);\n+    outStream.write(bytes);\n+  }\n+\n+  @Override\n+  public String decode(InputStream inStream) throws CoderException, IOException {\n+    byte[] bytes = new byte[VarInt.decodeInt(inStream)];\n+    inStream.read(bytes);\n+    return new String(bytes, StandardCharsets.UTF_8);\n+  }\n+}"
  },
  {
    "sha": "2875b50750b2d31774dae9d9edce95751ed23516",
    "filename": "chapter2/src/main/java/com/packtpub/beam/chapter2/TestKafkaWrite.java",
    "status": "added",
    "additions": 47,
    "deletions": 0,
    "changes": 47,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/main/java/com/packtpub/beam/chapter2/TestKafkaWrite.java",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/main/java/com/packtpub/beam/chapter2/TestKafkaWrite.java",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/chapter2/src/main/java/com/packtpub/beam/chapter2/TestKafkaWrite.java?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,47 @@\n+/**\n+ * Copyright 2021-2021 Packt Publishing Limited\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.packtpub.beam.chapter2;\n+\n+import java.util.Arrays;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.io.kafka.KafkaIO;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.Create.Values;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.TypeDescriptors;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+\n+public class TestKafkaWrite {\n+\n+  public static void main(String[] args) {\n+    Values<String> values = Create.of(Arrays.asList(Arrays.copyOfRange(args, 2, args.length)));\n+    Pipeline pipeline = Pipeline.create();\n+    pipeline\n+        .apply(values)\n+        .apply(\n+            MapElements.into(\n+                    TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.strings()))\n+                .via(e -> KV.of(\"key\", e)))\n+        .apply(\n+            KafkaIO.<String, String>write()\n+                .withValueSerializer(StringSerializer.class)\n+                .withKeySerializer(StringSerializer.class)\n+                .withBootstrapServers(args[0])\n+                .withTopic(args[1]));\n+    pipeline.run();\n+  }\n+}"
  },
  {
    "sha": "52fcd818d2a4b182cdad67acd2aaec283f662691",
    "filename": "chapter2/src/main/java/com/packtpub/beam/chapter2/TopNWords.java",
    "status": "added",
    "additions": 109,
    "deletions": 0,
    "changes": 109,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/main/java/com/packtpub/beam/chapter2/TopNWords.java",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/main/java/com/packtpub/beam/chapter2/TopNWords.java",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/chapter2/src/main/java/com/packtpub/beam/chapter2/TopNWords.java?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,109 @@\n+/**\n+ * Copyright 2021-2021 Packt Publishing Limited\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.packtpub.beam.chapter2;\n+\n+import avro.shaded.com.google.common.annotations.VisibleForTesting;\n+import com.packtpub.beam.chapter2.utils.MapToLines;\n+import com.packtpub.beam.util.Tokenize;\n+import java.io.Serializable;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import lombok.Value;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.io.kafka.KafkaIO;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.Flatten;\n+import org.apache.beam.sdk.transforms.Top;\n+import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n+import org.apache.beam.sdk.transforms.windowing.Window;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.joda.time.Duration;\n+\n+public class TopNWords {\n+\n+  public static void main(String[] args) {\n+    Params params = parseArgs(args);\n+    PipelineOptions options = PipelineOptionsFactory.fromArgs(params.getRemainingArgs()).create();\n+    Pipeline pipeline = Pipeline.create(options);\n+    PCollection<String> lines =\n+        pipeline\n+            .apply(\n+                KafkaIO.<String, String>read()\n+                    .withBootstrapServers(params.getBootstrapServer())\n+                    .withKeyDeserializer(StringDeserializer.class)\n+                    .withValueDeserializer(StringDeserializer.class)\n+                    .withTopic(params.getInputTopic()))\n+            .apply(MapToLines.of());\n+    PCollection<KV<String, Long>> output =\n+        countWordsInFixedWindows(lines, params.getWindowLength(), params.getN());\n+    output.apply(\n+        KafkaIO.<String, Long>write()\n+            .withBootstrapServers(params.getBootstrapServer())\n+            .withTopic(params.getOutputTopic())\n+            .withKeySerializer(StringSerializer.class)\n+            .withValueSerializer(LongSerializer.class));\n+    pipeline.run().waitUntilFinish();\n+  }\n+\n+  @VisibleForTesting\n+  static PCollection<KV<String, Long>> countWordsInFixedWindows(\n+      PCollection<String> lines, Duration windowLength, int n) {\n+\n+    return lines\n+        .apply(Window.into(FixedWindows.of(windowLength)))\n+        .apply(Tokenize.of())\n+        .apply(Count.perElement())\n+        .apply(\n+            Top.of(\n+                    n,\n+                    (Comparator<KV<String, Long>> & Serializable)\n+                        (a, b) -> Long.compare(a.getValue(), b.getValue()))\n+                .withoutDefaults())\n+        .apply(Flatten.iterables());\n+  }\n+\n+  @VisibleForTesting\n+  static Params parseArgs(String[] args) {\n+    if (args.length < 5) {\n+      throw new IllegalArgumentException(\n+          \"Expected at least 5 arguments: <windowLength> <bootstrapServer> <inputTopic> <outputTopic> <n>\");\n+    }\n+    return new Params(\n+        Duration.standardSeconds(Integer.parseInt(args[0])),\n+        args[1],\n+        args[2],\n+        args[3],\n+        Integer.parseInt(args[4]),\n+        Arrays.copyOfRange(args, 5, args.length));\n+  }\n+\n+  @Value\n+  @VisibleForTesting\n+  static class Params {\n+    Duration windowLength;\n+    String bootstrapServer;\n+    String inputTopic;\n+    String outputTopic;\n+    int n;\n+    String[] remainingArgs;\n+  }\n+}"
  },
  {
    "sha": "83a97d1d2933df672a58db00bb349dd242bb8234",
    "filename": "chapter2/src/main/java/com/packtpub/beam/chapter2/utils/MapToLines.java",
    "status": "added",
    "additions": 41,
    "deletions": 0,
    "changes": 41,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/main/java/com/packtpub/beam/chapter2/utils/MapToLines.java",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/main/java/com/packtpub/beam/chapter2/utils/MapToLines.java",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/chapter2/src/main/java/com/packtpub/beam/chapter2/utils/MapToLines.java?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,41 @@\n+/**\n+ * Copyright 2021-2021 Packt Publishing Limited\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.packtpub.beam.chapter2.utils;\n+\n+import org.apache.beam.sdk.io.kafka.KafkaRecord;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.TypeDescriptors;\n+\n+public class MapToLines<K, T>\n+    extends PTransform<PCollection<KafkaRecord<K, T>>, PCollection<String>> {\n+\n+  public static <K, V> MapToLines<K, V> of() {\n+    return new MapToLines<>();\n+  }\n+\n+  @Override\n+  public PCollection<String> expand(PCollection<KafkaRecord<K, T>> input) {\n+    return input.apply(\n+        MapElements.into(TypeDescriptors.strings())\n+            .via(r -> ifNotNull(r.getKV().getKey()) + \" \" + ifNotNull(r.getKV().getValue())));\n+  }\n+\n+  private static <T> String ifNotNull(T value) {\n+    return value != null ? value.toString() : \"\";\n+  }\n+}"
  },
  {
    "sha": "de30811fea0f0a19ab06889045dddd725f0e76e6",
    "filename": "chapter2/src/main/resources/log4j.properties",
    "status": "added",
    "additions": 6,
    "deletions": 0,
    "changes": 6,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/main/resources/log4j.properties",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/main/resources/log4j.properties",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/chapter2/src/main/resources/log4j.properties?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,6 @@\n+log4j.rootLogger=INFO, console\n+ \n+log4j.appender.console=org.apache.log4j.ConsoleAppender\n+log4j.appender.console.layout=org.apache.log4j.PatternLayout\n+log4j.appender.console.layout.ConversionPattern=[%t] %-5p %c %x - %m%n\n+ "
  },
  {
    "sha": "cb9f500f5a1cd28db375379bb34a78d711f0d878",
    "filename": "chapter2/src/test/java/com/packtpub/beam/chapter2/AverageWordLengthTest.java",
    "status": "added",
    "additions": 43,
    "deletions": 0,
    "changes": 43,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/test/java/com/packtpub/beam/chapter2/AverageWordLengthTest.java",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/test/java/com/packtpub/beam/chapter2/AverageWordLengthTest.java",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/chapter2/src/test/java/com/packtpub/beam/chapter2/AverageWordLengthTest.java?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,43 @@\n+/**\n+ * Copyright 2021-2021 Packt Publishing Limited\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.packtpub.beam.chapter2;\n+\n+import static com.packtpub.beam.chapter2.AverageWordLength.calculateAverageWordLength;\n+\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestStream;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.junit.jupiter.api.Test;\n+\n+public class AverageWordLengthTest {\n+\n+  @Test\n+  public void testAverageWordCalculation() {\n+    Pipeline pipeline = Pipeline.create();\n+    PCollection<String> input =\n+        pipeline.apply(\n+            TestStream.create(StringUtf8Coder.of())\n+                .addElements(\"a\")\n+                .addElements(\"bb\")\n+                .addElements(\"ccc\")\n+                .advanceWatermarkToInfinity());\n+    PCollection<Double> averages = calculateAverageWordLength(input);\n+    PAssert.that(averages).containsInAnyOrder(1.0, 1.5, 2.0);\n+    pipeline.run();\n+  }\n+}"
  },
  {
    "sha": "795768b5f79bcd5fa2800efb997e354b362dd950",
    "filename": "chapter2/src/test/java/com/packtpub/beam/chapter2/MaxWordLengthTest.java",
    "status": "added",
    "additions": 48,
    "deletions": 0,
    "changes": 48,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/test/java/com/packtpub/beam/chapter2/MaxWordLengthTest.java",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/test/java/com/packtpub/beam/chapter2/MaxWordLengthTest.java",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/chapter2/src/test/java/com/packtpub/beam/chapter2/MaxWordLengthTest.java?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,48 @@\n+/**\n+ * Copyright 2021-2021 Packt Publishing Limited\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.packtpub.beam.chapter2;\n+\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestStream;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.TimestampedValue;\n+import org.joda.time.Instant;\n+import org.junit.jupiter.api.Test;\n+\n+public class MaxWordLengthTest {\n+\n+  @Test\n+  public void testWordLength() {\n+    Instant now = Instant.now();\n+    TestStream<String> input =\n+        TestStream.create(StringUtf8Coder.of())\n+            .addElements(TimestampedValue.of(\"a\", now))\n+            .addElements(TimestampedValue.of(\"bb\", now.plus(10000)))\n+            .addElements(TimestampedValue.of(\"ccc\", now.plus(20000)))\n+            .addElements(TimestampedValue.of(\"d\", now.plus(30000)))\n+            .advanceWatermarkToInfinity();\n+\n+    Pipeline pipeline = Pipeline.create();\n+    PCollection<String> strings = pipeline.apply(input);\n+    PCollection<String> output = MaxWordLength.computeLongestWord(strings);\n+    PAssert.that(output).containsInAnyOrder(\"a\", \"bb\", \"ccc\", \"ccc\", \"ccc\");\n+    PAssert.that(output).inFinalPane(GlobalWindow.INSTANCE).containsInAnyOrder(\"ccc\");\n+    pipeline.run();\n+  }\n+}"
  },
  {
    "sha": "c1ba9596469a0435a60ba2f07f8a136728c6681e",
    "filename": "chapter2/src/test/java/com/packtpub/beam/chapter2/MaxWordLengthWithTimestampTest.java",
    "status": "added",
    "additions": 57,
    "deletions": 0,
    "changes": 57,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/test/java/com/packtpub/beam/chapter2/MaxWordLengthWithTimestampTest.java",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/test/java/com/packtpub/beam/chapter2/MaxWordLengthWithTimestampTest.java",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/chapter2/src/test/java/com/packtpub/beam/chapter2/MaxWordLengthWithTimestampTest.java?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,57 @@\n+/**\n+ * Copyright 2021-2021 Packt Publishing Limited\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.packtpub.beam.chapter2;\n+\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestStream;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.TimestampedValue;\n+import org.joda.time.Instant;\n+import org.junit.jupiter.api.Test;\n+\n+public class MaxWordLengthWithTimestampTest {\n+\n+  @Test\n+  public void testWordLength() {\n+    Instant now = Instant.now();\n+    TestStream<String> input =\n+        TestStream.create(StringUtf8Coder.of())\n+            .addElements(TimestampedValue.of(\"a\", now))\n+            .addElements(TimestampedValue.of(\"bb\", now.plus(10000)))\n+            .addElements(TimestampedValue.of(\"ccc\", now.plus(20000)))\n+            .addElements(TimestampedValue.of(\"d\", now.plus(30000)))\n+            .addElements(TimestampedValue.of(\"ccc\", now.plus(40000)))\n+            .advanceWatermarkToInfinity();\n+\n+    Pipeline pipeline = Pipeline.create();\n+    PCollection<String> strings = pipeline.apply(input);\n+    PCollection<KV<String, String>> output =\n+        MaxWordLengthWithTimestamp.computeLongestWordWithTimestamp(strings);\n+    PAssert.that(output)\n+        .containsInAnyOrder(\n+            KV.of(now.toString(), \"a\"),\n+            KV.of(now.plus(10000).toString(), \"bb\"),\n+            KV.of(now.plus(20000).toString(), \"ccc\"),\n+            KV.of(now.plus(30000).toString(), \"ccc\"),\n+            KV.of(now.plus(40000).toString(), \"ccc\"),\n+            KV.of(GlobalWindow.INSTANCE.maxTimestamp().toString(), \"ccc\"));\n+    pipeline.run();\n+  }\n+}"
  },
  {
    "sha": "4198fa3b5400289287af456476618641c7354a79",
    "filename": "chapter2/src/test/java/com/packtpub/beam/chapter2/MyStringCoderTest.java",
    "status": "added",
    "additions": 41,
    "deletions": 0,
    "changes": 41,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/test/java/com/packtpub/beam/chapter2/MyStringCoderTest.java",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/test/java/com/packtpub/beam/chapter2/MyStringCoderTest.java",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/chapter2/src/test/java/com/packtpub/beam/chapter2/MyStringCoderTest.java?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,41 @@\n+/**\n+ * Copyright 2021-2021 Packt Publishing Limited\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.packtpub.beam.chapter2;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.junit.jupiter.api.Test;\n+\n+public class MyStringCoderTest {\n+\n+  @Test\n+  public void testCoder() throws IOException {\n+    Coder<String> coder = new MyStringCoder();\n+    byte[] serialized;\n+    try (ByteArrayOutputStream baos = new ByteArrayOutputStream()) {\n+      coder.encode(\"This is string. \\uD83D\\uDE0A\", baos);\n+      baos.flush();\n+      serialized = baos.toByteArray();\n+    }\n+    try (ByteArrayInputStream bais = new ByteArrayInputStream(serialized)) {\n+      assertEquals(\"This is string. \\uD83D\\uDE0A\", coder.decode(bais));\n+    }\n+  }\n+}"
  },
  {
    "sha": "79722f3cc75242cf2d33b9bd57dfa2ae2c44c15a",
    "filename": "chapter2/src/test/java/com/packtpub/beam/chapter2/TopNWordsTest.java",
    "status": "added",
    "additions": 67,
    "deletions": 0,
    "changes": 67,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/test/java/com/packtpub/beam/chapter2/TopNWordsTest.java",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/chapter2/src/test/java/com/packtpub/beam/chapter2/TopNWordsTest.java",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/chapter2/src/test/java/com/packtpub/beam/chapter2/TopNWordsTest.java?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,67 @@\n+/**\n+ * Copyright 2021-2021 Packt Publishing Limited\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.packtpub.beam.chapter2;\n+\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestStream;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.TimestampedValue;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.junit.jupiter.api.Test;\n+\n+public class TopNWordsTest {\n+\n+  @Test\n+  public void testComputeStatsInWindows() {\n+    Duration windowDuration = Duration.standardSeconds(10);\n+    Instant now = Instant.now();\n+    Instant startOfTenSecondWindow = now.plus(-now.getMillis() % windowDuration.getMillis());\n+    TestStream<String> lines = createInput(startOfTenSecondWindow);\n+    Pipeline pipeline = Pipeline.create();\n+    PCollection<String> input = pipeline.apply(lines);\n+    PCollection<KV<String, Long>> output =\n+        TopNWords.countWordsInFixedWindows(input, windowDuration, 3);\n+    PAssert.that(output)\n+        .containsInAnyOrder(\n+            KV.of(\"line\", 3L),\n+            KV.of(\"first\", 3L),\n+            KV.of(\"the\", 3L),\n+            KV.of(\"line\", 3L),\n+            KV.of(\"in\", 2L),\n+            KV.of(\"window\", 2L));\n+    pipeline.run();\n+  }\n+\n+  private TestStream<String> createInput(Instant startOfTenSecondWindow) {\n+    return TestStream.create(StringUtf8Coder.of())\n+        .addElements(\n+            TimestampedValue.of(\"This is the first line.\", startOfTenSecondWindow),\n+            TimestampedValue.of(\n+                \"This is second line in the first window\", startOfTenSecondWindow.plus(1000)),\n+            TimestampedValue.of(\"Last line in the first window\", startOfTenSecondWindow.plus(2000)),\n+            TimestampedValue.of(\n+                \"This is another line, but in different window.\",\n+                startOfTenSecondWindow.plus(10000)),\n+            TimestampedValue.of(\n+                \"Last line, in the same window as previous line.\",\n+                startOfTenSecondWindow.plus(11000)))\n+        .advanceWatermarkToInfinity();\n+  }\n+}"
  },
  {
    "sha": "72e0844c3b60b59d5260c50360bc846747ea11c3",
    "filename": "copy-jars.sh",
    "status": "added",
    "additions": 7,
    "deletions": 0,
    "changes": 7,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/copy-jars.sh",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/copy-jars.sh",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/copy-jars.sh?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,7 @@\n+#!/bin/bash\n+\n+for t in $(find . -name target); do\n+  if [ -d $t ]; then\n+    cp $(find $t -name *.jar) /usr/local/lib/\n+  fi\n+done"
  },
  {
    "sha": "d9c50fd496c6e8edb69810b6df815e144dfea930",
    "filename": "docker/Dockerfile",
    "status": "added",
    "additions": 4,
    "deletions": 0,
    "changes": 4,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/docker/Dockerfile",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/docker/Dockerfile",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/docker/Dockerfile?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,4 @@\n+FROM packt-beam-base:latest\n+\n+ADD chapter*/target/*.jar /usr/local/lib/\n+"
  },
  {
    "sha": "2edfb55c93d6f5edf137da6f03d4008773abcecb",
    "filename": "docker/Dockerfile.base",
    "status": "added",
    "additions": 12,
    "deletions": 0,
    "changes": 12,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/docker/Dockerfile.base",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/docker/Dockerfile.base",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/docker/Dockerfile.base?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,12 @@\n+FROM openjdk:11\n+\n+ADD . /usr/src/beam-packt/\n+\n+RUN cd /usr/src/beam-packt/ \\\n+  && ./mvnw clean package -Pdocker\n+\n+RUN cd /usr/src/beam-packt/ \\\n+  && ./copy-jars.sh \\\n+  && cp bin/*.sh /usr/local/bin/\n+\n+ENTRYPOINT '/usr/local/bin/sleep-forever.sh'"
  },
  {
    "sha": "82dd2ae1cff0e578cdcc097e80a185b33bf06455",
    "filename": "env/build-docker-base.sh",
    "status": "added",
    "additions": 14,
    "deletions": 0,
    "changes": 14,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/env/build-docker-base.sh",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/env/build-docker-base.sh",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/env/build-docker-base.sh?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,14 @@\n+#!/bin/bash\n+\n+BASE=$(dirname $0)\n+ROOT=${BASE}/../\n+\n+set -e\n+if [[ $(minikube status -f='{{.Host}}') != \"Running\" ]]; then\n+  echo \"Please run minikube before running this script.\"\n+  exit 1\n+fi\n+\n+eval $(minikube docker-env)\n+\n+docker build -t packt-beam-base -f ${ROOT}/docker/Dockerfile.base ${ROOT}"
  },
  {
    "sha": "89db03653919cd7bb4d5793d3c2984a4b08ef0b8",
    "filename": "env/build-docker.sh",
    "status": "added",
    "additions": 25,
    "deletions": 0,
    "changes": 25,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/env/build-docker.sh",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/env/build-docker.sh",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/env/build-docker.sh?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,25 @@\n+#!/bin/bash\n+\n+BASE=$(dirname $0)\n+ROOT=${BASE}/../\n+\n+set -e\n+if [[ $(minikube status -f='{{.Host}}') != \"Running\" ]]; then\n+  echo \"Please run minikube before running this script.\"\n+  exit 1\n+fi\n+\n+eval $(minikube docker-env)\n+\n+if ! docker images | grep packt-beam-base > /dev/null; then\n+  ${BASE}/build-docker-base.sh\n+fi\n+\n+${ROOT}/mvnw package\n+\n+docker build -t packt-beam -f ${ROOT}/docker/Dockerfile ${ROOT}\n+\n+POD=$(kubectl get pods | grep packt-beam | cut -d' ' -f1)\n+if [[ ! -z $POD ]]; then\n+  kubectl delete pod $POD\n+fi"
  },
  {
    "sha": "0ac278fa82df653cbbbeb81e771328afbe3ccb18",
    "filename": "env/delay_input.sh",
    "status": "added",
    "additions": 8,
    "deletions": 0,
    "changes": 8,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/env/delay_input.sh",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/env/delay_input.sh",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/env/delay_input.sh?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,8 @@\n+#!/bin/bash\n+\n+IFS=$'\\n'\n+SLEEP=${1:-0.1}\n+for line in $(cat - | grep -v \"^ \\*$\"); do\n+  echo $line\n+  sleep $SLEEP\n+done"
  },
  {
    "sha": "fac5df85a67de9f276fa3f7a120fa4d466975697",
    "filename": "env/docker/kafka/Dockerfile",
    "status": "added",
    "additions": 24,
    "deletions": 0,
    "changes": 24,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/env/docker/kafka/Dockerfile",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/env/docker/kafka/Dockerfile",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/env/docker/kafka/Dockerfile?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,24 @@\n+FROM openjdk:11\n+ENV KAFKA_DATA_DIR=/var/lib/kafka/data \\\n+  KAFKA_HOME=/opt/kafka \\\n+  PATH=$PATH:/opt/kafka/bin\n+\n+ARG KAFKA_VERSION=2.5.1\n+ARG KAFKA_DIST=kafka_2.13-${KAFKA_VERSION}\n+RUN wget -q \"https://archive.apache.org/dist/kafka/$KAFKA_VERSION/$KAFKA_DIST.tgz\" \\\n+    && tar -xzf \"$KAFKA_DIST.tgz\" -C /opt \\\n+    && rm -r $KAFKA_DIST.tgz\n+\n+#COPY log4j.properties /opt/$KAFKA_DIST/config/\n+\n+RUN set -x \\\n+    && ln -s /opt/$KAFKA_DIST $KAFKA_HOME \\\n+    && mkdir -p $KAFKA_DATA_DIR \\\n+    && useradd kafka \\\n+    && chown kafka:kafka $KAFKA_DATA_DIR \\\n+    && chown kafka:kafka -R $KAFKA_HOME \\\n+    # change `log.dirs` settings\n+    && sed -i \"s/^log.dirs=.\\+/log.dirs=\\/var\\/lib\\/kafka\\/data\\//\" /opt/kafka/config/server.properties\n+\n+USER kafka\n+"
  },
  {
    "sha": "a5bd310daa7b20cc1c685f7154b7f2eb8d1192c2",
    "filename": "env/install-flink.sh",
    "status": "added",
    "additions": 9,
    "deletions": 0,
    "changes": 9,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/env/install-flink.sh",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/env/install-flink.sh",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/env/install-flink.sh?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,9 @@\n+#!/bin/bash\n+\n+BASE=$(dirname $0)\n+MANIFESTS=${BASE}/manifests\n+\n+set -e\n+\n+kubectl apply -f ${MANIFESTS}/flink.yaml\n+"
  },
  {
    "sha": "f72922474736b34ecdc18033d114d110e43abc6d",
    "filename": "env/install-kafka.sh",
    "status": "added",
    "additions": 18,
    "deletions": 0,
    "changes": 18,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/env/install-kafka.sh",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/env/install-kafka.sh",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/env/install-kafka.sh?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,18 @@\n+#!/bin/bash\n+\n+BASE=$(dirname $0)\n+DOCKER=${BASE}/docker\n+MANIFESTS=${BASE}/manifests\n+\n+set -e\n+\n+if [[ $(minikube status -f='{{.Host}}') != \"Running\" ]]; then\n+  echo \"Please run minikube before running this script.\"\n+  exit 1\n+fi\n+\n+eval $(minikube docker-env)\n+docker build -t kafka-packt ${DOCKER}/kafka\n+\n+kubectl apply -f ${MANIFESTS}/zookeeper_micro.yaml\n+kubectl apply -f ${MANIFESTS}/kafka_micro.yaml"
  },
  {
    "sha": "2cabff670fe31e85710b67312a512f3bf3cee097",
    "filename": "env/install-minikube.sh",
    "status": "added",
    "additions": 10,
    "deletions": 0,
    "changes": 10,
    "blob_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/blob/de1cc91fb7e69369fbb522161c47a5326e227dd3/env/install-minikube.sh",
    "raw_url": "https://github.com/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/raw/de1cc91fb7e69369fbb522161c47a5326e227dd3/env/install-minikube.sh",
    "contents_url": "https://api.github.com/repos/PacktPublishing/Building-Big-Data-Pipelines-with-Apache-Beam/contents/env/install-minikube.sh?ref=de1cc91fb7e69369fbb522161c47a5326e227dd3",
    "patch": "@@ -0,0 +1,10 @@\n+#!/bin/bash\n+\n+set -e\n+\n+DIR=$(pwd)\n+cd /tmp/\n+curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube_latest_amd64.deb\n+dpkg -i minikube_latest_amd64.deb\n+rm minikube_latest_amd64.deb\n+cd $DIR"
  }
]
