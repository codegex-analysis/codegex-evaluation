[
  {
    "sha": "f74ccfbf7dc271c8beb4acf120be70f54c3d9202",
    "filename": "solr/core/src/java/org/apache/solr/cloud/DistributedLock.java",
    "status": "added",
    "additions": 24,
    "deletions": 0,
    "changes": 24,
    "blob_url": "https://github.com/murblanc/lucene-solr/blob/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/DistributedLock.java",
    "raw_url": "https://github.com/murblanc/lucene-solr/raw/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/DistributedLock.java",
    "contents_url": "https://api.github.com/repos/murblanc/lucene-solr/contents/solr/core/src/java/org/apache/solr/cloud/DistributedLock.java?ref=d34680c2f671cf9f629a8837f3c658d362e708c6",
    "patch": "@@ -0,0 +1,24 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.solr.cloud;\n+\n+public interface DistributedLock {\n+  void waitUntilAcquired();\n+  void release();\n+  boolean isAcquired();\n+}"
  },
  {
    "sha": "78a312cf0819f3299dadce72f26151d7d89327d4",
    "filename": "solr/core/src/java/org/apache/solr/cloud/DistributedLockFactory.java",
    "status": "added",
    "additions": 53,
    "deletions": 0,
    "changes": 53,
    "blob_url": "https://github.com/murblanc/lucene-solr/blob/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/DistributedLockFactory.java",
    "raw_url": "https://github.com/murblanc/lucene-solr/raw/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/DistributedLockFactory.java",
    "contents_url": "https://api.github.com/repos/murblanc/lucene-solr/contents/solr/core/src/java/org/apache/solr/cloud/DistributedLockFactory.java?ref=d34680c2f671cf9f629a8837f3c658d362e708c6",
    "patch": "@@ -0,0 +1,53 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.solr.cloud;\n+\n+import org.apache.solr.cloud.api.collections.ApiLockFactory;\n+import org.apache.solr.common.params.CollectionParams;\n+\n+public interface DistributedLockFactory {\n+  /**\n+   * Create a new lock of the specified type (read or write) entering the \"competition\" for actually getting the lock at\n+   * the given level for the given path i.e. a lock at {@code collName} or a lock at {@code collName/shardId} or a lock\n+   * at {@code collName/shardId/replicaName}, depending on the passed {@code level}.<p>\n+   *\n+   * The paths are used to define which locks compete with each other (locks of equal paths compete).<p>\n+   *\n+   * Upon return from this call, the lock <b>has not been acquired</b> but the it had entered the lock aquiring \"competition\",\n+   * and the caller can decide to wait until the lock is granted by calling {@link DistributedLock#waitUntilAcquired()}.<br>\n+   * Separating the lock creation from lock acquisition allows a more deterministic release of the locks when/if they can't be\n+   * acquired.<p>\n+   *\n+   * Locks at different paths are independent of each other, multiple {@link DistributedLock} are therefore requested for\n+   * a single operation and are packaged together and returned as an {@link org.apache.solr.cloud.api.collections.ApiLockFactory.ApiLock},\n+   * see {@link ApiLockFactory#createCollectionApiLock}.\n+   * @param isWriteLock {@code true} if requesting a write lock, {@code false} for a read lock.\n+   * @param level The requested locking level. Can be one of:\n+   *              <ul><li>{@link org.apache.solr.common.params.CollectionParams.LockLevel#COLLECTION}</li>\n+   *              <li>{@link org.apache.solr.common.params.CollectionParams.LockLevel#SHARD}</li>\n+   *              <li>{@link org.apache.solr.common.params.CollectionParams.LockLevel#REPLICA}</li></ul>\n+   * @param collName the collection name, can never be {@code null} as is needed for all locks.\n+   * @param shardId is ignored and can be {@code null} if {@code level} is {@link org.apache.solr.common.params.CollectionParams.LockLevel#COLLECTION}\n+   * @param replicaName is ignored and can be {@code null} if {@code level} is {@link org.apache.solr.common.params.CollectionParams.LockLevel#COLLECTION}\n+   *                    or {@link org.apache.solr.common.params.CollectionParams.LockLevel#SHARD}\n+   * @return a lock instance that must be {@link DistributedLock#release()}'ed in a {@code finally},\n+   * regardless of the lock having been acquired or not.\n+   */\n+  DistributedLock createLock(boolean isWriteLock, CollectionParams.LockLevel level, String collName, String shardId,\n+                          String replicaName);\n+}"
  },
  {
    "sha": "a0ad0decd57e6534e3f349951df0cfa598a58b4f",
    "filename": "solr/core/src/java/org/apache/solr/cloud/OverseerNodePrioritizer.java",
    "status": "modified",
    "additions": 1,
    "deletions": 0,
    "changes": 1,
    "blob_url": "https://github.com/murblanc/lucene-solr/blob/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/OverseerNodePrioritizer.java",
    "raw_url": "https://github.com/murblanc/lucene-solr/raw/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/OverseerNodePrioritizer.java",
    "contents_url": "https://api.github.com/repos/murblanc/lucene-solr/contents/solr/core/src/java/org/apache/solr/cloud/OverseerNodePrioritizer.java?ref=d34680c2f671cf9f629a8837f3c658d362e708c6",
    "patch": "@@ -68,6 +68,7 @@ public synchronized void prioritizeOverseerNodes(String overseerId) throws Excep\n     if(overseerDesignates==null || overseerDesignates.isEmpty()) return;\n     String ldr = OverseerTaskProcessor.getLeaderNode(zk);\n     if(overseerDesignates.contains(ldr)) return;\n+    // note overseerId will have some default value when Collection API is distributed and not Overseer based\n     log.info(\"prioritizing overseer nodes at {} overseer designates are {}\", overseerId, overseerDesignates);\n     List<String> electionNodes = OverseerTaskProcessor.getSortedElectionNodes(zk, Overseer.OVERSEER_ELECT + LeaderElector.ELECTION_NODE);\n     if(electionNodes.size()<2) return;"
  },
  {
    "sha": "ed864da1f46d825f1e519bf66dbdffcb9ada6af2",
    "filename": "solr/core/src/java/org/apache/solr/cloud/ZkController.java",
    "status": "modified",
    "additions": 13,
    "deletions": 3,
    "changes": 16,
    "blob_url": "https://github.com/murblanc/lucene-solr/blob/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/ZkController.java",
    "raw_url": "https://github.com/murblanc/lucene-solr/raw/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/ZkController.java",
    "contents_url": "https://api.github.com/repos/murblanc/lucene-solr/contents/solr/core/src/java/org/apache/solr/cloud/ZkController.java?ref=d34680c2f671cf9f629a8837f3c658d362e708c6",
    "patch": "@@ -38,6 +38,7 @@\n import java.util.Locale;\n import java.util.Map;\n import java.util.Objects;\n+import java.util.Optional;\n import java.util.Set;\n import java.util.concurrent.Callable;\n import java.util.concurrent.ConcurrentHashMap;\n@@ -59,6 +60,7 @@\n import org.apache.solr.client.solrj.impl.SolrClientCloudManager;\n import org.apache.solr.client.solrj.impl.ZkClientClusterStateProvider;\n import org.apache.solr.client.solrj.request.CoreAdminRequest.WaitForState;\n+import org.apache.solr.cloud.api.collections.DistributedCollectionCommandRunner;\n import org.apache.solr.cloud.overseer.ClusterStateMutator;\n import org.apache.solr.cloud.overseer.OverseerAction;\n import org.apache.solr.cloud.overseer.SliceMutator;\n@@ -109,6 +111,7 @@\n import static org.apache.solr.common.cloud.ZkStateReader.NODE_NAME_PROP;\n import static org.apache.solr.common.cloud.ZkStateReader.REJOIN_AT_HEAD_PROP;\n import static org.apache.solr.common.cloud.ZkStateReader.SHARD_ID_PROP;\n+import static org.apache.solr.handler.admin.CollectionsHandler.DEFAULT_COLLECTION_OP_TIMEOUT;\n \n /**\n  * Handle ZooKeeper interactions.\n@@ -2245,7 +2248,7 @@ public void rejoinShardLeaderElection(SolrParams params) {\n     }\n   }\n \n-  public void checkOverseerDesignate() {\n+  public void checkOverseerDesignate(Optional<DistributedCollectionCommandRunner> distributedCollectionCommandRunner) {\n     try {\n       byte[] data = zkClient.getData(ZkStateReader.ROLES, null, new Stat(), true);\n       if (data == null) return;\n@@ -2256,11 +2259,18 @@ public void checkOverseerDesignate() {\n       List nodeList = (List) roles.get(\"overseer\");\n       if (nodeList == null) return;\n       if (nodeList.contains(getNodeName())) {\n-        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, CollectionParams.CollectionAction.ADDROLE.toString().toLowerCase(Locale.ROOT),\n+        final CollectionParams.CollectionAction addrole = CollectionParams.CollectionAction.ADDROLE;\n+        ZkNodeProps props = new ZkNodeProps(Overseer.QUEUE_OPERATION, addrole.toLower(),\n             \"node\", getNodeName(),\n             \"role\", \"overseer\");\n         log.info(\"Going to add role {} \", props);\n-        getOverseerCollectionQueue().offer(Utils.toJSON(props));\n+        if (distributedCollectionCommandRunner.isPresent()) {\n+          // We run in Distributed Collection API mode but we continue to support Overseer related activity for now until\n+          // we remove it for real. Some auxiliary processes use it still.\n+          distributedCollectionCommandRunner.get().runApiCommand(props, addrole, DEFAULT_COLLECTION_OP_TIMEOUT);\n+        } else {\n+          getOverseerCollectionQueue().offer(Utils.toJSON(props));\n+        }\n       }\n     } catch (NoNodeException nne) {\n       return;"
  },
  {
    "sha": "a5859e1ca0b11266db12c449572d7595acdbefb8",
    "filename": "solr/core/src/java/org/apache/solr/cloud/ZkDistributedLock.java",
    "status": "added",
    "additions": 228,
    "deletions": 0,
    "changes": 228,
    "blob_url": "https://github.com/murblanc/lucene-solr/blob/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/ZkDistributedLock.java",
    "raw_url": "https://github.com/murblanc/lucene-solr/raw/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/ZkDistributedLock.java",
    "contents_url": "https://api.github.com/repos/murblanc/lucene-solr/contents/solr/core/src/java/org/apache/solr/cloud/ZkDistributedLock.java?ref=d34680c2f671cf9f629a8837f3c658d362e708c6",
    "patch": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.solr.cloud;\n+\n+import java.util.List;\n+import java.util.concurrent.CountDownLatch;\n+\n+import org.apache.solr.common.SolrException;\n+import org.apache.solr.common.cloud.SolrZkClient;\n+import org.apache.zookeeper.CreateMode;\n+import org.apache.zookeeper.KeeperException;\n+import org.apache.zookeeper.WatchedEvent;\n+import org.apache.zookeeper.Watcher;\n+\n+import static org.apache.solr.common.SolrException.ErrorCode.SERVER_ERROR;\n+\n+abstract class ZkDistributedLock implements DistributedLock {\n+  /**\n+   * End of the lock node name prefix before the sequential part\n+   */\n+  static final char LOCK_PREFIX_SUFFIX = '_';\n+  /**\n+   * Prefix of EPHEMERAL read lock node names\n+   */\n+  static final String READ_LOCK_PREFIX = \"R\" + LOCK_PREFIX_SUFFIX;\n+  /**\n+   * Prefix of EPHEMERAL write lock node names\n+   */\n+  static final String WRITE_LOCK_PREFIX = \"W\" + LOCK_PREFIX_SUFFIX;\n+\n+  /**\n+   * Read lock.\n+   */\n+  static class Read extends ZkDistributedLock {\n+    protected Read(SolrZkClient zkClient, String lockPath) throws KeeperException, InterruptedException {\n+      super(zkClient, lockPath, READ_LOCK_PREFIX);\n+    }\n+\n+    @Override\n+    boolean isBlockedByNodeType(String otherLockName) {\n+      // A read lock is only blocked by a lower numbered write lock\n+      // Lower numbered read locks are ok, they can coexist.\n+      return otherLockName.startsWith(WRITE_LOCK_PREFIX);\n+    }\n+  }\n+\n+  /**\n+   * Write lock.\n+   */\n+  static class Write extends ZkDistributedLock {\n+    protected Write(SolrZkClient zkClient, String lockPath) throws KeeperException, InterruptedException {\n+      super(zkClient, lockPath, WRITE_LOCK_PREFIX);\n+    }\n+\n+    @Override\n+    boolean isBlockedByNodeType(String otherLockName) {\n+      // A write lock is blocked by another read or write lock with a lower sequence number\n+      return true;\n+    }\n+  }\n+\n+  private final SolrZkClient zkClient;\n+  private final String lockDir;\n+  private final String lockNode;\n+  protected final long sequence;\n+  protected volatile boolean released = false;\n+\n+  protected ZkDistributedLock(SolrZkClient zkClient, String lockDir, String lockNodePrefix) throws KeeperException, InterruptedException {\n+    this.zkClient = zkClient;\n+    this.lockDir = lockDir;\n+\n+    // Create the SEQUENTIAL EPHEMERAL node. We enter the locking rat race here. We MUST eventually call release() or we block others.\n+    lockNode = zkClient.create(lockDir + ZkDistributedLockFactory.ZK_PATH_SEPARATOR + lockNodePrefix, null,\n+        CreateMode.EPHEMERAL_SEQUENTIAL, false);\n+\n+    sequence = getSequenceFromNodename(lockNode);\n+  }\n+\n+  private static class DeletedNodeWatcher implements Watcher {\n+    final CountDownLatch latch;\n+    final String nodeBeingWatched;\n+    String errorMessage = null; // non null means error\n+\n+    DeletedNodeWatcher(String nodeBeingWatched) {\n+      this.latch = new CountDownLatch(1);\n+      this.nodeBeingWatched = nodeBeingWatched;\n+    }\n+\n+    @Override\n+    public void process(WatchedEvent event) {\n+      if (event.getType() == Event.EventType.None) {\n+        return;\n+      } else if (event.getType() != Event.EventType.NodeDeleted) {\n+        synchronized (this) {\n+          errorMessage = \"Received unexpected watch event \" + event.getType() + \" on \" + nodeBeingWatched;\n+        }\n+      }\n+      latch.countDown();\n+    }\n+\n+    void await() throws InterruptedException {\n+      latch.await();\n+      synchronized (this) {\n+        if (errorMessage != null) {\n+          throw new SolrException(SERVER_ERROR, errorMessage);\n+        }\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void waitUntilAcquired() {\n+    try {\n+      if (released) {\n+        throw new IllegalStateException(\"Bug. waitUntilAcquired() should not be called after release(). \" + lockNode);\n+      }\n+      String nodeToWatch = nodeToWatch();\n+      while (nodeToWatch != null) {\n+        final DeletedNodeWatcher watcher = new DeletedNodeWatcher(nodeToWatch);\n+        if (zkClient.exists(nodeToWatch, watcher, true) != null) {\n+          watcher.await();\n+        }\n+        nodeToWatch = nodeToWatch();\n+      }\n+    } catch (KeeperException e) {\n+      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n+    } catch (InterruptedException e) {\n+      Thread.currentThread().interrupt();\n+      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n+    }\n+  }\n+\n+  @Override\n+  public void release() {\n+    try {\n+      zkClient.delete(lockNode, -1, true);\n+      released = true;\n+    } catch (KeeperException e) {\n+      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n+    } catch (InterruptedException e) {\n+      Thread.currentThread().interrupt();\n+      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean isAcquired() {\n+    try {\n+      if (released) {\n+        throw new IllegalStateException(\"Bug. isAcquired() should not be called after release(). \" + lockNode);\n+      }\n+      return nodeToWatch() == null;\n+    } catch (KeeperException e) {\n+      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n+    } catch (InterruptedException e) {\n+      Thread.currentThread().interrupt();\n+      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n+    }\n+  }\n+\n+  /**\n+   * @return Another lock node (complete path) that must go away for us to acquire the lock, or {@code null} if the lock is ours.\n+   */\n+  String nodeToWatch() throws KeeperException, InterruptedException {\n+    List<String> locks = zkClient.getChildren(lockDir, null, true);\n+    boolean foundSelf = false; // For finding bugs or ZK bad behavior\n+    // We deviate from the ZK recipe here: we do not sort the list of nodes, and we stop waiting on the first one we find\n+    // that blocks us. This is done in O(n), whereas sorting is more expensive. And we iterate only once over the set of children.\n+    // Might cause more wakeups than needed though (multiple locks watching the same one rather than one another) but lock\n+    // contention expected low (how many concurrent and conflicting Collection API requests a reasonable user can issue?)\n+    // and the code below is simpler. Changing to a sorted \"optimal\" approach implies only changes in this method.\n+    for (String lock : locks) {\n+      long seq = getSequenceFromNodename(lock);\n+      if (seq == sequence) {\n+        foundSelf = true;\n+      } else if (seq < sequence && isBlockedByNodeType(lock)) {\n+        // Return the full path to the node to watch\n+        return lockDir + ZkDistributedLockFactory.ZK_PATH_SEPARATOR + lock;\n+      }\n+      // seq is bigger than sequence, can't block us. If the iteration was sorted we could avoid iterating over those.\n+    }\n+    if (!foundSelf) {\n+      // If this basic assumption doesn't hold with Zookeeper, we're in deep trouble. And not only here.\n+      throw new SolrException(SERVER_ERROR, \"Missing lock node \" + lockNode);\n+    }\n+\n+    // Didn't return early on any other blocking lock, means we own it\n+    return null;\n+  }\n+\n+  /**\n+   * @param otherLockName name of a competing lock node. Used to find out the type of the lock (read or write)\n+   * @return {@code true} if a lock node of the given type with a lower sequence number blocks us from acquiring the lock.\n+   */\n+  abstract boolean isBlockedByNodeType(String otherLockName);\n+\n+  static long getSequenceFromNodename(String lockNode) {\n+    // Javadoc of ZooKeeper.create() specifies \"The sequence number is always fixed length of 10 digits, 0 padded\"\n+    // for sequential nodes\n+    final int SEQUENCE_LENGTH = 10;\n+    // Before the sequence we have our specific char\n+    if (lockNode.charAt(lockNode.length() - SEQUENCE_LENGTH - 1) != LOCK_PREFIX_SUFFIX) {\n+      throw new SolrException(SERVER_ERROR, \"Unexpected lock node path \" + lockNode);\n+    }\n+\n+    return Long.decode(lockNode.substring(lockNode.length() - SEQUENCE_LENGTH));\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return lockNode;\n+  }\n+}"
  },
  {
    "sha": "7721a2e79e429c79a4cfe5f6f1edff1c2db9f300",
    "filename": "solr/core/src/java/org/apache/solr/cloud/ZkDistributedLockFactory.java",
    "status": "added",
    "additions": 150,
    "deletions": 0,
    "changes": 150,
    "blob_url": "https://github.com/murblanc/lucene-solr/blob/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/ZkDistributedLockFactory.java",
    "raw_url": "https://github.com/murblanc/lucene-solr/raw/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/ZkDistributedLockFactory.java",
    "contents_url": "https://api.github.com/repos/murblanc/lucene-solr/contents/solr/core/src/java/org/apache/solr/cloud/ZkDistributedLockFactory.java?ref=d34680c2f671cf9f629a8837f3c658d362e708c6",
    "patch": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.solr.cloud;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.solr.common.SolrException;\n+import org.apache.solr.common.cloud.SolrZkClient;\n+import org.apache.solr.common.cloud.ZkStateReader;\n+import org.apache.solr.common.params.CollectionParams;\n+import org.apache.zookeeper.CreateMode;\n+import org.apache.zookeeper.KeeperException;\n+\n+/**\n+ * A distributed lock implementation using Zookeeper \"directory\" nodes created within the collection znode hierarchy.\n+ * The locks are implemented using ephemeral nodes placed below the \"directory\" nodes.\n+ *\n+ * @see <a href=\"https://zookeeper.apache.org/doc/current/recipes.html#sc_recipes_Locks\">Zookeeper lock recipe</a>\n+ */\n+public class ZkDistributedLockFactory implements DistributedLockFactory {\n+\n+  static final String ZK_PATH_SEPARATOR = \"/\";\n+\n+  private final SolrZkClient zkClient;\n+\n+  public ZkDistributedLockFactory(SolrZkClient zkClient) {\n+    this.zkClient = zkClient;\n+  }\n+\n+\n+  public DistributedLock createLock(boolean isWriteLock, CollectionParams.LockLevel level, String collName, String shardId,\n+                                 String replicaName) {\n+    Preconditions.checkArgument(collName != null, \"collName can't be null\");\n+    Preconditions.checkArgument(level == CollectionParams.LockLevel.COLLECTION || shardId != null,\n+        \"shardId can't be null when getting lock for shard or replica\");\n+    Preconditions.checkArgument(level != CollectionParams.LockLevel.REPLICA || replicaName != null,\n+        \"replicaName can't be null when getting lock for replica\");\n+\n+    try {\n+      String lockPath = makeLockPath(level, collName, shardId, replicaName);\n+\n+      return isWriteLock ? new ZkDistributedLock.Write(zkClient, lockPath) : new ZkDistributedLock.Read(zkClient, lockPath);\n+    } catch (KeeperException e) {\n+      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n+    } catch (InterruptedException e) {\n+      Thread.currentThread().interrupt();\n+      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n+    }\n+  }\n+\n+  /**\n+   * Creates the Zookeeper path to the lock, creating missing nodes if needed, but NOT creating the collection node.\n+   * If the collection node does not exist, we don't want any locking operation to succeed.\n+   *\n+   * The tree of lock directories for a given collection {@code collName} is as follows:\n+   * <pre>\n+   *   /collections\n+   *      collName   <-- This node has to already exist and will NOT be created\n+   *         Locks   <-- EPHEMERAL collection level locks go here\n+   *         _ShardName1\n+   *            Locks   <-- EPHEMERAL shard level locks go here\n+   *            _replicaNameS1R1   <-- EPHEMERAL replica level locks go here\n+   *            _replicaNameS1R2   <-- EPHEMERAL replica level locks go here\n+   *         _ShardName2\n+   *            Locks   <-- EPHEMERAL shard level locks go here\n+   *            _replicaNameS2R1   <-- EPHEMERAL replica level locks go here\n+   *            _replicaNameS2R2   <-- EPHEMERAL replica level locks go here\n+   * </pre>\n+   * Depending on the requested lock level, this method will create the path (only the parts below {@code collName}\n+   * where the {@code EPHEMERAL} lock nodes should go, {@code /collections/collName} will have to already exist or\n+   * the call will fail). That path is:\n+   * <ul>\n+   *   <li>For {@link org.apache.solr.common.params.CollectionParams.LockLevel#COLLECTION} -\n+   *   {@code /collections/collName/Locks}</li>\n+   *   <li>For {@link org.apache.solr.common.params.CollectionParams.LockLevel#SHARD} -\n+   *   {@code /collections/collName/_shardName/Locks}</li>\n+   *   <li>For {@link org.apache.solr.common.params.CollectionParams.LockLevel#REPLICA} -\n+   *   {@code /collections/collName/_shardName/_replicaName}. There is no {@code Locks} subnode here because replicas do\n+   *   not have children so no need to separate {@code EPHEMERAL} lock nodes from children nodes as is the case for shards\n+   *   and collections</li>\n+   * </ul>\n+   * Note the {@code _} prefixing shards and replica names is to support shards or replicas called \"{@code Locks}\" (and\n+   * possibly, in a distant future, have other per shard files stored in Zookeeper, for example a shardState.json...).\n+   * Also note the returned path does not contain the separator ({@code \"/\"}) at the end.\n+   */\n+  private String makeLockPath(CollectionParams.LockLevel level, String collName, String shardId, String replicaName)\n+    throws KeeperException, InterruptedException {\n+    String collectionZnode = ZkStateReader.getCollectionPathRoot(collName);\n+\n+    final String LOCK_NODENAME = \"Locks\"; // Should not start with SUBNODE_PREFIX :)\n+    final String SUBNODE_PREFIX = \"_\";\n+\n+    final String[] pathElements;\n+    if (level == CollectionParams.LockLevel.COLLECTION) {\n+      pathElements = new String[]{ LOCK_NODENAME };\n+    } else if (level == CollectionParams.LockLevel.SHARD) {\n+      pathElements = new String[]{ SUBNODE_PREFIX + shardId, LOCK_NODENAME };\n+    } else if (level == CollectionParams.LockLevel.REPLICA) {\n+      pathElements = new String[]{ SUBNODE_PREFIX + shardId, SUBNODE_PREFIX + replicaName };\n+    } else {\n+      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unsupported lock level \" + level);\n+    }\n+\n+    // The complete path to the lock directory\n+    StringBuilder sb = new StringBuilder(collectionZnode);\n+    for (String pathElement : pathElements) {\n+      sb.append(ZK_PATH_SEPARATOR);\n+      sb.append(pathElement);\n+    }\n+    final String lockPath = sb.toString();\n+\n+    // If path already exists, bail early. This is the usual case once a given lock znode has been created once.\n+    // Otherwise walk the path and create missing bits. We can't call directly zkClient.makePath() because it would also\n+    // create the collectionZnode part of the path if it doesn't exist.\n+    if (!zkClient.exists(lockPath, true)) {\n+      // We want to create the missing elements in the path but NOT the collectionZnode part of the path\n+      StringBuilder walkThePath = new StringBuilder(collectionZnode);\n+      for (String pathElement : pathElements) {\n+        walkThePath.append(ZK_PATH_SEPARATOR);\n+        walkThePath.append(pathElement);\n+        try {\n+          // Create the next node in the path, but accept that it's already here (created previously or some other thread\n+          // beat us to it).\n+          // This call will throw a KeeperException with code() being KeeperException.Code.NONODE if the parent node does\n+          // not exist. In our case it's a missing /collections/collName node (or a rare/unlikely case of another thread deleting\n+          // the path as we create it)\n+          zkClient.create(walkThePath.toString(), null, CreateMode.PERSISTENT, true);\n+        } catch (KeeperException.NodeExistsException nee) {\n+          // If that element in the path already exists we're good, let's move to next.\n+        }\n+      }\n+    }\n+\n+    return lockPath;\n+  }\n+}"
  },
  {
    "sha": "d71a32e8ef45162a6672ae1fde608b601cbada28",
    "filename": "solr/core/src/java/org/apache/solr/cloud/api/collections/ApiLockFactory.java",
    "status": "added",
    "additions": 134,
    "deletions": 0,
    "changes": 134,
    "blob_url": "https://github.com/murblanc/lucene-solr/blob/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/api/collections/ApiLockFactory.java",
    "raw_url": "https://github.com/murblanc/lucene-solr/raw/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/api/collections/ApiLockFactory.java",
    "contents_url": "https://api.github.com/repos/murblanc/lucene-solr/contents/solr/core/src/java/org/apache/solr/cloud/api/collections/ApiLockFactory.java?ref=d34680c2f671cf9f629a8837f3c658d362e708c6",
    "patch": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.solr.cloud.api.collections;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.solr.cloud.DistributedLock;\n+import org.apache.solr.cloud.DistributedLockFactory;\n+import org.apache.solr.common.SolrException;\n+import org.apache.solr.common.params.CollectionParams;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * This class implements a higher level locking abstraction for the Collection API using lower level read and write locks.\n+ */\n+public class ApiLockFactory {\n+  /**\n+   * A lock as acquired for running a single Collection API command. Internally it is composed of multiple  {@link DistributedLock}'s.\n+   */\n+  static public class ApiLock {\n+    private static final Logger log = LoggerFactory.getLogger(MethodHandles.lookup().lookupClass());\n+    private final List<DistributedLock> locks;\n+    private volatile boolean isReleased = false;\n+\n+    private ApiLock(List<DistributedLock> locks) {\n+      this.locks = locks;\n+    }\n+\n+    void waitUntilAcquired() {\n+      if (isReleased) {\n+        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Released lock can't be waited upon\");\n+      }\n+\n+      for (DistributedLock lock : locks) {\n+        log.debug(\"ApiLock.waitUntilAcquired. About to wait on lock {}\", lock);\n+        lock.waitUntilAcquired();\n+        log.debug(\"ApiLock.waitUntilAcquired. Acquired lock {}\", lock);\n+      }\n+    }\n+\n+    void release() {\n+      isReleased = true;\n+      for (DistributedLock lock : locks) {\n+        lock.release();\n+      }\n+    }\n+\n+    boolean isAcquired() {\n+      if (isReleased) {\n+        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Released lock can't be tested\");\n+      }\n+      for (DistributedLock lock : locks) {\n+        if (!lock.isAcquired()) {\n+          return false;\n+        }\n+      }\n+      return true;\n+    }\n+\n+    @VisibleForTesting\n+    int getCountInternalLocks() {\n+      return locks.size();\n+    }\n+  }\n+\n+  private final DistributedLockFactory lockFactory;\n+\n+  ApiLockFactory(DistributedLockFactory lockFactory) {\n+    this.lockFactory = lockFactory;\n+  }\n+\n+  /**\n+   * For the {@link org.apache.solr.common.params.CollectionParams.LockLevel} of the passed {@code action}, obtains the\n+   * required locks (if any) and returns.<p>\n+   *\n+   * This method obtains a write lock at the actual level and path of the action, and also obtains read locks on \"lower\"\n+   * lock levels. For example for a lock at the shard level, a write lock will be requested at the corresponding shard path\n+   * and a read lock on the corresponding collection path (in order to prevent an operation locking at the collection level\n+   * from executing concurrently with an operation on one of the shards of the collection).\n+   * See documentation linked to SOLR-14840 regarding Collection API locking.\n+   *\n+   * @return a lock that once {@link ApiLock#isAcquired()} guarantees the corresponding Collection\n+   * API command can execute safely.\n+   * The returned lock <b>MUST</b> be {@link ApiLock#release()} no matter what once no longer needed as otherwise it would\n+   * prevent other threads from locking.\n+   */\n+  ApiLock createCollectionApiLock(CollectionParams.LockLevel lockLevel, String collName, String shardId, String replicaName) {\n+    if (lockLevel == CollectionParams.LockLevel.NONE) {\n+      return new ApiLock(List.of());\n+    }\n+\n+    if (lockLevel == CollectionParams.LockLevel.CLUSTER) {\n+      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Bug. Not expecting locking at cluster level.\");\n+    }\n+\n+    if (collName == null) {\n+      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Bug. collName can't be null\");\n+    }\n+\n+    // The first requested lock is a write one (on the target object for the action, depending on lock level), then requesting\n+    // read locks on \"higher\" levels (collection > shard > replica here for the level. Note LockLevel \"height\" is other way around).\n+    boolean requestWriteLock = true;\n+    final CollectionParams.LockLevel[] iterationOrder = { CollectionParams.LockLevel.REPLICA, CollectionParams.LockLevel.SHARD, CollectionParams.LockLevel.COLLECTION };\n+    List<DistributedLock> locks = new ArrayList<>(iterationOrder.length);\n+    for (CollectionParams.LockLevel level : iterationOrder) {\n+      // This comparison is based on the LockLevel height value that classifies replica > shard > collection.\n+      if (lockLevel.isHigherOrEqual(level)) {\n+        locks.add(lockFactory.createLock(requestWriteLock, level, collName, shardId, replicaName));\n+        requestWriteLock = false;\n+      }\n+    }\n+\n+    return new ApiLock(locks);\n+  }\n+}"
  },
  {
    "sha": "201f4509ce9e2a28aa5b6424e114507b085b4968",
    "filename": "solr/core/src/java/org/apache/solr/cloud/api/collections/CollApiCmds.java",
    "status": "modified",
    "additions": 78,
    "deletions": 6,
    "changes": 84,
    "blob_url": "https://github.com/murblanc/lucene-solr/blob/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/api/collections/CollApiCmds.java",
    "raw_url": "https://github.com/murblanc/lucene-solr/raw/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/api/collections/CollApiCmds.java",
    "contents_url": "https://api.github.com/repos/murblanc/lucene-solr/contents/solr/core/src/java/org/apache/solr/cloud/api/collections/CollApiCmds.java?ref=d34680c2f671cf9f629a8837f3c658d362e708c6",
    "patch": "@@ -24,9 +24,11 @@\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.TimeoutException;\n \n+import com.google.common.collect.ImmutableMap;\n import org.apache.commons.lang3.StringUtils;\n import org.apache.solr.cloud.DistributedClusterStateUpdater;\n import org.apache.solr.cloud.Overseer;\n+import org.apache.solr.cloud.OverseerNodePrioritizer;\n import org.apache.solr.common.SolrException;\n import org.apache.solr.common.cloud.ClusterState;\n import org.apache.solr.common.cloud.Replica;\n@@ -35,6 +37,7 @@\n import org.apache.solr.common.cloud.ZkNodeProps;\n import org.apache.solr.common.cloud.ZkStateReader;\n import org.apache.solr.common.params.CollectionAdminParams;\n+import org.apache.solr.common.params.CollectionParams;\n import org.apache.solr.common.params.CoreAdminParams;\n import org.apache.solr.common.params.ModifiableSolrParams;\n import org.apache.solr.common.util.NamedList;\n@@ -55,16 +58,16 @@\n import static org.apache.solr.common.cloud.ZkStateReader.REJOIN_AT_HEAD_PROP;\n import static org.apache.solr.common.cloud.ZkStateReader.REPLICA_PROP;\n import static org.apache.solr.common.cloud.ZkStateReader.SHARD_ID_PROP;\n-import static org.apache.solr.common.params.CollectionParams.CollectionAction.ADDREPLICAPROP;\n-import static org.apache.solr.common.params.CollectionParams.CollectionAction.BALANCESHARDUNIQUE;\n-import static org.apache.solr.common.params.CollectionParams.CollectionAction.DELETEREPLICAPROP;\n+import static org.apache.solr.common.params.CollectionParams.CollectionAction.*;\n import static org.apache.solr.common.params.CommonAdminParams.ASYNC;\n import static org.apache.solr.common.params.CommonParams.NAME;\n \n+\n /**\n- * This class contains \"smaller\" Collection API commands implementation as well as the interface implemented by all commands.\n- * Previously these implementations in {@link OverseerCollectionMessageHandler} were relying on methods implementing the\n- * functional interface.\n+ * This class contains \"smaller\" Collection API commands implementation, the interface implemented by all commands and the\n+ * class mapping a collection action to the actual command.\n+ * Previously the \"smaller\" command implementations in {@link OverseerCollectionMessageHandler} were relying on methods\n+ * implementing the functional interface.\n  */\n public class CollApiCmds {\n   private static final Logger log = LoggerFactory.getLogger(MethodHandles.lookup().lookupClass());\n@@ -76,6 +79,75 @@\n     void call(ClusterState state, ZkNodeProps message, @SuppressWarnings({\"rawtypes\"}) NamedList results) throws Exception;\n   }\n \n+  /**\n+   * Map {@link org.apache.solr.common.params.CollectionParams.CollectionAction} to instances of {@link CollApiCmds.CollectionApiCommand} and\n+   * being usable by both {@link OverseerCollectionMessageHandler} and {@link DistributedCollectionCommandRunner} so that\n+   * the mappings do not have to be maintained in two places.\n+   */\n+  protected static class CommandMap {\n+    final private Map<CollectionParams.CollectionAction, CollApiCmds.CollectionApiCommand> commandMap;\n+\n+    /**\n+     * Constructor used when Collection API is run on the Overseer. Called by {@link OverseerCollectionMessageHandler}\n+     */\n+    CommandMap(CollectionCommandContext ccc, OverseerNodePrioritizer overseerPrioritizer) {\n+      this(overseerPrioritizer, ccc);\n+      assert !ccc.isDistributedCollectionAPI();\n+    }\n+\n+    /**\n+     * Constructor used when Collection API execution is distributed\n+     */\n+    CommandMap(CollectionCommandContext ccc) {\n+      // Overseer prioritizer will not be used. Running in distributed Collection API mode\n+      this(null, ccc);\n+      assert ccc.isDistributedCollectionAPI();\n+    }\n+\n+    private CommandMap(OverseerNodePrioritizer overseerPrioritizer, CollectionCommandContext ccc) {\n+      commandMap = new ImmutableMap.Builder<CollectionParams.CollectionAction, CollApiCmds.CollectionApiCommand>()\n+          .put(REPLACENODE, new ReplaceNodeCmd(ccc))\n+          .put(DELETENODE, new DeleteNodeCmd(ccc))\n+          .put(BACKUP, new BackupCmd(ccc))\n+          .put(RESTORE, new RestoreCmd(ccc))\n+          .put(DELETEBACKUP, new DeleteBackupCmd(ccc))\n+          .put(CREATESNAPSHOT, new CreateSnapshotCmd(ccc))\n+          .put(DELETESNAPSHOT, new DeleteSnapshotCmd(ccc))\n+          .put(SPLITSHARD, new SplitShardCmd(ccc))\n+          .put(ADDROLE, new OverseerRoleCmd(ccc, ADDROLE, overseerPrioritizer))\n+          .put(REMOVEROLE, new OverseerRoleCmd(ccc, REMOVEROLE, overseerPrioritizer))\n+          .put(MOCK_COLL_TASK, new CollApiCmds.MockOperationCmd())\n+          .put(MOCK_SHARD_TASK, new CollApiCmds.MockOperationCmd())\n+          .put(MOCK_REPLICA_TASK, new CollApiCmds.MockOperationCmd())\n+          .put(CREATESHARD, new CreateShardCmd(ccc))\n+          .put(MIGRATE, new MigrateCmd(ccc))\n+          .put(CREATE, new CreateCollectionCmd(ccc))\n+          .put(MODIFYCOLLECTION, new CollApiCmds.ModifyCollectionCmd(ccc))\n+          .put(ADDREPLICAPROP, new CollApiCmds.AddReplicaPropCmd(ccc))\n+          .put(DELETEREPLICAPROP, new CollApiCmds.DeleteReplicaPropCmd(ccc))\n+          .put(BALANCESHARDUNIQUE, new CollApiCmds.BalanceShardsUniqueCmd(ccc))\n+          .put(REBALANCELEADERS, new CollApiCmds.RebalanceLeadersCmd(ccc))\n+          .put(RELOAD, new CollApiCmds.ReloadCollectionCmd(ccc))\n+          .put(DELETE, new DeleteCollectionCmd(ccc))\n+          .put(CREATEALIAS, new CreateAliasCmd(ccc))\n+          .put(DELETEALIAS, new DeleteAliasCmd(ccc))\n+          .put(ALIASPROP, new SetAliasPropCmd(ccc))\n+          .put(MAINTAINROUTEDALIAS, new MaintainRoutedAliasCmd(ccc))\n+          .put(OVERSEERSTATUS, new OverseerStatusCmd(ccc))\n+          .put(DELETESHARD, new DeleteShardCmd(ccc))\n+          .put(DELETEREPLICA, new DeleteReplicaCmd(ccc))\n+          .put(ADDREPLICA, new AddReplicaCmd(ccc))\n+          .put(MOVEREPLICA, new MoveReplicaCmd(ccc))\n+          .put(REINDEXCOLLECTION, new ReindexCollectionCmd(ccc))\n+          .put(RENAME, new RenameCmd(ccc))\n+          .build();\n+    }\n+\n+    CollApiCmds.CollectionApiCommand getActionCommand(CollectionParams.CollectionAction action) {\n+      return commandMap.get(action);\n+    }\n+  }\n+\n   static public class MockOperationCmd implements CollectionApiCommand {\n     @SuppressForbidden(reason = \"Needs currentTimeMillis for mock requests\")\n     @SuppressWarnings({\"unchecked\"})"
  },
  {
    "sha": "95d7b6907081b8f1ed8ef39c96c9a4e32d8e3084",
    "filename": "solr/core/src/java/org/apache/solr/cloud/api/collections/CollectionCommandContext.java",
    "status": "modified",
    "additions": 9,
    "deletions": 3,
    "changes": 12,
    "blob_url": "https://github.com/murblanc/lucene-solr/blob/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/api/collections/CollectionCommandContext.java",
    "raw_url": "https://github.com/murblanc/lucene-solr/raw/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/api/collections/CollectionCommandContext.java",
    "contents_url": "https://api.github.com/repos/murblanc/lucene-solr/contents/solr/core/src/java/org/apache/solr/cloud/api/collections/CollectionCommandContext.java?ref=d34680c2f671cf9f629a8837f3c658d362e708c6",
    "patch": "@@ -37,6 +37,11 @@\n  * distributed way, unrelated to and not depending upon Overseer abstractions (including overseer collection message handling).\n  */\n public interface CollectionCommandContext {\n+  /**\n+   * When this method returns {@code true}, Overseer specific actions do not make sense and commands should not be doing them.\n+   */\n+  boolean isDistributedCollectionAPI();\n+\n   ShardHandler getShardHandler();\n \n   SolrCloudManager getSolrCloudManager();\n@@ -61,11 +66,12 @@ default String getAdminPath() {\n   ExecutorService getExecutorService();\n \n   /**\n-   * This method enables the commands to enqueue to the overseer cluster state update. This should only be used when the command\n-   * is running in the Overseer (and will throw an exception if called when Collection API is distributed)\n+   * This method enables the commands to enqueue to the overseer cluster state update. This should only be used when the\n+   * cluster state update is running in the Overseer (and will throw an exception if called when Collection API is distributed\n+   * given that distributed Collection API implies distributed Cluster State Update)\n    */\n   default void offerStateUpdate(byte[] data) throws KeeperException, InterruptedException {\n-    throw new IllegalStateException(\"Bug! offerStateUpdate() should not be called when distributed state updates are enabled\");\n+    throw new IllegalStateException(\"Bug! offerStateUpdate() should not be called when distributed cluster state updates are enabled\");\n   }\n \n   default String getOverseerId() {"
  },
  {
    "sha": "e87c5937b13295b7a981260c8f7cb347e6e953ff",
    "filename": "solr/core/src/java/org/apache/solr/cloud/api/collections/CreateCollectionCmd.java",
    "status": "modified",
    "additions": 2,
    "deletions": 0,
    "changes": 2,
    "blob_url": "https://github.com/murblanc/lucene-solr/blob/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/api/collections/CreateCollectionCmd.java",
    "raw_url": "https://github.com/murblanc/lucene-solr/raw/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/api/collections/CreateCollectionCmd.java",
    "contents_url": "https://api.github.com/repos/murblanc/lucene-solr/contents/solr/core/src/java/org/apache/solr/cloud/api/collections/CreateCollectionCmd.java?ref=d34680c2f671cf9f629a8837f3c658d362e708c6",
    "patch": "@@ -170,6 +170,8 @@ public void call(ClusterState clusterState, ZkNodeProps message, @SuppressWarnin\n         // When cluster state updates are handled by Overseer, ask it to load that collection it doesn't know about.\n         // When cluster state updates are distributed, ZK is the source of truth for all nodes so no reload needed.\n         if (!ccc.getDistributedClusterStateUpdater().isDistributedStateUpdate()) {\n+          // If cluster state update is not distributed and we execute here, the Collection API is not distributed either\n+          // and this execution happens on the Overseer node, so direct memory access as done below is ok.\n           ccc.submitIntraProcessMessage(new RefreshCollectionMessage(collectionName));\n         }\n       } else {"
  },
  {
    "sha": "aaaa9a7268433d3cc97dc8707ac7ea8906608a14",
    "filename": "solr/core/src/java/org/apache/solr/cloud/api/collections/DistributedCollectionCommandContext.java",
    "status": "added",
    "additions": 86,
    "deletions": 0,
    "changes": 86,
    "blob_url": "https://github.com/murblanc/lucene-solr/blob/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/api/collections/DistributedCollectionCommandContext.java",
    "raw_url": "https://github.com/murblanc/lucene-solr/raw/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/api/collections/DistributedCollectionCommandContext.java",
    "contents_url": "https://api.github.com/repos/murblanc/lucene-solr/contents/solr/core/src/java/org/apache/solr/cloud/api/collections/DistributedCollectionCommandContext.java?ref=d34680c2f671cf9f629a8837f3c658d362e708c6",
    "patch": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.solr.cloud.api.collections;\n+\n+import java.util.concurrent.ExecutorService;\n+\n+import org.apache.solr.client.solrj.cloud.SolrCloudManager;\n+import org.apache.solr.cloud.DistributedClusterStateUpdater;\n+import org.apache.solr.common.SolrCloseable;\n+import org.apache.solr.common.cloud.ZkStateReader;\n+import org.apache.solr.core.CoreContainer;\n+import org.apache.solr.handler.component.ShardHandler;\n+\n+public class DistributedCollectionCommandContext implements CollectionCommandContext {\n+  private final ShardHandler shardHandler;\n+  private final SolrCloudManager solrCloudManager;\n+  private final CoreContainer coreContainer;\n+  private final ZkStateReader zkStateReader;\n+  private final DistributedClusterStateUpdater getDistributedClusterStateUpdater;\n+  private final ExecutorService executorService;\n+\n+  public DistributedCollectionCommandContext(CoreContainer coreContainer, ExecutorService executorService) {\n+    this.shardHandler = coreContainer.getShardHandlerFactory().getShardHandler();;\n+    this.solrCloudManager = coreContainer.getZkController().getSolrCloudManager();\n+    this.coreContainer = coreContainer;\n+    this.zkStateReader = coreContainer.getZkController().getZkStateReader();\n+    this.getDistributedClusterStateUpdater = new DistributedClusterStateUpdater(coreContainer.getConfig().getCloudConfig().getDistributedClusterStateUpdates());;\n+    this.executorService = executorService;\n+  }\n+\n+  @Override\n+  public boolean isDistributedCollectionAPI() {\n+    return true;\n+  }\n+\n+  @Override\n+  public ShardHandler getShardHandler() {\n+    return this.shardHandler;\n+  }\n+\n+  @Override\n+  public SolrCloudManager getSolrCloudManager() {\n+    return this.solrCloudManager;\n+  }\n+\n+  @Override\n+  public CoreContainer getCoreContainer() {\n+    return this.coreContainer;\n+  }\n+\n+  @Override\n+  public ZkStateReader getZkStateReader() {\n+    return this.zkStateReader;\n+  }\n+\n+  @Override\n+  public DistributedClusterStateUpdater getDistributedClusterStateUpdater() {\n+    return this.getDistributedClusterStateUpdater;\n+  }\n+\n+  @Override\n+  public SolrCloseable getCloseableToLatchOn() {\n+    // Debatable: SolrCloudManager instance is tracked for closing to interrupt some command execution. Could very well monitor something else.\n+    return this.solrCloudManager;\n+  }\n+\n+  @Override\n+  public ExecutorService getExecutorService() {\n+    return this.executorService;\n+  }\n+}"
  },
  {
    "sha": "4928e94eb20e8f472c481c831a17d256a1f2ccc2",
    "filename": "solr/core/src/java/org/apache/solr/cloud/api/collections/DistributedCollectionCommandRunner.java",
    "status": "added",
    "additions": 185,
    "deletions": 0,
    "changes": 185,
    "blob_url": "https://github.com/murblanc/lucene-solr/blob/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/api/collections/DistributedCollectionCommandRunner.java",
    "raw_url": "https://github.com/murblanc/lucene-solr/raw/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/api/collections/DistributedCollectionCommandRunner.java",
    "contents_url": "https://api.github.com/repos/murblanc/lucene-solr/contents/solr/core/src/java/org/apache/solr/cloud/api/collections/DistributedCollectionCommandRunner.java?ref=d34680c2f671cf9f629a8837f3c658d362e708c6",
    "patch": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.solr.cloud.api.collections;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.SynchronousQueue;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.solr.cloud.OverseerSolrResponse;\n+import org.apache.solr.cloud.ZkDistributedLockFactory;\n+import org.apache.solr.common.SolrException;\n+import org.apache.solr.common.cloud.ZkNodeProps;\n+import org.apache.solr.common.params.CollectionParams;\n+import org.apache.solr.common.util.ExecutorUtil;\n+import org.apache.solr.common.util.NamedList;\n+import org.apache.solr.common.util.SimpleOrderedMap;\n+import org.apache.solr.common.util.SolrNamedThreadFactory;\n+import org.apache.solr.core.CoreContainer;\n+import org.apache.solr.logging.MDCLoggingContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.solr.common.cloud.ZkStateReader.COLLECTION_PROP;\n+import static org.apache.solr.common.cloud.ZkStateReader.REPLICA_PROP;\n+import static org.apache.solr.common.cloud.ZkStateReader.SHARD_ID_PROP;\n+import static org.apache.solr.common.params.CommonParams.NAME;\n+\n+/**\n+ * Class for execution Collection API commands in a distributed way, without going through Overseer and\n+ * {@link OverseerCollectionMessageHandler}.<p>\n+ *\n+ * This class is only called when Collection API calls are configured to be distributed, which implies cluster state\n+ * updates are distributed as well.\n+ */\n+public class DistributedCollectionCommandRunner {\n+  private static final Logger log = LoggerFactory.getLogger(MethodHandles.lookup().lookupClass());\n+\n+  private final ExecutorService distributedCollectionApiExecutorService;\n+  private final CoreContainer coreContainer;\n+  final private CollApiCmds.CommandMap commandMapper;\n+  private final CollectionCommandContext ccc;\n+  private final ApiLockFactory apiLockingFactory;\n+\n+  private volatile boolean shuttingDown = false;\n+\n+  public DistributedCollectionCommandRunner(CoreContainer coreContainer) {\n+    this.coreContainer = coreContainer;\n+\n+    // TODO we should look at how everything is getting closed when the node is shutdown.\n+    //  But it seems that CollectionsHandler (that creates instances of this class) is not really closed, so maybe it doesn't matter?\n+    // With distributed Collection API execution, each node will have such an executor but given how thread pools work,\n+    // threads will only be created if needed (including the corePoolSize threads).\n+    distributedCollectionApiExecutorService = new ExecutorUtil.MDCAwareThreadPoolExecutor(5, 10, 0L, TimeUnit.MILLISECONDS,\n+        new SynchronousQueue<>(),\n+        new SolrNamedThreadFactory(\"DistributedCollectionCommandRunnerThreadFactory\"));\n+\n+    ccc = new DistributedCollectionCommandContext(this.coreContainer, this.distributedCollectionApiExecutorService);\n+    commandMapper = new CollApiCmds.CommandMap(ccc);\n+\n+    apiLockingFactory = new ApiLockFactory(new ZkDistributedLockFactory(ccc.getZkStateReader().getZkClient()));\n+  }\n+\n+  /**\n+   * When {@link org.apache.solr.handler.admin.CollectionsHandler#invokeAction} does not enqueue to overseer queue and\n+   * instead calls this method, this method is expected to do the equivalent of what Overseer does in\n+   * {@link org.apache.solr.cloud.api.collections.OverseerCollectionMessageHandler#processMessage}.\n+   * <p>\n+   * The steps leading to that call in the Overseer execution path are (and the equivalent is done here):\n+   * <ul>\n+   * <li>{@link org.apache.solr.cloud.OverseerTaskProcessor#run()} gets the message from the ZK queue, grabs the\n+   * corresponding lock (Collection API calls do locking to prevent non compatible concurrent modifications of a collection),\n+   * marks the async id of the task as running then executes the command using an executor service</li>\n+   * <li>In {@link org.apache.solr.cloud.OverseerTaskProcessor}.{@code Runner.run()} (run on an executor thread) a call is made to\n+   * {@link org.apache.solr.cloud.api.collections.OverseerCollectionMessageHandler#processMessage} which sets the logging\n+   * context, calls {@link CollApiCmds.CollectionApiCommand#call}\n+   * TODO and anything else?</li>\n+   * </ul>\n+   */\n+  @SuppressWarnings(\"unchecked\")\n+  public OverseerSolrResponse runApiCommand(ZkNodeProps message, CollectionParams.CollectionAction action, long timeoutMs) {\n+    // We refuse new tasks, but will wait for already submitted ones (i.e. those that made it through this method earlier).\n+    // See stopAndWaitForPendingTasksToComplete() below\n+    if (shuttingDown) {\n+      throw new SolrException(SolrException.ErrorCode.CONFLICT, \"Solr is shutting down, no more Collection API tasks may be executed\");\n+    }\n+\n+    final String collName = getCollectionName(message);\n+    final String shardId = message.getStr(SHARD_ID_PROP);\n+    final String replicaName = message.getStr(REPLICA_PROP);\n+\n+    // TODO Execute on another thread ALWAYS and have this thread wait for at most timeoutMs if call is not async.\n+\n+    MDCLoggingContext.setCollection(collName);\n+    MDCLoggingContext.setShard(shardId);\n+    MDCLoggingContext.setReplica(replicaName);\n+\n+    // Create API lock for executing the command. This call is non blocking (not blocked on waiting for a lock to be acquired anyway,\n+    // might be blocked on access to ZK etc)\n+    ApiLockFactory.ApiLock lock = apiLockingFactory.createCollectionApiLock(action.lockLevel, collName, shardId, replicaName);\n+\n+    log.debug(\"DistributedCollectionCommandRunner.runApiCommand. About to acquire lock for action {} lock level {}. {}/{}/{}\",\n+        action, action.lockLevel, collName, shardId, replicaName);\n+\n+    // Block this thread until all required locks are acquired.\n+    // This thread is either a CollectionsHandler thread and a client is waiting for a response, or this thread comes from a pool (WHERE?) and is running an async task.\n+    lock.waitUntilAcquired();\n+\n+    // TODO handle async tasks... Before and after command call\n+    // Note async tasks will be executed in the order enqueued on a node (no guarantees for enqueues on different nodes)\n+    // But async vs non async task, no guarantees either. The non async task might get executed first. So when combining both\n+    // should document that need to wait for completion first (if order matters)\n+\n+    log.debug(\"DistributedCollectionCommandRunner.runApiCommand. Lock acquired. Calling: {}, {}\", action, message);\n+\n+    @SuppressWarnings({\"rawtypes\"})\n+    NamedList results = new NamedList();\n+    try {\n+      try {\n+        CollApiCmds.CollectionApiCommand command = commandMapper.getActionCommand(action);\n+        if (command != null) {\n+          command.call(ccc.getSolrCloudManager().getClusterStateProvider().getClusterState(), message, results);\n+        } else {\n+          // Seeing this is a bug, not bad user data\n+          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Unknown operation: \" + action);\n+        }\n+      } finally {\n+        lock.release();\n+      }\n+    } catch (Exception e) {\n+      if (e instanceof InterruptedException) {\n+        Thread.currentThread().interrupt();\n+      }\n+      // Output some error logs\n+      if (collName == null) {\n+        SolrException.log(log, \"Operation \" + action + \" failed\", e);\n+      } else  {\n+        SolrException.log(log, \"Collection \" + collName + \", operation \" + action + \" failed\", e);\n+      }\n+\n+      results.add(\"Operation \" + action + \" caused exception:\", e);\n+      SimpleOrderedMap<Object> nl = new SimpleOrderedMap<>();\n+      nl.add(\"msg\", e.getMessage());\n+      nl.add(\"rspCode\", e instanceof SolrException ? ((SolrException)e).code() : -1);\n+      results.add(\"exception\", nl);\n+    }\n+    return new OverseerSolrResponse(results);\n+  }\n+\n+  public void stopAndWaitForPendingTasksToComplete() {\n+    shuttingDown = true;\n+\n+    // TODO Wait (for a while?) for already submitted tasks to complete, or to time out.\n+    // We likely do not want to wait for async tasks to complete, and given this is called when the node is being shut down,\n+    // it means the async tasks will be considered failed and client would have to deal with it. Client must be prepared for\n+    // such an outcome. With Overseer based Collection API execution, this can happen if Overseer is being shut down.\n+\n+    // TODO Do it!\n+\n+  }\n+\n+  /**\n+   * Collection name can be found in either of two message parameters (why??). Return it from where it's defined.\n+   * (see also parameter {@code collectionNameParamName} of {@link org.apache.solr.cloud.DistributedClusterStateUpdater.MutatingCommand#MutatingCommand(CollectionParams.CollectionAction, String)})\n+   */\n+  public static String getCollectionName(ZkNodeProps message) {\n+    return message.containsKey(COLLECTION_PROP) ?\n+        message.getStr(COLLECTION_PROP) : message.getStr(NAME);\n+  }\n+}"
  },
  {
    "sha": "0df599a6cf04244bb3d5fc772abd75e5f76a56ab",
    "filename": "solr/core/src/java/org/apache/solr/cloud/api/collections/MaintainRoutedAliasCmd.java",
    "status": "modified",
    "additions": 3,
    "deletions": 3,
    "changes": 6,
    "blob_url": "https://github.com/murblanc/lucene-solr/blob/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/api/collections/MaintainRoutedAliasCmd.java",
    "raw_url": "https://github.com/murblanc/lucene-solr/raw/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/api/collections/MaintainRoutedAliasCmd.java",
    "contents_url": "https://api.github.com/repos/murblanc/lucene-solr/contents/solr/core/src/java/org/apache/solr/cloud/api/collections/MaintainRoutedAliasCmd.java?ref=d34680c2f671cf9f629a8837f3c658d362e708c6",
    "patch": "@@ -58,12 +58,12 @@\n    */\n   static void remoteInvoke(CollectionsHandler collHandler, String aliasName, String targetCol)\n       throws Exception {\n-    final String operation = CollectionParams.CollectionAction.MAINTAINROUTEDALIAS.toLower();\n+    final CollectionParams.CollectionAction maintainroutedalias = CollectionParams.CollectionAction.MAINTAINROUTEDALIAS;\n     Map<String, Object> msg = new HashMap<>();\n-    msg.put(Overseer.QUEUE_OPERATION, operation);\n+    msg.put(Overseer.QUEUE_OPERATION, maintainroutedalias.toLower());\n     msg.put(CollectionParams.NAME, aliasName);\n     msg.put(MaintainRoutedAliasCmd.ROUTED_ALIAS_TARGET_COL, targetCol);\n-    final SolrResponse rsp = collHandler.sendToOCPQueue(new ZkNodeProps(msg));\n+    final SolrResponse rsp = collHandler.submitCollectionApiCommand(new ZkNodeProps(msg), maintainroutedalias);\n     if (rsp.getException() != null) {\n       throw rsp.getException();\n     }"
  },
  {
    "sha": "18f60fb2595c3452d620abd538c307459d2e4cdd",
    "filename": "solr/core/src/java/org/apache/solr/cloud/api/collections/OcmhCollectionCommandContext.java",
    "status": "modified",
    "additions": 5,
    "deletions": 0,
    "changes": 5,
    "blob_url": "https://github.com/murblanc/lucene-solr/blob/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/api/collections/OcmhCollectionCommandContext.java",
    "raw_url": "https://github.com/murblanc/lucene-solr/raw/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/api/collections/OcmhCollectionCommandContext.java",
    "contents_url": "https://api.github.com/repos/murblanc/lucene-solr/contents/solr/core/src/java/org/apache/solr/cloud/api/collections/OcmhCollectionCommandContext.java?ref=d34680c2f671cf9f629a8837f3c658d362e708c6",
    "patch": "@@ -39,6 +39,11 @@ public OcmhCollectionCommandContext(OverseerCollectionMessageHandler ocmh) {\n     this.ocmh = ocmh;\n   }\n \n+  @Override\n+  public  boolean isDistributedCollectionAPI() {\n+    return false;\n+  }\n+\n   @Override\n   public ShardHandler getShardHandler() {\n     return ocmh.shardHandlerFactory.getShardHandler();"
  },
  {
    "sha": "2b78f70d910e302d57a002fa5175645f29e7c46a",
    "filename": "solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerCollectionMessageHandler.java",
    "status": "modified",
    "additions": 5,
    "deletions": 45,
    "changes": 50,
    "blob_url": "https://github.com/murblanc/lucene-solr/blob/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerCollectionMessageHandler.java",
    "raw_url": "https://github.com/murblanc/lucene-solr/raw/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerCollectionMessageHandler.java",
    "contents_url": "https://api.github.com/repos/murblanc/lucene-solr/contents/solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerCollectionMessageHandler.java?ref=d34680c2f671cf9f629a8837f3c658d362e708c6",
    "patch": "@@ -16,7 +16,6 @@\n  */\n package org.apache.solr.cloud.api.collections;\n \n-import com.google.common.collect.ImmutableMap;\n import org.apache.solr.client.solrj.cloud.SolrCloudManager;\n import org.apache.solr.cloud.LockTree;\n import org.apache.solr.cloud.Overseer;\n@@ -43,20 +42,19 @@\n import java.io.IOException;\n import java.lang.invoke.MethodHandles;\n import java.util.Arrays;\n-import java.util.Map;\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.SynchronousQueue;\n import java.util.concurrent.TimeUnit;\n \n import static org.apache.solr.common.cloud.ZkStateReader.*;\n import static org.apache.solr.common.params.CollectionAdminParams.COLLECTION;\n-import static org.apache.solr.common.params.CollectionParams.CollectionAction.*;\n import static org.apache.solr.common.params.CommonParams.NAME;\n \n /**\n  * A {@link OverseerMessageHandler} that handles Collections API related overseer messages.<p>\n  *\n- * A lot of the content that was in this class got moved to {@link CollectionHandlingUtils} and {@link CollApiCmds}.\n+ * A lot of the content that was in this class got moved to {@link CollectionHandlingUtils} and {@link CollApiCmds}.<p>\n+ * The equivalent of this class for distributed Collection API command execution is {@link DistributedCollectionCommandRunner}.\n  */\n public class OverseerCollectionMessageHandler implements OverseerMessageHandler, SolrCloseable {\n \n@@ -71,7 +69,6 @@\n   String myId;\n   Stats stats;\n   TimeSource timeSource;\n-  private final CollectionCommandContext ccc;\n \n   // Set that tracks collections that are currently being processed by a running task.\n   // This is used for handling mutual exclusion of the tasks.\n@@ -81,7 +78,7 @@\n       new SynchronousQueue<>(),\n       new SolrNamedThreadFactory(\"OverseerCollectionMessageHandlerThreadFactory\"));\n \n-  final private Map<CollectionAction, CollApiCmds.CollectionApiCommand> commandMap;\n+  final private CollApiCmds.CommandMap commandMapper;\n \n   private volatile boolean isClosed;\n \n@@ -100,44 +97,7 @@ public OverseerCollectionMessageHandler(ZkStateReader zkStateReader, String myId\n     this.cloudManager = overseer.getSolrCloudManager();\n     this.timeSource = cloudManager.getTimeSource();\n     this.isClosed = false;\n-    ccc = new OcmhCollectionCommandContext(this);\n-    commandMap = new ImmutableMap.Builder<CollectionAction, CollApiCmds.CollectionApiCommand>()\n-        .put(REPLACENODE, new ReplaceNodeCmd(ccc))\n-        .put(DELETENODE, new DeleteNodeCmd(ccc))\n-        .put(BACKUP, new BackupCmd(ccc))\n-        .put(RESTORE, new RestoreCmd(ccc))\n-        .put(DELETEBACKUP, new DeleteBackupCmd(ccc))\n-        .put(CREATESNAPSHOT, new CreateSnapshotCmd(ccc))\n-        .put(DELETESNAPSHOT, new DeleteSnapshotCmd(ccc))\n-        .put(SPLITSHARD, new SplitShardCmd(ccc))\n-        .put(ADDROLE, new OverseerRoleCmd(ccc, ADDROLE, overseerPrioritizer))\n-        .put(REMOVEROLE, new OverseerRoleCmd(ccc, REMOVEROLE, overseerPrioritizer))\n-        .put(MOCK_COLL_TASK, new CollApiCmds.MockOperationCmd())\n-        .put(MOCK_SHARD_TASK, new CollApiCmds.MockOperationCmd())\n-        .put(MOCK_REPLICA_TASK, new CollApiCmds.MockOperationCmd())\n-        .put(CREATESHARD, new CreateShardCmd(ccc))\n-        .put(MIGRATE, new MigrateCmd(ccc))\n-        .put(CREATE, new CreateCollectionCmd(ccc))\n-        .put(MODIFYCOLLECTION, new CollApiCmds.ModifyCollectionCmd(ccc))\n-        .put(ADDREPLICAPROP, new CollApiCmds.AddReplicaPropCmd(ccc))\n-        .put(DELETEREPLICAPROP, new CollApiCmds.DeleteReplicaPropCmd(ccc))\n-        .put(BALANCESHARDUNIQUE, new CollApiCmds.BalanceShardsUniqueCmd(ccc))\n-        .put(REBALANCELEADERS, new CollApiCmds.RebalanceLeadersCmd(ccc))\n-        .put(RELOAD, new CollApiCmds.ReloadCollectionCmd(ccc))\n-        .put(DELETE, new DeleteCollectionCmd(ccc))\n-        .put(CREATEALIAS, new CreateAliasCmd(ccc))\n-        .put(DELETEALIAS, new DeleteAliasCmd(ccc))\n-        .put(ALIASPROP, new SetAliasPropCmd(ccc))\n-        .put(MAINTAINROUTEDALIAS, new MaintainRoutedAliasCmd(ccc))\n-        .put(OVERSEERSTATUS, new OverseerStatusCmd(ccc))\n-        .put(DELETESHARD, new DeleteShardCmd(ccc))\n-        .put(DELETEREPLICA, new DeleteReplicaCmd(ccc))\n-        .put(ADDREPLICA, new AddReplicaCmd(ccc))\n-        .put(MOVEREPLICA, new MoveReplicaCmd(ccc))\n-        .put(REINDEXCOLLECTION, new ReindexCollectionCmd(ccc))\n-        .put(RENAME, new RenameCmd(ccc))\n-        .build()\n-    ;\n+    commandMapper = new CollApiCmds.CommandMap(new OcmhCollectionCommandContext(this), overseerPrioritizer);\n   }\n \n   @Override\n@@ -152,7 +112,7 @@ public OverseerSolrResponse processMessage(ZkNodeProps message, String operation\n     NamedList results = new NamedList();\n     try {\n       CollectionAction action = getCollectionAction(operation);\n-      CollApiCmds.CollectionApiCommand command = commandMap.get(action);\n+      CollApiCmds.CollectionApiCommand command = commandMapper.getActionCommand(action);\n       if (command != null) {\n         command.call(cloudManager.getClusterStateProvider().getClusterState(), message, results);\n       } else {"
  },
  {
    "sha": "fd0a1d8caf842823ae8fcfd941444a781f04eb1b",
    "filename": "solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerRoleCmd.java",
    "status": "modified",
    "additions": 8,
    "deletions": 1,
    "changes": 9,
    "blob_url": "https://github.com/murblanc/lucene-solr/blob/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerRoleCmd.java",
    "raw_url": "https://github.com/murblanc/lucene-solr/raw/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerRoleCmd.java",
    "contents_url": "https://api.github.com/repos/murblanc/lucene-solr/contents/solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerRoleCmd.java?ref=d34680c2f671cf9f629a8837f3c658d362e708c6",
    "patch": "@@ -84,11 +84,18 @@ public void call(ClusterState state, ZkNodeProps message, NamedList results) thr\n     } else {\n       zkClient.create(ZkStateReader.ROLES, Utils.toJSON(roles), CreateMode.PERSISTENT, true);\n     }\n+\n+    // overseerId happens to not be used but only logged. So when Collection API is distributed we pass a default value.\n+    // This allows doing the prioritization logic (assuming Overseer is still running and doing other things even though\n+    // Collection API and cluster state updates are distributed - at least until we completely remove Overseer)\n+    final String overseerId = ccc.getCoreContainer().getDistributedCollectionCommandRunner().isEmpty() ?\n+        ccc.getOverseerId() : \"N/A\";\n+\n     //if there are too many nodes this command may time out. And most likely dedicated\n     // overseers are created when there are too many nodes  . So , do this operation in a separate thread\n     new Thread(() -> {\n       try {\n-        overseerPrioritizer.prioritizeOverseerNodes(ccc.getOverseerId());\n+        overseerPrioritizer.prioritizeOverseerNodes(overseerId);\n       } catch (Exception e) {\n         log.error(\"Error in prioritizing Overseer\", e);\n       }"
  },
  {
    "sha": "6cc6b9bdfe5ccd43c57cbbcee7ebaee1b099de19",
    "filename": "solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerStatusCmd.java",
    "status": "modified",
    "additions": 21,
    "deletions": 2,
    "changes": 23,
    "blob_url": "https://github.com/murblanc/lucene-solr/blob/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerStatusCmd.java",
    "raw_url": "https://github.com/murblanc/lucene-solr/raw/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerStatusCmd.java",
    "contents_url": "https://api.github.com/repos/murblanc/lucene-solr/contents/solr/core/src/java/org/apache/solr/cloud/api/collections/OverseerStatusCmd.java?ref=d34680c2f671cf9f629a8837f3c658d362e708c6",
    "patch": "@@ -41,6 +41,18 @@\n  * <b>within the current Overseer node</b> (this is important because distributed operations occurring on other nodes\n  * are <b>not included</b> in these stats, for example distributed cluster state updates or Per Replica States updates).<p>\n  *\n+ * More fundamentally, when the Collection API command execution is distributed, this specific command is <b>not</b> being\n+ * run on the Overseer anyway (but then not much is running on the overseer as cluster state updates are distributed as well)\n+ * so Overseer stats and status can't be returned and actually do not even make sense. Zookeeper based queue metrics do not\n+ * make sense either because Zookeeper queues are then not used.<p>\n+ *\n+ * The {@link Stats} instance returned by {@link CollectionCommandContext#getOverseerStats()} when running in the Overseer\n+ * is created in Overseer.start() and passed to the cluster state updater from where it is also propagated to the various\n+ * Zookeeper queues to register various events. This class is the only place where it is used in the Collection API implementation,\n+ * and only to return results.\n+ *\n+ * TODO: create a new command returning node specific Collection API/Config set API/cluster state updates stats such as success and failures?<p>\n+ *\n  * The structure of the returned results is as follows:\n  * <ul>\n  *   <li><b>{@code leader}:</b> {@code ID} of the current overseer leader node</li>\n@@ -59,8 +71,8 @@\n  *       (not all of them!) and {@link org.apache.solr.cloud.overseer.OverseerAction} (the complete list: {@code create},\n  *       {@code delete}, {@code createshard}, {@code deleteshard}, {@code addreplica}, {@code addreplicaprop}, {@code deletereplicaprop},\n  *       {@code balanceshardunique}, {@code modifycollection}, {@code state}, {@code leader}, {@code deletecore}, {@code addroutingrule},\n- *       {@code removeroutingrule}, {@code updateshardstate}, {@code downnode} and {@code quit} with this like one unlikely\n- *       to be observed since the Overseer is existing right away)</li>\n+ *       {@code removeroutingrule}, {@code updateshardstate}, {@code downnode} and {@code quit} with this last one unlikely\n+ *       to be observed since the Overseer is exiting right away)</li>\n  *       <li>{@code update_state} (when Overseer cluster state updater persists changes in Zookeeper)</li>\n  *     </ul>\n  *     For each key, the value is a map composed of:\n@@ -137,6 +149,13 @@ public OverseerStatusCmd(CollectionCommandContext ccc) {\n   @Override\n   @SuppressWarnings(\"unchecked\")\n   public void call(ClusterState state, ZkNodeProps message, @SuppressWarnings({\"rawtypes\"})NamedList results) throws Exception {\n+    // If Collection API execution is distributed, we're not running on the Overseer node so can't return any Overseer stats.\n+    if (ccc.getCoreContainer().getDistributedCollectionCommandRunner().isPresent()) {\n+      // TODO: introduce a per node status command allowing insight into how Cluster state updates, Collection API and\n+      //  config set API execution went on that node...\n+      return;\n+    }\n+\n     ZkStateReader zkStateReader = ccc.getZkStateReader();\n     String leaderNode = OverseerTaskProcessor.getLeaderNode(zkStateReader.getZkClient());\n     results.add(\"leader\", leaderNode);"
  },
  {
    "sha": "fdfa9a4bd87875f470970fdbb107712b2f6137d2",
    "filename": "solr/core/src/java/org/apache/solr/core/CloudConfig.java",
    "status": "modified",
    "additions": 20,
    "deletions": 2,
    "changes": 22,
    "blob_url": "https://github.com/murblanc/lucene-solr/blob/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/core/CloudConfig.java",
    "raw_url": "https://github.com/murblanc/lucene-solr/raw/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/core/CloudConfig.java",
    "contents_url": "https://api.github.com/repos/murblanc/lucene-solr/contents/solr/core/src/java/org/apache/solr/core/CloudConfig.java?ref=d34680c2f671cf9f629a8837f3c658d362e708c6",
    "patch": "@@ -50,10 +50,12 @@\n \n   private final boolean useDistributedClusterStateUpdates;\n \n+  private final boolean useDistributedCollectionConfigSetExecution;\n+\n   CloudConfig(String zkHost, int zkClientTimeout, int hostPort, String hostName, String hostContext, boolean useGenericCoreNames,\n               int leaderVoteWait, int leaderConflictResolveWait, String zkCredentialsProviderClass, String zkACLProviderClass,\n               int createCollectionWaitTimeTillActive, boolean createCollectionCheckLeaderActive, String pkiHandlerPrivateKeyPath,\n-              String pkiHandlerPublicKeyPath, boolean useDistributedClusterStateUpdates) {\n+              String pkiHandlerPublicKeyPath, boolean useDistributedClusterStateUpdates, boolean useDistributedCollectionConfigSetExecution) {\n     this.zkHost = zkHost;\n     this.zkClientTimeout = zkClientTimeout;\n     this.hostPort = hostPort;\n@@ -69,6 +71,11 @@\n     this.pkiHandlerPrivateKeyPath = pkiHandlerPrivateKeyPath;\n     this.pkiHandlerPublicKeyPath = pkiHandlerPublicKeyPath;\n     this.useDistributedClusterStateUpdates = useDistributedClusterStateUpdates;\n+    this.useDistributedCollectionConfigSetExecution = useDistributedCollectionConfigSetExecution;\n+\n+    if (useDistributedCollectionConfigSetExecution && !useDistributedClusterStateUpdates) {\n+      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"'useDistributedCollectionConfigSetExecution' can't be true if useDistributedClusterStateUpdates is false\");\n+    }\n \n     if (this.hostPort == -1)\n       throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"'hostPort' must be configured to run SolrCloud\");\n@@ -136,6 +143,10 @@ public boolean getDistributedClusterStateUpdates() {\n     return useDistributedClusterStateUpdates;\n   }\n \n+  public boolean getDistributedCollectionConfigSetExecution() {\n+    return useDistributedCollectionConfigSetExecution;\n+  }\n+\n   public static class CloudConfigBuilder {\n \n     private static final int DEFAULT_ZK_CLIENT_TIMEOUT = 45000;\n@@ -159,6 +170,7 @@ public boolean getDistributedClusterStateUpdates() {\n     private String pkiHandlerPrivateKeyPath;\n     private String pkiHandlerPublicKeyPath;\n     private boolean useDistributedClusterStateUpdates = false;\n+    private boolean useDistributedCollectionConfigSetExecution = false;\n \n     public CloudConfigBuilder(String hostName, int hostPort) {\n       this(hostName, hostPort, null);\n@@ -230,10 +242,16 @@ public CloudConfigBuilder setUseDistributedClusterStateUpdates(boolean useDistri\n       return this;\n     }\n \n+    public CloudConfigBuilder setUseDistributedCollectionConfigSetExecution(boolean useDistributedCollectionConfigSetExecution) {\n+      this.useDistributedCollectionConfigSetExecution = useDistributedCollectionConfigSetExecution;\n+      return this;\n+    }\n+\n     public CloudConfig build() {\n       return new CloudConfig(zkHost, zkClientTimeout, hostPort, hostName, hostContext, useGenericCoreNames, leaderVoteWait,\n           leaderConflictResolveWait, zkCredentialsProviderClass, zkACLProviderClass, createCollectionWaitTimeTillActive,\n-          createCollectionCheckLeaderActive, pkiHandlerPrivateKeyPath, pkiHandlerPublicKeyPath, useDistributedClusterStateUpdates);\n+          createCollectionCheckLeaderActive, pkiHandlerPrivateKeyPath, pkiHandlerPublicKeyPath,\n+          useDistributedClusterStateUpdates, useDistributedCollectionConfigSetExecution);\n     }\n   }\n }"
  },
  {
    "sha": "ce2f8389ae8b51bfcf1f7f40a260913aa49aee13",
    "filename": "solr/core/src/java/org/apache/solr/core/CoreContainer.java",
    "status": "modified",
    "additions": 32,
    "deletions": 4,
    "changes": 36,
    "blob_url": "https://github.com/murblanc/lucene-solr/blob/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/core/CoreContainer.java",
    "raw_url": "https://github.com/murblanc/lucene-solr/raw/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/core/CoreContainer.java",
    "contents_url": "https://api.github.com/repos/murblanc/lucene-solr/contents/solr/core/src/java/org/apache/solr/core/CoreContainer.java?ref=d34680c2f671cf9f629a8837f3c658d362e708c6",
    "patch": "@@ -71,6 +71,7 @@\n import org.apache.solr.cloud.ClusterSingleton;\n import org.apache.solr.cloud.OverseerTaskQueue;\n import org.apache.solr.cloud.ZkController;\n+import org.apache.solr.cloud.api.collections.DistributedCollectionCommandRunner;\n import org.apache.solr.cluster.events.ClusterEventProducer;\n import org.apache.solr.cluster.events.impl.ClusterEventProducerFactory;\n import org.apache.solr.cluster.placement.PlacementPluginConfig;\n@@ -275,6 +276,11 @@ public CoreLoadFailure(CoreDescriptor cd, Exception loadFailure) {\n \n   private ExecutorService coreContainerAsyncTaskExecutor = ExecutorUtil.newMDCAwareCachedThreadPool(\"Core Container Async Task\");\n \n+  /**\n+   * Non empty if the Collection API is executed in a distributed way and not on Overseer\n+   */\n+  private final Optional<DistributedCollectionCommandRunner> distributedCollectionCommandRunner;\n+\n   private enum CoreInitFailedAction {fromleader, none}\n \n   /**\n@@ -352,9 +358,20 @@ public CoreContainer(NodeConfig config, CoresLocator locator) {\n   }\n \n   public CoreContainer(NodeConfig config, CoresLocator locator, boolean asyncSolrCoreLoad) {\n+    this.cfg = requireNonNull(config);\n     this.loader = config.getSolrResourceLoader();\n     this.solrHome = config.getSolrHome();\n-    this.cfg = requireNonNull(config);\n+\n+    // Some SolrCloud tests do not need Zookeeper and end up with a null cloudConfig in NodeConfig (because\n+    // TestHarness.buildTestNodeConfig() uses the zkHost to decide it's SolrCloud).\n+    // These tests do not use Zookeeper and do not do Collection API or state updates (see subclasses of TestBaseStatsCacheCloud).\n+    // Some non SolrCloud tests do not even pass a config at all, so let be cautious.\n+    if (config.getCloudConfig() == null || !config.getCloudConfig().getDistributedCollectionConfigSetExecution()) {\n+      this.distributedCollectionCommandRunner = Optional.empty();\n+    } else {\n+      this.distributedCollectionCommandRunner = Optional.of(new DistributedCollectionCommandRunner(this));\n+    }\n+\n     try {\n       containerHandlers.put(PublicKeyHandler.PATH, new PublicKeyHandler(cfg.getCloudConfig()));\n     } catch (IOException | InvalidKeySpecException e) {\n@@ -585,6 +602,7 @@ protected CoreContainer(Object testConstructor) {\n     cfg = null;\n     containerProperties = null;\n     replayUpdatesExecutor = null;\n+    distributedCollectionCommandRunner = Optional.empty();\n   }\n \n   public static CoreContainer createAndLoad(Path solrHome) {\n@@ -930,7 +948,7 @@ public void load() {\n       });\n \n       clusterSingletons.setReady();\n-      zkSys.getZkController().checkOverseerDesignate();\n+      zkSys.getZkController().checkOverseerDesignate(distributedCollectionCommandRunner);\n \n     }\n     // This is a bit redundant but these are two distinct concepts for all they're accomplished at the same time.\n@@ -1030,8 +1048,14 @@ public void shutdown() {\n \n     ZkController zkController = getZkController();\n     if (zkController != null) {\n-      OverseerTaskQueue overseerCollectionQueue = zkController.getOverseerCollectionQueue();\n-      overseerCollectionQueue.allowOverseerPendingTasksToComplete();\n+      if (distributedCollectionCommandRunner.isPresent()) {\n+        // Local (i.e. distributed) Collection API processing\n+        distributedCollectionCommandRunner.get().stopAndWaitForPendingTasksToComplete();\n+      } else {\n+        // Overseer based processing\n+        OverseerTaskQueue overseerCollectionQueue = zkController.getOverseerCollectionQueue();\n+        overseerCollectionQueue.allowOverseerPendingTasksToComplete();\n+      }\n     }\n     if (log.isInfoEnabled()) {\n       log.info(\"Shutting down CoreContainer instance={}\", System.identityHashCode(this));\n@@ -2205,6 +2229,10 @@ public ClusterEventProducer getClusterEventProducer() {\n     return placementPluginFactory;\n   }\n \n+  public Optional<DistributedCollectionCommandRunner> getDistributedCollectionCommandRunner() {\n+    return this.distributedCollectionCommandRunner;\n+  }\n+\n   static {\n     ExecutorUtil.addThreadLocalProvider(SolrRequestInfo.getInheritableThreadLocalProvider());\n   }"
  },
  {
    "sha": "878623d769cb6147310a717e7b4045813464d022",
    "filename": "solr/core/src/java/org/apache/solr/core/SolrXmlConfig.java",
    "status": "modified",
    "additions": 6,
    "deletions": 0,
    "changes": 6,
    "blob_url": "https://github.com/murblanc/lucene-solr/blob/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/core/SolrXmlConfig.java",
    "raw_url": "https://github.com/murblanc/lucene-solr/raw/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/core/SolrXmlConfig.java",
    "contents_url": "https://api.github.com/repos/murblanc/lucene-solr/contents/solr/core/src/java/org/apache/solr/core/SolrXmlConfig.java?ref=d34680c2f671cf9f629a8837f3c658d362e708c6",
    "patch": "@@ -491,6 +491,12 @@ private static CloudConfig fillSolrCloudSection(NamedList<Object> nl, XmlConfigF\n         case \"distributedClusterStateUpdates\":\n           builder.setUseDistributedClusterStateUpdates(Boolean.parseBoolean(value));\n           break;\n+        case \"distributedCollectionConfigSetExecution\":\n+          builder.setUseDistributedCollectionConfigSetExecution(Boolean.parseBoolean(value));\n+          break;\n+\n+\n+\n         default:\n           throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unknown configuration parameter in <solrcloud> section of solr.xml: \" + name);\n       }"
  },
  {
    "sha": "7c99718baf1d7be35da083f793789b505ba11abf",
    "filename": "solr/core/src/java/org/apache/solr/handler/admin/CollectionsHandler.java",
    "status": "modified",
    "additions": 74,
    "deletions": 71,
    "changes": 145,
    "blob_url": "https://github.com/murblanc/lucene-solr/blob/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/handler/admin/CollectionsHandler.java",
    "raw_url": "https://github.com/murblanc/lucene-solr/raw/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/handler/admin/CollectionsHandler.java",
    "contents_url": "https://api.github.com/repos/murblanc/lucene-solr/contents/solr/core/src/java/org/apache/solr/handler/admin/CollectionsHandler.java?ref=d34680c2f671cf9f629a8837f3c658d362e708c6",
    "patch": "@@ -28,14 +28,14 @@\n import org.apache.solr.client.solrj.request.CoreAdminRequest.RequestSyncShard;\n import org.apache.solr.client.solrj.response.RequestStatusState;\n import org.apache.solr.client.solrj.util.SolrIdentifierValidator;\n-import org.apache.solr.cloud.DistributedClusterStateUpdater;\n import org.apache.solr.cloud.OverseerSolrResponse;\n import org.apache.solr.cloud.OverseerSolrResponseSerializer;\n import org.apache.solr.cloud.OverseerTaskQueue;\n import org.apache.solr.cloud.OverseerTaskQueue.QueueEvent;\n import org.apache.solr.cloud.ZkController;\n import org.apache.solr.cloud.ZkController.NotInClusterStateException;\n import org.apache.solr.cloud.ZkShardTerms;\n+import org.apache.solr.cloud.api.collections.DistributedCollectionCommandRunner;\n import org.apache.solr.cloud.api.collections.ReindexCollectionCmd;\n import org.apache.solr.cloud.api.collections.RoutedAlias;\n import org.apache.solr.cloud.overseer.SliceMutator;\n@@ -99,6 +99,7 @@\n import java.util.List;\n import java.util.Locale;\n import java.util.Map;\n+import java.util.Optional;\n import java.util.Set;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.TimeoutException;\n@@ -137,7 +138,7 @@\n \n   protected final CoreContainer coreContainer;\n   private final CollectionHandlerApi v2Handler;\n-  private final DistributedClusterStateUpdater distributedClusterStateUpdater;\n+  private final Optional<DistributedCollectionCommandRunner> distributedCollectionCommandRunner;\n \n   public CollectionsHandler() {\n     // Unlike most request handlers, CoreContainer initialization\n@@ -154,17 +155,7 @@ public CollectionsHandler() {\n   public CollectionsHandler(final CoreContainer coreContainer) {\n     this.coreContainer = coreContainer;\n     v2Handler = new CollectionHandlerApi(this);\n-    // Get the state change factory to know if need to enqueue to Overseer or process distributed.\n-    // Some SolrCloud tests do not need Zookeeper and end up with a null cloudConfig in NodeConfig (because\n-    // TestHarness.buildTestNodeConfig() uses the zkHost to decide it's SolrCloud).\n-    // These tests do not use Zookeeper and do not do state updates (see subclasses of TestBaseStatsCacheCloud).\n-    // Some non SolrCloud tests do not even pass a config at all, so let be cautious here (code is not pretty).\n-    // We do want to initialize here and not do it lazy to not deal with synchronization for actual prod code.\n-    if (coreContainer == null || coreContainer.getConfig() == null || coreContainer.getConfig().getCloudConfig() == null) {\n-      distributedClusterStateUpdater = null;\n-    } else {\n-      distributedClusterStateUpdater = new DistributedClusterStateUpdater(coreContainer.getConfig().getCloudConfig().getDistributedClusterStateUpdates());\n-    }\n+    distributedCollectionCommandRunner = coreContainer != null ? coreContainer.getDistributedCollectionCommandRunner() : Optional.empty();\n   }\n \n   @Override\n@@ -179,7 +170,7 @@ public CollectionsHandler(final CoreContainer coreContainer) {\n   }\n \n   @Override\n-  final public void init(@SuppressWarnings({\"rawtypes\"})NamedList args) {\n+  final public void init(@SuppressWarnings({\"rawtypes\"}) NamedList args) {\n \n   }\n \n@@ -259,7 +250,10 @@ void invokeAction(SolrQueryRequest req, SolrQueryResponse rsp, CoreContainer cor\n     props.put(QUEUE_OPERATION, operation.action.toLower());\n \n     ZkNodeProps zkProps = new ZkNodeProps(props);\n-    SolrResponse overseerResponse = sendToOCPQueue(zkProps, operation.timeOut);\n+    final SolrResponse overseerResponse;\n+\n+    overseerResponse = submitCollectionApiCommand(zkProps, operation.action, operation.timeOut);\n+\n     rsp.getValues().addAll(overseerResponse.getResponse());\n     Exception exp = overseerResponse.getException();\n     if (exp != null) {\n@@ -279,74 +273,83 @@ void invokeAction(SolrQueryRequest req, SolrQueryResponse rsp, CoreContainer cor\n \n   public static long DEFAULT_COLLECTION_OP_TIMEOUT = 180 * 1000;\n \n-  public SolrResponse sendToOCPQueue(ZkNodeProps m) throws KeeperException, InterruptedException {\n-    return sendToOCPQueue(m, DEFAULT_COLLECTION_OP_TIMEOUT);\n+  public SolrResponse submitCollectionApiCommand(ZkNodeProps m, CollectionAction action) throws KeeperException, InterruptedException {\n+    return submitCollectionApiCommand(m, action, DEFAULT_COLLECTION_OP_TIMEOUT);\n   }\n \n-  public SolrResponse sendToOCPQueue(ZkNodeProps m, long timeout) throws KeeperException, InterruptedException {\n-    String operation = m.getStr(QUEUE_OPERATION);\n-    if (operation == null) {\n-      throw new SolrException(ErrorCode.BAD_REQUEST, \"missing key \" + QUEUE_OPERATION);\n-    }\n-    if (m.get(ASYNC) != null) {\n-\n-      String asyncId = m.getStr(ASYNC);\n-\n-      if (asyncId.equals(\"-1\")) {\n-        throw new SolrException(ErrorCode.BAD_REQUEST, \"requestid can not be -1. It is reserved for cleanup purposes.\");\n+  public SolrResponse submitCollectionApiCommand(ZkNodeProps m, CollectionAction action, long timeout) throws KeeperException, InterruptedException {\n+    // Collection API messages are either sent to Overseer and processed there, or processed locally.\n+    // Distributing Collection API implies we're also distributing Cluster State Updates. Indeed collection creation\n+    // with non distributed cluster state updates requires for \"Per Replica States\" that the Collection API be running\n+    // on Overseer, which means that it is not possible to distributed Collection API while keeping cluster state updates\n+    // on Overseer. See the call to CollectionCommandContext.submitIntraProcessMessage() in CreateCollectionCmd.call() which\n+    // can only be done if the Collection API command runs on the same JVM as the Overseer based cluster state update...\n+    // The configuration handling includes these checks to not allow distributing collection API without distributing\n+    // cluster state updates (but the other way around is ok). See constructor of CloudConfig.\n+    if (distributedCollectionCommandRunner.isPresent()) {\n+      if (log.isInfoEnabled()) {\n+        log.info(\"Running Collection API locally for \" + action.name()); // nowarn\n       }\n-\n-      NamedList<String> r = new NamedList<>();\n-\n-\n-      if (coreContainer.getZkController().claimAsyncId(asyncId)) {\n-        boolean success = false;\n-        try {\n-          coreContainer.getZkController().getOverseerCollectionQueue()\n-              .offer(Utils.toJSON(m));\n-          success = true;\n-        } finally {\n-          if (!success) {\n-            try {\n-              coreContainer.getZkController().clearAsyncId(asyncId);\n-            } catch (Exception e) {\n-              // let the original exception bubble up\n-              log.error(\"Unable to release async ID={}\", asyncId, e);\n-              SolrZkClient.checkInterrupted(e);\n+      return distributedCollectionCommandRunner.get().runApiCommand(m, action, timeout);\n+    } else { // Sending the Collection API message to Overseer via a Zookeeper queue\n+      String operation = m.getStr(QUEUE_OPERATION);\n+      if (operation == null) {\n+        throw new SolrException(ErrorCode.BAD_REQUEST, \"missing key \" + QUEUE_OPERATION);\n+      }\n+      if (m.get(ASYNC) != null) {\n+        String asyncId = m.getStr(ASYNC);\n+        NamedList<String> r = new NamedList<>();\n+\n+        if (coreContainer.getZkController().claimAsyncId(asyncId)) {\n+          boolean success = false;\n+          try {\n+            coreContainer.getZkController().getOverseerCollectionQueue()\n+                .offer(Utils.toJSON(m));\n+            success = true;\n+          } finally {\n+            if (!success) {\n+              try {\n+                coreContainer.getZkController().clearAsyncId(asyncId);\n+              } catch (Exception e) {\n+                // let the original exception bubble up\n+                log.error(\"Unable to release async ID={}\", asyncId, e);\n+                SolrZkClient.checkInterrupted(e);\n+              }\n             }\n           }\n+        } else {\n+          r.add(\"error\", \"Task with the same requestid already exists.\");\n         }\n-      } else {\n-        r.add(\"error\", \"Task with the same requestid already exists.\");\n-      }\n-      r.add(CoreAdminParams.REQUESTID, (String) m.get(ASYNC));\n+        r.add(CoreAdminParams.REQUESTID, (String) m.get(ASYNC));\n \n-      return new OverseerSolrResponse(r);\n-    }\n+        return new OverseerSolrResponse(r);\n+      }\n \n-    long time = System.nanoTime();\n-    QueueEvent event = coreContainer.getZkController()\n-        .getOverseerCollectionQueue()\n-        .offer(Utils.toJSON(m), timeout);\n-    if (event.getBytes() != null) {\n-      return OverseerSolrResponseSerializer.deserialize(event.getBytes());\n-    } else {\n-      if (System.nanoTime() - time >= TimeUnit.NANOSECONDS.convert(timeout, TimeUnit.MILLISECONDS)) {\n-        throw new SolrException(ErrorCode.SERVER_ERROR, operation\n-            + \" the collection time out:\" + timeout / 1000 + \"s\");\n-      } else if (event.getWatchedEvent() != null) {\n-        throw new SolrException(ErrorCode.SERVER_ERROR, operation\n-            + \" the collection error [Watcher fired on path: \"\n-            + event.getWatchedEvent().getPath() + \" state: \"\n-            + event.getWatchedEvent().getState() + \" type \"\n-            + event.getWatchedEvent().getType() + \"]\");\n+      long time = System.nanoTime();\n+      QueueEvent event = coreContainer.getZkController()\n+          .getOverseerCollectionQueue()\n+          .offer(Utils.toJSON(m), timeout);\n+      if (event.getBytes() != null) {\n+        return OverseerSolrResponseSerializer.deserialize(event.getBytes());\n       } else {\n-        throw new SolrException(ErrorCode.SERVER_ERROR, operation\n-            + \" the collection unknown case\");\n+        if (System.nanoTime() - time >= TimeUnit.NANOSECONDS.convert(timeout, TimeUnit.MILLISECONDS)) {\n+          throw new SolrException(ErrorCode.SERVER_ERROR, operation\n+              + \" the collection time out:\" + timeout / 1000 + \"s\");\n+        } else if (event.getWatchedEvent() != null) {\n+          throw new SolrException(ErrorCode.SERVER_ERROR, operation\n+              + \" the collection error [Watcher fired on path: \"\n+              + event.getWatchedEvent().getPath() + \" state: \"\n+              + event.getWatchedEvent().getState() + \" type \"\n+              + event.getWatchedEvent().getType() + \"]\");\n+        } else {\n+          throw new SolrException(ErrorCode.SERVER_ERROR, operation\n+              + \" the collection unknown case\");\n+        }\n       }\n     }\n   }\n \n+  // TODO need to fix (and all the rest of ASYNC!) this for async in case of distributed Collection API\n   private boolean overseerCollectionQueueContains(String asyncId) throws KeeperException, InterruptedException {\n     OverseerTaskQueue collectionQueue = coreContainer.getZkController().getOverseerCollectionQueue();\n     return collectionQueue.containsTaskWithRequestId(ASYNC, asyncId);\n@@ -810,7 +813,7 @@ private static void addStatusToResponse(NamedList<Object> results, RequestStatus\n       return null;\n     }),\n     @SuppressWarnings({\"unchecked\"})\n-    REQUESTSTATUS_OP(REQUESTSTATUS, (req, rsp, h) -> {\n+    REQUESTSTATUS_OP(REQUESTSTATUS, (req, rsp, h) -> { // TODO need to fix this for async in case of distributed Collection API\n       req.getParams().required().check(REQUESTID);\n \n       final CoreContainer coreContainer1 = h.coreContainer;"
  },
  {
    "sha": "45cba5907e6a4dd2ebe5b4a6c3220e63ac9efa70",
    "filename": "solr/core/src/java/org/apache/solr/handler/admin/RebalanceLeaders.java",
    "status": "modified",
    "additions": 5,
    "deletions": 3,
    "changes": 8,
    "blob_url": "https://github.com/murblanc/lucene-solr/blob/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/handler/admin/RebalanceLeaders.java",
    "raw_url": "https://github.com/murblanc/lucene-solr/raw/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/java/org/apache/solr/handler/admin/RebalanceLeaders.java",
    "contents_url": "https://api.github.com/repos/murblanc/lucene-solr/contents/solr/core/src/java/org/apache/solr/handler/admin/RebalanceLeaders.java?ref=d34680c2f671cf9f629a8837f3c658d362e708c6",
    "patch": "@@ -37,6 +37,7 @@\n import org.apache.solr.common.cloud.Slice;\n import org.apache.solr.common.cloud.ZkNodeProps;\n import org.apache.solr.common.cloud.ZkStateReader;\n+import org.apache.solr.common.params.CollectionParams;\n import org.apache.solr.common.util.SimpleOrderedMap;\n import org.apache.solr.core.CoreContainer;\n import org.apache.solr.request.SolrQueryRequest;\n@@ -403,20 +404,21 @@ int waitForNodeChange(Slice slice, String electionNode) throws InterruptedExcept\n   private void rejoinElectionQueue(Slice slice, String electionNode, String core, boolean rejoinAtHead)\n       throws KeeperException, InterruptedException {\n     Replica replica = slice.getReplica(LeaderElector.getNodeName(electionNode));\n+    final CollectionParams.CollectionAction rebalanceleaders = REBALANCELEADERS;\n     Map<String, Object> propMap = new HashMap<>();\n     propMap.put(COLLECTION_PROP, collectionName);\n     propMap.put(SHARD_ID_PROP, slice.getName());\n-    propMap.put(QUEUE_OPERATION, REBALANCELEADERS.toLower());\n+    propMap.put(QUEUE_OPERATION, rebalanceleaders.toLower());\n     propMap.put(CORE_NAME_PROP, core);\n     propMap.put(CORE_NODE_NAME_PROP, replica.getName());\n     propMap.put(ZkStateReader.NODE_NAME_PROP, replica.getNodeName());\n     propMap.put(REJOIN_AT_HEAD_PROP, Boolean.toString(rejoinAtHead)); // Get ourselves to be first in line.\n     propMap.put(ELECTION_NODE_PROP, electionNode);\n-    String asyncId = REBALANCELEADERS.toLower() + \"_\" + core + \"_\" + Math.abs(System.nanoTime());\n+    String asyncId = rebalanceleaders.toLower() + \"_\" + core + \"_\" + Math.abs(System.nanoTime());\n     propMap.put(ASYNC, asyncId);\n     asyncRequests.add(asyncId);\n \n-    collectionsHandler.sendToOCPQueue(new ZkNodeProps(propMap)); // ignore response; we construct our own\n+    collectionsHandler.submitCollectionApiCommand(new ZkNodeProps(propMap), rebalanceleaders); // ignore response; we construct our own\n   }\n \n   // maxWaitSecs - How long are we going to wait? Defaults to 30 seconds."
  },
  {
    "sha": "d66062ecd3fa516a97c6e21343c098907aed0c31",
    "filename": "solr/core/src/test/org/apache/solr/cloud/ZkDistributedLockTest.java",
    "status": "added",
    "additions": 164,
    "deletions": 0,
    "changes": 164,
    "blob_url": "https://github.com/murblanc/lucene-solr/blob/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/test/org/apache/solr/cloud/ZkDistributedLockTest.java",
    "raw_url": "https://github.com/murblanc/lucene-solr/raw/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/test/org/apache/solr/cloud/ZkDistributedLockTest.java",
    "contents_url": "https://api.github.com/repos/murblanc/lucene-solr/contents/solr/core/src/test/org/apache/solr/cloud/ZkDistributedLockTest.java?ref=d34680c2f671cf9f629a8837f3c658d362e708c6",
    "patch": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.solr.cloud;\n+\n+import java.nio.file.Path;\n+import java.util.concurrent.CountDownLatch;\n+\n+import org.apache.solr.SolrTestCaseJ4;\n+import org.apache.solr.common.SolrException;\n+import org.apache.solr.common.cloud.SolrZkClient;\n+import org.apache.solr.common.cloud.ZkStateReader;\n+import org.apache.solr.common.params.CollectionParams;\n+import org.apache.zookeeper.CreateMode;\n+import org.junit.Test;\n+\n+public class ZkDistributedLockTest extends SolrTestCaseJ4 {\n+\n+  private static final String COLLECTION_NAME = \"lockColl\";\n+  final String SHARD_NAME = \"lockShard\";\n+  final String REPLICA_NAME = \"lockReplica\";\n+\n+  static final int TIMEOUT = 10000;\n+\n+  /**\n+   * Tests the obtention of a single read or write lock at a specific hierarchical level.\n+   * Tests the logic with a single thread, then tests multithreaded wait for lock acquire works.\n+   * Tests grouped to pay setup only once.\n+   */\n+  @Test\n+  public void testSingleLocks() throws Exception {\n+    Path zkDir = createTempDir(\"zkData\");\n+\n+    ZkTestServer server = new ZkTestServer(zkDir);\n+    try {\n+      server.run();\n+      try (SolrZkClient zkClient = new SolrZkClient(server.getZkAddress(), TIMEOUT)) {\n+        DistributedLockFactory factory = new ZkDistributedLockFactory(zkClient);\n+\n+        try {\n+          factory.createLock(true, CollectionParams.LockLevel.COLLECTION, COLLECTION_NAME, null, null);\n+          fail(\"Collection does not exist, lock creation should have failed\");\n+        } catch (SolrException expected) {\n+        }\n+\n+        zkClient.makePath(ZkStateReader.COLLECTIONS_ZKNODE + \"/\" + COLLECTION_NAME, null, CreateMode.PERSISTENT, true);\n+\n+        monothreadedTests(factory);\n+        multithreadedTests(factory);\n+      }\n+    } finally {\n+      server.shutdown();\n+    }\n+  }\n+\n+  private void monothreadedTests(DistributedLockFactory factory) {\n+    // Collection level locks\n+    DistributedLock collRL1 = factory.createLock(false, CollectionParams.LockLevel.COLLECTION, COLLECTION_NAME, null, null);\n+    assertTrue(\"collRL1 should have been acquired\", collRL1.isAcquired());\n+\n+    DistributedLock collRL2 = factory.createLock(false, CollectionParams.LockLevel.COLLECTION, COLLECTION_NAME, null, null);\n+    assertTrue(\"collRL1 should have been acquired\", collRL2.isAcquired());\n+\n+    DistributedLock collWL3 = factory.createLock(true, CollectionParams.LockLevel.COLLECTION, COLLECTION_NAME, null, null);\n+    assertFalse(\"collWL3 should not have been acquired, due to collRL1 and collRL2\", collWL3.isAcquired());\n+\n+    assertTrue(\"collRL2 should have been acquired, that should not have changed\", collRL2.isAcquired());\n+\n+    collRL1.release();\n+    collRL2.release();\n+    assertTrue(\"collWL3 should have been acquired, collRL1 and collRL2 were released\", collWL3.isAcquired());\n+\n+    DistributedLock collRL4 = factory.createLock(false, CollectionParams.LockLevel.COLLECTION, COLLECTION_NAME, null, null);\n+    assertFalse(\"collRL4 should not have been acquired, due to collWL3 locking the collection\", collRL4.isAcquired());\n+\n+    // Collection is write locked by collWL3 and collRL4 read lock waiting behind. Now moving to request shard level locks.\n+    // These are totally independent from the Collection level locks so should see no impact.\n+    DistributedLock shardWL5 = factory.createLock(true, CollectionParams.LockLevel.SHARD, COLLECTION_NAME, SHARD_NAME, null);\n+    assertTrue(\"shardWL5 should have been acquired, there is no lock on that shard\", shardWL5.isAcquired());\n+\n+    DistributedLock shardWL6 = factory.createLock(true, CollectionParams.LockLevel.SHARD, COLLECTION_NAME, SHARD_NAME, null);\n+    assertFalse(\"shardWL6 should not have been acquired, shardWL5 is locking that shard\", shardWL6.isAcquired());\n+\n+    // Get a lock on a Replica. Again this is independent of collection or shard level\n+    DistributedLock replicaRL7 = factory.createLock(false, CollectionParams.LockLevel.REPLICA, COLLECTION_NAME, SHARD_NAME, REPLICA_NAME);\n+    assertTrue(\"replicaRL7 should have been acquired\", replicaRL7.isAcquired());\n+\n+    DistributedLock replicaWL8 = factory.createLock(true, CollectionParams.LockLevel.REPLICA, COLLECTION_NAME, SHARD_NAME, REPLICA_NAME);\n+    assertFalse(\"replicaWL8 should not have been acquired, replicaRL7 is read locking that replica\", replicaWL8.isAcquired());\n+\n+    replicaRL7.release();\n+    assertTrue(\"replicaWL8 should have been acquired, as replicaRL7 got released\", replicaWL8.isAcquired());\n+\n+\n+    collWL3.release();\n+    assertTrue(\"collRL4 should have been acquired given collWL3 released\", collRL4.isAcquired());\n+    shardWL5.release();\n+    assertTrue(\"shardWL6 should have been acquired, now that shardWL5 was released\", shardWL6.isAcquired());\n+\n+    replicaWL8.release();\n+    try {\n+      replicaWL8.isAcquired();\n+      fail(\"isAcquired() called after release() on a lock should have thrown exception\");\n+    } catch (IllegalStateException ise) {\n+      // expected\n+    }\n+\n+    // Releasing the collection lock used in the multithreaded phase\n+    collRL4.release();\n+  }\n+\n+  private void multithreadedTests(DistributedLockFactory factory) throws Exception {\n+    // Acquiring right away a read lock\n+    DistributedLock readLock = factory.createLock(false, CollectionParams.LockLevel.COLLECTION, COLLECTION_NAME, null, null);\n+    assertTrue(\"readLock should have been acquired\", readLock.isAcquired());\n+\n+    // And now creating a write lock, that can't be acquired just yet, because of the read lock\n+    DistributedLock writeLock = factory.createLock(true, CollectionParams.LockLevel.COLLECTION, COLLECTION_NAME, null, null);\n+    assertFalse(\"writeLock should not have been acquired\", writeLock.isAcquired());\n+\n+    // Wait for acquisition of the write lock on another thread (and be notified via a latch)\n+    final CountDownLatch latch = new CountDownLatch(1);\n+    new Thread(() -> {\n+      writeLock.waitUntilAcquired();\n+      // countDown() will not be called if waitUntilAcquired() threw exception of any kind\n+      latch.countDown();\n+    }).start();\n+\n+    // Wait for the thread to start and to get blocked in waitUntilAcquired()\n+    // (thread start could have been checked more reliably using another latch, and verifying the thread is in waitUntilAcquired\n+    // done through that thread stacktrace, but that would be overkill compared to the very slight race condition of waiting 30ms,\n+    // but a race that would not cause the test to fail since we're testing... that nothing happened yet).\n+    Thread.sleep(30);\n+\n+    assertEquals(\"we should not have been notified that writeLock was acquired\", 1, latch.getCount());\n+    assertFalse(\"writeLock should not have been acquired\", writeLock.isAcquired());\n+\n+    readLock.release();\n+    assertTrue(\"writeLock should have been acquired now that readlock was released\", writeLock.isAcquired());\n+\n+    // Wait for the Zookeeper watch to fire + the thread to be unblocked and countdown the latch\n+    // We'll wait up to 10 seconds here, so should be safe even if GC is extraordinarily high with a pause\n+    int i = 0;\n+    while (i < 1000 && latch.getCount() != 0) {\n+      Thread.sleep(10);\n+      i++;\n+    }\n+    assertEquals(\"we should have been notified that writeLock was acquired\", 0, latch.getCount());\n+  }\n+}\n\\ No newline at end of file"
  },
  {
    "sha": "037250e403d9f98053827a850cbc47ff478408b1",
    "filename": "solr/core/src/test/org/apache/solr/cloud/api/collections/ApiLockingTest.java",
    "status": "added",
    "additions": 162,
    "deletions": 0,
    "changes": 162,
    "blob_url": "https://github.com/murblanc/lucene-solr/blob/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/test/org/apache/solr/cloud/api/collections/ApiLockingTest.java",
    "raw_url": "https://github.com/murblanc/lucene-solr/raw/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/core/src/test/org/apache/solr/cloud/api/collections/ApiLockingTest.java",
    "contents_url": "https://api.github.com/repos/murblanc/lucene-solr/contents/solr/core/src/test/org/apache/solr/cloud/api/collections/ApiLockingTest.java?ref=d34680c2f671cf9f629a8837f3c658d362e708c6",
    "patch": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.solr.cloud.api.collections;\n+\n+import java.nio.file.Path;\n+import java.util.concurrent.CountDownLatch;\n+\n+import org.apache.solr.SolrTestCaseJ4;\n+import org.apache.solr.cloud.ZkDistributedLockFactory;\n+import org.apache.solr.cloud.ZkTestServer;\n+import org.apache.solr.common.SolrException;\n+import org.apache.solr.common.cloud.SolrZkClient;\n+import org.apache.solr.common.cloud.ZkStateReader;\n+import org.apache.solr.common.params.CollectionParams;\n+import org.apache.zookeeper.CreateMode;\n+import org.junit.Test;\n+\n+public class ApiLockingTest  extends SolrTestCaseJ4 {\n+  private static final String COLLECTION_NAME = \"lockColl\";\n+  final String SHARD1_NAME = \"lockShard1\";\n+  final String SHARD2_NAME = \"lockShard2\";\n+  final String REPLICA_NAME = \"lockReplica\";\n+\n+  static final int TIMEOUT = 10000;\n+\n+  /**\n+   * Tests the Collection API locking. All tests run from single test method to save on setup time.\n+   */\n+  @Test\n+  public void monothreadedApiLockTests() throws Exception {\n+    Path zkDir = createTempDir(\"zkData\");\n+\n+    ZkTestServer server = new ZkTestServer(zkDir);\n+    try {\n+      server.run();\n+      try (SolrZkClient zkClient = new SolrZkClient(server.getZkAddress(), TIMEOUT)) {\n+        ApiLockFactory apiLockFactory = new ApiLockFactory(new ZkDistributedLockFactory(zkClient));\n+\n+        try {\n+          apiLockFactory.createCollectionApiLock(CollectionParams.LockLevel.COLLECTION, COLLECTION_NAME, null, null);\n+          fail(\"Collection does not exist, lock creation should have failed\");\n+        } catch (SolrException expected) {\n+        }\n+\n+        zkClient.makePath(ZkStateReader.COLLECTIONS_ZKNODE + \"/\" + COLLECTION_NAME, null, CreateMode.PERSISTENT, true);\n+\n+        monothreadedTests(apiLockFactory);\n+        multithreadedTests(apiLockFactory);\n+      }\n+    } finally {\n+      server.shutdown();\n+    }\n+  }\n+\n+  private void monothreadedTests(ApiLockFactory apiLockingHelper) throws Exception {\n+    // Lock at collection level (which prevents locking + acquiring on any other level of the hierarchy)\n+    ApiLockFactory.ApiLock collLock = apiLockingHelper.createCollectionApiLock(CollectionParams.LockLevel.COLLECTION, COLLECTION_NAME, null, null);\n+    assertTrue(\"Collection should have been acquired\", collLock.isAcquired());\n+    assertEquals(\"Lock at collection level expected to need one distributed lock\", 1, collLock.getCountInternalLocks());\n+\n+    // Request a shard lock. Will not be acquired as long as we don't release the collection lock above\n+    ApiLockFactory.ApiLock shard1Lock = apiLockingHelper.createCollectionApiLock(CollectionParams.LockLevel.SHARD, COLLECTION_NAME, SHARD1_NAME, null);\n+    assertFalse(\"Shard1 should not have been acquired\", shard1Lock.isAcquired());\n+    assertEquals(\"Lock at shard level expected to need two distributed locks\", 2, shard1Lock.getCountInternalLocks());\n+\n+    // Request a lock on another shard. Will not be acquired as long as we don't release the collection lock above\n+    ApiLockFactory.ApiLock shard2Lock = apiLockingHelper.createCollectionApiLock(CollectionParams.LockLevel.SHARD, COLLECTION_NAME, SHARD2_NAME, null);\n+    assertFalse(\"Shard2 should not have been acquired\", shard2Lock.isAcquired());\n+\n+    assertTrue(\"Collection should still be acquired\", collLock.isAcquired());\n+\n+    collLock.release();\n+\n+    assertTrue(\"Shard1 should have been acquired now that collection lock released\", shard1Lock.isAcquired());\n+    assertTrue(\"Shard2 should have been acquired now that collection lock released\", shard2Lock.isAcquired());\n+\n+    // Request a lock on replica of shard1\n+    ApiLockFactory.ApiLock replicaShard1Lock = apiLockingHelper.createCollectionApiLock(CollectionParams.LockLevel.SHARD, COLLECTION_NAME, SHARD1_NAME, REPLICA_NAME);\n+    assertFalse(\"replicaShard1Lock should not have been acquired, shard1 is locked\", replicaShard1Lock.isAcquired());\n+\n+    // Now ask for a new lock on the collection\n+    collLock = apiLockingHelper.createCollectionApiLock(CollectionParams.LockLevel.COLLECTION, COLLECTION_NAME, null, null);\n+\n+    assertFalse(\"Collection should not have been acquired, shard1 and shard2 locks preventing it\", collLock.isAcquired());\n+\n+    shard1Lock.release();\n+    assertTrue(\"replicaShard1Lock should have been acquired, as shard1 got released\", replicaShard1Lock.isAcquired());\n+    assertFalse(\"Collection should not have been acquired, shard2 lock is preventing it\", collLock.isAcquired());\n+\n+    replicaShard1Lock.release();\n+\n+    // Request a lock on replica of shard2\n+    ApiLockFactory.ApiLock replicaShard2Lock = apiLockingHelper.createCollectionApiLock(CollectionParams.LockLevel.SHARD, COLLECTION_NAME, SHARD2_NAME, REPLICA_NAME);\n+    assertFalse(\"replicaShard2Lock should not have been acquired, shard2 is locked\", replicaShard2Lock.isAcquired());\n+\n+    shard2Lock.release();\n+\n+    assertTrue(\"Collection should have been acquired as shard2 got released and replicaShard2Locks was requested after the collection lock\", collLock.isAcquired());\n+    assertFalse(\"replicaShard2Lock should not have been acquired, collLock is locked\", replicaShard2Lock.isAcquired());\n+\n+    collLock.release();\n+    assertTrue(\"replicaShard2Lock should have been acquired, the collection lock got released\", replicaShard2Lock.isAcquired());\n+\n+    // Release remaining lock to allow the multithreaded locking to succeed\n+    replicaShard2Lock.release();\n+  }\n+\n+  private void multithreadedTests(ApiLockFactory apiLockingHelper) throws Exception {\n+    // Lock on collection...\n+    ApiLockFactory.ApiLock collLock = apiLockingHelper.createCollectionApiLock(CollectionParams.LockLevel.COLLECTION, COLLECTION_NAME, null, null);\n+    assertTrue(\"Collection should have been acquired\", collLock.isAcquired());\n+\n+    // ...blocks a lock on replica from being acquired\n+    final ApiLockFactory.ApiLock replicaShard1Lock = apiLockingHelper.createCollectionApiLock(CollectionParams.LockLevel.SHARD, COLLECTION_NAME, SHARD1_NAME, REPLICA_NAME);\n+    assertFalse(\"replicaShard1Lock should not have been acquired, because collection is locked\", replicaShard1Lock.isAcquired());\n+\n+    // Wait for acquisition of the replica lock on another thread (and be notified via a latch)\n+    final CountDownLatch latch = new CountDownLatch(1);\n+    new Thread(() -> {\n+      replicaShard1Lock.waitUntilAcquired();\n+      // countDown() will not be called if waitUntilAcquired() threw exception of any kind\n+      latch.countDown();\n+    }).start();\n+\n+    // Wait for the thread to start and to get blocked in waitUntilAcquired()\n+    // (thread start could have been checked more reliably using another latch, and verifying the thread is in waitUntilAcquired\n+    // done through that thread stacktrace, but that would be overkill compared to the very slight race condition of waiting 30ms,\n+    // but a race that would not cause the test to fail since we're testing... that nothing happened yet).\n+    Thread.sleep(30);\n+\n+    assertEquals(\"we should not have been notified that replica was acquired\", 1, latch.getCount());\n+    assertFalse(\"replica lock should not have been acquired\", replicaShard1Lock.isAcquired());\n+\n+    collLock.release();\n+    assertTrue(\"replica lock should have been acquired now that collection lock was released\", replicaShard1Lock.isAcquired());\n+\n+    // Wait for the Zookeeper watch to fire + the thread to be unblocked and countdown the latch\n+    // We'll wait up to 10 seconds here, so should be safe even if GC is extraordinarily high with a pause\n+    int i = 0;\n+    while (i < 1000 && latch.getCount() != 0) {\n+      Thread.sleep(10);\n+      i++;\n+    }\n+    assertEquals(\"we should have been notified that replica lock was acquired\", 0, latch.getCount());\n+  }\n+\n+}"
  },
  {
    "sha": "8c73d838fc1f52d073d5d11e6152155f3c3c5cd3",
    "filename": "solr/server/solr/solr.xml",
    "status": "modified",
    "additions": 1,
    "deletions": 0,
    "changes": 1,
    "blob_url": "https://github.com/murblanc/lucene-solr/blob/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/server/solr/solr.xml",
    "raw_url": "https://github.com/murblanc/lucene-solr/raw/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/server/solr/solr.xml",
    "contents_url": "https://api.github.com/repos/murblanc/lucene-solr/contents/solr/server/solr/solr.xml?ref=d34680c2f671cf9f629a8837f3c658d362e708c6",
    "patch": "@@ -46,6 +46,7 @@\n     <str name=\"zkCredentialsProvider\">${zkCredentialsProvider:org.apache.solr.common.cloud.DefaultZkCredentialsProvider}</str>\n     <str name=\"zkACLProvider\">${zkACLProvider:org.apache.solr.common.cloud.DefaultZkACLProvider}</str>\n     <bool name=\"distributedClusterStateUpdates\">${distributedClusterStateUpdates:false}</bool>\n+    <bool name=\"distributedCollectionConfigSetExecution\">${distributedCollectionConfigSetExecution:false}</bool>\n \n   </solrcloud>\n "
  },
  {
    "sha": "e80c0c5ea811c351c7564d1475e34cb347854f65",
    "filename": "solr/solrj/src/java/org/apache/solr/common/params/CollectionParams.java",
    "status": "modified",
    "additions": 4,
    "deletions": 0,
    "changes": 4,
    "blob_url": "https://github.com/murblanc/lucene-solr/blob/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/solrj/src/java/org/apache/solr/common/params/CollectionParams.java",
    "raw_url": "https://github.com/murblanc/lucene-solr/raw/d34680c2f671cf9f629a8837f3c658d362e708c6/solr/solrj/src/java/org/apache/solr/common/params/CollectionParams.java",
    "contents_url": "https://api.github.com/repos/murblanc/lucene-solr/contents/solr/solrj/src/java/org/apache/solr/common/params/CollectionParams.java?ref=d34680c2f671cf9f629a8837f3c658d362e708c6",
    "patch": "@@ -157,6 +157,10 @@ public boolean isEqual(String s) {\n     public String toLower() {\n       return lowerName;\n     }\n+\n+    public String toString() {\n+      return lowerName;\n+    }\n   }\n \n   Map<String, CollectionAction> actions = Collections.unmodifiableMap("
  }
]
