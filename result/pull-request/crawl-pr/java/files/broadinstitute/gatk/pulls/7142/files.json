[
  {
    "sha": "40489e35f7993ad5fc9ddacd497e82aed883fced",
    "filename": "src/main/java/org/broadinstitute/hellbender/engine/FeatureDataSource.java",
    "status": "modified",
    "additions": 14,
    "deletions": 4,
    "changes": 18,
    "blob_url": "https://github.com/broadinstitute/gatk/blob/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/engine/FeatureDataSource.java",
    "raw_url": "https://github.com/broadinstitute/gatk/raw/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/engine/FeatureDataSource.java",
    "contents_url": "https://api.github.com/repos/broadinstitute/gatk/contents/src/main/java/org/broadinstitute/hellbender/engine/FeatureDataSource.java?ref=304d73f3bd34f59732b9e3d7676b373d0e688f9b",
    "patch": "@@ -20,10 +20,13 @@\n import org.broadinstitute.hellbender.tools.IndexFeatureFile;\n import org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBConstants;\n import org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBOptions;\n+import org.broadinstitute.hellbender.utils.CollatingInterval;\n import org.broadinstitute.hellbender.utils.IndexUtils;\n import org.broadinstitute.hellbender.utils.SimpleInterval;\n import org.broadinstitute.hellbender.utils.Utils;\n import org.broadinstitute.hellbender.utils.gcs.BucketUtils;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Reader;\n import org.broadinstitute.hellbender.utils.io.IOUtils;\n import org.genomicsdb.model.GenomicsDBExportConfiguration;\n import org.genomicsdb.reader.GenomicsDBFeatureReader;\n@@ -42,6 +45,7 @@\n import java.util.function.Function;\n \n import static org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBUtils.createExportConfiguration;\n+import static org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.BCI_FILE_EXTENSION;\n \n /**\n  * Enables traversals and queries over sources of Features, which are metadata associated with a location\n@@ -286,12 +290,13 @@ public FeatureDataSource(final FeatureInput<T> featureInput, final int queryLook\n                 BucketUtils.getPrefetchingWrapper(cloudIndexPrefetchBuffer),\n                 genomicsDBOptions);\n \n-        if (IOUtils.isGenomicsDBPath(featureInput)) {\n+        if (IOUtils.isGenomicsDBPath(featureInput) ||\n+                featureInput.getFeaturePath().endsWith(BCI_FILE_EXTENSION)) {\n             //genomics db uri's have no associated index file to read from, but they do support random access\n             this.hasIndex = false;\n             this.supportsRandomAccess = true;\n         } else if (featureReader instanceof AbstractFeatureReader) {\n-            this.hasIndex = ((AbstractFeatureReader<T, ?>) featureReader).hasIndex();\n+            this.hasIndex = ((AbstractFeatureReader<T, ?>)featureReader).hasIndex();\n             this.supportsRandomAccess = hasIndex;\n         } else {\n             throw new GATKException(\"Found a feature input that was neither GenomicsDB or a Tribble AbstractFeatureReader.  Input was \" + featureInput.toString() + \".\");\n@@ -312,7 +317,7 @@ final void printCacheStats() {\n         queryCache.printCacheStatistics( getName() );\n     }\n \n-    @SuppressWarnings(\"unchecked\")\n+    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n     private static <T extends Feature> FeatureReader<T> getFeatureReader(final FeatureInput<T> featureInput, final Class<? extends Feature> targetFeatureType,\n                                                                          final Function<SeekableByteChannel, SeekableByteChannel> cloudWrapper,\n                                                                          final Function<SeekableByteChannel, SeekableByteChannel> cloudIndexWrapper,\n@@ -334,6 +339,9 @@ final void printCacheStats() {\n             }\n         } else {\n             final FeatureCodec<T, ?> codec = getCodecForFeatureInput(featureInput, targetFeatureType);\n+            if ( featureInput.getFeaturePath().endsWith(BCI_FILE_EXTENSION) ) {\n+                return new Reader(featureInput, codec);\n+            }\n             return getTribbleFeatureReader(featureInput, codec, cloudWrapper, cloudIndexWrapper);\n         }\n     }\n@@ -422,7 +430,9 @@ final void printCacheStats() {\n     public SAMSequenceDictionary getSequenceDictionary() {\n         SAMSequenceDictionary dict = null;\n         final Object header = getHeader();\n-        if (header instanceof VCFHeader) {\n+        if ( header instanceof BlockCompressedIntervalStream.Header ) {\n+            dict = ((BlockCompressedIntervalStream.Header)header).getDictionary();\n+        } else if (header instanceof VCFHeader) {\n             dict = ((VCFHeader) header).getSequenceDictionary();\n         }\n         if (dict != null && !dict.isEmpty()) {"
  },
  {
    "sha": "5ea7dec4cdfd62372748caf69b405f2378d6b344",
    "filename": "src/main/java/org/broadinstitute/hellbender/engine/FeatureManager.java",
    "status": "modified",
    "additions": 16,
    "deletions": 0,
    "changes": 16,
    "blob_url": "https://github.com/broadinstitute/gatk/blob/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/engine/FeatureManager.java",
    "raw_url": "https://github.com/broadinstitute/gatk/raw/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/engine/FeatureManager.java",
    "contents_url": "https://api.github.com/repos/broadinstitute/gatk/contents/src/main/java/org/broadinstitute/hellbender/engine/FeatureManager.java?ref=304d73f3bd34f59732b9e3d7676b373d0e688f9b",
    "patch": "@@ -375,6 +375,22 @@ public boolean isEmpty() {\n         return dataSource.iterator();\n     }\n \n+    /**\n+     * As above, but takes an optional list of intervals to examine.\n+     * @param featureDescriptor FeatureInput to scan\n+     * @param intervals The userIntervals to examine (may be null)\n+     * @param <T> Feature type\n+     * @return An iterator over the Features\n+     */\n+    public <T extends Feature> Iterator<T> getFeatureIterator( final FeatureInput<T> featureDescriptor,\n+                                                               final List<SimpleInterval> intervals ) {\n+        final FeatureDataSource<T> dataSource = lookupDataSource(featureDescriptor);\n+        if ( intervals != null ) {\n+            dataSource.setIntervalsForTraversal(intervals);\n+        }\n+        return dataSource.iterator();\n+    }\n+\n     /**\n      * Get the header associated with a particular FeatureInput\n      *"
  },
  {
    "sha": "432f829b7a21098e1232cf2c441820cf61459342",
    "filename": "src/main/java/org/broadinstitute/hellbender/engine/FeatureWalker.java",
    "status": "modified",
    "additions": 16,
    "deletions": 23,
    "changes": 39,
    "blob_url": "https://github.com/broadinstitute/gatk/blob/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/engine/FeatureWalker.java",
    "raw_url": "https://github.com/broadinstitute/gatk/raw/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/engine/FeatureWalker.java",
    "contents_url": "https://api.github.com/repos/broadinstitute/gatk/contents/src/main/java/org/broadinstitute/hellbender/engine/FeatureWalker.java?ref=304d73f3bd34f59732b9e3d7676b373d0e688f9b",
    "patch": "@@ -5,7 +5,8 @@\n import org.broadinstitute.hellbender.engine.filters.CountingReadFilter;\n import org.broadinstitute.hellbender.exceptions.UserException;\n import org.broadinstitute.hellbender.utils.SimpleInterval;\n-import org.broadinstitute.hellbender.utils.Utils;\n+\n+import java.util.Iterator;\n \n /**\n  * A FeatureWalker is a tool that processes a {@link Feature} at a time from a source of Features, with\n@@ -20,7 +21,7 @@\n  */\n public abstract class FeatureWalker<F extends Feature> extends WalkerBase {\n \n-    private FeatureDataSource<F> drivingFeatures;\n+    private FeatureInput<F> drivingFeaturesInput;\n     private Object header;\n \n     @Override\n@@ -46,20 +47,13 @@ void initializeFeatures() {\n     @Override\n     protected final void onStartup() {\n         super.onStartup();\n-        // set the intervals for the feature here, because they are not initialized when initialize features is set\n-        if ( hasUserSuppliedIntervals() ) {\n-            drivingFeatures.setIntervalsForTraversal(userIntervals);\n-        }\n     }\n \n-    @SuppressWarnings(\"unchecked\")\n     private void initializeDrivingFeatures() {\n         final GATKPath drivingPath = getDrivingFeaturePath();\n         final FeatureCodec<? extends Feature, ?> codec = FeatureManager.getCodecForFile(drivingPath.toPath());\n         if (isAcceptableFeatureType(codec.getFeatureType())) {\n-            drivingFeatures = new FeatureDataSource<>(new FeatureInput<>(drivingPath), FeatureDataSource.DEFAULT_QUERY_LOOKAHEAD_BASES, null, cloudPrefetchBuffer, cloudIndexPrefetchBuffer, referenceArguments.getReferencePath());\n-\n-            final FeatureInput<F> drivingFeaturesInput = new FeatureInput<>(drivingPath, \"drivingFeatureFile\");\n+            drivingFeaturesInput = new FeatureInput<>(drivingPath, \"drivingFeatureFile\");\n             features.addToFeatureSources(0, drivingFeaturesInput, codec.getFeatureType(), cloudPrefetchBuffer, cloudIndexPrefetchBuffer,\n                                          referenceArguments.getReferencePath());\n             header = getHeaderForFeatures(drivingFeaturesInput);\n@@ -84,16 +78,19 @@ private void initializeDrivingFeatures() {\n      */\n     @Override\n     public void traverse() {\n-        CountingReadFilter readFilter = makeReadFilter();\n+        final CountingReadFilter readFilter = makeReadFilter();\n         // Process each feature in the input stream.\n-        Utils.stream(drivingFeatures).forEach(feature -> {\n-                    final SimpleInterval featureInterval = makeFeatureInterval(feature);\n-                    apply(feature,\n-                            new ReadsContext(reads, featureInterval, readFilter),\n-                            new ReferenceContext(reference, featureInterval),\n-                            new FeatureContext(features, featureInterval));\n-                    progressMeter.update(feature);\n-                });\n+        final Iterator<F> featureItr =\n+                features.getFeatureIterator(drivingFeaturesInput, userIntervals);\n+        while ( featureItr.hasNext() ) {\n+            final F feature = featureItr.next();\n+            final SimpleInterval featureInterval = makeFeatureInterval(feature);\n+            apply(feature,\n+                  new ReadsContext(reads, featureInterval, readFilter),\n+                  new ReferenceContext(reference, featureInterval),\n+                  new FeatureContext(features, featureInterval));\n+            progressMeter.update(feature);\n+        }\n     }\n \n     /**\n@@ -136,10 +133,6 @@ public void traverse() {\n     @Override\n     protected final void onShutdown() {\n         super.onShutdown();\n-\n-        if ( drivingFeatures != null ) {\n-            drivingFeatures.close();\n-        }\n     }\n \n     /**"
  },
  {
    "sha": "724ce5151a0940a3f37710cfa2684c53e3820289",
    "filename": "src/main/java/org/broadinstitute/hellbender/engine/GATKTool.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/broadinstitute/gatk/blob/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/engine/GATKTool.java",
    "raw_url": "https://github.com/broadinstitute/gatk/raw/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/engine/GATKTool.java",
    "contents_url": "https://api.github.com/repos/broadinstitute/gatk/contents/src/main/java/org/broadinstitute/hellbender/engine/GATKTool.java?ref=304d73f3bd34f59732b9e3d7676b373d0e688f9b",
    "patch": "@@ -649,7 +649,7 @@ public SAMSequenceDictionary getBestAvailableSequenceDictionary() {\n         } else if (hasReads()){\n             return reads.getSequenceDictionary();\n         } else if (hasFeatures()){\n-            final List<SAMSequenceDictionary> dictionaries = features.getVariantSequenceDictionaries();\n+            final List<SAMSequenceDictionary> dictionaries = features.getAllSequenceDictionaries();\n             //If there is just one, it clearly is the best. Otherwise, none is best.\n             if (dictionaries.size() == 1){\n                 return dictionaries.get(0);"
  },
  {
    "sha": "5bdfb27d8de5304a8e90084a5f7701c3ad8f9916",
    "filename": "src/main/java/org/broadinstitute/hellbender/tools/sv/BafEvidence.java",
    "status": "modified",
    "additions": 2,
    "deletions": 1,
    "changes": 3,
    "blob_url": "https://github.com/broadinstitute/gatk/blob/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/tools/sv/BafEvidence.java",
    "raw_url": "https://github.com/broadinstitute/gatk/raw/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/tools/sv/BafEvidence.java",
    "contents_url": "https://api.github.com/repos/broadinstitute/gatk/contents/src/main/java/org/broadinstitute/hellbender/tools/sv/BafEvidence.java?ref=304d73f3bd34f59732b9e3d7676b373d0e688f9b",
    "patch": "@@ -6,12 +6,13 @@\n import java.util.Objects;\n \n public final class BafEvidence implements Feature {\n-\n     final String sample;\n     final String contig;\n     final int position;\n     final double value;\n \n+    public final static String BCI_VERSION = \"1.0\";\n+\n     public BafEvidence(final String sample, final String contig, final int position, final double value) {\n         Utils.nonNull(sample);\n         Utils.nonNull(contig);"
  },
  {
    "sha": "3e9c046273ca27b00436c710946d7cf5a569e87b",
    "filename": "src/main/java/org/broadinstitute/hellbender/tools/sv/DepthEvidence.java",
    "status": "modified",
    "additions": 2,
    "deletions": 0,
    "changes": 2,
    "blob_url": "https://github.com/broadinstitute/gatk/blob/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/tools/sv/DepthEvidence.java",
    "raw_url": "https://github.com/broadinstitute/gatk/raw/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/tools/sv/DepthEvidence.java",
    "contents_url": "https://api.github.com/repos/broadinstitute/gatk/contents/src/main/java/org/broadinstitute/hellbender/tools/sv/DepthEvidence.java?ref=304d73f3bd34f59732b9e3d7676b373d0e688f9b",
    "patch": "@@ -13,6 +13,8 @@\n     final int end;\n     final int[] counts;\n \n+    public static final String BCI_VERSION = \"1.0\";\n+\n     public DepthEvidence(final String contig, int start, final int end, final int[] counts) {\n         Utils.nonNull(contig);\n         Utils.nonNull(counts);"
  },
  {
    "sha": "ae9e40671df09fbb7d58787163a1202d0ed91c9a",
    "filename": "src/main/java/org/broadinstitute/hellbender/tools/sv/DiscordantPairEvidence.java",
    "status": "modified",
    "additions": 2,
    "deletions": 0,
    "changes": 2,
    "blob_url": "https://github.com/broadinstitute/gatk/blob/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/tools/sv/DiscordantPairEvidence.java",
    "raw_url": "https://github.com/broadinstitute/gatk/raw/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/tools/sv/DiscordantPairEvidence.java",
    "contents_url": "https://api.github.com/repos/broadinstitute/gatk/contents/src/main/java/org/broadinstitute/hellbender/tools/sv/DiscordantPairEvidence.java?ref=304d73f3bd34f59732b9e3d7676b373d0e688f9b",
    "patch": "@@ -16,6 +16,8 @@\n     final boolean startStrand;\n     final boolean endStrand;\n \n+    public final static String BCI_VERSION = \"1.0\";\n+\n     public DiscordantPairEvidence(final String sample, final String startContig, final int start, final boolean startStrand,\n                                   final String endContig, final int end, final boolean endStrand) {\n         Utils.nonNull(sample);"
  },
  {
    "sha": "de7c27b04388dbd8fa6705c1e36bc3eb7b2be359",
    "filename": "src/main/java/org/broadinstitute/hellbender/tools/sv/PrintSVEvidence.java",
    "status": "modified",
    "additions": 99,
    "deletions": 54,
    "changes": 153,
    "blob_url": "https://github.com/broadinstitute/gatk/blob/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/tools/sv/PrintSVEvidence.java",
    "raw_url": "https://github.com/broadinstitute/gatk/raw/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/tools/sv/PrintSVEvidence.java",
    "contents_url": "https://api.github.com/repos/broadinstitute/gatk/contents/src/main/java/org/broadinstitute/hellbender/tools/sv/PrintSVEvidence.java?ref=304d73f3bd34f59732b9e3d7676b373d0e688f9b",
    "patch": "@@ -10,12 +10,16 @@\n import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n import org.broadinstitute.hellbender.engine.*;\n import org.broadinstitute.hellbender.exceptions.UserException;\n-import org.broadinstitute.hellbender.utils.codecs.BafEvidenceCodec;\n-import org.broadinstitute.hellbender.utils.codecs.DepthEvidenceCodec;\n-import org.broadinstitute.hellbender.utils.codecs.DiscordantPairEvidenceCodec;\n-import org.broadinstitute.hellbender.utils.codecs.SplitReadEvidenceCodec;\n+import org.broadinstitute.hellbender.utils.codecs.*;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Header;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Writer;\n import org.broadinstitute.hellbender.utils.io.FeatureOutputStream;\n \n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.function.Function;\n+\n /**\n  * Prints SV evidence records. Can be used with -L to retrieve records on a set of intervals. Supports streaming input\n  * from GCS buckets.\n@@ -69,14 +73,15 @@\n                     + SplitReadEvidenceCodec.FORMAT_SUFFIX + \"', '\"\n                     + DiscordantPairEvidenceCodec.FORMAT_SUFFIX + \"', '\"\n                     + BafEvidenceCodec.FORMAT_SUFFIX + \"', or '\"\n-                    + DepthEvidenceCodec.FORMAT_SUFFIX + \"' (may be gzipped).\",\n+                    + DepthEvidenceCodec.FORMAT_SUFFIX + \"' (may be gzipped). \"\n+                    + \"Can also handle bci rather than txt files.\",\n             fullName = EVIDENCE_FILE_NAME\n     )\n     private GATKPath inputFilePath;\n \n     @Argument(\n             doc = \"Output file with an evidence extension matching the input. Will be indexed if it has a \" +\n-                    \"block-compressed extension (e.g. '.gz').\",\n+                    \"block-compressed extension (e.g. '.gz' or '.bci').\",\n             fullName = StandardArgumentDefinitions.OUTPUT_LONG_NAME,\n             shortName = StandardArgumentDefinitions.OUTPUT_SHORT_NAME\n     )\n@@ -89,13 +94,28 @@\n     )\n     private int compressionLevel = 4;\n \n-    private FeatureOutputStream<DiscordantPairEvidence> peStream;\n-    private FeatureOutputStream<SplitReadEvidence> srStream;\n-    private FeatureOutputStream<BafEvidence> bafStream;\n-    private FeatureOutputStream<DepthEvidence> rdStream;\n+    @Argument(doc = \"List of sample names\", fullName = \"sample-names\", optional = true)\n+    private List<String> sampleNames = new ArrayList<>();\n+\n+    @SuppressWarnings(\"rawtypes\")\n+    private FeatureOutputStream foStream;\n+    @SuppressWarnings(\"rawtypes\")\n+    private Writer bciWriter;\n     private FeatureCodec<? extends Feature, ?> featureCodec;\n     private Class<? extends Feature> evidenceClass;\n \n+    private static final List<FeatureCodec<? extends Feature, ?>> outputCodecs = new ArrayList<>(8);\n+    static {\n+        outputCodecs.add(new BafEvidenceCodec());\n+        outputCodecs.add(new DepthEvidenceCodec());\n+        outputCodecs.add(new DiscordantPairEvidenceCodec());\n+        outputCodecs.add(new SplitReadEvidenceCodec());\n+        outputCodecs.add(new BafEvidenceBCICodec());\n+        outputCodecs.add(new DepthEvidenceBCICodec());\n+        outputCodecs.add(new DiscordantPairEvidenceBCICodec());\n+        outputCodecs.add(new SplitReadEvidenceBCICodec());\n+    }\n+\n     @Override\n     protected boolean isAcceptableFeatureType(final Class<? extends Feature> featureType) {\n         return featureType.equals(BafEvidence.class) || featureType.equals(DepthEvidence.class)\n@@ -113,73 +133,98 @@ public void onTraversalStart() {\n         featureCodec = FeatureManager.getCodecForFile(inputFilePath.toPath());\n         evidenceClass = featureCodec.getFeatureType();\n         initializeOutput();\n-        writeHeader();\n+    }\n+\n+    private static FeatureCodec<? extends Feature, ?> findOutputCodec( final GATKPath outputFilePath ) {\n+        final String outputFileName = outputFilePath.toString();\n+        for ( final FeatureCodec<? extends Feature, ?> codec : outputCodecs ) {\n+            if ( codec.canDecode(outputFileName) ) {\n+                return codec;\n+            }\n+        }\n+        throw new UserException(\"no codec found for path \" + outputFileName);\n     }\n \n     private void initializeOutput() {\n-        if (evidenceClass.equals(DiscordantPairEvidence.class)) {\n-            peStream = new FeatureOutputStream<>(outputFilePath, featureCodec, DiscordantPairEvidenceCodec::encode,\n-                    getBestAvailableSequenceDictionary(), compressionLevel);\n-        } else if (evidenceClass.equals(SplitReadEvidence.class)) {\n-            srStream = new FeatureOutputStream<>(outputFilePath, featureCodec, SplitReadEvidenceCodec::encode,\n-                    getBestAvailableSequenceDictionary(), compressionLevel);\n-        } else if (evidenceClass.equals(BafEvidence.class)) {\n-            bafStream = new FeatureOutputStream<>(outputFilePath, featureCodec, BafEvidenceCodec::encode,\n-                    getBestAvailableSequenceDictionary(), compressionLevel);\n-        } else if (evidenceClass.equals(DepthEvidence.class)) {\n-            rdStream = new FeatureOutputStream<>(outputFilePath, featureCodec, DepthEvidenceCodec::encode,\n-                    getBestAvailableSequenceDictionary(), compressionLevel);\n+        final FeatureCodec<? extends Feature, ?> outputCodec = findOutputCodec(outputFilePath);\n+        final Class<? extends Feature> outputClass = outputCodec.getFeatureType();\n+        if ( evidenceClass != outputClass ) {\n+            throw new UserException(\"input file contains \" + evidenceClass.getSimpleName() +\n+                    \" but output file would be expected to contain \" + outputClass.getSimpleName());\n+        }\n+\n+        if ( outputCodec instanceof AbstractBCICodec ) {\n+            final AbstractBCICodec<? extends Feature> bciCodec =\n+                    (AbstractBCICodec<? extends Feature>)outputCodec;\n+            final Object headerObj = getDrivingFeaturesHeader();\n+            final Header header;\n+            if ( headerObj instanceof Header ) {\n+                header = (Header)headerObj;\n+            } else {\n+                header = new Header(outputClass.getSimpleName(), bciCodec.getVersion(),\n+                                   getBestAvailableSequenceDictionary(), sampleNames);\n+            }\n+            bciWriter = new Writer<>(outputFilePath, header, bciCodec::encode, compressionLevel);\n         } else {\n-            throw new UserException.BadInput(\"Unsupported evidence type: \" + evidenceClass.getSimpleName());\n+            if ( evidenceClass.equals(DiscordantPairEvidence.class) ) {\n+                initializeFOStream(new DiscordantPairEvidenceCodec(), DiscordantPairEvidenceCodec::encode);\n+            } else if ( evidenceClass.equals(SplitReadEvidence.class) ) {\n+                initializeFOStream(new SplitReadEvidenceCodec(), SplitReadEvidenceCodec::encode);\n+            } else if ( evidenceClass.equals(BafEvidence.class) ) {\n+                initializeFOStream(new BafEvidenceCodec(), BafEvidenceCodec::encode);\n+            } else if ( evidenceClass.equals(DepthEvidence.class) ) {\n+                initializeFOStream(new DepthEvidenceCodec(), DepthEvidenceCodec::encode);\n+                writeDepthEvidenceHeader();\n+            } else {\n+                throw new UserException.BadInput(\"Unsupported evidence type: \" + evidenceClass.getSimpleName());\n+            }\n         }\n     }\n \n-    private void writeHeader() {\n+    private <F extends Feature> void initializeFOStream( final FeatureCodec<F, ?> codec,\n+                                                         final Function<F, String> encoder ) {\n+        foStream = new FeatureOutputStream<>(outputFilePath, codec, encoder,\n+                getBestAvailableSequenceDictionary(), compressionLevel);\n+    }\n+\n+    private void writeDepthEvidenceHeader() {\n         final Object header = getDrivingFeaturesHeader();\n-        if (header != null) {\n-            if (header instanceof String) {\n-                if (peStream != null) {\n-                    peStream.writeHeader((String) header);\n-                } else if (srStream != null) {\n-                    srStream.writeHeader((String) header);\n-                } else if (bafStream != null) {\n-                    bafStream.writeHeader((String) header);\n-                } else {\n-                    rdStream.writeHeader((String) header);\n-                }\n-            } else {\n-                throw new IllegalArgumentException(\"Expected header object of type \" + String.class.getSimpleName());\n+        if ( header instanceof Header ) {\n+            final StringBuilder sb = new StringBuilder(\"#Chr\\tStart\\tEnd\");\n+            final Header hdr = (Header)header;\n+            for ( final String sampleName : hdr.getSampleNames() ) {\n+                sb.append('\\t').append(sampleName);\n             }\n+            foStream.writeHeader(sb.toString());\n+        } else if (header instanceof String) {\n+            foStream.writeHeader((String) header);\n+        } else {\n+            throw new IllegalArgumentException(\"Expected header object of type String\");\n         }\n     }\n \n     @Override\n+    @SuppressWarnings(\"unchecked\")\n     public void apply(final Feature feature,\n                       final ReadsContext readsContext,\n                       final ReferenceContext referenceContext,\n                       final FeatureContext featureContext) {\n-        if (peStream != null) {\n-            peStream.add((DiscordantPairEvidence) feature);\n-        } else if (srStream != null) {\n-            srStream.add((SplitReadEvidence) feature);\n-        } else if (bafStream != null) {\n-            bafStream.add((BafEvidence) feature);\n-        } else {\n-            rdStream.add((DepthEvidence) feature);\n+        if ( foStream != null ) {\n+            foStream.add(feature);\n+        }\n+        if ( bciWriter != null ) {\n+            bciWriter.write(feature);\n         }\n     }\n \n     @Override\n     public Object onTraversalSuccess() {\n         super.onTraversalSuccess();\n-        if (peStream != null) {\n-            peStream.close();\n-        } else if (srStream != null) {\n-            srStream.close();\n-        } else if (bafStream != null) {\n-            bafStream.close();\n-        } else {\n-            rdStream.close();\n+        if ( foStream != null ) {\n+            foStream.close();\n+        }\n+        if ( bciWriter != null ) {\n+            bciWriter.close();\n         }\n         return null;\n     }"
  },
  {
    "sha": "124cca34ed5e9052e8054310ce32164a366b417e",
    "filename": "src/main/java/org/broadinstitute/hellbender/tools/sv/SplitReadEvidence.java",
    "status": "modified",
    "additions": 2,
    "deletions": 0,
    "changes": 2,
    "blob_url": "https://github.com/broadinstitute/gatk/blob/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/tools/sv/SplitReadEvidence.java",
    "raw_url": "https://github.com/broadinstitute/gatk/raw/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/tools/sv/SplitReadEvidence.java",
    "contents_url": "https://api.github.com/repos/broadinstitute/gatk/contents/src/main/java/org/broadinstitute/hellbender/tools/sv/SplitReadEvidence.java?ref=304d73f3bd34f59732b9e3d7676b373d0e688f9b",
    "patch": "@@ -13,6 +13,8 @@\n     final int count;\n     final boolean strand;\n \n+    public final static String BCI_VERSION = \"1.0\";\n+\n     public SplitReadEvidence(final String sample, final String contig, final int position, final int count, final boolean strand) {\n         Utils.nonNull(sample);\n         Utils.nonNull(contig);"
  },
  {
    "sha": "4d4f864f8cb4995042ab29c13703966d3074b32d",
    "filename": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/PairedEndAndSplitReadEvidenceCollection.java",
    "status": "modified",
    "additions": 309,
    "deletions": 37,
    "changes": 346,
    "blob_url": "https://github.com/broadinstitute/gatk/blob/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/PairedEndAndSplitReadEvidenceCollection.java",
    "raw_url": "https://github.com/broadinstitute/gatk/raw/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/PairedEndAndSplitReadEvidenceCollection.java",
    "contents_url": "https://api.github.com/repos/broadinstitute/gatk/contents/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/PairedEndAndSplitReadEvidenceCollection.java?ref=304d73f3bd34f59732b9e3d7676b373d0e688f9b",
    "patch": "@@ -4,26 +4,33 @@\n import htsjdk.samtools.CigarElement;\n import htsjdk.samtools.CigarOperator;\n import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.util.BlockCompressedOutputStream;\n+import htsjdk.samtools.util.Locatable;\n+import htsjdk.tribble.Feature;\n+import htsjdk.variant.variantcontext.VariantContext;\n import org.broadinstitute.barclay.argparser.Argument;\n import org.broadinstitute.barclay.argparser.BetaFeature;\n import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n-import org.broadinstitute.hellbender.engine.FeatureContext;\n-import org.broadinstitute.hellbender.engine.GATKPath;\n-import org.broadinstitute.hellbender.engine.ReadWalker;\n-import org.broadinstitute.hellbender.engine.ReferenceContext;\n+import org.broadinstitute.hellbender.engine.*;\n import org.broadinstitute.hellbender.engine.filters.ReadFilter;\n import org.broadinstitute.hellbender.engine.filters.ReadFilterLibrary;\n-import org.broadinstitute.hellbender.tools.sv.DiscordantPairEvidence;\n-import org.broadinstitute.hellbender.tools.sv.SplitReadEvidence;\n-import org.broadinstitute.hellbender.utils.codecs.DiscordantPairEvidenceCodec;\n-import org.broadinstitute.hellbender.utils.codecs.SplitReadEvidenceCodec;\n-import org.broadinstitute.hellbender.utils.io.FeatureOutputStream;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.utils.CollatingInterval;\n+import org.broadinstitute.hellbender.utils.Nucleotide;\n+import org.broadinstitute.hellbender.utils.codecs.LocusDepthCodec;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Header;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Writer;\n import org.broadinstitute.hellbender.utils.read.GATKRead;\n \n+import java.io.*;\n import java.util.*;\n import java.util.function.Predicate;\n \n+import static org.broadinstitute.hellbender.utils.read.ReadUtils.isBaseInsideAdaptor;\n+\n /**\n  * Creates discordant read pair and split read evidence files for use in the GATK-SV pipeline.\n  *\n@@ -69,30 +76,53 @@\n     public static final String PAIRED_END_FILE_ARGUMENT_LONG_NAME = \"pe-file\";\n     public static final String SPLIT_READ_FILE_ARGUMENT_SHORT_NAME = \"SR\";\n     public static final String SPLIT_READ_FILE_ARGUMENT_LONG_NAME = \"sr-file\";\n+    public static final String ALLELE_COUNT_OUTPUT_ARGUMENT_SHORT_NAME = \"AC\";\n+    public static final String ALLELE_COUNT_OUTPUT_ARGUMENT_LONG_NAME = \"allele-count-file\";\n+    public static final String ALLELE_COUNT_INPUT_ARGUMENT_SHORT_NAME = \"F\";\n+    public static final String ALLELE_COUNT_INPUT_ARGUMENT_LONG_NAME = \"allele-count-vcf\";\n     public static final String SAMPLE_NAME_ARGUMENT_LONG_NAME = \"sample-name\";\n-    public static final String COMPRESSION_LEVEL_ARGUMENT_LONG_NAME = \"compression-level\";\n \n     @Argument(shortName = PAIRED_END_FILE_ARGUMENT_SHORT_NAME, fullName = PAIRED_END_FILE_ARGUMENT_LONG_NAME, doc = \"Output file for paired end evidence\", optional=false)\n-    public GATKPath peFile;\n+    public String peFile;\n \n     @Argument(shortName = SPLIT_READ_FILE_ARGUMENT_SHORT_NAME, fullName = SPLIT_READ_FILE_ARGUMENT_LONG_NAME, doc = \"Output file for split read evidence\", optional=false)\n-    public GATKPath srFile;\n+    public String srFile;\n+\n+    @Argument(shortName = ALLELE_COUNT_OUTPUT_ARGUMENT_SHORT_NAME,\n+            fullName = ALLELE_COUNT_OUTPUT_ARGUMENT_LONG_NAME,\n+            doc = \"Output file for allele counts\",\n+            optional = true)\n+    public String alleleCountOutputFilename;\n+\n+    @Argument(shortName = ALLELE_COUNT_INPUT_ARGUMENT_SHORT_NAME,\n+            fullName = ALLELE_COUNT_INPUT_ARGUMENT_LONG_NAME,\n+            doc = \"Input VCF of SNPs marking loci for allele counts\",\n+            optional = true)\n+    public String alleleCountInputFilename;\n+\n+    @Argument(fullName = \"allele-count-min-mapq\",\n+            doc = \"minimum mapping quality for read to be allele-counted\",\n+            optional = true)\n+    public int minMapQ = 30;\n+\n+    @Argument(fullName = \"allele-count-min-baseq\",\n+            doc = \"minimum base call quality for SNP to be allele-counted\",\n+            optional = true)\n+    public int minQ = 20;\n \n     @Argument(fullName = SAMPLE_NAME_ARGUMENT_LONG_NAME, doc = \"Sample name\")\n     String sampleName = null;\n \n-    @Argument(fullName = COMPRESSION_LEVEL_ARGUMENT_LONG_NAME, doc = \"Output compression level\")\n-    int compressionLevel = 4;\n-\n     final Set<String> observedDiscordantNames = new HashSet<>();\n     final PriorityQueue<SplitPos> splitPosBuffer = new PriorityQueue<>(new SplitPosComparator());\n     final List<DiscordantRead> discordantPairs = new ArrayList<>();\n \n     int currentDiscordantPosition = -1;\n     String currentChrom = null;\n \n-    private FeatureOutputStream<DiscordantPairEvidence> peWriter;\n-    private FeatureOutputStream<SplitReadEvidence> srWriter;\n+    private BufferedWriter peWriter;\n+    private BufferedWriter srWriter;\n+    private AlleleCounter alleleCounter;\n \n     private SAMSequenceDictionary sequenceDictionary;\n \n@@ -105,30 +135,53 @@ public boolean requiresReads() {\n     @Override\n     public void onTraversalStart() {\n         super.onTraversalStart();\n+\n+        peWriter = createOutputFile(peFile);\n+        srWriter = createOutputFile(srFile);\n+\n         sequenceDictionary = getBestAvailableSequenceDictionary();\n-        peWriter = new FeatureOutputStream<>(peFile, new DiscordantPairEvidenceCodec(), DiscordantPairEvidenceCodec::encode, sequenceDictionary, compressionLevel);\n-        srWriter = new FeatureOutputStream<>(srFile, new SplitReadEvidenceCodec(), SplitReadEvidenceCodec::encode, sequenceDictionary, compressionLevel);\n+        if ( alleleCountInputFilename != null && alleleCountOutputFilename != null ) {\n+            alleleCounter = new AlleleCounter(sequenceDictionary,\n+                                                alleleCountInputFilename, alleleCountOutputFilename,\n+                                                minMapQ, minQ);\n+        }\n     }\n \n     @Override\n     public List<ReadFilter> getDefaultReadFilters() {\n         final List<ReadFilter> readFilters = new ArrayList<>(super.getDefaultReadFilters());\n-        readFilters.add(ReadFilterLibrary.MATE_UNMAPPED_AND_UNMAPPED_READ_FILTER);\n+        readFilters.add(ReadFilterLibrary.MAPPED);\n         readFilters.add(ReadFilterLibrary.NOT_DUPLICATE);\n-        readFilters.add(ReadFilterLibrary.NOT_SECONDARY_ALIGNMENT);\n-        readFilters.add(ReadFilterLibrary.NOT_SUPPLEMENTARY_ALIGNMENT);\n         return readFilters;\n     }\n \n     @Override\n     public void apply(final GATKRead read, final ReferenceContext referenceContext, final FeatureContext featureContext) {\n-        if (isSoftClipped(read)) {\n-            countSplitRead(read, splitPosBuffer, srWriter);\n+        if ( !(read.isPaired() && read.mateIsUnmapped()) &&\n+                !read.isSupplementaryAlignment() &&\n+                !read.isSecondaryAlignment() ) {\n+            if ( isSoftClipped(read) ) {\n+                countSplitRead(read, splitPosBuffer, srWriter);\n+            }\n+\n+            if ( !read.isProperlyPaired() ) {\n+                reportDiscordantReadPair(read);\n+            }\n+        }\n+\n+        if ( alleleCounter != null ) {\n+            alleleCounter.apply(read);\n         }\n+    }\n \n-        if (! read.isProperlyPaired()) {\n-            reportDiscordantReadPair(read);\n+    private static BufferedWriter createOutputFile( final String fileName ) {\n+        final GATKPath path = new GATKPath(fileName);\n+        if ( fileName.endsWith(\".gz\") ) {\n+            return new BufferedWriter(\n+                    new OutputStreamWriter(\n+                            new BlockCompressedOutputStream(path.getOutputStream(), path.toPath(), 6)));\n         }\n+        return new BufferedWriter(new OutputStreamWriter(path.getOutputStream()));\n     }\n \n     private void reportDiscordantReadPair(final GATKRead read) {\n@@ -176,19 +229,23 @@ private void flushDiscordantReadPairs() {\n     }\n \n     private void writeDiscordantPair(final DiscordantRead r) {\n-        final boolean strandA = !r.isReadReverseStrand();\n-        final boolean strandB = !r.isMateReverseStrand();\n+        final String strandA = r.isReadReverseStrand() ? \"-\" : \"+\";\n+        final String strandB = r.isMateReverseStrand() ? \"-\" : \"+\";\n \n-        final DiscordantPairEvidence discordantPair = new DiscordantPairEvidence(sampleName, r.getContig(), r.getStart(), strandA, r.getMateContig(), r.getMateStart(), strandB);\n-        peWriter.add(discordantPair);\n+        try {\n+            // subtract 1 from positions to match pysam output\n+            peWriter.write(r.getContig() + \"\\t\" + (r.getStart() - 1) + \"\\t\" + strandA + \"\\t\" + r.getMateContig() + \"\\t\" + (r.getMateStart() - 1) + \"\\t\" + strandB + \"\\t\" + sampleName + \"\\n\");\n+        } catch (IOException e) {\n+            throw new GATKException(\"Could not write to PE file\", e);\n+        }\n     }\n \n     /**\n      * Adds split read information about the current read to the counts in splitCounts. Flushes split read counts to\n      * srWriter if necessary.\n      */\n     @VisibleForTesting\n-    public void countSplitRead(final GATKRead read, final PriorityQueue<SplitPos> splitCounts, final FeatureOutputStream<SplitReadEvidence> srWriter) {\n+    public void countSplitRead(final GATKRead read, final PriorityQueue<SplitPos> splitCounts, final BufferedWriter srWriter) {\n         final SplitPos splitPosition = getSplitPosition(read);\n         final int readStart = read.getStart();\n         if (splitPosition.direction == POSITION.MIDDLE) {\n@@ -206,7 +263,7 @@ public void countSplitRead(final GATKRead read, final PriorityQueue<SplitPos> sp\n         splitCounts.add(splitPosition);\n     }\n \n-    private void flushSplitCounts(final Predicate<SplitPos> flushablePosition, final FeatureOutputStream<SplitReadEvidence> srWriter, final PriorityQueue<SplitPos> splitCounts) {\n+    private void flushSplitCounts(final Predicate<SplitPos> flushablePosition, final BufferedWriter srWriter, final PriorityQueue<SplitPos> splitCounts) {\n \n         while (splitCounts.size() > 0 && flushablePosition.test(splitCounts.peek())) {\n             SplitPos pos = splitCounts.poll();\n@@ -215,8 +272,11 @@ private void flushSplitCounts(final Predicate<SplitPos> flushablePosition, final\n                 countAtPos++;\n                 splitCounts.poll();\n             }\n-            final SplitReadEvidence splitRead = new SplitReadEvidence(sampleName, currentChrom, pos.pos, countAtPos, pos.direction.equals(POSITION.RIGHT));\n-            srWriter.add(splitRead);\n+            try {\n+                srWriter.write(currentChrom + \"\\t\" + (pos.pos - 1) + \"\\t\" + pos.direction.getDescription() + \"\\t\" + countAtPos + \"\\t\" + sampleName + \"\\n\");\n+            } catch (IOException e) {\n+                throw new GATKException(\"Could not write to sr file\", e);\n+            }\n         }\n     }\n \n@@ -242,17 +302,20 @@ private boolean isSoftClipped(final GATKRead read) {\n     public Object onTraversalSuccess() {\n         flushSplitCounts(splitPos -> true, srWriter, splitPosBuffer);\n         flushDiscordantReadPairs();\n+        if ( alleleCounter != null ) {\n+            alleleCounter.close();\n+        }\n         return null;\n     }\n \n     @Override\n     public void closeTool() {\n         super.closeTool();\n-        if (peWriter != null) {\n+        try {\n             peWriter.close();\n-        }\n-        if (srWriter != null) {\n             srWriter.close();\n+        } catch (IOException e) {\n+            throw new GATKException(\"error closing output file\", e);\n         }\n     }\n \n@@ -434,4 +497,213 @@ public int compare(final DiscordantRead o1, final DiscordantRead o2) {\n             return internalComparator.compare(o1, o2);\n         }\n     }\n+\n+    @VisibleForTesting\n+    final static class AlleleCounter {\n+        private final SAMSequenceDictionary dict;\n+        private final Writer<LocusDepth> writer;\n+        private final int minMapQ;\n+        private final int minQ;\n+        private final Iterator<VariantContext> snpSourceItr;\n+        private final Deque<LocusDepth> locusDepthQueue;\n+\n+        public AlleleCounter( final SAMSequenceDictionary dict,\n+                              final String inputFilename,\n+                              final String outputFilename,\n+                              final int minMapQ,\n+                              final int minQ ) {\n+            this.dict = dict;\n+            final Header header =\n+                    new Header(LocusDepth.class.getSimpleName(), LocusDepth.BCI_VERSION,\n+                                dict, Collections.emptyList());\n+            final LocusDepthCodec codec = new LocusDepthCodec();\n+            this.writer = new Writer<>(new GATKPath(outputFilename), header, codec::encode);\n+            this.minMapQ = minMapQ;\n+            this.minQ = minQ;\n+            final FeatureDataSource<VariantContext> snpSource =\n+                    new FeatureDataSource<>(inputFilename);\n+            dict.assertSameDictionary(snpSource.getSequenceDictionary());\n+            this.snpSourceItr = snpSource.iterator();\n+            this.locusDepthQueue = new ArrayDeque<>(100);\n+            readNextLocus();\n+        }\n+\n+        public void apply( final GATKRead read ) {\n+            if ( read.getMappingQuality() < minMapQ || locusDepthQueue.isEmpty() ) {\n+                return;\n+            }\n+\n+            // clean queue of LocusCounts that precede the current read\n+            final CollatingInterval readInterval = new CollatingInterval(dict, read);\n+            while ( true ) {\n+                final LocusDepth locusDepth = locusDepthQueue.getFirst();\n+                final SAMSequenceRecord rec = locusDepth.getSequenceRecord();\n+                if ( readInterval.compareLocus(rec, locusDepth.getStart()) >= 0 ) {\n+                    break;\n+                }\n+                writer.write(locusDepthQueue.removeFirst());\n+                if ( locusDepthQueue.isEmpty() ) {\n+                    if ( !readNextLocus() ) {\n+                        return;\n+                    }\n+                }\n+            }\n+\n+            // make sure that the last LocusCount in the queue occurs after the current read\n+            //  if such a LocusCount is available\n+            while ( true ) {\n+                final LocusDepth locusDepth = locusDepthQueue.getLast();\n+                final SAMSequenceRecord rec = locusDepth.getSequenceRecord();\n+                if ( readInterval.compareLocus(rec, locusDepth.getStart()) > 0 ||\n+                        !readNextLocus() ) {\n+                    break;\n+                }\n+            }\n+\n+            walkReadMatches(read);\n+        }\n+\n+        public void walkReadMatches( final GATKRead read ) {\n+            int opStart = read.getStart();\n+            int readIdx = 0;\n+            final byte[] calls = read.getBasesNoCopy();\n+            final byte[] quals = read.getBaseQualitiesNoCopy();\n+            final SAMSequenceRecord contig = dict.getSequence(read.getContig());\n+            for ( final CigarElement cigEle : read.getCigar().getCigarElements() ) {\n+                final int eleLen = cigEle.getLength();\n+                final CigarOperator cigOp = cigEle.getOperator();\n+                if ( cigOp.isAlignment() ) {\n+                    final int opEnd = opStart + eleLen - 1;\n+                    final CollatingInterval opInterval =\n+                            new CollatingInterval(contig, opStart, opEnd);\n+                    for ( final LocusDepth locusDepth : locusDepthQueue ) {\n+                        final SAMSequenceRecord locusContig = locusDepth.getSequenceRecord();\n+                        final int cmp = opInterval.compareLocus(locusContig, locusDepth.getStart());\n+                        if ( cmp > 0 ) {\n+                            break;\n+                        }\n+                        if ( cmp == 0 && !isBaseInsideAdaptor(read, locusDepth.getStart()) ) {\n+                            final int callIdx = readIdx + locusDepth.getStart() - opStart;\n+                            if ( quals[callIdx] < minQ ) {\n+                                continue;\n+                            }\n+                            final Nucleotide call = Nucleotide.decode(calls[callIdx]);\n+                            if ( call.isStandard() ) {\n+                                locusDepth.observe(call.ordinal());\n+                            }\n+                        }\n+                    }\n+                }\n+                if ( cigOp.consumesReadBases() ) {\n+                    readIdx += eleLen;\n+                }\n+                if ( cigOp.consumesReferenceBases() ) {\n+                    opStart += eleLen;\n+                }\n+            }\n+        }\n+\n+        public void close() {\n+            while ( !locusDepthQueue.isEmpty() ) {\n+                writer.write(locusDepthQueue.removeFirst());\n+            }\n+            writer.close();\n+        }\n+\n+        private boolean readNextLocus() {\n+            if ( !snpSourceItr.hasNext() ) {\n+                return false;\n+            }\n+            VariantContext snp = snpSourceItr.next();\n+            while ( !snp.isSNP() ) {\n+                if ( !snpSourceItr.hasNext() ) {\n+                    return false;\n+                }\n+                snp = snpSourceItr.next();\n+            }\n+            final byte[] refSeq = snp.getReference().getBases();\n+            final Nucleotide refCall = Nucleotide.decode(refSeq[0]);\n+            if ( !refCall.isStandard() ) {\n+                throw new UserException(\"vcf contains a SNP with a non-standard reference base \" +\n+                        refCall + \" at locus \" + snp.getContig() + \":\" + snp.getStart());\n+            }\n+            final byte[] altSeq = snp.getAlternateAllele(0).getBases();\n+            final Nucleotide altCall = Nucleotide.decode(altSeq[0]);\n+            if ( !altCall.isStandard() ) {\n+                throw new UserException(\"vcf contains a SNP with a non-standard alt base\" +\n+                        altCall + \" at locus \" + snp.getContig() + \":\" + snp.getStart());\n+            }\n+            final LocusDepth locusDepth =\n+                    new LocusDepth(dict, snp, refCall.ordinal(), altCall.ordinal());\n+            locusDepthQueue.add(locusDepth);\n+            return true;\n+        }\n+    }\n+\n+    @VisibleForTesting\n+    public final static class LocusDepth implements Feature {\n+        private final SAMSequenceRecord contig;\n+        private final int position;\n+        private final int refIdx;\n+        private final int altIdx;\n+        private int totalDepth;\n+        private int altDepth;\n+        public final static String BCI_VERSION = \"1.0\";\n+\n+        public LocusDepth( final SAMSequenceDictionary dict, final Locatable loc,\n+                           final int refIdx, final int altIdx ) {\n+            this(dict, loc.getContig(), loc.getStart(), refIdx, altIdx, 0, 0);\n+        }\n+\n+        public LocusDepth( final SAMSequenceDictionary dict,\n+                           final String contigName, final int position,\n+                           final int refIdx, final int altIdx,\n+                           final int totalDepth, final int altDepth ) {\n+            this.contig = dict.getSequence(contigName);\n+            this.position = position;\n+            this.refIdx = refIdx;\n+            this.altIdx = altIdx;\n+            this.totalDepth = totalDepth;\n+            this.altDepth = altDepth;\n+        }\n+\n+        public LocusDepth( final SAMSequenceDictionary dict,\n+                           final DataInputStream dis ) throws IOException {\n+            contig = dict.getSequence(dis.readInt());\n+            position = dis.readInt();\n+            refIdx = dis.readByte();\n+            altIdx = dis.readByte();\n+            totalDepth = dis.readInt();\n+            altDepth = dis.readInt();\n+        }\n+\n+        public void observe( final int idx ) {\n+            if ( idx == altIdx ) altDepth += 1;\n+            totalDepth += 1;\n+        }\n+\n+        public SAMSequenceRecord getSequenceRecord() { return contig; }\n+        @Override public String getContig() { return contig.getSequenceName(); }\n+        @Override public int getEnd() { return position; }\n+        @Override public int getStart() { return position; }\n+\n+        public int getRefIdx() { return refIdx; }\n+        public int getAltIdx() { return altIdx; }\n+        public int getAltDepth() { return altDepth; }\n+        public int getTotalDepth() { return totalDepth; }\n+\n+        public void write( final DataOutputStream dos ) throws IOException {\n+            dos.writeInt(contig.getSequenceIndex());\n+            dos.writeInt(position);\n+            dos.writeByte(refIdx);\n+            dos.writeByte(altIdx);\n+            dos.writeInt(totalDepth);\n+            dos.writeInt(altDepth);\n+        }\n+\n+        public String toString() {\n+            return getContig() + \"\\t\" + getStart() + \"\\t\" + \"ACGT\".charAt(refIdx) + \"\\t\" +\n+                    \"ACGT\".charAt(altIdx) + \"\\t\" + totalDepth + \"\\t\" + altDepth;\n+        }\n+    }\n }"
  },
  {
    "sha": "c49b91edf42525de2c2901df2db770f0fa4ab95e",
    "filename": "src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/PrintAlleleCounts.java",
    "status": "added",
    "additions": 42,
    "deletions": 0,
    "changes": 42,
    "blob_url": "https://github.com/broadinstitute/gatk/blob/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/PrintAlleleCounts.java",
    "raw_url": "https://github.com/broadinstitute/gatk/raw/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/PrintAlleleCounts.java",
    "contents_url": "https://api.github.com/repos/broadinstitute/gatk/contents/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/PrintAlleleCounts.java?ref=304d73f3bd34f59732b9e3d7676b373d0e688f9b",
    "patch": "@@ -0,0 +1,42 @@\n+package org.broadinstitute.hellbender.tools.walkers.sv;\n+\n+import htsjdk.tribble.Feature;\n+import org.broadinstitute.barclay.argparser.Argument;\n+import org.broadinstitute.barclay.argparser.BetaFeature;\n+import org.broadinstitute.barclay.argparser.CommandLineProgramProperties;\n+import org.broadinstitute.hellbender.cmdline.programgroups.StructuralVariantDiscoveryProgramGroup;\n+import org.broadinstitute.hellbender.engine.*;\n+import org.broadinstitute.hellbender.tools.walkers.sv.PairedEndAndSplitReadEvidenceCollection.LocusDepth;\n+\n+@BetaFeature\n+@CommandLineProgramProperties(\n+        summary = \"Prints allele counts.\",\n+        oneLineSummary = \"Prints allele counts.\",\n+        programGroup = StructuralVariantDiscoveryProgramGroup.class\n+)\n+public class PrintAlleleCounts extends FeatureWalker<LocusDepth> {\n+    public static final String ALLELE_COUNT_INPUT_ARGUMENT_SHORT_NAME = \"F\";\n+    public static final String ALLELE_COUNT_INPUT_ARGUMENT_LONG_NAME = \"allele-count-vcf\";\n+\n+    @Argument(shortName = ALLELE_COUNT_INPUT_ARGUMENT_SHORT_NAME,\n+            fullName = ALLELE_COUNT_INPUT_ARGUMENT_LONG_NAME,\n+            doc = \"Input VCF of SNPs marking loci for allele counts\")\n+    public String alleleCountInputFilename;\n+\n+    @Override\n+    protected boolean isAcceptableFeatureType( final Class<? extends Feature> featureType ) {\n+        return featureType == LocusDepth.class;\n+    }\n+\n+    @Override\n+    public void apply( final LocusDepth feature,\n+                       final ReadsContext readsContext,\n+                       final ReferenceContext referenceContext, FeatureContext featureContext ) {\n+        System.out.println(feature.toString());\n+    }\n+\n+    @Override\n+    public GATKPath getDrivingFeaturePath() {\n+        return new GATKPath(alleleCountInputFilename);\n+    }\n+}"
  },
  {
    "sha": "bc21248cb87812ff1cd692448f645074c623408e",
    "filename": "src/main/java/org/broadinstitute/hellbender/utils/CollatingInterval.java",
    "status": "added",
    "additions": 168,
    "deletions": 0,
    "changes": 168,
    "blob_url": "https://github.com/broadinstitute/gatk/blob/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/utils/CollatingInterval.java",
    "raw_url": "https://github.com/broadinstitute/gatk/raw/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/utils/CollatingInterval.java",
    "contents_url": "https://api.github.com/repos/broadinstitute/gatk/contents/src/main/java/org/broadinstitute/hellbender/utils/CollatingInterval.java?ref=304d73f3bd34f59732b9e3d7676b373d0e688f9b",
    "patch": "@@ -0,0 +1,168 @@\n+package org.broadinstitute.hellbender.utils;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.util.Locatable;\n+import htsjdk.tribble.Feature;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.function.Supplier;\n+\n+public class CollatingInterval implements Feature, Comparable<CollatingInterval> {\n+    private final SAMSequenceRecord contig;\n+    private final int start;\n+    private final int end;\n+\n+    public CollatingInterval( final SAMSequenceDictionary dict, final Locatable loc ) {\n+        contig = dict.getSequence(loc.getContig());\n+        start = loc.getStart();\n+        end = loc.getEnd();\n+        validate(() -> loc.getContig() + \" not in dictionary\");\n+    }\n+\n+    public CollatingInterval( final SAMSequenceDictionary dict, final String contigName,\n+                              final int start, final int end ) {\n+        this.contig = dict.getSequence(contigName);\n+        this.start = start;\n+        this.end = end;\n+        validate(() -> contigName + \" not in dictionary\");\n+    }\n+\n+    public CollatingInterval( final SAMSequenceRecord contig, final int start, final int end ) {\n+        this.contig = contig;\n+        this.start = start;\n+        this.end = end;\n+        validate(() -> \"null contig\");\n+    }\n+\n+    public CollatingInterval( final SAMSequenceDictionary dict, final DataInputStream dis )\n+        throws IOException {\n+        final int contigId = dis.readInt();\n+        contig = dict.getSequence(contigId);\n+        start = dis.readInt();\n+        end = dis.readInt();\n+        validate(() -> \"dictionary does not contain a contig with id \" + contigId);\n+    }\n+\n+    @Override public String getContig() { return contig.getSequenceName(); }\n+    @Override public int getStart() { return start; }\n+    @Override public int getEnd() { return end; }\n+    @Override public boolean overlaps( final Locatable that ) {\n+        return contigsMatch(that) && start <= that.getEnd() && that.getStart() <= end;\n+    }\n+    @Override public boolean contains( final Locatable that ) {\n+        return contigsMatch(that) && that.getStart() >= start && that.getEnd() <= end;\n+    }\n+    @Override public boolean contigsMatch( final Locatable that ) {\n+        final String thisContig = contig.getSequenceName();\n+        final String thatContig = that.getContig();\n+        return thisContig == thatContig || thisContig.equals(thatContig);\n+    }\n+\n+    /**\n+     * Compares contig, start, and stop, in that order.\n+     * The contig comparison is done relative to some dictionary's order.\n+     */\n+    @Override public int compareTo( final CollatingInterval that ) {\n+        int result = Integer.compare(this.contig.getSequenceIndex(), that.contig.getSequenceIndex());\n+        if ( result == 0 ) {\n+            result = Integer.compare(this.start, that.start);\n+            if ( result == 0 ) {\n+                result = Integer.compare(this.end, that.end);\n+            }\n+        }\n+        return result;\n+    }\n+\n+    @Override public boolean equals( final Object obj ) {\n+        if ( this == obj ) return true;\n+        if ( !(obj instanceof CollatingInterval) ) return false;\n+        final CollatingInterval that = (CollatingInterval)obj;\n+        return contigsMatch(that) && this.start == that.start && this.end == that.end;\n+    }\n+\n+    @Override public int hashCode() {\n+        return 241*(241*(241*contig.getSequenceIndex() + start) + end);\n+    }\n+\n+    @Override public String toString() {\n+        return contig.getSequenceName() + \":\" + start + \"-\" + end;\n+    }\n+\n+    /**\n+     * Returns a value less than 0 for a locus earlier in the genome than any in this interval,\n+     * the value 0 for a locus that overlaps this interval,\n+     * and a value greater than 0 for a locus later than any any in this interval.\n+     */\n+    public int compareLocus( final SAMSequenceRecord thatContig, final int thatPosition ) {\n+        int result = Integer.compare(thatContig.getSequenceIndex(), contig.getSequenceIndex());\n+        if ( result != 0 ) {\n+            return result;\n+        }\n+        if ( thatPosition < start ) {\n+            return -1;\n+        }\n+        if ( thatPosition > end ) {\n+            return 1;\n+        }\n+        return 0;\n+    }\n+\n+    /**\n+     * Upstream means not overlapping, and\n+     * 1) this is on an earlier (relative to some dictionary's order) contig than that, or\n+     * 2) if on the same contig, this ends earlier than that starts\n+     */\n+    public boolean isUpstreamOf( final CollatingInterval that ) {\n+        final int thisContigId = contig.getSequenceIndex();\n+        final int thatContigId = that.contig.getSequenceIndex();\n+        if ( thisContigId < thatContigId ) {\n+            return true;\n+        }\n+        if ( thisContigId == thatContigId ) {\n+            return end < that.getStart();\n+        }\n+        return false;\n+    }\n+\n+    /**\n+     * If the contigs are not the same, the laterEnding interval is the one with the later (relative\n+     * to some dictionary) contig.\n+     * If the contigs are the same, the laterEnding interval is the one with the greater end position.\n+     */\n+    public static CollatingInterval laterEnding( final CollatingInterval interval1,\n+                                                 final CollatingInterval interval2 ) {\n+        final int contigId1 = interval1.contig.getSequenceIndex();\n+        final int contigId2 = interval2.contig.getSequenceIndex();\n+        if ( contigId1 == contigId2 ) {\n+            return interval1.end > interval2.end ? interval1 : interval2;\n+        }\n+        if ( contigId1 > contigId2 ) {\n+            return interval1;\n+        }\n+        return interval2;\n+    }\n+\n+    public void write( final DataOutputStream dos ) throws IOException {\n+        dos.writeInt(contig.getSequenceIndex());\n+        dos.writeInt(start);\n+        dos.writeInt(end);\n+    }\n+\n+    private void validate( final Supplier<String> badContigMessage ) {\n+        if ( contig == null ) {\n+            throw new GATKException(badContigMessage.get());\n+        }\n+        final int sequenceLength = contig.getSequenceLength();\n+        if ( start < 1 || start > sequenceLength ) {\n+            throw new GATKException(\"starting coordinate \" + start + \" is not within contig bounds\");\n+        }\n+        if ( end < start || end > sequenceLength ) {\n+            throw new GATKException(\"ending coordinate \" + end +\n+                    \" is less than start or greater than contig length\");\n+        }\n+    }\n+}"
  },
  {
    "sha": "844741e54645415da93de5e35b4ac79922316756",
    "filename": "src/main/java/org/broadinstitute/hellbender/utils/codecs/AbstractBCICodec.java",
    "status": "added",
    "additions": 47,
    "deletions": 0,
    "changes": 47,
    "blob_url": "https://github.com/broadinstitute/gatk/blob/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/utils/codecs/AbstractBCICodec.java",
    "raw_url": "https://github.com/broadinstitute/gatk/raw/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/utils/codecs/AbstractBCICodec.java",
    "contents_url": "https://api.github.com/repos/broadinstitute/gatk/contents/src/main/java/org/broadinstitute/hellbender/utils/codecs/AbstractBCICodec.java?ref=304d73f3bd34f59732b9e3d7676b373d0e688f9b",
    "patch": "@@ -0,0 +1,47 @@\n+package org.broadinstitute.hellbender.utils.codecs;\n+\n+import htsjdk.samtools.util.LocationAware;\n+import htsjdk.tribble.Feature;\n+import htsjdk.tribble.FeatureCodec;\n+import htsjdk.tribble.FeatureCodecHeader;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Reader;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Writer;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+import static htsjdk.tribble.FeatureCodecHeader.EMPTY_HEADER;\n+\n+public abstract class AbstractBCICodec <T extends Feature> implements FeatureCodec<T, Reader<T>> {\n+\n+    @Override\n+    public Feature decodeLoc( final Reader<T> reader ) throws IOException {\n+        return decode(reader);\n+    }\n+\n+    @Override\n+    public FeatureCodecHeader readHeader( final Reader<T> reader ) throws IOException {\n+        return EMPTY_HEADER;\n+    }\n+\n+    @Override\n+    public Reader<T> makeSourceFromStream( final InputStream is ) {\n+        throw new GATKException(\"wasn't expecting to execute this code path\");\n+    }\n+\n+    @Override\n+    public LocationAware makeIndexableSourceFromStream( final InputStream is ) {\n+        throw new GATKException(\"wasn't expecting to execute this code path\");\n+    }\n+\n+    @Override\n+    public boolean isDone( final Reader<T> reader ) { return !reader.hasNext(); }\n+\n+    @Override\n+    public void close( final Reader<T> reader ) { reader.close(); }\n+\n+    abstract public void encode( T feature, Writer<T> writer ) throws IOException;\n+\n+    abstract public String getVersion();\n+}"
  },
  {
    "sha": "b3d35ae1d0a9bff7dbac04313930a53549a10404",
    "filename": "src/main/java/org/broadinstitute/hellbender/utils/codecs/BafEvidenceBCICodec.java",
    "status": "added",
    "additions": 49,
    "deletions": 0,
    "changes": 49,
    "blob_url": "https://github.com/broadinstitute/gatk/blob/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/utils/codecs/BafEvidenceBCICodec.java",
    "raw_url": "https://github.com/broadinstitute/gatk/raw/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/utils/codecs/BafEvidenceBCICodec.java",
    "contents_url": "https://api.github.com/repos/broadinstitute/gatk/contents/src/main/java/org/broadinstitute/hellbender/utils/codecs/BafEvidenceBCICodec.java?ref=304d73f3bd34f59732b9e3d7676b373d0e688f9b",
    "patch": "@@ -0,0 +1,49 @@\n+package org.broadinstitute.hellbender.utils.codecs;\n+\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.sv.BafEvidence;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Reader;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Writer;\n+\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+\n+public class BafEvidenceBCICodec extends AbstractBCICodec<BafEvidence> {\n+    private boolean versionChecked = false;\n+    private static final String BAF_BCI_FILE_EXTENSION = \".baf.bci\";\n+\n+    @Override\n+    public BafEvidence decode( final Reader<BafEvidence> reader ) throws IOException {\n+        if ( !versionChecked ) {\n+            if ( !BafEvidence.BCI_VERSION.equals(reader.getVersion()) ) {\n+                throw new UserException(\"baf.bci file has wrong version: expected \" +\n+                        BafEvidence.BCI_VERSION + \" but found \" + reader.getVersion());\n+            }\n+            versionChecked = true;\n+        }\n+        final DataInputStream dis = reader.getStream();\n+        final String sample = reader.getSampleNames().get(dis.readInt());\n+        final String contig = reader.getDictionary().getSequence(dis.readInt()).getSequenceName();\n+        final int position = dis.readInt();\n+        final double value = dis.readDouble();\n+        return new BafEvidence(sample, contig, position, value);\n+    }\n+\n+    @Override\n+    public Class<BafEvidence> getFeatureType() { return BafEvidence.class; }\n+\n+    @Override\n+    public boolean canDecode( final String path ) { return path.endsWith(BAF_BCI_FILE_EXTENSION); }\n+\n+    public void encode( final BafEvidence bafEvidence,\n+                        final Writer<BafEvidence> writer ) throws IOException {\n+        final DataOutputStream dos = writer.getStream();\n+        dos.writeInt(writer.getSampleIndex(bafEvidence.getSample()));\n+        dos.writeInt(writer.getContigIndex(bafEvidence.getContig()));\n+        dos.writeInt(bafEvidence.getStart());\n+        dos.writeDouble(bafEvidence.getValue());\n+    }\n+\n+    public String getVersion() { return BafEvidence.BCI_VERSION; }\n+}"
  },
  {
    "sha": "973e56e2e9b5f01cdcbe9526dfc08379e6c99c83",
    "filename": "src/main/java/org/broadinstitute/hellbender/utils/codecs/DepthEvidenceBCICodec.java",
    "status": "added",
    "additions": 58,
    "deletions": 0,
    "changes": 58,
    "blob_url": "https://github.com/broadinstitute/gatk/blob/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/utils/codecs/DepthEvidenceBCICodec.java",
    "raw_url": "https://github.com/broadinstitute/gatk/raw/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/utils/codecs/DepthEvidenceBCICodec.java",
    "contents_url": "https://api.github.com/repos/broadinstitute/gatk/contents/src/main/java/org/broadinstitute/hellbender/utils/codecs/DepthEvidenceBCICodec.java?ref=304d73f3bd34f59732b9e3d7676b373d0e688f9b",
    "patch": "@@ -0,0 +1,58 @@\n+package org.broadinstitute.hellbender.utils.codecs;\n+\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.sv.BafEvidence;\n+import org.broadinstitute.hellbender.tools.sv.DepthEvidence;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Reader;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Writer;\n+\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+\n+public class DepthEvidenceBCICodec extends AbstractBCICodec<DepthEvidence> {\n+    private boolean versionChecked = false;\n+    private static final String RD_BCI_FILE_EXTENSION = \".rd.bci\";\n+\n+    @Override\n+    public DepthEvidence decode( final Reader<DepthEvidence> reader ) throws IOException {\n+        if ( !versionChecked ) {\n+            if ( !DepthEvidence.BCI_VERSION.equals(reader.getVersion()) ) {\n+                throw new UserException(\"baf.bci file has wrong version: expected \" +\n+                        DepthEvidence.BCI_VERSION + \" but found \" + reader.getVersion());\n+            }\n+            versionChecked = true;\n+        }\n+        final DataInputStream dis = reader.getStream();\n+        final String contig = reader.getDictionary().getSequence(dis.readInt()).getSequenceName();\n+        final int start = dis.readInt();\n+        final int end = dis.readInt();\n+        final int nCounts = dis.readInt();\n+        final int[] counts = new int[nCounts];\n+        for ( int idx = 0; idx != nCounts; ++idx ) {\n+            counts[idx] = dis.readInt();\n+        }\n+        return new DepthEvidence(contig, start, end, counts);\n+    }\n+\n+    @Override\n+    public Class<DepthEvidence> getFeatureType() { return DepthEvidence.class; }\n+\n+    @Override\n+    public boolean canDecode( final String path ) { return path.endsWith(RD_BCI_FILE_EXTENSION); }\n+\n+    public void encode( final DepthEvidence depthEvidence,\n+                        final Writer<DepthEvidence> writer ) throws IOException {\n+        final DataOutputStream dos = writer.getStream();\n+        dos.writeInt(writer.getContigIndex(depthEvidence.getContig()));\n+        dos.writeInt(depthEvidence.getStart());\n+        dos.writeInt(depthEvidence.getEnd());\n+        final int[] counts = depthEvidence.getCounts();\n+        dos.writeInt(counts.length);\n+        for ( final int count : counts ) {\n+            dos.writeInt(count);\n+        }\n+    }\n+\n+    public String getVersion() { return DepthEvidence.BCI_VERSION; }\n+}"
  },
  {
    "sha": "9cb15091b2de5ae6f627d6ecb6b77192741cee9e",
    "filename": "src/main/java/org/broadinstitute/hellbender/utils/codecs/DepthEvidenceCodec.java",
    "status": "modified",
    "additions": 3,
    "deletions": 0,
    "changes": 3,
    "blob_url": "https://github.com/broadinstitute/gatk/blob/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/utils/codecs/DepthEvidenceCodec.java",
    "raw_url": "https://github.com/broadinstitute/gatk/raw/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/utils/codecs/DepthEvidenceCodec.java",
    "contents_url": "https://api.github.com/repos/broadinstitute/gatk/contents/src/main/java/org/broadinstitute/hellbender/utils/codecs/DepthEvidenceCodec.java?ref=304d73f3bd34f59732b9e3d7676b373d0e688f9b",
    "patch": "@@ -28,6 +28,9 @@ public TabixFormat getTabixFormat() {\n \n     @Override\n     public DepthEvidence decode(final String line) {\n+        if ( line.startsWith(\"#Chr\") ) {\n+            return null;\n+        }\n         final List<String> tokens = splitter.splitToList(line);\n         if (tokens.size() < 3) {\n             throw new IllegalArgumentException(\"Expected at least 3 columns but found \" + tokens.size());"
  },
  {
    "sha": "2a9126501eb2f673ed845ccd498dd1e1ab5f9a2f",
    "filename": "src/main/java/org/broadinstitute/hellbender/utils/codecs/DiscordantPairEvidenceBCICodec.java",
    "status": "added",
    "additions": 61,
    "deletions": 0,
    "changes": 61,
    "blob_url": "https://github.com/broadinstitute/gatk/blob/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/utils/codecs/DiscordantPairEvidenceBCICodec.java",
    "raw_url": "https://github.com/broadinstitute/gatk/raw/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/utils/codecs/DiscordantPairEvidenceBCICodec.java",
    "contents_url": "https://api.github.com/repos/broadinstitute/gatk/contents/src/main/java/org/broadinstitute/hellbender/utils/codecs/DiscordantPairEvidenceBCICodec.java?ref=304d73f3bd34f59732b9e3d7676b373d0e688f9b",
    "patch": "@@ -0,0 +1,61 @@\n+package org.broadinstitute.hellbender.utils.codecs;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.sv.BafEvidence;\n+import org.broadinstitute.hellbender.tools.sv.DiscordantPairEvidence;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Reader;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Writer;\n+\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+\n+public class DiscordantPairEvidenceBCICodec extends AbstractBCICodec<DiscordantPairEvidence> {\n+    private boolean versionChecked = false;\n+\n+    private static final String PE_BCI_FILE_EXTENSION = \".pe.bci\";\n+\n+    @Override\n+    public DiscordantPairEvidence decode( final Reader<DiscordantPairEvidence> reader )\n+            throws IOException {\n+        if ( !versionChecked ) {\n+            if ( !DiscordantPairEvidence.BCI_VERSION.equals(reader.getVersion()) ) {\n+                throw new UserException(\"baf.bci file has wrong version: expected \" +\n+                        DiscordantPairEvidence.BCI_VERSION + \" but found \" + reader.getVersion());\n+            }\n+            versionChecked = true;\n+        }\n+        final DataInputStream dis = reader.getStream();\n+        final String sample = reader.getSampleNames().get(dis.readInt());\n+        final SAMSequenceDictionary dict = reader.getDictionary();\n+        final String startContig = dict.getSequence(dis.readInt()).getSequenceName();\n+        final int start = dis.readInt();\n+        final boolean startStrand = dis.readBoolean();\n+        final String endContig = dict.getSequence(dis.readInt()).getSequenceName();\n+        final int end = dis.readInt();\n+        final boolean endStrand = dis.readBoolean();\n+        return new DiscordantPairEvidence(sample, startContig, start, startStrand,\n+                                            endContig, end, endStrand);\n+    }\n+\n+    @Override\n+    public Class<DiscordantPairEvidence> getFeatureType() { return DiscordantPairEvidence.class; }\n+\n+    @Override\n+    public boolean canDecode( final String path ) { return path.endsWith(PE_BCI_FILE_EXTENSION); }\n+\n+    public void encode( final DiscordantPairEvidence peEvidence,\n+                        final Writer<DiscordantPairEvidence> writer ) throws IOException {\n+        final DataOutputStream dos = writer.getStream();\n+        dos.writeInt(writer.getSampleIndex(peEvidence.getSample()));\n+        dos.writeInt(writer.getContigIndex(peEvidence.getContig()));\n+        dos.writeInt(peEvidence.getStart());\n+        dos.writeBoolean(peEvidence.getStartStrand());\n+        dos.writeInt(writer.getContigIndex(peEvidence.getEndContig()));\n+        dos.writeInt(peEvidence.getEndPosition());\n+        dos.writeBoolean(peEvidence.getEndStrand());\n+    }\n+\n+    public String getVersion() { return DiscordantPairEvidence.BCI_VERSION; }\n+}"
  },
  {
    "sha": "d1b3510ae979ebbbc08a0939fb7b70b02de096c4",
    "filename": "src/main/java/org/broadinstitute/hellbender/utils/codecs/LocusDepthCodec.java",
    "status": "added",
    "additions": 39,
    "deletions": 0,
    "changes": 39,
    "blob_url": "https://github.com/broadinstitute/gatk/blob/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/utils/codecs/LocusDepthCodec.java",
    "raw_url": "https://github.com/broadinstitute/gatk/raw/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/utils/codecs/LocusDepthCodec.java",
    "contents_url": "https://api.github.com/repos/broadinstitute/gatk/contents/src/main/java/org/broadinstitute/hellbender/utils/codecs/LocusDepthCodec.java?ref=304d73f3bd34f59732b9e3d7676b373d0e688f9b",
    "patch": "@@ -0,0 +1,39 @@\n+package org.broadinstitute.hellbender.utils.codecs;\n+\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.sv.BafEvidence;\n+import org.broadinstitute.hellbender.tools.walkers.sv.PairedEndAndSplitReadEvidenceCollection.LocusDepth;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Reader;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Writer;\n+\n+import java.io.IOException;\n+\n+public class LocusDepthCodec extends AbstractBCICodec<LocusDepth> {\n+    private boolean versionChecked = false;\n+    private static final String LD_BCI_FILE_EXTENSION = \".ld.bci\";\n+\n+    @Override\n+    public LocusDepth decode( final Reader<LocusDepth> reader ) throws IOException {\n+        if ( !versionChecked ) {\n+            if ( !LocusDepth.BCI_VERSION.equals(reader.getVersion()) ) {\n+                throw new UserException(\"bci file has wrong version: expected \" +\n+                        LocusDepth.BCI_VERSION + \" but found \" + reader.getVersion());\n+            }\n+            versionChecked = true;\n+        }\n+        return new LocusDepth(reader.getDictionary(), reader.getStream());\n+    }\n+\n+    @Override\n+    public Class<LocusDepth> getFeatureType() { return LocusDepth.class; }\n+\n+    @Override\n+    public boolean canDecode( final String path ) { return path.endsWith(LD_BCI_FILE_EXTENSION); }\n+\n+    public void encode( final LocusDepth locusDepth, final Writer<LocusDepth> writer )\n+            throws IOException {\n+        locusDepth.write(writer.getStream());\n+    }\n+\n+    public String getVersion() { return LocusDepth.BCI_VERSION; }\n+}"
  },
  {
    "sha": "62f14ea9720885c0ae049c50ac696c997477f84a",
    "filename": "src/main/java/org/broadinstitute/hellbender/utils/codecs/SplitReadEvidenceBCICodec.java",
    "status": "added",
    "additions": 52,
    "deletions": 0,
    "changes": 52,
    "blob_url": "https://github.com/broadinstitute/gatk/blob/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/utils/codecs/SplitReadEvidenceBCICodec.java",
    "raw_url": "https://github.com/broadinstitute/gatk/raw/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/utils/codecs/SplitReadEvidenceBCICodec.java",
    "contents_url": "https://api.github.com/repos/broadinstitute/gatk/contents/src/main/java/org/broadinstitute/hellbender/utils/codecs/SplitReadEvidenceBCICodec.java?ref=304d73f3bd34f59732b9e3d7676b373d0e688f9b",
    "patch": "@@ -0,0 +1,52 @@\n+package org.broadinstitute.hellbender.utils.codecs;\n+\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.tools.sv.BafEvidence;\n+import org.broadinstitute.hellbender.tools.sv.SplitReadEvidence;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Reader;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Writer;\n+\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+\n+public class SplitReadEvidenceBCICodec extends AbstractBCICodec<SplitReadEvidence> {\n+    private boolean versionChecked = false;\n+    private static final String SR_BCI_FILE_EXTENSION = \".sr.bci\";\n+\n+    @Override\n+    public SplitReadEvidence decode( final Reader<SplitReadEvidence> reader ) throws IOException {\n+        if ( !versionChecked ) {\n+            if ( !SplitReadEvidence.BCI_VERSION.equals(reader.getVersion()) ) {\n+                throw new UserException(\"baf.bci file has wrong version: expected \" +\n+                        SplitReadEvidence.BCI_VERSION + \" but found \" + reader.getVersion());\n+            }\n+            versionChecked = true;\n+        }\n+        final DataInputStream dis = reader.getStream();\n+        final String sample = reader.getSampleNames().get(dis.readInt());\n+        final String contig = reader.getDictionary().getSequence(dis.readInt()).getSequenceName();\n+        final int position = dis.readInt();\n+        final int count = dis.readInt();\n+        final boolean strand = dis.readBoolean();\n+        return new SplitReadEvidence(sample, contig, position, count, strand);\n+    }\n+\n+    @Override\n+    public Class<SplitReadEvidence> getFeatureType() { return SplitReadEvidence.class; }\n+\n+    @Override\n+    public boolean canDecode( final String path ) { return path.endsWith(SR_BCI_FILE_EXTENSION); }\n+\n+    public void encode( final SplitReadEvidence srEvidence,\n+                        final Writer<SplitReadEvidence> writer ) throws IOException {\n+        final DataOutputStream dos = writer.getStream();\n+        dos.writeInt(writer.getSampleIndex(srEvidence.getSample()));\n+        dos.writeInt(writer.getContigIndex(srEvidence.getContig()));\n+        dos.writeInt(srEvidence.getStart());\n+        dos.writeInt(srEvidence.getCount());\n+        dos.writeBoolean(srEvidence.getStrand());\n+    }\n+\n+    public String getVersion() { return SplitReadEvidence.BCI_VERSION; }\n+}"
  },
  {
    "sha": "73158cae7c21c134f322f1987ec423caaa25f8ff",
    "filename": "src/main/java/org/broadinstitute/hellbender/utils/collections/IntervalTree.java",
    "status": "added",
    "additions": 999,
    "deletions": 0,
    "changes": 999,
    "blob_url": "https://github.com/broadinstitute/gatk/blob/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/utils/collections/IntervalTree.java",
    "raw_url": "https://github.com/broadinstitute/gatk/raw/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/utils/collections/IntervalTree.java",
    "contents_url": "https://api.github.com/repos/broadinstitute/gatk/contents/src/main/java/org/broadinstitute/hellbender/utils/collections/IntervalTree.java?ref=304d73f3bd34f59732b9e3d7676b373d0e688f9b",
    "patch": "@@ -0,0 +1,999 @@\n+package org.broadinstitute.hellbender.utils.collections;\n+\n+import org.broadinstitute.hellbender.utils.CollatingInterval;\n+\n+import java.util.ConcurrentModificationException;\n+import java.util.Iterator;\n+import java.util.NoSuchElementException;\n+import java.util.Objects;\n+\n+/**\n+ * A Red-Black tree with CollatingIntervals for keys.\n+ * Intervals are kept in dictionary order, and there are no comparator overrides.\n+ * Not thread-safe, and cannot be made so.  You must synchronize externally.\n+ * <p>\n+ * There's some weird stuff about sentinel values for the put and remove methods.  Here's what's up with that:\n+ * When you update the value associated with some interval by doing a put with an interval that's already in the\n+ * tree, the old value is returned to you.  But maybe you've put some nulls into the tree as values.  (That's legal.)\n+ * In that case, when you get a null value returned by put you can't tell whether the interval was inserted into the tree\n+ * and there was no old value to return to you or whether you just updated an existing interval that had a null value\n+ * associated with it.  (Both situations return null.)  IF you're inserting nulls as values, and IF you need to be able\n+ * to tell whether the put operation did an insert or an update, you can do a special thing so that you can distinguish\n+ * these cases:  set the sentinel value for the tree to some singleton object that you never ever use as a legitimate\n+ * value.  Then when you call put you'll get your sentinel value back for an insert, but you'll get null back for an\n+ * update of a formerly-null value.  Same thing happens for remove:  set the sentinel IF you've used nulls for values,\n+ * and IF you need to be able to tell the difference between remove not finding the interval and remove removing an\n+ * interval that has a null value associated with it.\n+ * If you're not using nulls as values, or if you don't care to disambiguate these cases, then just forget about\n+ * all this weirdness.  The sentinel value is null by default, so put and remove will behave like you might expect them\n+ * to if you're not worrying about this stuff:  they'll return null for novel insertions and failed deletions.\n+ */\n+public final class IntervalTree<V> implements Iterable<IntervalTree.Entry<V>> {\n+    private Node<V> root;\n+    private V sentinel;\n+\n+    public IntervalTree() {}\n+\n+    /**\n+     * Return the number of intervals in the tree.\n+     *\n+     * @return The number of intervals.\n+     */\n+    public int size() {\n+        return root == null ? 0 : root.getSize();\n+    }\n+\n+    /**\n+     * Remove all entries.\n+     */\n+    public void clear() {\n+        root = null;\n+    }\n+\n+    /**\n+     * Put a new interval into the tree (or update the value associated with an existing interval).\n+     * If the interval is novel, the special sentinel value (which is null by default) is returned.\n+     *\n+     * @param interval The interval.\n+     * @param value    The associated value.\n+     * @return The old value associated with that interval, or the sentinel value.\n+     */\n+    public V put( final CollatingInterval interval, final V value ) {\n+        V result = sentinel;\n+\n+        if ( root == null ) {\n+            root = new Node<>(interval, value);\n+        } else {\n+            Node<V> parent = null;\n+            Node<V> node = root;\n+            int cmpVal = 0;\n+\n+            while ( node != null ) {\n+                parent = node; // last non-null node\n+                cmpVal = interval.compareTo(node.getInterval());\n+                if ( cmpVal == 0 ) {\n+                    break;\n+                }\n+                node = cmpVal < 0 ? node.getLeft() : node.getRight();\n+            }\n+\n+            if ( cmpVal == 0 ) {\n+                result = parent.setValue(value);\n+            } else if ( cmpVal < 0 ) {\n+                root = parent.insertLeft(interval, value, root);\n+            } else {\n+                root = parent.insertRight(interval, value, root);\n+            }\n+        }\n+\n+        return result;\n+    }\n+\n+    /**\n+     * Remove an interval from the tree.\n+     * If the interval is not found, the special sentinel value (which is null by default) is returned.\n+     *\n+     * @param interval The interval to remove.\n+     * @return The value associated with the deleted interval, or the sentinel value.\n+     */\n+    public V remove( final CollatingInterval interval ) {\n+        V result = sentinel;\n+        Node<V> node = root;\n+\n+        while ( node != null ) {\n+            final int cmpVal = interval.compareTo(node.getInterval());\n+            if ( cmpVal == 0 ) {\n+                result = node.getValue();\n+                root = node.remove(root);\n+                break;\n+            }\n+            node = cmpVal < 0 ? node.getLeft() : node.getRight();\n+        }\n+\n+        return result;\n+    }\n+\n+    /**\n+     * Find an interval.\n+     *\n+     * @param interval The interval sought.\n+     * @return The Entry that represents that interval, or null.\n+     */\n+    public Entry<V> find( final CollatingInterval interval ) {\n+        Node<V> node = root;\n+\n+        while ( node != null ) {\n+            final int cmpVal = interval.compareTo(node.getInterval());\n+            if ( cmpVal == 0 ) {\n+                break;\n+            }\n+            node = cmpVal < 0 ? node.getLeft() : node.getRight();\n+        }\n+\n+        return node;\n+    }\n+\n+    /**\n+     * Find the nth interval in the tree.\n+     *\n+     * @param idx The rank of the interval sought (from 0 to size()-1).\n+     * @return The Entry that represents the nth interval.\n+     */\n+    public Entry<V> findByIndex( final int idx ) {\n+        return Node.findByRank(root, idx + 1);\n+    }\n+\n+    /**\n+     * Find the rank of the specified interval.  If the specified interval is not in the\n+     * tree, then -1 is returned.\n+     *\n+     * @param interval The interval for which the index is sought.\n+     * @return The rank of that interval, or -1.\n+     */\n+    public int getIndex( final CollatingInterval interval ) {\n+        return Node.getRank(root, interval) - 1;\n+    }\n+\n+    /**\n+     * Find the least interval in the tree.\n+     *\n+     * @return The Entry with the earliest interval, or null if the tree is empty.\n+     */\n+    public Entry<V> min() {\n+        Node<V> result = null;\n+        Node<V> node = root;\n+\n+        while ( node != null ) {\n+            result = node;\n+            node = node.getLeft();\n+        }\n+\n+        return result;\n+    }\n+\n+    /**\n+     * Find the earliest interval in the tree greater than or equal to the specified interval.\n+     *\n+     * @param interval The interval sought.\n+     * @return The Entry with the earliest interval >= the specified interval, or null if there is none.\n+     */\n+    public Entry<V> min( final CollatingInterval interval ) {\n+        Node<V> result = null;\n+        Node<V> node = root;\n+        int cmpVal = 0;\n+\n+        while ( node != null ) {\n+            result = node;\n+            cmpVal = interval.compareTo(node.getInterval());\n+            if ( cmpVal == 0 ) {\n+                break;\n+            }\n+            node = cmpVal < 0 ? node.getLeft() : node.getRight();\n+        }\n+\n+        if ( cmpVal > 0 ) {\n+            result = result.getNext();\n+        }\n+\n+        return result;\n+    }\n+\n+    /**\n+     * Check the tree against a probe interval to see if there's an overlapping interval.\n+     *\n+     * @param interval The interval sought.\n+     * @return Whether or not there's an overlapping interval in this tree.\n+     */\n+    public boolean hasOverlapper( final CollatingInterval interval ) {\n+        Node<V> node = root;\n+\n+        if ( node != null && !node.getMaxEndInterval().isUpstreamOf(interval) ) {\n+            while ( true ) {\n+                if ( node.getInterval().overlaps(interval) ) {\n+                    return true;\n+                } else { // no overlap.  if there might be a left sub-tree overlapper, consider the left sub-tree.\n+                    final Node<V> left = node.getLeft();\n+                    if ( left != null && !left.getMaxEndInterval().isUpstreamOf(interval) ) {\n+                        node = left;\n+                    } else { // left sub-tree cannot contain an overlapper.  consider the right sub-tree.\n+                        // if everything in the right sub-tree is past the end of the query interval, then break\n+                        if ( interval.isUpstreamOf(node.getInterval()) ) {\n+                            break;\n+                        }\n+\n+                        node = node.getRight();\n+                        // if no right sub-tree or all nodes end too early, then break\n+                        if ( node == null || node.getMaxEndInterval().isUpstreamOf(interval) ) {\n+                            break;\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+        return false;\n+    }\n+\n+    /**\n+     * Find the earliest interval in the tree that overlaps the specified interval.\n+     *\n+     * @param interval The interval sought.\n+     * @return The Entry with the earliest overlapping interval, or null if there is none.\n+     */\n+    public Entry<V> minOverlapper( final CollatingInterval interval ) {\n+        Node<V> result = null;\n+        Node<V> node = root;\n+\n+        if ( node != null && !node.getMaxEndInterval().isUpstreamOf(interval) ) {\n+            while ( true ) {\n+                if ( node.getInterval().overlaps(interval) ) {\n+                    // this node overlaps.  however, there might be an earlier overlapper down the left sub-tree.\n+                    // no need to consider the right sub-tree:  even if there's an overlapper, if won't be minimal\n+                    result = node;\n+                    node = node.getLeft();\n+                    // if no left sub-tree or all nodes end too early, then break\n+                    if ( node == null || node.getMaxEndInterval().isUpstreamOf(interval) ) {\n+                        break;\n+                    }\n+                } else { // no overlap.  if there might be a left sub-tree overlapper, consider the left sub-tree.\n+                    final Node<V> left = node.getLeft();\n+                    if ( left != null && !left.getMaxEndInterval().isUpstreamOf(interval) ) {\n+                        node = left;\n+                    } else { // left sub-tree cannot contain an overlapper.  consider the right sub-tree.\n+                        // if everything in the right sub-tree is past the end of the query interval, then break\n+                        if ( interval.isUpstreamOf(node.getInterval()) ) {\n+                            break;\n+                        }\n+\n+                        node = node.getRight();\n+                        // if no right sub-tree or all nodes end too early, then break\n+                        if ( node == null || node.getMaxEndInterval().isUpstreamOf(interval) ) {\n+                            break;\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+\n+        return result;\n+    }\n+\n+    /**\n+     * Find the greatest interval in the tree.\n+     *\n+     * @return The Entry with the latest interval, or null if the tree is empty.\n+     */\n+    public Entry<V> max() {\n+        Node<V> result = null;\n+        Node<V> node = root;\n+\n+        while ( node != null ) {\n+            result = node;\n+            node = node.getRight();\n+        }\n+\n+        return result;\n+    }\n+\n+    /**\n+     * Find the latest interval in the tree less than or equal to the specified interval.\n+     *\n+     * @param interval The interval sought.\n+     * @return The Entry with the latest interval <= the specified interval, or null if there is none.\n+     */\n+    public Entry<V> max( final CollatingInterval interval ) {\n+        Node<V> result = null;\n+        Node<V> node = root;\n+        int cmpVal = 0;\n+\n+        while ( node != null ) {\n+            result = node;\n+            cmpVal = interval.compareTo(node.getInterval());\n+            if ( cmpVal == 0 ) {\n+                break;\n+            }\n+\n+            node = cmpVal < 0 ? node.getLeft() : node.getRight();\n+        }\n+\n+        if ( cmpVal < 0 ) {\n+            result = result.getPrev();\n+        }\n+\n+        return result;\n+    }\n+\n+    /**\n+     * Return the interval having the largest ending value.\n+     * This will be null if the tree is empty.\n+     */\n+    public CollatingInterval maxEnd() {\n+        return root == null ? null : root.getMaxEndInterval();\n+    }\n+\n+    /**\n+     * Return an iterator over the entire tree.\n+     *\n+     * @return An iterator.\n+     */\n+    public Iterator<Entry<V>> iterator() { return new FwdIterator((Node<V>)min()); }\n+\n+    /**\n+     * Return an iterator over all intervals greater than or equal to the specified interval.\n+     *\n+     * @param interval The minimum interval.\n+     * @return An iterator.\n+     */\n+    public Iterator<Entry<V>> iterator( final CollatingInterval interval ) {\n+        return new FwdIterator((Node<V>)min(interval));\n+    }\n+\n+    /**\n+     * Return an iterator over all intervals overlapping the specified interval.\n+     *\n+     * @param interval Interval to overlap.\n+     * @return An iterator.\n+     */\n+    public Iterator<Entry<V>> overlappers( final CollatingInterval interval ) {\n+        return new OverlapIterator(interval);\n+    }\n+\n+    /**\n+     * Return an iterator over the entire tree that returns intervals in reverse order.\n+     *\n+     * @return An iterator.\n+     */\n+    public Iterator<Entry<V>> reverseIterator() {\n+        return new RevIterator((Node<V>)max());\n+    }\n+\n+    /**\n+     * Return an iterator over all intervals less than or equal to the specified interval, in reverse order.\n+     *\n+     * @param interval The maximum interval.\n+     * @return An iterator.\n+     */\n+    public Iterator<Entry<V>> reverseIterator( final CollatingInterval interval ) {\n+        return new RevIterator((Node<V>)max(interval));\n+    }\n+\n+    /**\n+     * Get the special sentinel value that will be used to signal novelty when putting a new interval\n+     * into the tree, or to signal \"not found\" when removing an interval.  This is null by default.\n+     *\n+     * @return The sentinel value.\n+     */\n+    public V getSentinel() {\n+        return sentinel;\n+    }\n+\n+    /**\n+     * Set the special sentinel value that will be used to signal novelty when putting a new interval\n+     * into the tree, or to signal \"not found\" when removing an interval.\n+     *\n+     * @param sentinel The new sentinel value.\n+     * @return The old sentinel value.\n+     */\n+    public V setSentinel( final V sentinel ) {\n+        final V result = this.sentinel;\n+        this.sentinel = sentinel;\n+        return result;\n+    }\n+\n+    /**\n+     * The fraction of the intervals in this tree that overlap with intervals in some other tree.\n+     *\n+     * @param that Some other interval tree.\n+     * @return The fraction of intervals in this tree overlapping those in some other tree.\n+     */\n+    public float overlapFraction( final IntervalTree<?> that ) {\n+        int count = 0;\n+        for ( final Entry<V> entry : this ) {\n+            if ( that.hasOverlapper(entry.getInterval()) ) count += 1;\n+        }\n+        return (float)count/size();\n+    }\n+\n+    void removeNode( final Node<V> node ) {\n+        root = node.remove(root);\n+    }\n+\n+    public interface Entry<V1> {\n+        CollatingInterval getInterval();\n+        V1 getValue();\n+        V1 setValue( final V1 value );\n+    }\n+\n+    static final class Node<V1> implements Entry<V1> {\n+        private final CollatingInterval interval;\n+        private V1 value;\n+        private Node<V1> parent;\n+        private Node<V1> left;\n+        private Node<V1> right;\n+        private int size;\n+        private CollatingInterval maxEndInterval; // interval in this sub-tree having the greatest endpoint\n+        private boolean isBlack;\n+\n+        /** make a root node */\n+        Node( final CollatingInterval interval, final V1 value ) {\n+            this(null, interval, value);\n+            isBlack = true;\n+        }\n+\n+        /** make a leaf node */\n+        Node( final Node<V1> parent, final CollatingInterval interval, final V1 value ) {\n+            this.interval = interval;\n+            this.value = value;\n+            this.parent = parent;\n+            size = 1;\n+            maxEndInterval = interval;\n+        }\n+\n+        @Override public CollatingInterval getInterval() { return interval; }\n+        @Override public V1 getValue() { return value; }\n+        @Override public V1 setValue( final V1 value ) {\n+            final V1 result = this.value;\n+            this.value = value;\n+            return result;\n+        }\n+\n+        int getSize() { return size; }\n+        CollatingInterval getMaxEndInterval() { return maxEndInterval; }\n+        Node<V1> getLeft() { return left; }\n+\n+        Node<V1> insertLeft( final CollatingInterval interval, final V1 value, final Node<V1> root ) {\n+            left = new Node<>(this, interval, value);\n+            return insertFixup(left, root);\n+        }\n+\n+        Node<V1> getRight() {\n+            return right;\n+        }\n+\n+        Node<V1> insertRight( final CollatingInterval interval, final V1 value, final Node<V1> root ) {\n+            right = new Node<>(this, interval, value);\n+            return insertFixup(right, root);\n+        }\n+\n+        Node<V1> getNext() {\n+            Node<V1> result;\n+\n+            if ( right != null ) {\n+                result = right;\n+                while ( result.left != null ) {\n+                    result = result.left;\n+                }\n+            } else {\n+                Node<V1> node = this;\n+                result = parent;\n+                while ( result != null && node == result.right ) {\n+                    node = result;\n+                    result = result.parent;\n+                }\n+            }\n+\n+            return result;\n+        }\n+\n+        Node<V1> getPrev() {\n+            Node<V1> result;\n+\n+            if ( left != null ) {\n+                result = left;\n+                while ( result.right != null ) {\n+                    result = result.right;\n+                }\n+            } else {\n+                Node<V1> node = this;\n+                result = parent;\n+                while ( result != null && node == result.left ) {\n+                    node = result;\n+                    result = result.parent;\n+                }\n+            }\n+\n+            return result;\n+        }\n+\n+        boolean wasRemoved() {\n+            return size == 0;\n+        }\n+\n+        Node<V1> remove( final Node<V1> initialRoot ) {\n+            Node<V1> root = initialRoot;\n+            if ( size == 0 ) {\n+                throw new IllegalStateException(\"Entry was already removed.\");\n+            }\n+\n+            if ( left == null ) {\n+                if ( right == null ) { // no children\n+                    if ( parent == null ) {\n+                        root = null;\n+                    } else if ( parent.left == this ) {\n+                        parent.left = null;\n+                        fixup(parent);\n+\n+                        if ( isBlack ) {\n+                            root = removeFixup(parent, null, root);\n+                        }\n+                    } else {\n+                        parent.right = null;\n+                        fixup(parent);\n+\n+                        if ( isBlack ) {\n+                            root = removeFixup(parent, null, root);\n+                        }\n+                    }\n+                } else { // single child on right\n+                    root = spliceOut(right, root);\n+                }\n+            } else if ( right == null ) { // single child on left\n+                root = spliceOut(left, root);\n+            } else { // two children\n+                final Node<V1> next = getNext();\n+                root = next.remove(root);\n+\n+                // put next into tree in same position as this, effectively removing this\n+                if ( (next.parent = parent) == null ) {\n+                    root = next;\n+                } else if ( parent.left == this ) {\n+                    parent.left = next;\n+                } else {\n+                    parent.right = next;\n+                }\n+\n+                if ( (next.left = left) != null ) {\n+                    left.parent = next;\n+                }\n+\n+                if ( (next.right = right) != null ) {\n+                    right.parent = next;\n+                }\n+\n+                next.isBlack = isBlack;\n+                next.size = size;\n+                fixup(next);\n+            }\n+\n+            size = 0;\n+            return root;\n+        }\n+\n+        static <V1> Node<V1> getNextOverlapper( final Node<V1> startingNode, final CollatingInterval interval ) {\n+            Node<V1> node = startingNode;\n+            do {\n+                Node<V1> nextNode = node.right;\n+                if ( nextNode != null && !nextNode.maxEndInterval.isUpstreamOf(interval) ) {\n+                    node = nextNode;\n+                    while ( (nextNode = node.left) != null && !nextNode.maxEndInterval.isUpstreamOf(interval) )\n+                        node = nextNode;\n+                } else {\n+                    nextNode = node;\n+                    while ( (node = nextNode.parent) != null && node.right == nextNode )\n+                        nextNode = node;\n+                }\n+\n+                if ( node != null && interval.isUpstreamOf(node.interval) ) {\n+                    node = null;\n+                }\n+            }\n+            while ( node != null && !interval.overlaps(node.interval) );\n+\n+            return node;\n+        }\n+\n+        static <V1> Node<V1> findByRank( final Node<V1> startingNode, final int initialRank ) {\n+            Node<V1> node = startingNode;\n+            int rank = initialRank;\n+            while ( node != null ) {\n+                final int nodeRank = node.getRank();\n+                if ( rank == nodeRank ) {\n+                    break;\n+                }\n+\n+                if ( rank < nodeRank ) {\n+                    node = node.left;\n+                } else {\n+                    node = node.right;\n+                    rank -= nodeRank;\n+                }\n+            }\n+\n+            return node;\n+        }\n+\n+        static <V1> int getRank( final Node<V1> startingNode, final CollatingInterval interval ) {\n+            Node<V1> node = startingNode;\n+            int rank = 0;\n+\n+            while ( node != null ) {\n+                final int cmpVal = interval.compareTo(node.getInterval());\n+                if ( cmpVal < 0 ) {\n+                    node = node.left;\n+                } else {\n+                    rank += node.getRank();\n+                    if ( cmpVal == 0 ) {\n+                        return rank; // EARLY RETURN!!!\n+                    }\n+\n+                    node = node.right;\n+                }\n+            }\n+\n+            return 0;\n+        }\n+\n+        private int getRank() {\n+            int result = 1;\n+            if ( left != null ) {\n+                result = left.size + 1;\n+            }\n+            return result;\n+        }\n+\n+        private Node<V1> spliceOut( final Node<V1> child, final Node<V1> initialRoot ) {\n+            Node<V1> root = initialRoot;\n+            if ( (child.parent = parent) == null ) {\n+                root = child;\n+                child.isBlack = true;\n+            } else {\n+                if ( parent.left == this ) {\n+                    parent.left = child;\n+                } else {\n+                    parent.right = child;\n+                }\n+                fixup(parent);\n+\n+                if ( isBlack ) {\n+                    root = removeFixup(parent, child, root);\n+                }\n+            }\n+\n+            return root;\n+        }\n+\n+        private Node<V1> rotateLeft( final Node<V1> initialRoot ) {\n+            Node<V1> root = initialRoot;\n+            final Node<V1> child = right;\n+\n+            final int childSize = child.size;\n+            child.size = size;\n+            size -= childSize;\n+\n+            if ( (right = child.left) != null ) {\n+                right.parent = this;\n+                size += right.size;\n+            }\n+\n+            if ( (child.parent = parent) == null ) {\n+                root = child;\n+            } else if ( this == parent.left ) {\n+                parent.left = child;\n+            } else {\n+                parent.right = child;\n+            }\n+\n+            child.left = this;\n+            parent = child;\n+\n+            setMaxEnd();\n+            child.setMaxEnd();\n+\n+            return root;\n+        }\n+\n+        private Node<V1> rotateRight( final Node<V1> initialRoot ) {\n+            Node<V1> root = initialRoot;\n+            final Node<V1> child = left;\n+\n+            final int childSize = child.size;\n+            child.size = size;\n+            size -= childSize;\n+\n+            if ( (left = child.right) != null ) {\n+                left.parent = this;\n+                size += left.size;\n+            }\n+\n+            if ( (child.parent = parent) == null ) {\n+                root = child;\n+            } else if ( this == parent.left ) {\n+                parent.left = child;\n+            } else {\n+                parent.right = child;\n+            }\n+\n+            child.right = this;\n+            parent = child;\n+\n+            setMaxEnd();\n+            child.setMaxEnd();\n+\n+            return root;\n+        }\n+\n+        private void setMaxEnd() {\n+            maxEndInterval = interval;\n+            if ( left != null ) {\n+                maxEndInterval = CollatingInterval.laterEnding(maxEndInterval, left.maxEndInterval);\n+            }\n+            if ( right != null ) {\n+                maxEndInterval = CollatingInterval.laterEnding(maxEndInterval, right.maxEndInterval);\n+            }\n+        }\n+\n+        private static <V1> void fixup( final Node<V1> initialNode ) {\n+            Node<V1> node = initialNode;\n+            do {\n+                node.size = 1;\n+                if ( node.left != null ) {\n+                    node.size += node.left.size;\n+                }\n+                if ( node.right != null ) {\n+                    node.size += node.right.size;\n+                }\n+                node.setMaxEnd();\n+            }\n+            while ( (node = node.parent) != null );\n+        }\n+\n+        private static <V1> Node<V1> insertFixup( final Node<V1> initialDaughter, final Node<V1> initialRoot ) {\n+            Node<V1> daughter = initialDaughter;\n+            Node<V1> root = initialRoot;\n+            Node<V1> mom = daughter.parent;\n+            fixup(mom);\n+\n+            while ( mom != null && !mom.isBlack ) {\n+                final Node<V1> gramma = mom.parent;\n+                Node<V1> auntie = gramma.left;\n+                if ( auntie == mom ) {\n+                    auntie = gramma.right;\n+                    if ( auntie != null && !auntie.isBlack ) {\n+                        mom.isBlack = true;\n+                        auntie.isBlack = true;\n+                        gramma.isBlack = false;\n+                        daughter = gramma;\n+                    } else {\n+                        if ( daughter == mom.right ) {\n+                            root = mom.rotateLeft(root);\n+                            mom = daughter;\n+                        }\n+                        mom.isBlack = true;\n+                        gramma.isBlack = false;\n+                        root = gramma.rotateRight(root);\n+                        break;\n+                    }\n+                } else {\n+                    if ( auntie != null && !auntie.isBlack ) {\n+                        mom.isBlack = true;\n+                        auntie.isBlack = true;\n+                        gramma.isBlack = false;\n+                        daughter = gramma;\n+                    } else {\n+                        if ( daughter == mom.left ) {\n+                            root = mom.rotateRight(root);\n+                            mom = daughter;\n+                        }\n+                        mom.isBlack = true;\n+                        gramma.isBlack = false;\n+                        root = gramma.rotateLeft(root);\n+                        break;\n+                    }\n+                }\n+                mom = daughter.parent;\n+            }\n+            root.isBlack = true;\n+            return root;\n+        }\n+\n+        private static <V1> Node<V1> removeFixup( final Node<V1> initialParent,\n+                                                                                                                    final Node<V1> initialNode, final Node<V1> initialRoot ) {\n+            Node<V1> parent = initialParent;\n+            Node<V1> node = initialNode;\n+            Node<V1> root = initialRoot;\n+            do {\n+                if ( node == parent.left ) {\n+                    Node<V1> sister = parent.right;\n+                    if ( !sister.isBlack ) {\n+                        sister.isBlack = true;\n+                        parent.isBlack = false;\n+                        root = parent.rotateLeft(root);\n+                        sister = parent.right;\n+                    }\n+                    if ( (sister.left == null || sister.left.isBlack) && (sister.right == null || sister.right.isBlack) ) {\n+                        sister.isBlack = false;\n+                        node = parent;\n+                    } else {\n+                        if ( sister.right == null || sister.right.isBlack ) {\n+                            sister.left.isBlack = true;\n+                            sister.isBlack = false;\n+                            root = sister.rotateRight(root);\n+                            sister = parent.right;\n+                        }\n+                        sister.isBlack = parent.isBlack;\n+                        parent.isBlack = true;\n+                        sister.right.isBlack = true;\n+                        root = parent.rotateLeft(root);\n+                        node = root;\n+                    }\n+                } else {\n+                    Node<V1> sister = parent.left;\n+                    if ( !sister.isBlack ) {\n+                        sister.isBlack = true;\n+                        parent.isBlack = false;\n+                        root = parent.rotateRight(root);\n+                        sister = parent.left;\n+                    }\n+                    if ( (sister.left == null || sister.left.isBlack) && (sister.right == null || sister.right.isBlack) ) {\n+                        sister.isBlack = false;\n+                        node = parent;\n+                    } else {\n+                        if ( sister.left == null || sister.left.isBlack ) {\n+                            sister.right.isBlack = true;\n+                            sister.isBlack = false;\n+                            root = sister.rotateLeft(root);\n+                            sister = parent.left;\n+                        }\n+                        sister.isBlack = parent.isBlack;\n+                        parent.isBlack = true;\n+                        sister.left.isBlack = true;\n+                        root = parent.rotateRight(root);\n+                        node = root;\n+                    }\n+                }\n+                parent = node.parent;\n+            }\n+            while ( parent != null && node.isBlack );\n+\n+            node.isBlack = true;\n+            return root;\n+        }\n+    }\n+\n+    abstract class IteratorBase implements Iterator<Entry<V>> {\n+        protected Node<V> next;\n+        protected Node<V> last;\n+\n+        protected IteratorBase( final Node<V> node ) { next = node; }\n+\n+        @Override\n+        public boolean hasNext() {\n+            return next != null;\n+        }\n+\n+        @Override\n+        public void remove() {\n+            if ( last == null ) {\n+                throw new IllegalStateException(\"No entry to remove.\");\n+            }\n+            removeNode(last);\n+            last = null;\n+        }\n+\n+        // equality of iterators is defined as having the same current position in the tree\n+        @Override\n+        public boolean equals( final Object obj ) {\n+            return obj instanceof IntervalTree<?>.IteratorBase &&\n+                    ((IntervalTree<?>.IteratorBase)obj).next == next;\n+        }\n+\n+        @Override\n+        public int hashCode() {\n+            return Objects.hashCode(next);\n+        }\n+    }\n+\n+    public final class FwdIterator extends IteratorBase {\n+\n+        public FwdIterator( final Node<V> node ) {\n+            super(node);\n+        }\n+\n+        @Override\n+        public Node<V> next() {\n+            if ( next == null ) {\n+                throw new NoSuchElementException(\"No next element.\");\n+            }\n+\n+            if ( next.wasRemoved() ) {\n+                next = (Node<V>)min(next.getInterval());\n+                if ( next == null ) {\n+                    throw new ConcurrentModificationException(\"Current element was removed, and there are no more elements.\");\n+                }\n+            }\n+            last = next;\n+            next = next.getNext();\n+            return last;\n+        }\n+    }\n+\n+    public final class RevIterator extends IteratorBase {\n+\n+        public RevIterator( final Node<V> node ) {\n+            super(node);\n+        }\n+\n+        @Override\n+        public Node<V> next() {\n+            if ( next == null ) {\n+                throw new NoSuchElementException(\"No next element.\");\n+            }\n+\n+            if ( next.wasRemoved() ) {\n+                next = (Node<V>)max(next.getInterval());\n+                if ( next == null ) {\n+                    throw new ConcurrentModificationException(\"Current element was removed, and there are no more elements.\");\n+                }\n+            }\n+            last = next;\n+            next = next.getPrev();\n+            return last;\n+        }\n+    }\n+\n+    public final class OverlapIterator extends IteratorBase {\n+        private final CollatingInterval interval;\n+\n+        public OverlapIterator( final CollatingInterval interval ) {\n+            super((Node<V>)minOverlapper(interval));\n+            this.interval = interval;\n+        }\n+\n+        @Override\n+        public Node<V> next() {\n+            if ( next == null ) {\n+                throw new NoSuchElementException(\"No next element.\");\n+            }\n+\n+            if ( next.wasRemoved() ) {\n+                throw new ConcurrentModificationException(\"Current element was removed.\");\n+            }\n+\n+            last = next;\n+            next = Node.getNextOverlapper(next, interval);\n+            return last;\n+        }\n+    }\n+\n+    public final static class ValuesIterator<V1> implements Iterator<V1> {\n+        private final Iterator<Entry<V1>> itr;\n+\n+        public ValuesIterator( final Iterator<Entry<V1>> itr ) {\n+            this.itr = itr;\n+        }\n+\n+        @Override\n+        public boolean hasNext() {\n+            return itr.hasNext();\n+        }\n+\n+        @Override\n+        public V1 next() {\n+            return itr.next().getValue();\n+        }\n+\n+        @Override\n+        public void remove() {\n+            itr.remove();\n+        }\n+    }\n+}"
  },
  {
    "sha": "5c803e46e225915f4864aad83554d8f67ff4f4b8",
    "filename": "src/main/java/org/broadinstitute/hellbender/utils/io/BlockCompressedIntervalStream.java",
    "status": "added",
    "additions": 626,
    "deletions": 0,
    "changes": 626,
    "blob_url": "https://github.com/broadinstitute/gatk/blob/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/utils/io/BlockCompressedIntervalStream.java",
    "raw_url": "https://github.com/broadinstitute/gatk/raw/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/main/java/org/broadinstitute/hellbender/utils/io/BlockCompressedIntervalStream.java",
    "contents_url": "https://api.github.com/repos/broadinstitute/gatk/contents/src/main/java/org/broadinstitute/hellbender/utils/io/BlockCompressedIntervalStream.java?ref=304d73f3bd34f59732b9e3d7676b373d0e688f9b",
    "patch": "@@ -0,0 +1,626 @@\n+package org.broadinstitute.hellbender.utils.io;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.seekablestream.SeekableBufferedStream;\n+import htsjdk.samtools.seekablestream.SeekableStream;\n+import htsjdk.samtools.seekablestream.SeekableStreamFactory;\n+import htsjdk.samtools.util.BlockCompressedInputStream;\n+import htsjdk.samtools.util.BlockCompressedOutputStream;\n+import htsjdk.samtools.util.BlockCompressedStreamConstants;\n+import htsjdk.tribble.CloseableTribbleIterator;\n+import htsjdk.tribble.Feature;\n+import htsjdk.tribble.FeatureCodec;\n+import htsjdk.tribble.FeatureReader;\n+import org.broadinstitute.hellbender.engine.FeatureInput;\n+import org.broadinstitute.hellbender.engine.GATKPath;\n+import org.broadinstitute.hellbender.exceptions.GATKException;\n+import org.broadinstitute.hellbender.exceptions.UserException;\n+import org.broadinstitute.hellbender.utils.CollatingInterval;\n+import org.broadinstitute.hellbender.utils.collections.IntervalTree;\n+\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.file.Path;\n+import java.util.*;\n+\n+public class BlockCompressedIntervalStream {\n+\n+    // our final empty block adds to the extra information\n+    // it has a virtual file pointer to the start of the index\n+    public static final byte[] EMPTY_GZIP_BLOCK_WITH_INDEX_POINTER = {\n+            BlockCompressedStreamConstants.GZIP_ID1,\n+            (byte)BlockCompressedStreamConstants.GZIP_ID2,\n+            BlockCompressedStreamConstants.GZIP_CM_DEFLATE,\n+            BlockCompressedStreamConstants.GZIP_FLG,\n+            0, 0, 0, 0, // modification time\n+            BlockCompressedStreamConstants.GZIP_XFL,\n+            (byte)BlockCompressedStreamConstants.GZIP_OS_UNKNOWN,\n+            BlockCompressedStreamConstants.GZIP_XLEN + 12, 0,\n+            BlockCompressedStreamConstants.BGZF_ID1,\n+            BlockCompressedStreamConstants.BGZF_ID2,\n+            BlockCompressedStreamConstants.BGZF_LEN, 0,\n+            39, 0, // Total block size - 1 as little-endian short\n+            (byte)'I', (byte)'P', 8, 0, // index pointer extra data\n+\n+            // 8-byte little-endian long representing file-pointer to beginning of index\n+            // (this data gets overwritten each time a stream is closed)\n+            1, 2, 3, 4, 5, 6, 7, 8,\n+\n+            3, 0, // empty payload\n+            0, 0, 0, 0, // crc\n+            0, 0, 0, 0, // uncompressedSize\n+    };\n+    public static final int FILE_POINTER_OFFSET = 22;\n+\n+    public static final String BCI_FILE_EXTENSION = \".bci\";\n+\n+    // each compressed block of data will have (at least) one of these as a part of the index\n+    // for each contig that appears in a compressed block the CollatingInterval tracks the smallest\n+    //   starting coordinate and largest end coordinate of any object in the block\n+    // the filePosition member is the virtual file offset of the first object in the block (or, the\n+    //   first object for a new contig, if there are multiple contigs represented within the block)\n+    public final static class IndexEntry {\n+        final CollatingInterval interval;\n+        long filePosition;\n+\n+        public IndexEntry( final CollatingInterval interval, final long filePosition ) {\n+            this.interval = interval;\n+            this.filePosition = filePosition;\n+        }\n+\n+        public IndexEntry( final DataInputStream dis,\n+                           final SAMSequenceDictionary dict ) throws IOException {\n+            this.interval = new CollatingInterval(dict, dis);\n+            this.filePosition = dis.readLong();\n+        }\n+\n+        public CollatingInterval getInterval() { return interval; }\n+        public long getFilePosition() { return filePosition; }\n+\n+        public void write( final DataOutputStream dos ) throws IOException {\n+            interval.write(dos);\n+            dos.writeLong(filePosition);\n+        }\n+    }\n+\n+    @FunctionalInterface\n+    public interface WriteFunc<T extends Feature> {\n+        void write( T tee, Writer<T> writer ) throws IOException;\n+    }\n+\n+    public static class Header {\n+        private final String className;\n+        private final String version;\n+        private final SAMSequenceDictionary dictionary;\n+        private final List<String> sampleNames;\n+\n+        public Header( final String className, final String version,\n+                       final SAMSequenceDictionary dictionary, final List<String> sampleNames ) {\n+            this.className = className;\n+            this.version = version;\n+            this.dictionary = dictionary;\n+            this.sampleNames = sampleNames;\n+        }\n+        public String getClassName() { return className; }\n+        public String getVersion() { return version; }\n+        public SAMSequenceDictionary getDictionary() { return dictionary; }\n+        public List<String> getSampleNames() { return sampleNames; }\n+    }\n+\n+    // a class for writing arbitrary objects to a block compressed stream with a self-contained index\n+    // the only restriction is that you must supply a lambda that writes enough state to a DataOutputStream\n+    //   to allow you to reconstitute the object when you read it back in later AND you have to\n+    //   return an CollatingInterval so that we can do indexing.\n+    public static class Writer <T extends Feature> {\n+        final String path;\n+        final SAMSequenceDictionary dict;\n+        final Map<String, Integer> sampleMap;\n+        final WriteFunc<T> writeFunc;\n+        final OutputStream os;\n+        final BlockCompressedOutputStream bcos;\n+        final DataOutputStream dos;\n+        Feature lastInterval;\n+        final List<IndexEntry> indexEntries;\n+        long blockFilePosition;\n+        String blockContig;\n+        int blockStart;\n+        int blockEnd;\n+        boolean firstBlockMember;\n+\n+        public final static int DEFAULT_COMPRESSION_LEVEL = 6;\n+\n+        public Writer( final GATKPath path,\n+                       final Header header,\n+                       final WriteFunc<T> writeFunc ) {\n+            this(path, header, writeFunc, DEFAULT_COMPRESSION_LEVEL);\n+        }\n+\n+        public Writer( final GATKPath path,\n+                       final Header header,\n+                       final WriteFunc<T> writeFunc,\n+                       final int compressionLevel ) {\n+            this.path = path.toString();\n+            this.dict = header.getDictionary();\n+            this.sampleMap = createSampleMap(header.getSampleNames());\n+            this.writeFunc = writeFunc;\n+            this.os = path.getOutputStream();\n+            this.bcos = new BlockCompressedOutputStream(os, (Path)null, compressionLevel);\n+            this.dos = new DataOutputStream(bcos);\n+            this.lastInterval = null;\n+            this.indexEntries = new ArrayList<>();\n+            this.firstBlockMember = true;\n+            writeHeader(header);\n+        }\n+\n+        @VisibleForTesting\n+        public Writer( final String streamSource,\n+                final OutputStream os,\n+                final Header header,\n+                final WriteFunc<T> writeFunc ) {\n+            this.path = streamSource;\n+            this.dict = header.getDictionary();\n+            this.sampleMap = createSampleMap(header.getSampleNames());\n+            this.writeFunc = writeFunc;\n+            this.os = os;\n+            this.bcos = new BlockCompressedOutputStream(os, (Path)null, DEFAULT_COMPRESSION_LEVEL);\n+            this.dos = new DataOutputStream(bcos);\n+            this.lastInterval = null;\n+            this.indexEntries = new ArrayList<>();\n+            this.firstBlockMember = true;\n+            writeHeader(header);\n+        }\n+\n+        private Map<String, Integer> createSampleMap( final List<String> sampleNames ) {\n+            final Map<String, Integer> sampleMap = new HashMap<>(sampleNames.size() * 3 / 2);\n+            for ( final String sampleName : sampleNames ) {\n+                sampleMap.put(sampleName, sampleMap.size());\n+            }\n+            return sampleMap;\n+        }\n+\n+        private void writeHeader( final Header header ) {\n+            try {\n+                writeClassAndVersion(header.getClassName(), header.getVersion());\n+                writeSamples(header.getSampleNames());\n+                writeDictionary(header.getDictionary());\n+                dos.flush();\n+            } catch ( final IOException ioe ) {\n+                throw new UserException(\"can't write header to \" + path, ioe);\n+            }\n+        }\n+\n+        private void writeClassAndVersion( final String className, final String version )\n+                throws IOException {\n+            dos.writeUTF(className);\n+            dos.writeUTF(version);\n+        }\n+\n+        private void writeSamples( final List<String> sampleNames ) throws IOException {\n+            dos.writeInt(sampleNames.size());\n+            for ( final String sampleName : sampleNames ) {\n+                dos.writeUTF(sampleName);\n+            }\n+        }\n+\n+        private void writeDictionary( final SAMSequenceDictionary dictionary ) throws IOException {\n+            dos.writeInt(dictionary.size());\n+            for ( final SAMSequenceRecord rec : dictionary.getSequences() ) {\n+                dos.writeInt(rec.getSequenceLength());\n+                dos.writeUTF(rec.getSequenceName());\n+            }\n+        }\n+\n+        public DataOutputStream getStream() { return dos; }\n+\n+        public int getSampleIndex( final String sampleName ) {\n+            final Integer sampleIndex = sampleMap.get(sampleName);\n+            if ( sampleIndex == null ) {\n+                throw new UserException(\"can't find index for sampleName \" + sampleName);\n+            }\n+            return sampleIndex;\n+        }\n+\n+        public int getContigIndex( final String contigName ) {\n+            final SAMSequenceRecord contig = dict.getSequence(contigName);\n+            if ( contig == null ) {\n+                throw new UserException(\"can't find contig name \" + contigName);\n+            }\n+            return contig.getSequenceIndex();\n+        }\n+\n+        public void write( final T interval ) {\n+            final long prevFilePosition = bcos.getPosition();\n+            // write the object\n+            try {\n+                writeFunc.write(interval, this);\n+            } catch ( final IOException ioe ) {\n+                throw new UserException(\"can't write to \" + path, ioe);\n+            }\n+\n+            // if this is the first interval we've seen in a block, just capture the block-start data\n+            if ( firstBlockMember || lastInterval == null ) {\n+                startBlock(prevFilePosition, interval);\n+                return;\n+            }\n+\n+            // if the contig changes emit a new index entry (for the previous contig) and\n+            //   restart tracking of the block\n+            if ( !interval.contigsMatch(lastInterval) ) {\n+                addIndexEntry();\n+                startBlock(prevFilePosition, interval);\n+                return;\n+            }\n+\n+            // extend the tracked interval, as necessary\n+            blockEnd = Math.max(blockEnd, interval.getEnd());\n+            lastInterval = interval;\n+\n+            // if writing this element caused a new block to be compressed and added to the file\n+            if ( isNewBlock(prevFilePosition, bcos.getPosition()) ) {\n+                addIndexEntry();\n+                firstBlockMember = true;\n+            }\n+        }\n+\n+        public void close() {\n+            // take care of any pending index entry, if necessary\n+            if ( !firstBlockMember ) {\n+                addIndexEntry();\n+            }\n+\n+            try {\n+                dos.flush(); // complete the data block\n+\n+                long indexPosition = bcos.getPosition(); // current position is the start of the index\n+\n+                // write the index entries\n+                dos.writeInt(indexEntries.size());\n+                for ( final IndexEntry indexEntry : indexEntries ) {\n+                    indexEntry.write(dos);\n+                }\n+                dos.flush(); // and complete the block\n+\n+                // write a 0-length terminator block at the end that captures the index position\n+                final byte[] emptyBlockWithIndexPointer =\n+                        Arrays.copyOf(EMPTY_GZIP_BLOCK_WITH_INDEX_POINTER,\n+                                EMPTY_GZIP_BLOCK_WITH_INDEX_POINTER.length);\n+                for ( int idx = FILE_POINTER_OFFSET; idx != FILE_POINTER_OFFSET + 8; ++idx ) {\n+                    emptyBlockWithIndexPointer[idx] = (byte)indexPosition;\n+                    indexPosition >>>= 8;\n+                }\n+                os.write(emptyBlockWithIndexPointer);\n+\n+                bcos.close(false); // we've already handled the terminator block\n+            } catch ( final IOException ioe ) {\n+                throw new UserException(\"unable to add index and close \" + path, ioe);\n+            }\n+        }\n+\n+        private void startBlock( final long filePosition, final T interval ) {\n+            blockFilePosition = filePosition;\n+            lastInterval = interval;\n+            blockContig = interval.getContig();\n+            blockStart = interval.getStart();\n+            blockEnd = interval.getEnd();\n+            firstBlockMember = false;\n+        }\n+\n+        private void addIndexEntry() {\n+            final CollatingInterval blockInterval =\n+                    new CollatingInterval(dict, blockContig, blockStart, blockEnd);\n+            indexEntries.add(new IndexEntry(blockInterval, blockFilePosition));\n+        }\n+    }\n+\n+    // a class for reading arbitrary objects from a block compressed stream with a self-contained index\n+    // the only restriction is that you must supply a lambda that reads from a DataInputStream\n+    //   to reconstitute the object.\n+    public static final class Reader <T extends Feature> implements FeatureReader<T> {\n+        final String path;\n+        final FeatureCodec<T, Reader<T>> codec;\n+        final long indexFilePointer;\n+        final BlockCompressedInputStream bcis;\n+        final DataInputStream dis;\n+        final Header header;\n+        final long dataFilePointer;\n+        IntervalTree<Long> index;\n+        boolean usedByIterator;\n+\n+        public Reader( final FeatureInput<T> inputDescriptor, final FeatureCodec<T, Reader<T>> codec ) {\n+            this.path = inputDescriptor.getRawInputString();\n+            this.codec = codec;\n+            final SeekableStream ss;\n+            try {\n+                ss = SeekableStreamFactory.getInstance().getStreamFor(path);\n+            } catch ( final IOException ioe ) {\n+                throw new UserException(\"unable to open \" + path, ioe);\n+            }\n+            this.indexFilePointer = findIndexFilePointer(ss);\n+            this.bcis = new BlockCompressedInputStream(new SeekableBufferedStream(ss));\n+            this.dis = new DataInputStream(bcis);\n+            this.header = readHeader();\n+            this.dataFilePointer = bcis.getPosition(); // having read header, we're pointing at the data\n+            this.index = null;\n+            this.usedByIterator = false;\n+            final String expectedClassName = codec.getFeatureType().getSimpleName();\n+            if ( !header.getClassName().equals(expectedClassName) ) {\n+                throw new UserException(\"can't use \" + path + \" to read \" + expectedClassName +\n+                        \" features -- it contains \" + header.getClassName() + \" features\");\n+            }\n+        }\n+\n+        public Reader( final Reader<T> reader ) {\n+            this.path = reader.path;\n+            this.codec = reader.codec;\n+            this.indexFilePointer = reader.indexFilePointer;\n+            try {\n+                this.bcis = new BlockCompressedInputStream(\n+                        new SeekableBufferedStream(\n+                                SeekableStreamFactory.getInstance().getStreamFor(path)));\n+            } catch ( final IOException ioe ) {\n+                throw new UserException(\"unable to clone stream for \" + path, ioe);\n+            }\n+            this.dis = new DataInputStream(bcis);\n+            this.header = reader.header;\n+            this.dataFilePointer = reader.dataFilePointer;\n+            this.index = reader.index;\n+            this.usedByIterator = true;\n+        }\n+\n+        @VisibleForTesting\n+        public Reader( final String inputStreamName,\n+                       final SeekableStream ss,\n+                       final FeatureCodec<T, Reader<T>> codec ) {\n+            this.path = inputStreamName;\n+            this.codec = codec;\n+            this.indexFilePointer = findIndexFilePointer(ss);\n+            this.bcis = new BlockCompressedInputStream(new SeekableBufferedStream(ss));\n+            this.dis = new DataInputStream(bcis);\n+            this.header = readHeader();\n+            this.dataFilePointer = bcis.getPosition();\n+            this.index = null;\n+            this.usedByIterator = false;\n+        }\n+\n+        public DataInputStream getStream() { return dis; }\n+        public List<String> getSampleNames() { return header.getSampleNames(); }\n+        public SAMSequenceDictionary getDictionary() { return header.getDictionary(); }\n+        public String getVersion() { return header.getVersion(); }\n+\n+        public boolean hasNext() {\n+            final long position = bcis.getPosition();\n+            // A BlockCompressedInputStream returns a 0 position when closed\n+            return position > 0 && position < indexFilePointer;\n+        }\n+\n+        @Override\n+        public CloseableTribbleIterator<T> query( final String chr, final int start, final int end )\n+                throws IOException {\n+            if ( index == null ) {\n+                loadIndex(bcis);\n+            }\n+            final CollatingInterval interval =\n+                    new CollatingInterval(getDictionary(), chr, start, end);\n+            return new OverlapIterator<>(interval, this);\n+        }\n+\n+        @Override public CloseableTribbleIterator<T> iterator() {\n+            return new CompleteIterator<>(this);\n+        }\n+\n+        @Override public void close() {\n+            try {\n+                dis.close();\n+            } catch ( final IOException ioe ) {\n+                throw new UserException(\"unable to close \" + path, ioe);\n+            }\n+        }\n+\n+        @Override public List<String> getSequenceNames() {\n+            final SAMSequenceDictionary dict = getDictionary();\n+            final List<String> names = new ArrayList<>(dict.size());\n+            for ( final SAMSequenceRecord rec : dict.getSequences() ) {\n+                names.add(rec.getSequenceName());\n+            }\n+            return names;\n+        }\n+\n+        @Override public Object getHeader() { return header; }\n+\n+        @Override public boolean isQueryable() { return true; }\n+\n+        public long getPosition() { return bcis.getPosition(); }\n+\n+        public void seekStream( final long filePointer ) {\n+            try {\n+                bcis.seek(filePointer);\n+            } catch ( final IOException ioe ) {\n+                throw new UserException(\"unable to position stream for \" + path, ioe);\n+            }\n+        }\n+\n+        public T readStream() {\n+            try {\n+                return codec.decode(this);\n+            } catch ( final IOException ioe ) {\n+                throw new GATKException(\"can't read \" + path, ioe);\n+            }\n+        }\n+\n+        private long findIndexFilePointer( final SeekableStream ss ) {\n+            final int finalBlockLen = EMPTY_GZIP_BLOCK_WITH_INDEX_POINTER.length;\n+            final byte[] finalBlock = new byte[finalBlockLen];\n+            try {\n+                ss.seek(ss.length() - finalBlockLen);\n+                ss.readFully(finalBlock);\n+                ss.seek(0);\n+            } catch ( final IOException ioe ) {\n+                throw new UserException(\"unable to read final bgzip block from \" + path, ioe);\n+            }\n+            for ( int idx = 0; idx != FILE_POINTER_OFFSET; ++idx ) {\n+                if ( EMPTY_GZIP_BLOCK_WITH_INDEX_POINTER[idx] != finalBlock[idx] ) {\n+                    throw new UserException(\n+                            \"unable to recover index pointer from final block of \" + path);\n+                }\n+            }\n+            for ( int idx = FILE_POINTER_OFFSET + 8; idx != finalBlockLen; ++idx ) {\n+                if ( EMPTY_GZIP_BLOCK_WITH_INDEX_POINTER[idx] != finalBlock[idx] ) {\n+                    throw new UserException(\n+                            \"unable to recover index pointer from final block of \" + path);\n+                }\n+            }\n+            long indexFilePointer = 0;\n+            int idx = FILE_POINTER_OFFSET + 8;\n+            while ( --idx >= FILE_POINTER_OFFSET ) {\n+                indexFilePointer <<= 8;\n+                indexFilePointer |= finalBlock[idx] & 0xFFL;\n+            }\n+            return indexFilePointer;\n+        }\n+\n+        private Header readHeader() {\n+            try {\n+                final String className = dis.readUTF();\n+                final String version = dis.readUTF();\n+                final List<String> sampleNames = readSampleNames(dis);\n+                final SAMSequenceDictionary dictionary = readDictionary(dis);\n+                return new Header(className, version, dictionary, sampleNames);\n+            } catch ( final IOException ioe ) {\n+                throw new UserException(\"can't read header from \" + path, ioe);\n+            }\n+        }\n+\n+        private List<String> readSampleNames( final DataInputStream dis ) throws IOException {\n+            final int nSamples = dis.readInt();\n+            final List<String> sampleNames = new ArrayList<>(nSamples);\n+            for ( int sampleId = 0; sampleId < nSamples; ++sampleId ) {\n+                sampleNames.add(dis.readUTF());\n+            }\n+            return sampleNames;\n+        }\n+\n+        private SAMSequenceDictionary readDictionary( final DataInputStream dis ) throws IOException {\n+            final int nRecs = dis.readInt();\n+            final List<SAMSequenceRecord> seqRecs = new ArrayList<>(nRecs);\n+            for ( int idx = 0; idx != nRecs; ++idx ) {\n+                final int contigSize = dis.readInt();\n+                final String contigName = dis.readUTF();\n+                seqRecs.add(new SAMSequenceRecord(contigName, contigSize));\n+            }\n+            return new SAMSequenceDictionary(seqRecs);\n+        }\n+\n+        private void loadIndex( final BlockCompressedInputStream bcis ) {\n+            final IntervalTree<Long> intervalTree = new IntervalTree<>();\n+            try {\n+                bcis.seek(indexFilePointer);\n+                final DataInputStream dis = new DataInputStream(bcis);\n+                final SAMSequenceDictionary dict = getDictionary();\n+                int nEntries = dis.readInt();\n+                while ( nEntries-- > 0 ) {\n+                    final IndexEntry entry = new IndexEntry(dis, dict);\n+                    intervalTree.put(entry.getInterval(), entry.getFilePosition());\n+                }\n+                bcis.seek(dataFilePointer);\n+            } catch ( final IOException ioe ) {\n+                throw new UserException(\"unable to read index from \" + path, ioe);\n+            }\n+            index = intervalTree;\n+        }\n+\n+        private Reader<T> getReaderForIterator() {\n+            if ( !usedByIterator ) {\n+                usedByIterator = true;\n+                return this;\n+            }\n+            return new Reader<>(this);\n+        }\n+\n+        private static class CompleteIterator <T extends Feature>\n+                implements CloseableTribbleIterator<T> {\n+            final Reader<T> reader;\n+            public CompleteIterator( final Reader<T> reader ) {\n+                this.reader = reader.getReaderForIterator();\n+                reader.seekStream(reader.dataFilePointer);\n+            }\n+\n+            @Override public Iterator<T> iterator() {\n+                return new CompleteIterator<>(reader);\n+            }\n+\n+            @Override public boolean hasNext() {\n+                return reader.hasNext();\n+            }\n+\n+            @Override public T next() {\n+                if ( !hasNext() ) {\n+                    throw new NoSuchElementException(\"feature iterator has no next element\");\n+                }\n+                return reader.readStream();\n+            }\n+\n+            @Override public void close() { reader.close(); }\n+        }\n+        // find all the objects in the stream inflating just those blocks that might have relevant objects\n+        private static class OverlapIterator <T extends Feature>\n+                implements CloseableTribbleIterator<T> {\n+            final CollatingInterval interval;\n+            final Reader<T> reader;\n+            final Iterator<IntervalTree.Entry<Long>> indexEntryIterator;\n+            long blockStartPosition;\n+            T nextT;\n+\n+            public OverlapIterator( final CollatingInterval interval, final Reader<T> reader ) {\n+                this.interval = interval;\n+                this.reader = reader.getReaderForIterator();\n+                this.indexEntryIterator = reader.index.overlappers(interval);\n+                this.blockStartPosition = -1;\n+                advance();\n+            }\n+\n+            @Override public boolean hasNext() { return nextT != null; }\n+\n+            @Override public T next() {\n+                final T result = nextT;\n+                if ( result == null ) {\n+                    throw new NoSuchElementException(\"overlapper iterator has no next element\");\n+                }\n+                advance();\n+                return result; }\n+\n+            @Override public void close() { reader.close(); nextT = null; }\n+\n+            @Override public CloseableTribbleIterator<T> iterator() {\n+                return new OverlapIterator<>(interval, reader);\n+            }\n+\n+            private void advance() {\n+                do {\n+                    if ( isNewBlock(blockStartPosition, reader.getPosition()) ) {\n+                        if ( !indexEntryIterator.hasNext() ) {\n+                            nextT = null;\n+                            return;\n+                        }\n+                        blockStartPosition = indexEntryIterator.next().getValue();\n+                        reader.seekStream(blockStartPosition);\n+                    }\n+                    nextT = reader.readStream();\n+                    if ( !interval.contigsMatch(nextT) || interval.getEnd() < nextT.getStart() ) {\n+                        nextT = null;\n+                        return;\n+                    }\n+                } while ( !interval.overlaps(nextT) );\n+            }\n+        }\n+    }\n+\n+    public static boolean isNewBlock( final long filePosition1, final long filePosition2 ) {\n+        // upper 48 bits contain the block offset\n+        // check to see if there are any bit differences in those upper 48 bits\n+        return ((filePosition1 ^ filePosition2) & ~0xffffL) != 0;\n+    }\n+}"
  },
  {
    "sha": "78c2ea8df347235ee387b4477e4b26ce701c29c9",
    "filename": "src/test/java/org/broadinstitute/hellbender/tools/sv/BafEvidenceUnitTest.java",
    "status": "added",
    "additions": 77,
    "deletions": 0,
    "changes": 77,
    "blob_url": "https://github.com/broadinstitute/gatk/blob/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/test/java/org/broadinstitute/hellbender/tools/sv/BafEvidenceUnitTest.java",
    "raw_url": "https://github.com/broadinstitute/gatk/raw/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/test/java/org/broadinstitute/hellbender/tools/sv/BafEvidenceUnitTest.java",
    "contents_url": "https://api.github.com/repos/broadinstitute/gatk/contents/src/test/java/org/broadinstitute/hellbender/tools/sv/BafEvidenceUnitTest.java?ref=304d73f3bd34f59732b9e3d7676b373d0e688f9b",
    "patch": "@@ -0,0 +1,77 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.seekablestream.ByteArraySeekableStream;\n+import org.broadinstitute.hellbender.utils.codecs.BafEvidenceBCICodec;\n+import org.broadinstitute.hellbender.utils.codecs.BafEvidenceCodec;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Header;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Reader;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Writer;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class BafEvidenceUnitTest {\n+    private static final SAMSequenceDictionary dict = new SAMSequenceDictionary();\n+    private static final List<String> samples = new ArrayList<>(3);\n+    private static final List<BafEvidence> bafs = new ArrayList<>(18);\n+    static {\n+        dict.addSequence(new SAMSequenceRecord(\"21\", 46709983));\n+        dict.addSequence(new SAMSequenceRecord(\"22\", 50818468));\n+        samples.add(\"A\");\n+        samples.add(\"B\");\n+        samples.add(\"C\");\n+        bafs.add(new BafEvidence(\"B\",\"21\",25993443,0.5185185185185185));\n+        bafs.add(new BafEvidence(\"A\",\"21\",25995506,0.43333333333333335));\n+        bafs.add(new BafEvidence(\"A\",\"21\",25996383,0.46875));\n+        bafs.add(new BafEvidence(\"A\",\"21\",25996446,0.6));\n+        bafs.add(new BafEvidence(\"A\",\"21\",25996679,0.5263157894736842));\n+        bafs.add(new BafEvidence(\"A\",\"21\",25996772,0.3751515151515151));\n+        bafs.add(new BafEvidence(\"A\",\"21\",25996907,0.5660377358490566));\n+        bafs.add(new BafEvidence(\"A\",\"21\",25997040,0.4753753753753754));\n+        bafs.add(new BafEvidence(\"A\",\"21\",25997094,0.518984337921215));\n+        bafs.add(new BafEvidence(\"B\",\"22\",30000353,0.5666666666666667));\n+        bafs.add(new BafEvidence(\"B\",\"22\",30003011,0.5555555555555556));\n+        bafs.add(new BafEvidence(\"B\",\"22\",30004712,0.5526315789473685));\n+        bafs.add(new BafEvidence(\"B\",\"22\",30005667,0.5925925925925926));\n+        bafs.add(new BafEvidence(\"B\",\"22\",30007780,0.36363636363636365));\n+        bafs.add(new BafEvidence(\"C\",\"22\",30010391,0.514162077104642));\n+        bafs.add(new BafEvidence(\"B\",\"22\",30012721,0.34782608695652173));\n+        bafs.add(new BafEvidence(\"C\",\"22\",30012825,0.6266666666666667));\n+        bafs.add(new BafEvidence(\"B\",\"22\",30016476,0.18181818181818182));\n+    }\n+\n+    @Test\n+    public void testTextRoundTrip() {\n+        final BafEvidenceCodec codec = new BafEvidenceCodec();\n+        for ( final BafEvidence be : bafs ) {\n+            Assert.assertEquals(codec.decode(BafEvidenceCodec.encode(be)), be);\n+        }\n+    }\n+\n+    @Test\n+    public void testBinaryRoundTrip() {\n+        final BafEvidenceBCICodec codec = new BafEvidenceBCICodec();\n+        final ByteArrayOutputStream os = new ByteArrayOutputStream(1024);\n+        final Header header =\n+                new Header(BafEvidence.class.getSimpleName(), BafEvidence.BCI_VERSION, dict, samples);\n+        final Writer<BafEvidence> writer =\n+                new Writer<>(\"in-memory stream\", os, header, codec::encode);\n+        for ( final BafEvidence be : bafs ) {\n+            writer.write(be);\n+        }\n+        writer.close();\n+        final ByteArraySeekableStream ss = new ByteArraySeekableStream(os.toByteArray());\n+        final Reader<BafEvidence> reader =\n+                new Reader<>(\"in-memory stream\", ss, codec);\n+        final List<BafEvidence> recoveredBafs = new ArrayList<>(18);\n+        while ( reader.hasNext() ) {\n+            recoveredBafs.add(reader.readStream());\n+        }\n+        Assert.assertEquals(recoveredBafs, bafs);\n+    }\n+}"
  },
  {
    "sha": "96cf9bc62bbde2445b8c1fe151ae676e5024edfb",
    "filename": "src/test/java/org/broadinstitute/hellbender/tools/sv/DepthEvidenceUnitTest.java",
    "status": "added",
    "additions": 77,
    "deletions": 0,
    "changes": 77,
    "blob_url": "https://github.com/broadinstitute/gatk/blob/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/test/java/org/broadinstitute/hellbender/tools/sv/DepthEvidenceUnitTest.java",
    "raw_url": "https://github.com/broadinstitute/gatk/raw/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/test/java/org/broadinstitute/hellbender/tools/sv/DepthEvidenceUnitTest.java",
    "contents_url": "https://api.github.com/repos/broadinstitute/gatk/contents/src/test/java/org/broadinstitute/hellbender/tools/sv/DepthEvidenceUnitTest.java?ref=304d73f3bd34f59732b9e3d7676b373d0e688f9b",
    "patch": "@@ -0,0 +1,77 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.seekablestream.ByteArraySeekableStream;\n+import org.broadinstitute.hellbender.utils.codecs.DepthEvidenceBCICodec;\n+import org.broadinstitute.hellbender.utils.codecs.DepthEvidenceCodec;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Header;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Reader;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Writer;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class DepthEvidenceUnitTest {\n+    private static final SAMSequenceDictionary dict = new SAMSequenceDictionary();\n+    private static final List<String> samples = new ArrayList<>(3);\n+    private static final List<DepthEvidence> depths = new ArrayList<>(18);\n+    static {\n+        dict.addSequence(new SAMSequenceRecord(\"21\", 46709983));\n+        dict.addSequence(new SAMSequenceRecord(\"22\", 50818468));\n+        samples.add(\"A\");\n+        samples.add(\"B\");\n+        samples.add(\"C\");\n+        depths.add(new DepthEvidence(\"21\",25999208,25999308,new int[]{15,23,18}));\n+        depths.add(new DepthEvidence(\"21\",25999308,25999408,new int[]{22,22,32}));\n+        depths.add(new DepthEvidence(\"21\",25999408,25999508,new int[]{28,18,19}));\n+        depths.add(new DepthEvidence(\"21\",25999508,25999608,new int[]{17,20,29}));\n+        depths.add(new DepthEvidence(\"21\",25999608,25999708,new int[]{22,17,24}));\n+        depths.add(new DepthEvidence(\"21\",25999708,25999808,new int[]{26,27,26}));\n+        depths.add(new DepthEvidence(\"21\",25999808,25999908,new int[]{22,20,22}));\n+        depths.add(new DepthEvidence(\"21\",25999908,26000008,new int[]{24,20,29}));\n+        depths.add(new DepthEvidence(\"22\",29999964,30000064,new int[]{29,20,33}));\n+        depths.add(new DepthEvidence(\"22\",30000064,30000164,new int[]{25,18,25}));\n+        depths.add(new DepthEvidence(\"22\",30000164,30000264,new int[]{21,21,23}));\n+        depths.add(new DepthEvidence(\"22\",30000264,30000364,new int[]{35,22,32}));\n+        depths.add(new DepthEvidence(\"22\",30000364,30000464,new int[]{58,52,51}));\n+        depths.add(new DepthEvidence(\"22\",30000464,30000564,new int[]{35,35,32}));\n+        depths.add(new DepthEvidence(\"22\",30000564,30000664,new int[]{20,15,16}));\n+        depths.add(new DepthEvidence(\"22\",30000664,30000764,new int[]{36,22,26}));\n+        depths.add(new DepthEvidence(\"22\",30000764,30000864,new int[]{23,20,25}));\n+        depths.add(new DepthEvidence(\"22\",30000864,30000964,new int[]{16,18,30}));\n+    }\n+\n+    @Test\n+    public void testTextRoundTrip() {\n+        final DepthEvidenceCodec codec = new DepthEvidenceCodec();\n+        for ( final DepthEvidence de : depths ) {\n+            Assert.assertEquals(codec.decode(DepthEvidenceCodec.encode(de)), de);\n+        }\n+    }\n+\n+    @Test\n+    public void testBinaryRoundTrip() {\n+        final DepthEvidenceBCICodec codec = new DepthEvidenceBCICodec();\n+        final ByteArrayOutputStream os = new ByteArrayOutputStream(1024);\n+        final Header header =\n+                new Header(DepthEvidence.class.getSimpleName(), DepthEvidence.BCI_VERSION, dict, samples);\n+        final Writer<DepthEvidence> writer =\n+                new Writer<>(\"in-memory stream\", os, header, codec::encode);\n+        for ( final DepthEvidence de : depths ) {\n+            writer.write(de);\n+        }\n+        writer.close();\n+        final ByteArraySeekableStream ss = new ByteArraySeekableStream(os.toByteArray());\n+        final Reader<DepthEvidence> reader =\n+                new Reader<>(\"in-memory stream\", ss, codec);\n+        final List<DepthEvidence> recoveredDepths = new ArrayList<>(18);\n+        while ( reader.hasNext() ) {\n+            recoveredDepths.add(reader.readStream());\n+        }\n+        Assert.assertEquals(recoveredDepths, depths);\n+    }\n+}"
  },
  {
    "sha": "c5c45425d70de100dc96aeeaa36c3e320116e86c",
    "filename": "src/test/java/org/broadinstitute/hellbender/tools/sv/DiscordantPairEvidenceUnitTest.java",
    "status": "added",
    "additions": 78,
    "deletions": 0,
    "changes": 78,
    "blob_url": "https://github.com/broadinstitute/gatk/blob/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/test/java/org/broadinstitute/hellbender/tools/sv/DiscordantPairEvidenceUnitTest.java",
    "raw_url": "https://github.com/broadinstitute/gatk/raw/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/test/java/org/broadinstitute/hellbender/tools/sv/DiscordantPairEvidenceUnitTest.java",
    "contents_url": "https://api.github.com/repos/broadinstitute/gatk/contents/src/test/java/org/broadinstitute/hellbender/tools/sv/DiscordantPairEvidenceUnitTest.java?ref=304d73f3bd34f59732b9e3d7676b373d0e688f9b",
    "patch": "@@ -0,0 +1,78 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.seekablestream.ByteArraySeekableStream;\n+import org.broadinstitute.hellbender.utils.codecs.DiscordantPairEvidenceBCICodec;\n+import org.broadinstitute.hellbender.utils.codecs.DiscordantPairEvidenceCodec;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Header;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Reader;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Writer;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class DiscordantPairEvidenceUnitTest {\n+    private static final SAMSequenceDictionary dict = new SAMSequenceDictionary();\n+    private static final List<String> samples = new ArrayList<>(3);\n+    private static final List<DiscordantPairEvidence> pairs = new ArrayList<>(18);\n+    static {\n+        dict.addSequence(new SAMSequenceRecord(\"21\", 46709983));\n+        dict.addSequence(new SAMSequenceRecord(\"22\", 50818468));\n+        dict.addSequence(new SAMSequenceRecord(\"X\", 156040895));\n+        samples.add(\"A\");\n+        samples.add(\"B\");\n+        samples.add(\"C\");\n+        pairs.add(new DiscordantPairEvidence(\"C\",\"21\",25972981,false,\"X\",123051950,true));\n+        pairs.add(new DiscordantPairEvidence(\"C\",\"21\",25974416,false,\"X\",433179,false));\n+        pairs.add(new DiscordantPairEvidence(\"B\",\"21\",25974526,true,\"X\",7510256,false));\n+        pairs.add(new DiscordantPairEvidence(\"A\",\"21\",25978689,false,\"X\",23061675,true));\n+        pairs.add(new DiscordantPairEvidence(\"C\",\"21\",25980279,true,\"X\",118908694,true));\n+        pairs.add(new DiscordantPairEvidence(\"C\",\"21\",25991097,false,\"X\",19552859,true));\n+        pairs.add(new DiscordantPairEvidence(\"B\",\"21\",25996526,true,\"21\",25997312,false));\n+        pairs.add(new DiscordantPairEvidence(\"B\",\"21\",25998677,true,\"21\",25999518,false));\n+        pairs.add(new DiscordantPairEvidence(\"A\",\"21\",25999457,true,\"21\",26000320,false));\n+        pairs.add(new DiscordantPairEvidence(\"B\",\"22\",30000459,true,\"X\",112737301,false));\n+        pairs.add(new DiscordantPairEvidence(\"A\",\"22\",30000461,false,\"X\",70214634,false));\n+        pairs.add(new DiscordantPairEvidence(\"C\",\"22\",30000464,true,\"X\",1113557,false));\n+        pairs.add(new DiscordantPairEvidence(\"B\",\"22\",30000670,true,\"22\",30001447,false));\n+        pairs.add(new DiscordantPairEvidence(\"C\",\"22\",30000827,false,\"X\",100936820,false));\n+        pairs.add(new DiscordantPairEvidence(\"A\",\"22\",30003590,false,\"X\",10293,false));\n+        pairs.add(new DiscordantPairEvidence(\"A\",\"22\",30006140,true,\"22\",30007026,false));\n+        pairs.add(new DiscordantPairEvidence(\"B\",\"22\",30006209,false,\"X\",116263582,false));\n+        pairs.add(new DiscordantPairEvidence(\"C\",\"22\",30009296,false,\"X\",141138844,true));\n+    }\n+\n+    @Test\n+    public void testTextRoundTrip() {\n+        final DiscordantPairEvidenceCodec codec = new DiscordantPairEvidenceCodec();\n+        for ( final DiscordantPairEvidence pair : pairs ) {\n+            Assert.assertEquals(codec.decode(DiscordantPairEvidenceCodec.encode(pair)), pair);\n+        }\n+    }\n+\n+    @Test\n+    public void testBinaryRoundTrip() {\n+        final DiscordantPairEvidenceBCICodec codec = new DiscordantPairEvidenceBCICodec();\n+        final ByteArrayOutputStream os = new ByteArrayOutputStream(1024);\n+        final Header header = new Header(DiscordantPairEvidence.class.getSimpleName(),\n+                                            DiscordantPairEvidence.BCI_VERSION, dict, samples);\n+        final Writer<DiscordantPairEvidence> writer =\n+                new Writer<>(\"in-memory stream\", os, header, codec::encode);\n+        for ( final DiscordantPairEvidence be : pairs ) {\n+            writer.write(be);\n+        }\n+        writer.close();\n+        final ByteArraySeekableStream ss = new ByteArraySeekableStream(os.toByteArray());\n+        final Reader<DiscordantPairEvidence> reader =\n+                new Reader<>(\"in-memory stream\", ss, codec);\n+        final List<DiscordantPairEvidence> recoveredDiscordantPairs = new ArrayList<>(18);\n+        while ( reader.hasNext() ) {\n+            recoveredDiscordantPairs.add(reader.readStream());\n+        }\n+        Assert.assertEquals(recoveredDiscordantPairs, pairs);\n+    }\n+}"
  },
  {
    "sha": "f1106437f0fc1b53eece290d01486c3a4f1a90ab",
    "filename": "src/test/java/org/broadinstitute/hellbender/tools/sv/SplitReadEvidenceUnitTest.java",
    "status": "added",
    "additions": 77,
    "deletions": 0,
    "changes": 77,
    "blob_url": "https://github.com/broadinstitute/gatk/blob/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/test/java/org/broadinstitute/hellbender/tools/sv/SplitReadEvidenceUnitTest.java",
    "raw_url": "https://github.com/broadinstitute/gatk/raw/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/test/java/org/broadinstitute/hellbender/tools/sv/SplitReadEvidenceUnitTest.java",
    "contents_url": "https://api.github.com/repos/broadinstitute/gatk/contents/src/test/java/org/broadinstitute/hellbender/tools/sv/SplitReadEvidenceUnitTest.java?ref=304d73f3bd34f59732b9e3d7676b373d0e688f9b",
    "patch": "@@ -0,0 +1,77 @@\n+package org.broadinstitute.hellbender.tools.sv;\n+\n+import htsjdk.samtools.SAMSequenceDictionary;\n+import htsjdk.samtools.SAMSequenceRecord;\n+import htsjdk.samtools.seekablestream.ByteArraySeekableStream;\n+import org.broadinstitute.hellbender.utils.codecs.SplitReadEvidenceBCICodec;\n+import org.broadinstitute.hellbender.utils.codecs.SplitReadEvidenceCodec;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Header;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Reader;\n+import org.broadinstitute.hellbender.utils.io.BlockCompressedIntervalStream.Writer;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class SplitReadEvidenceUnitTest {\n+    private static final SAMSequenceDictionary dict = new SAMSequenceDictionary();\n+    private static final List<String> samples = new ArrayList<>(3);\n+    private static final List<SplitReadEvidence> splits = new ArrayList<>(18);\n+    static {\n+        dict.addSequence(new SAMSequenceRecord(\"21\", 46709983));\n+        dict.addSequence(new SAMSequenceRecord(\"22\", 50818468));\n+        samples.add(\"A\");\n+        samples.add(\"B\");\n+        samples.add(\"C\");\n+        splits.add(new SplitReadEvidence(\"B\",\"21\",25998421,1,true));\n+        splits.add(new SplitReadEvidence(\"B\",\"21\",25998456,1,true));\n+        splits.add(new SplitReadEvidence(\"B\",\"21\",25998529,1,true));\n+        splits.add(new SplitReadEvidence(\"B\",\"21\",25998668,1,false));\n+        splits.add(new SplitReadEvidence(\"B\",\"21\",25998825,1,true));\n+        splits.add(new SplitReadEvidence(\"C\",\"21\",25999220,1,false));\n+        splits.add(new SplitReadEvidence(\"B\",\"21\",25999674,1,false));\n+        splits.add(new SplitReadEvidence(\"B\",\"21\",25999957,1,false));\n+        splits.add(new SplitReadEvidence(\"C\",\"22\",30000009,1,false));\n+        splits.add(new SplitReadEvidence(\"B\",\"22\",30000115,1,false));\n+        splits.add(new SplitReadEvidence(\"B\",\"22\",30000125,1,false));\n+        splits.add(new SplitReadEvidence(\"A\",\"22\",30000133,1,false));\n+        splits.add(new SplitReadEvidence(\"B\",\"22\",30000137,1,false));\n+        splits.add(new SplitReadEvidence(\"B\",\"22\",30000189,1,false));\n+        splits.add(new SplitReadEvidence(\"B\",\"22\",30000192,1,false));\n+        splits.add(new SplitReadEvidence(\"A\",\"22\",30000196,1,false));\n+        splits.add(new SplitReadEvidence(\"B\",\"22\",30000335,1,false));\n+        splits.add(new SplitReadEvidence(\"A\",\"22\",30000338,1,false));\n+    }\n+\n+    @Test\n+    public void testTextRoundTrip() {\n+        final SplitReadEvidenceCodec codec = new SplitReadEvidenceCodec();\n+        for ( final SplitReadEvidence sr : splits ) {\n+            Assert.assertEquals(codec.decode(SplitReadEvidenceCodec.encode(sr)), sr);\n+        }\n+    }\n+\n+    @Test\n+    public void testBinaryRoundTrip() {\n+        final SplitReadEvidenceBCICodec codec = new SplitReadEvidenceBCICodec();\n+        final ByteArrayOutputStream os = new ByteArrayOutputStream(1024);\n+        final Header header =\n+                new Header(SplitReadEvidence.class.getSimpleName(), SplitReadEvidence.BCI_VERSION, dict, samples);\n+        final Writer<SplitReadEvidence> writer =\n+                new Writer<>(\"in-memory stream\", os, header, codec::encode);\n+        for ( final SplitReadEvidence sr : splits ) {\n+            writer.write(sr);\n+        }\n+        writer.close();\n+        final ByteArraySeekableStream ss = new ByteArraySeekableStream(os.toByteArray());\n+        final Reader<SplitReadEvidence> reader =\n+                new Reader<>(\"in-memory stream\", ss, codec);\n+        final List<SplitReadEvidence> recoveredSplitReads = new ArrayList<>(18);\n+        while ( reader.hasNext() ) {\n+            recoveredSplitReads.add(reader.readStream());\n+        }\n+        Assert.assertEquals(recoveredSplitReads, splits);\n+    }\n+}"
  },
  {
    "sha": "3b1c5c4c9cb2ab497e00ee6b55a013b718674108",
    "filename": "src/test/java/org/broadinstitute/hellbender/tools/walkers/sv/PairedEndAndSplitReadEvidenceCollectionUnitTest.java",
    "status": "modified",
    "additions": 4,
    "deletions": 5,
    "changes": 9,
    "blob_url": "https://github.com/broadinstitute/gatk/blob/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/test/java/org/broadinstitute/hellbender/tools/walkers/sv/PairedEndAndSplitReadEvidenceCollectionUnitTest.java",
    "raw_url": "https://github.com/broadinstitute/gatk/raw/304d73f3bd34f59732b9e3d7676b373d0e688f9b/src/test/java/org/broadinstitute/hellbender/tools/walkers/sv/PairedEndAndSplitReadEvidenceCollectionUnitTest.java",
    "contents_url": "https://api.github.com/repos/broadinstitute/gatk/contents/src/test/java/org/broadinstitute/hellbender/tools/walkers/sv/PairedEndAndSplitReadEvidenceCollectionUnitTest.java?ref=304d73f3bd34f59732b9e3d7676b373d0e688f9b",
    "patch": "@@ -13,6 +13,7 @@\n import org.testng.Assert;\n import org.testng.annotations.Test;\n \n+import java.io.BufferedWriter;\n import java.util.HashSet;\n import java.util.Map;\n import java.util.PriorityQueue;\n@@ -98,7 +99,7 @@ public void testCountSplitRead() throws Exception {\n         final GATKRead rightClip = ArtificialReadUtils.createArtificialRead(header, \"rightClip\", 0, 1000, ArtificialReadUtils.createRandomReadBases(151, false),\n                 ArtificialReadUtils.createRandomReadQuals(151), \"100M51S\");\n \n-        final FeatureOutputStream<SplitReadEvidence> mockSrWriter = Mockito.mock(SplitReadFeatureOutputStream.class);\n+        final BufferedWriter mockSrWriter = Mockito.mock(BufferedWriter.class);\n \n         PairedEndAndSplitReadEvidenceCollection tool = new PairedEndAndSplitReadEvidenceCollection();\n         final PriorityQueue<PairedEndAndSplitReadEvidenceCollection.SplitPos> splitCounts = new PriorityQueue<>(new PairedEndAndSplitReadEvidenceCollection.SplitPosComparator());\n@@ -131,10 +132,8 @@ public void testCountSplitRead() throws Exception {\n         Assert.assertFalse(counts.containsKey(new PairedEndAndSplitReadEvidenceCollection.SplitPos(1100, PairedEndAndSplitReadEvidenceCollection.POSITION.RIGHT)));\n         Assert.assertFalse(counts.containsKey(new PairedEndAndSplitReadEvidenceCollection.SplitPos(1100, PairedEndAndSplitReadEvidenceCollection.POSITION.LEFT)));\n         Assert.assertEquals(counts.get(new PairedEndAndSplitReadEvidenceCollection.SplitPos(1600, PairedEndAndSplitReadEvidenceCollection.POSITION.LEFT)).intValue(), 1);\n-        final SplitReadEvidence splitRead1 = new SplitReadEvidence(\"sample\", \"1\", 1100, 1, false);\n-        final SplitReadEvidence splitRead2 = new SplitReadEvidence(\"sample\", \"1\", 1100, 2, true);\n-        Mockito.verify(mockSrWriter).add(splitRead1);\n-        Mockito.verify(mockSrWriter).add(splitRead2);\n+        Mockito.verify(mockSrWriter).write(\"1\t1099\tleft\t1\tsample\\n\");\n+        Mockito.verify(mockSrWriter).write(\"1\t1099\tright\t2\tsample\\n\");\n         Mockito.verifyNoMoreInteractions(mockSrWriter);\n \n     }"
  }
]
