[
  {
    "sha": "1aa41e4f842593691f615477d6e3b39346cbee7b",
    "filename": "clirr-ignored-differences.xml",
    "status": "added",
    "additions": 15,
    "deletions": 0,
    "changes": 15,
    "blob_url": "https://github.com/googleapis/java-pubsublite-spark/blob/e408b3e6987977c82700732492cad881a431185d/clirr-ignored-differences.xml",
    "raw_url": "https://github.com/googleapis/java-pubsublite-spark/raw/e408b3e6987977c82700732492cad881a431185d/clirr-ignored-differences.xml",
    "contents_url": "https://api.github.com/repos/googleapis/java-pubsublite-spark/contents/clirr-ignored-differences.xml?ref=e408b3e6987977c82700732492cad881a431185d",
    "patch": "@@ -0,0 +1,15 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!-- see http://www.mojohaus.org/clirr-maven-plugin/examples/ignored-differences.html -->\n+<differences>\n+    <difference>\n+        <differenceType>7004</differenceType>\n+        <className>com/google/cloud/pubsublite/spark/*Reader</className>\n+        <method>*</method>\n+    </difference>\n+    <difference>\n+        <differenceType>7005</differenceType>\n+        <className>com/google/cloud/pubsublite/spark/*Reader</className>\n+        <method>*</method>\n+        <to>*</to>\n+    </difference>\n+</differences>\n\\ No newline at end of file"
  },
  {
    "sha": "35555805ff4fb408c2760a40b09e744430bb51b8",
    "filename": "src/main/java/com/google/cloud/pubsublite/spark/CachedPartitionCountReader.java",
    "status": "added",
    "additions": 47,
    "deletions": 0,
    "changes": 47,
    "blob_url": "https://github.com/googleapis/java-pubsublite-spark/blob/e408b3e6987977c82700732492cad881a431185d/src/main/java/com/google/cloud/pubsublite/spark/CachedPartitionCountReader.java",
    "raw_url": "https://github.com/googleapis/java-pubsublite-spark/raw/e408b3e6987977c82700732492cad881a431185d/src/main/java/com/google/cloud/pubsublite/spark/CachedPartitionCountReader.java",
    "contents_url": "https://api.github.com/repos/googleapis/java-pubsublite-spark/contents/src/main/java/com/google/cloud/pubsublite/spark/CachedPartitionCountReader.java?ref=e408b3e6987977c82700732492cad881a431185d",
    "patch": "@@ -0,0 +1,47 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.google.cloud.pubsublite.spark;\n+\n+import com.google.cloud.pubsublite.AdminClient;\n+import com.google.cloud.pubsublite.PartitionLookupUtils;\n+import com.google.cloud.pubsublite.TopicPath;\n+import com.google.common.base.Supplier;\n+import com.google.common.base.Suppliers;\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+@ThreadSafe\n+public class CachedPartitionCountReader implements PartitionCountReader {\n+  private final AdminClient adminClient;\n+  private final Supplier<Integer> supplier;\n+\n+  public CachedPartitionCountReader(AdminClient adminClient, TopicPath topicPath) {\n+    this.adminClient = adminClient;\n+    this.supplier =\n+        Suppliers.memoizeWithExpiration(\n+            () -> PartitionLookupUtils.numPartitions(topicPath, adminClient), 1, TimeUnit.MINUTES);\n+  }\n+\n+  @Override\n+  public void close() {\n+    adminClient.close();\n+  }\n+\n+  public int getPartitionCount() {\n+    return supplier.get();\n+  }\n+}"
  },
  {
    "sha": "7bad0ffc55df4a78d8288a94cb22f448443eaa76",
    "filename": "src/main/java/com/google/cloud/pubsublite/spark/LimitingHeadOffsetReader.java",
    "status": "modified",
    "additions": 15,
    "deletions": 5,
    "changes": 20,
    "blob_url": "https://github.com/googleapis/java-pubsublite-spark/blob/e408b3e6987977c82700732492cad881a431185d/src/main/java/com/google/cloud/pubsublite/spark/LimitingHeadOffsetReader.java",
    "raw_url": "https://github.com/googleapis/java-pubsublite-spark/raw/e408b3e6987977c82700732492cad881a431185d/src/main/java/com/google/cloud/pubsublite/spark/LimitingHeadOffsetReader.java",
    "contents_url": "https://api.github.com/repos/googleapis/java-pubsublite-spark/contents/src/main/java/com/google/cloud/pubsublite/spark/LimitingHeadOffsetReader.java?ref=e408b3e6987977c82700732492cad881a431185d",
    "patch": "@@ -27,7 +27,9 @@\n import com.google.cloud.pubsublite.internal.TopicStatsClient;\n import com.google.cloud.pubsublite.proto.Cursor;\n import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.flogger.GoogleLogger;\n import com.google.common.util.concurrent.MoreExecutors;\n+import java.io.Closeable;\n import java.util.HashSet;\n import java.util.Map;\n import java.util.Set;\n@@ -40,18 +42,22 @@\n  * offsets for the topic at most once per minute.\n  */\n public class LimitingHeadOffsetReader implements PerTopicHeadOffsetReader {\n+  private static final GoogleLogger log = GoogleLogger.forEnclosingClass();\n \n   private final TopicStatsClient topicStatsClient;\n   private final TopicPath topic;\n-  private final long topicPartitionCount;\n+  private final PartitionCountReader partitionCountReader;\n   private final AsyncLoadingCache<Partition, Offset> cachedHeadOffsets;\n \n   @VisibleForTesting\n   public LimitingHeadOffsetReader(\n-      TopicStatsClient topicStatsClient, TopicPath topic, long topicPartitionCount, Ticker ticker) {\n+      TopicStatsClient topicStatsClient,\n+      TopicPath topic,\n+      PartitionCountReader partitionCountReader,\n+      Ticker ticker) {\n     this.topicStatsClient = topicStatsClient;\n     this.topic = topic;\n-    this.topicPartitionCount = topicPartitionCount;\n+    this.partitionCountReader = partitionCountReader;\n     this.cachedHeadOffsets =\n         Caffeine.newBuilder()\n             .ticker(ticker)\n@@ -82,7 +88,7 @@ public void onSuccess(Cursor c) {\n   @Override\n   public PslSourceOffset getHeadOffset() {\n     Set<Partition> keySet = new HashSet<>();\n-    for (int i = 0; i < topicPartitionCount; i++) {\n+    for (int i = 0; i < partitionCountReader.getPartitionCount(); i++) {\n       keySet.add(Partition.of(i));\n     }\n     CompletableFuture<Map<Partition, Offset>> future = cachedHeadOffsets.getAll(keySet);\n@@ -95,6 +101,10 @@ public PslSourceOffset getHeadOffset() {\n \n   @Override\n   public void close() {\n-    topicStatsClient.close();\n+    try (AutoCloseable a = topicStatsClient;\n+        Closeable b = partitionCountReader) {\n+    } catch (Exception e) {\n+      log.atWarning().withCause(e).log(\"Unable to close LimitingHeadOffsetReader.\");\n+    }\n   }\n }"
  },
  {
    "sha": "7ebec8916ab3873c3a0196073c38d66a4449b6c5",
    "filename": "src/main/java/com/google/cloud/pubsublite/spark/MultiPartitionCommitterImpl.java",
    "status": "modified",
    "additions": 69,
    "deletions": 4,
    "changes": 73,
    "blob_url": "https://github.com/googleapis/java-pubsublite-spark/blob/e408b3e6987977c82700732492cad881a431185d/src/main/java/com/google/cloud/pubsublite/spark/MultiPartitionCommitterImpl.java",
    "raw_url": "https://github.com/googleapis/java-pubsublite-spark/raw/e408b3e6987977c82700732492cad881a431185d/src/main/java/com/google/cloud/pubsublite/spark/MultiPartitionCommitterImpl.java",
    "contents_url": "https://api.github.com/repos/googleapis/java-pubsublite-spark/contents/src/main/java/com/google/cloud/pubsublite/spark/MultiPartitionCommitterImpl.java?ref=e408b3e6987977c82700732492cad881a431185d",
    "patch": "@@ -25,30 +25,95 @@\n import com.google.common.flogger.GoogleLogger;\n import com.google.common.util.concurrent.MoreExecutors;\n import java.util.HashMap;\n+import java.util.HashSet;\n import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.concurrent.GuardedBy;\n \n+/**\n+ * A {@link MultiPartitionCommitter} that lazily adjusts for partition changes when {@link\n+ * MultiPartitionCommitter#commit(PslSourceOffset)} is called.\n+ */\n public class MultiPartitionCommitterImpl implements MultiPartitionCommitter {\n   private static final GoogleLogger log = GoogleLogger.forEnclosingClass();\n \n+  private final CommitterFactory committerFactory;\n+\n+  @GuardedBy(\"this\")\n   private final Map<Partition, Committer> committerMap = new HashMap<>();\n \n+  @GuardedBy(\"this\")\n+  private final Set<Partition> partitionsCleanUp = new HashSet<>();\n+\n+  public MultiPartitionCommitterImpl(long topicPartitionCount, CommitterFactory committerFactory) {\n+    this(\n+        topicPartitionCount,\n+        committerFactory,\n+        MoreExecutors.getExitingScheduledExecutorService(new ScheduledThreadPoolExecutor(1)));\n+  }\n+\n   @VisibleForTesting\n-  MultiPartitionCommitterImpl(long topicPartitionCount, CommitterFactory committerFactory) {\n+  MultiPartitionCommitterImpl(\n+      long topicPartitionCount,\n+      CommitterFactory committerFactory,\n+      ScheduledExecutorService executorService) {\n+    this.committerFactory = committerFactory;\n     for (int i = 0; i < topicPartitionCount; i++) {\n       Partition p = Partition.of(i);\n-      Committer committer = committerFactory.newCommitter(p);\n-      committer.startAsync().awaitRunning();\n-      committerMap.put(p, committer);\n+      committerMap.put(p, createCommitter(p));\n     }\n+    executorService.scheduleWithFixedDelay(this::cleanUpCommitterMap, 10, 10, TimeUnit.MINUTES);\n   }\n \n   @Override\n   public synchronized void close() {\n     committerMap.values().forEach(c -> c.stopAsync().awaitTerminated());\n   }\n \n+  /** Adjust committerMap based on the partitions that needs to be committed. */\n+  private synchronized void updateCommitterMap(PslSourceOffset offset) {\n+    int currentPartitions = committerMap.size();\n+    int newPartitions = offset.partitionOffsetMap().size();\n+\n+    if (currentPartitions == newPartitions) {\n+      return;\n+    }\n+    if (currentPartitions < newPartitions) {\n+      for (int i = currentPartitions; i < newPartitions; i++) {\n+        Partition p = Partition.of(i);\n+        if (!committerMap.containsKey(p)) {\n+          committerMap.put(p, createCommitter(p));\n+        }\n+        partitionsCleanUp.remove(p);\n+      }\n+      return;\n+    }\n+    partitionsCleanUp.clear();\n+    for (int i = newPartitions; i < currentPartitions; i++) {\n+      partitionsCleanUp.add(Partition.of(i));\n+    }\n+  }\n+\n+  private synchronized Committer createCommitter(Partition p) {\n+    Committer committer = committerFactory.newCommitter(p);\n+    committer.startAsync().awaitRunning();\n+    return committer;\n+  }\n+\n+  private synchronized void cleanUpCommitterMap() {\n+    for (Partition p : partitionsCleanUp) {\n+      committerMap.get(p).stopAsync();\n+      committerMap.remove(p);\n+    }\n+    partitionsCleanUp.clear();\n+  }\n+\n   @Override\n   public synchronized void commit(PslSourceOffset offset) {\n+    updateCommitterMap(offset);\n     offset\n         .partitionOffsetMap()\n         .forEach("
  },
  {
    "sha": "934d40be73ead27e1c0232960d5d96b9db54e17c",
    "filename": "src/main/java/com/google/cloud/pubsublite/spark/PartitionCountReader.java",
    "status": "added",
    "additions": 26,
    "deletions": 0,
    "changes": 26,
    "blob_url": "https://github.com/googleapis/java-pubsublite-spark/blob/e408b3e6987977c82700732492cad881a431185d/src/main/java/com/google/cloud/pubsublite/spark/PartitionCountReader.java",
    "raw_url": "https://github.com/googleapis/java-pubsublite-spark/raw/e408b3e6987977c82700732492cad881a431185d/src/main/java/com/google/cloud/pubsublite/spark/PartitionCountReader.java",
    "contents_url": "https://api.github.com/repos/googleapis/java-pubsublite-spark/contents/src/main/java/com/google/cloud/pubsublite/spark/PartitionCountReader.java?ref=e408b3e6987977c82700732492cad881a431185d",
    "patch": "@@ -0,0 +1,26 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.google.cloud.pubsublite.spark;\n+\n+import java.io.Closeable;\n+\n+public interface PartitionCountReader extends Closeable {\n+  int getPartitionCount();\n+\n+  @Override\n+  void close();\n+}"
  },
  {
    "sha": "659530318e455a2f72a7cb4aac4a211ac7b8c083",
    "filename": "src/main/java/com/google/cloud/pubsublite/spark/PslContinuousReader.java",
    "status": "modified",
    "additions": 10,
    "deletions": 3,
    "changes": 13,
    "blob_url": "https://github.com/googleapis/java-pubsublite-spark/blob/e408b3e6987977c82700732492cad881a431185d/src/main/java/com/google/cloud/pubsublite/spark/PslContinuousReader.java",
    "raw_url": "https://github.com/googleapis/java-pubsublite-spark/raw/e408b3e6987977c82700732492cad881a431185d/src/main/java/com/google/cloud/pubsublite/spark/PslContinuousReader.java",
    "contents_url": "https://api.github.com/repos/googleapis/java-pubsublite-spark/contents/src/main/java/com/google/cloud/pubsublite/spark/PslContinuousReader.java?ref=e408b3e6987977c82700732492cad881a431185d",
    "patch": "@@ -41,8 +41,9 @@\n   private final PartitionSubscriberFactory partitionSubscriberFactory;\n   private final SubscriptionPath subscriptionPath;\n   private final FlowControlSettings flowControlSettings;\n-  private final long topicPartitionCount;\n   private SparkSourceOffset startOffset;\n+  private final PartitionCountReader partitionCountReader;\n+  private final long topicPartitionCount;\n \n   @VisibleForTesting\n   public PslContinuousReader(\n@@ -51,13 +52,14 @@ public PslContinuousReader(\n       PartitionSubscriberFactory partitionSubscriberFactory,\n       SubscriptionPath subscriptionPath,\n       FlowControlSettings flowControlSettings,\n-      long topicPartitionCount) {\n+      PartitionCountReader partitionCountReader) {\n     this.cursorClient = cursorClient;\n     this.committer = committer;\n     this.partitionSubscriberFactory = partitionSubscriberFactory;\n     this.subscriptionPath = subscriptionPath;\n     this.flowControlSettings = flowControlSettings;\n-    this.topicPartitionCount = topicPartitionCount;\n+    this.partitionCountReader = partitionCountReader;\n+    this.topicPartitionCount = partitionCountReader.getPartitionCount();\n   }\n \n   @Override\n@@ -126,4 +128,9 @@ public StructType readSchema() {\n     }\n     return list;\n   }\n+\n+  @Override\n+  public boolean needsReconfiguration() {\n+    return partitionCountReader.getPartitionCount() != topicPartitionCount;\n+  }\n }"
  },
  {
    "sha": "08a96ee8eacc0f2d50b643f125a76b38a8588812",
    "filename": "src/main/java/com/google/cloud/pubsublite/spark/PslDataSource.java",
    "status": "modified",
    "additions": 13,
    "deletions": 11,
    "changes": 24,
    "blob_url": "https://github.com/googleapis/java-pubsublite-spark/blob/e408b3e6987977c82700732492cad881a431185d/src/main/java/com/google/cloud/pubsublite/spark/PslDataSource.java",
    "raw_url": "https://github.com/googleapis/java-pubsublite-spark/raw/e408b3e6987977c82700732492cad881a431185d/src/main/java/com/google/cloud/pubsublite/spark/PslDataSource.java",
    "contents_url": "https://api.github.com/repos/googleapis/java-pubsublite-spark/contents/src/main/java/com/google/cloud/pubsublite/spark/PslDataSource.java?ref=e408b3e6987977c82700732492cad881a431185d",
    "patch": "@@ -21,7 +21,6 @@\n import com.github.benmanes.caffeine.cache.Ticker;\n import com.google.auto.service.AutoService;\n import com.google.cloud.pubsublite.AdminClient;\n-import com.google.cloud.pubsublite.PartitionLookupUtils;\n import com.google.cloud.pubsublite.SubscriptionPath;\n import com.google.cloud.pubsublite.TopicPath;\n import java.util.Objects;\n@@ -55,17 +54,21 @@ public ContinuousReader createContinuousReader(\n     PslDataSourceOptions pslDataSourceOptions =\n         PslDataSourceOptions.fromSparkDataSourceOptions(options);\n     SubscriptionPath subscriptionPath = pslDataSourceOptions.subscriptionPath();\n-    long topicPartitionCount;\n+    TopicPath topicPath;\n     try (AdminClient adminClient = pslDataSourceOptions.newAdminClient()) {\n-      topicPartitionCount = PartitionLookupUtils.numPartitions(subscriptionPath, adminClient);\n+      topicPath = TopicPath.parse(adminClient.getSubscription(subscriptionPath).get().getTopic());\n+    } catch (Throwable t) {\n+      throw toCanonical(t).underlying;\n     }\n+    PartitionCountReader partitionCountReader =\n+        new CachedPartitionCountReader(pslDataSourceOptions.newAdminClient(), topicPath);\n     return new PslContinuousReader(\n         pslDataSourceOptions.newCursorClient(),\n-        pslDataSourceOptions.newMultiPartitionCommitter(topicPartitionCount),\n+        pslDataSourceOptions.newMultiPartitionCommitter(partitionCountReader.getPartitionCount()),\n         pslDataSourceOptions.getSubscriberFactory(),\n         subscriptionPath,\n         Objects.requireNonNull(pslDataSourceOptions.flowControlSettings()),\n-        topicPartitionCount);\n+        partitionCountReader);\n   }\n \n   @Override\n@@ -80,25 +83,24 @@ public MicroBatchReader createMicroBatchReader(\n         PslDataSourceOptions.fromSparkDataSourceOptions(options);\n     SubscriptionPath subscriptionPath = pslDataSourceOptions.subscriptionPath();\n     TopicPath topicPath;\n-    long topicPartitionCount;\n     try (AdminClient adminClient = pslDataSourceOptions.newAdminClient()) {\n       topicPath = TopicPath.parse(adminClient.getSubscription(subscriptionPath).get().getTopic());\n-      topicPartitionCount = PartitionLookupUtils.numPartitions(topicPath, adminClient);\n     } catch (Throwable t) {\n       throw toCanonical(t).underlying;\n     }\n+    PartitionCountReader partitionCountReader =\n+        new CachedPartitionCountReader(pslDataSourceOptions.newAdminClient(), topicPath);\n     return new PslMicroBatchReader(\n         pslDataSourceOptions.newCursorClient(),\n-        pslDataSourceOptions.newMultiPartitionCommitter(topicPartitionCount),\n+        pslDataSourceOptions.newMultiPartitionCommitter(partitionCountReader.getPartitionCount()),\n         pslDataSourceOptions.getSubscriberFactory(),\n         new LimitingHeadOffsetReader(\n             pslDataSourceOptions.newTopicStatsClient(),\n             topicPath,\n-            topicPartitionCount,\n+            partitionCountReader,\n             Ticker.systemTicker()),\n         subscriptionPath,\n         Objects.requireNonNull(pslDataSourceOptions.flowControlSettings()),\n-        pslDataSourceOptions.maxMessagesPerBatch(),\n-        topicPartitionCount);\n+        pslDataSourceOptions.maxMessagesPerBatch());\n   }\n }"
  },
  {
    "sha": "b2a346c0cb0c2b746053279503fc4e42f158e96e",
    "filename": "src/main/java/com/google/cloud/pubsublite/spark/PslMicroBatchReader.java",
    "status": "modified",
    "additions": 33,
    "deletions": 25,
    "changes": 58,
    "blob_url": "https://github.com/googleapis/java-pubsublite-spark/blob/e408b3e6987977c82700732492cad881a431185d/src/main/java/com/google/cloud/pubsublite/spark/PslMicroBatchReader.java",
    "raw_url": "https://github.com/googleapis/java-pubsublite-spark/raw/e408b3e6987977c82700732492cad881a431185d/src/main/java/com/google/cloud/pubsublite/spark/PslMicroBatchReader.java",
    "contents_url": "https://api.github.com/repos/googleapis/java-pubsublite-spark/contents/src/main/java/com/google/cloud/pubsublite/spark/PslMicroBatchReader.java?ref=e408b3e6987977c82700732492cad881a431185d",
    "patch": "@@ -19,6 +19,7 @@\n import static com.google.common.base.Preconditions.checkArgument;\n import static com.google.common.base.Preconditions.checkState;\n \n+import com.google.cloud.pubsublite.Partition;\n import com.google.cloud.pubsublite.SubscriptionPath;\n import com.google.cloud.pubsublite.cloudpubsub.FlowControlSettings;\n import com.google.cloud.pubsublite.internal.CursorClient;\n@@ -34,14 +35,12 @@\n import org.apache.spark.sql.types.StructType;\n \n public class PslMicroBatchReader implements MicroBatchReader {\n-\n   private final CursorClient cursorClient;\n   private final MultiPartitionCommitter committer;\n   private final PartitionSubscriberFactory partitionSubscriberFactory;\n   private final PerTopicHeadOffsetReader headOffsetReader;\n   private final SubscriptionPath subscriptionPath;\n   private final FlowControlSettings flowControlSettings;\n-  private final long topicPartitionCount;\n   private final long maxMessagesPerBatch;\n   @Nullable private SparkSourceOffset startOffset = null;\n   private SparkSourceOffset endOffset;\n@@ -53,41 +52,45 @@ public PslMicroBatchReader(\n       PerTopicHeadOffsetReader headOffsetReader,\n       SubscriptionPath subscriptionPath,\n       FlowControlSettings flowControlSettings,\n-      long maxMessagesPerBatch,\n-      long topicPartitionCount) {\n+      long maxMessagesPerBatch) {\n     this.cursorClient = cursorClient;\n     this.committer = committer;\n     this.partitionSubscriberFactory = partitionSubscriberFactory;\n     this.headOffsetReader = headOffsetReader;\n     this.subscriptionPath = subscriptionPath;\n     this.flowControlSettings = flowControlSettings;\n-    this.topicPartitionCount = topicPartitionCount;\n     this.maxMessagesPerBatch = maxMessagesPerBatch;\n   }\n \n   @Override\n   public void setOffsetRange(Optional<Offset> start, Optional<Offset> end) {\n+    int currentTopicPartitionCount;\n+    if (end.isPresent()) {\n+      checkArgument(\n+          end.get() instanceof SparkSourceOffset,\n+          \"end offset is not instance of SparkSourceOffset.\");\n+      endOffset = (SparkSourceOffset) end.get();\n+      currentTopicPartitionCount = ((SparkSourceOffset) end.get()).getPartitionOffsetMap().size();\n+    } else {\n+      endOffset = PslSparkUtils.toSparkSourceOffset(headOffsetReader.getHeadOffset());\n+      currentTopicPartitionCount = endOffset.getPartitionOffsetMap().size();\n+    }\n+\n     if (start.isPresent()) {\n       checkArgument(\n           start.get() instanceof SparkSourceOffset,\n           \"start offset is not instance of SparkSourceOffset.\");\n       startOffset = (SparkSourceOffset) start.get();\n     } else {\n       startOffset =\n-          PslSparkUtils.getSparkStartOffset(cursorClient, subscriptionPath, topicPartitionCount);\n-    }\n-    if (end.isPresent()) {\n-      checkArgument(\n-          end.get() instanceof SparkSourceOffset,\n-          \"end offset is not instance of SparkSourceOffset.\");\n-      endOffset = (SparkSourceOffset) end.get();\n-    } else {\n-      SparkSourceOffset headOffset =\n-          PslSparkUtils.toSparkSourceOffset(headOffsetReader.getHeadOffset());\n-      endOffset =\n-          PslSparkUtils.getSparkEndOffset(\n-              headOffset, startOffset, maxMessagesPerBatch, topicPartitionCount);\n+          PslSparkUtils.getSparkStartOffset(\n+              cursorClient, subscriptionPath, currentTopicPartitionCount);\n     }\n+\n+    // Limit endOffset by maxMessagesPerBatch.\n+    endOffset =\n+        PslSparkUtils.getSparkEndOffset(\n+            endOffset, startOffset, maxMessagesPerBatch, currentTopicPartitionCount);\n   }\n \n   @Override\n@@ -126,23 +129,28 @@ public StructType readSchema() {\n \n   @Override\n   public List<InputPartition<InternalRow>> planInputPartitions() {\n-    checkState(startOffset != null);\n+    checkState(startOffset != null && endOffset != null);\n+\n     List<InputPartition<InternalRow>> list = new ArrayList<>();\n-    for (SparkPartitionOffset offset : startOffset.getPartitionOffsetMap().values()) {\n-      SparkPartitionOffset endPartitionOffset =\n-          endOffset.getPartitionOffsetMap().get(offset.partition());\n-      if (offset.equals(endPartitionOffset)) {\n+    // Since this is called right after setOffsetRange, we could use partitions in endOffset as\n+    // current partition count.\n+    for (SparkPartitionOffset endPartitionOffset : endOffset.getPartitionOffsetMap().values()) {\n+      Partition p = endPartitionOffset.partition();\n+      SparkPartitionOffset startPartitionOffset =\n+          startOffset.getPartitionOffsetMap().getOrDefault(p, SparkPartitionOffset.create(p, -1L));\n+      if (startPartitionOffset.equals(endPartitionOffset)) {\n         // There is no message to pull for this partition.\n         continue;\n       }\n       PartitionSubscriberFactory partitionSubscriberFactory = this.partitionSubscriberFactory;\n       SubscriberFactory subscriberFactory =\n-          (consumer) -> partitionSubscriberFactory.newSubscriber(offset.partition(), consumer);\n+          (consumer) ->\n+              partitionSubscriberFactory.newSubscriber(endPartitionOffset.partition(), consumer);\n       list.add(\n           new PslMicroBatchInputPartition(\n               subscriptionPath,\n               flowControlSettings,\n-              offset,\n+              startPartitionOffset,\n               endPartitionOffset,\n               subscriberFactory));\n     }"
  },
  {
    "sha": "dcc3025a95fc8fdb7f5b202e24bcf7166f00e8bc",
    "filename": "src/test/java/com/google/cloud/pubsublite/spark/LimitingHeadOffsetReaderTest.java",
    "status": "modified",
    "additions": 32,
    "deletions": 1,
    "changes": 33,
    "blob_url": "https://github.com/googleapis/java-pubsublite-spark/blob/e408b3e6987977c82700732492cad881a431185d/src/test/java/com/google/cloud/pubsublite/spark/LimitingHeadOffsetReaderTest.java",
    "raw_url": "https://github.com/googleapis/java-pubsublite-spark/raw/e408b3e6987977c82700732492cad881a431185d/src/test/java/com/google/cloud/pubsublite/spark/LimitingHeadOffsetReaderTest.java",
    "contents_url": "https://api.github.com/repos/googleapis/java-pubsublite-spark/contents/src/test/java/com/google/cloud/pubsublite/spark/LimitingHeadOffsetReaderTest.java?ref=e408b3e6987977c82700732492cad881a431185d",
    "patch": "@@ -38,12 +38,15 @@\n \n   private final FakeTicker ticker = new FakeTicker();\n   private final TopicStatsClient topicStatsClient = mock(TopicStatsClient.class);\n+  private final PartitionCountReader partitionReader = mock(PartitionCountReader.class);\n   private final LimitingHeadOffsetReader reader =\n       new LimitingHeadOffsetReader(\n-          topicStatsClient, UnitTestExamples.exampleTopicPath(), 1, ticker::read);\n+          topicStatsClient, UnitTestExamples.exampleTopicPath(), partitionReader, ticker::read);\n \n   @Test\n   public void testRead() {\n+    when(partitionReader.getPartitionCount()).thenReturn(1);\n+\n     Cursor cursor1 = Cursor.newBuilder().setOffset(10).build();\n     Cursor cursor2 = Cursor.newBuilder().setOffset(13).build();\n     when(topicStatsClient.computeHeadCursor(UnitTestExamples.exampleTopicPath(), Partition.of(0)))\n@@ -66,4 +69,32 @@ public void testRead() {\n         .containsExactly(Partition.of(0), Offset.of(cursor2.getOffset()));\n     verify(topicStatsClient).computeHeadCursor(any(), any());\n   }\n+\n+  @Test\n+  public void testPartitionChange() {\n+    when(partitionReader.getPartitionCount()).thenReturn(1);\n+\n+    Cursor cursor1 = Cursor.newBuilder().setOffset(10).build();\n+    when(topicStatsClient.computeHeadCursor(UnitTestExamples.exampleTopicPath(), Partition.of(0)))\n+        .thenReturn(ApiFutures.immediateFuture(cursor1));\n+    assertThat(reader.getHeadOffset().partitionOffsetMap())\n+        .containsExactly(Partition.of(0), Offset.of(10));\n+    verify(topicStatsClient).computeHeadCursor(any(), any());\n+\n+    when(partitionReader.getPartitionCount()).thenReturn(3);\n+\n+    for (int i = 0; i < 3; i++) {\n+      when(topicStatsClient.computeHeadCursor(UnitTestExamples.exampleTopicPath(), Partition.of(i)))\n+          .thenReturn(ApiFutures.immediateFuture(cursor1));\n+    }\n+    assertThat(reader.getHeadOffset().partitionOffsetMap())\n+        .containsExactly(\n+            Partition.of(0),\n+            Offset.of(10),\n+            Partition.of(1),\n+            Offset.of(10),\n+            Partition.of(2),\n+            Offset.of(10));\n+    verify(topicStatsClient, times(3)).computeHeadCursor(any(), any());\n+  }\n }"
  },
  {
    "sha": "65b4675a8f40e8e90bb1a8c691ece999501c4e16",
    "filename": "src/test/java/com/google/cloud/pubsublite/spark/MultiPartitionCommitterImplTest.java",
    "status": "modified",
    "additions": 95,
    "deletions": 46,
    "changes": 141,
    "blob_url": "https://github.com/googleapis/java-pubsublite-spark/blob/e408b3e6987977c82700732492cad881a431185d/src/test/java/com/google/cloud/pubsublite/spark/MultiPartitionCommitterImplTest.java",
    "raw_url": "https://github.com/googleapis/java-pubsublite-spark/raw/e408b3e6987977c82700732492cad881a431185d/src/test/java/com/google/cloud/pubsublite/spark/MultiPartitionCommitterImplTest.java",
    "contents_url": "https://api.github.com/repos/googleapis/java-pubsublite-spark/contents/src/test/java/com/google/cloud/pubsublite/spark/MultiPartitionCommitterImplTest.java?ref=e408b3e6987977c82700732492cad881a431185d",
    "patch": "@@ -16,75 +16,124 @@\n \n package com.google.cloud.pubsublite.spark;\n \n+import static com.google.cloud.pubsublite.spark.TestingUtils.createPslSourceOffset;\n import static org.mockito.ArgumentMatchers.eq;\n import static org.mockito.Mockito.*;\n \n import com.google.api.core.SettableApiFuture;\n import com.google.cloud.pubsublite.*;\n import com.google.cloud.pubsublite.internal.wire.Committer;\n-import com.google.common.collect.ImmutableMap;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n import org.junit.Test;\n+import org.mockito.ArgumentCaptor;\n \n public class MultiPartitionCommitterImplTest {\n \n-  @Test\n-  public void testCommit() {\n-    Committer committer1 = mock(Committer.class);\n-    Committer committer2 = mock(Committer.class);\n-    when(committer1.startAsync())\n-        .thenReturn(committer1)\n-        .thenThrow(new IllegalStateException(\"should only init once\"));\n-    when(committer2.startAsync())\n-        .thenReturn(committer2)\n-        .thenThrow(new IllegalStateException(\"should only init once\"));\n+  private Runnable task;\n+  private List<Committer> committerList;\n+\n+  private MultiPartitionCommitterImpl createCommitter(int initialPartitions, int available) {\n+    committerList = new ArrayList<>();\n+    for (int i = 0; i < available; i++) {\n+      Committer committer = mock(Committer.class);\n+      when(committer.startAsync())\n+          .thenReturn(committer)\n+          .thenThrow(new IllegalStateException(\"should only init once\"));\n+      when(committer.commitOffset(eq(Offset.of(10L)))).thenReturn(SettableApiFuture.create());\n+      committerList.add(committer);\n+    }\n+    ScheduledExecutorService mockExecutor = mock(ScheduledExecutorService.class);\n+    ArgumentCaptor<Runnable> taskCaptor = ArgumentCaptor.forClass(Runnable.class);\n+    when(mockExecutor.scheduleWithFixedDelay(\n+            taskCaptor.capture(), anyLong(), anyLong(), any(TimeUnit.class)))\n+        .thenReturn(null);\n     MultiPartitionCommitterImpl multiCommitter =\n         new MultiPartitionCommitterImpl(\n-            2,\n-            (p) -> {\n-              if (p.value() == 0L) {\n-                return committer1;\n-              } else {\n-                return committer2;\n-              }\n-            });\n-    verify(committer1, times(1)).startAsync();\n-    verify(committer2, times(1)).startAsync();\n-\n-    PslSourceOffset offset =\n-        PslSourceOffset.builder()\n-            .partitionOffsetMap(\n-                ImmutableMap.of(\n-                    Partition.of(0), Offset.of(10L),\n-                    Partition.of(1), Offset.of(8L)))\n-            .build();\n+            initialPartitions, p -> committerList.get((int) p.value()), mockExecutor);\n+    task = taskCaptor.getValue();\n+    return multiCommitter;\n+  }\n+\n+  private MultiPartitionCommitterImpl createCommitter(int initialPartitions) {\n+    return createCommitter(initialPartitions, initialPartitions);\n+  }\n+\n+  @Test\n+  public void testCommit() {\n+    MultiPartitionCommitterImpl multiCommitter = createCommitter(2);\n+\n+    verify(committerList.get(0)).startAsync();\n+    verify(committerList.get(1)).startAsync();\n+\n+    PslSourceOffset offset = createPslSourceOffset(10L, 8L);\n     SettableApiFuture<Void> future1 = SettableApiFuture.create();\n     SettableApiFuture<Void> future2 = SettableApiFuture.create();\n-    when(committer1.commitOffset(eq(Offset.of(10L)))).thenReturn(future1);\n-    when(committer2.commitOffset(eq(Offset.of(8L)))).thenReturn(future2);\n+    when(committerList.get(0).commitOffset(eq(Offset.of(10L)))).thenReturn(future1);\n+    when(committerList.get(1).commitOffset(eq(Offset.of(8L)))).thenReturn(future2);\n     multiCommitter.commit(offset);\n-    verify(committer1, times(1)).commitOffset(eq(Offset.of(10L)));\n-    verify(committer2, times(1)).commitOffset(eq(Offset.of(8L)));\n+    verify(committerList.get(0)).commitOffset(eq(Offset.of(10L)));\n+    verify(committerList.get(1)).commitOffset(eq(Offset.of(8L)));\n   }\n \n   @Test\n   public void testClose() {\n-    Committer committer = mock(Committer.class);\n-    when(committer.startAsync())\n-        .thenReturn(committer)\n-        .thenThrow(new IllegalStateException(\"should only init once\"));\n-    MultiPartitionCommitterImpl multiCommitter =\n-        new MultiPartitionCommitterImpl(1, (p) -> committer);\n+    MultiPartitionCommitterImpl multiCommitter = createCommitter(1);\n \n-    PslSourceOffset offset =\n-        PslSourceOffset.builder()\n-            .partitionOffsetMap(ImmutableMap.of(Partition.of(0), Offset.of(10L)))\n-            .build();\n+    PslSourceOffset offset = createPslSourceOffset(10L);\n     SettableApiFuture<Void> future1 = SettableApiFuture.create();\n-    when(committer.commitOffset(eq(Offset.of(10L)))).thenReturn(future1);\n-    when(committer.stopAsync()).thenReturn(committer);\n+    when(committerList.get(0).commitOffset(eq(Offset.of(10L)))).thenReturn(future1);\n     multiCommitter.commit(offset);\n+    when(committerList.get(0).stopAsync()).thenReturn(committerList.get(0));\n \n     multiCommitter.close();\n-    verify(committer, times(1)).stopAsync();\n+    verify(committerList.get(0)).stopAsync();\n+  }\n+\n+  @Test\n+  public void testPartitionChange() {\n+    // Creates committer with 2 partitions\n+    MultiPartitionCommitterImpl multiCommitter = createCommitter(2, 4);\n+    for (int i = 0; i < 2; i++) {\n+      verify(committerList.get(i)).startAsync();\n+    }\n+    for (int i = 2; i < 4; i++) {\n+      verify(committerList.get(i), times(0)).startAsync();\n+    }\n+\n+    // Partitions increased to 4.\n+    multiCommitter.commit(createPslSourceOffset(10L, 10L, 10L, 10L));\n+    for (int i = 0; i < 2; i++) {\n+      verify(committerList.get(i)).commitOffset(eq(Offset.of(10L)));\n+    }\n+    for (int i = 2; i < 4; i++) {\n+      verify(committerList.get(i)).startAsync();\n+      verify(committerList.get(i)).commitOffset(eq(Offset.of(10L)));\n+    }\n+\n+    // Partitions decreased to 2\n+    multiCommitter.commit(createPslSourceOffset(10L, 10L));\n+    for (int i = 0; i < 2; i++) {\n+      verify(committerList.get(i), times(2)).commitOffset(eq(Offset.of(10L)));\n+    }\n+    task.run();\n+    for (int i = 2; i < 4; i++) {\n+      verify(committerList.get(i)).stopAsync();\n+    }\n+  }\n+\n+  @Test\n+  public void testDelayedPartitionRemoval() {\n+    // Creates committer with 4 partitions, then decrease to 2, then increase to 3.\n+    MultiPartitionCommitterImpl multiCommitter = createCommitter(4);\n+    multiCommitter.commit(createPslSourceOffset(10L, 10L));\n+    multiCommitter.commit(createPslSourceOffset(10L, 10L, 10L));\n+    task.run();\n+    verify(committerList.get(2)).startAsync();\n+    verify(committerList.get(2), times(0)).stopAsync();\n+    verify(committerList.get(3)).startAsync();\n+    verify(committerList.get(3)).stopAsync();\n   }\n }"
  },
  {
    "sha": "36bcdf917f62be67990af56089eafa1a70a99a1e",
    "filename": "src/test/java/com/google/cloud/pubsublite/spark/PslContinuousReaderTest.java",
    "status": "modified",
    "additions": 15,
    "deletions": 1,
    "changes": 16,
    "blob_url": "https://github.com/googleapis/java-pubsublite-spark/blob/e408b3e6987977c82700732492cad881a431185d/src/test/java/com/google/cloud/pubsublite/spark/PslContinuousReaderTest.java",
    "raw_url": "https://github.com/googleapis/java-pubsublite-spark/raw/e408b3e6987977c82700732492cad881a431185d/src/test/java/com/google/cloud/pubsublite/spark/PslContinuousReaderTest.java",
    "contents_url": "https://api.github.com/repos/googleapis/java-pubsublite-spark/contents/src/test/java/com/google/cloud/pubsublite/spark/PslContinuousReaderTest.java?ref=e408b3e6987977c82700732492cad881a431185d",
    "patch": "@@ -38,14 +38,22 @@\n   private final MultiPartitionCommitter committer = mock(MultiPartitionCommitter.class);\n   private final PartitionSubscriberFactory partitionSubscriberFactory =\n       mock(PartitionSubscriberFactory.class);\n+  private final PartitionCountReader partitionCountReader;\n+\n+  {\n+    PartitionCountReader mock = mock(PartitionCountReader.class);\n+    when(mock.getPartitionCount()).thenReturn(2);\n+    partitionCountReader = mock;\n+  }\n+\n   private final PslContinuousReader reader =\n       new PslContinuousReader(\n           cursorClient,\n           committer,\n           partitionSubscriberFactory,\n           UnitTestExamples.exampleSubscriptionPath(),\n           OPTIONS.flowControlSettings(),\n-          2);\n+          partitionCountReader);\n \n   @Test\n   public void testEmptyStartOffset() {\n@@ -122,4 +130,10 @@ public void testCommit() {\n     reader.commit(offset);\n     verify(committer, times(1)).commit(eq(expectedCommitOffset));\n   }\n+\n+  @Test\n+  public void testPartitionIncrease() {\n+    when(partitionCountReader.getPartitionCount()).thenReturn(4);\n+    assertThat(reader.needsReconfiguration()).isTrue();\n+  }\n }"
  },
  {
    "sha": "3692e7a58fa4722792d21ee9785f466e3eba9653",
    "filename": "src/test/java/com/google/cloud/pubsublite/spark/PslMicroBatchReaderTest.java",
    "status": "modified",
    "additions": 75,
    "deletions": 24,
    "changes": 99,
    "blob_url": "https://github.com/googleapis/java-pubsublite-spark/blob/e408b3e6987977c82700732492cad881a431185d/src/test/java/com/google/cloud/pubsublite/spark/PslMicroBatchReaderTest.java",
    "raw_url": "https://github.com/googleapis/java-pubsublite-spark/raw/e408b3e6987977c82700732492cad881a431185d/src/test/java/com/google/cloud/pubsublite/spark/PslMicroBatchReaderTest.java",
    "contents_url": "https://api.github.com/repos/googleapis/java-pubsublite-spark/contents/src/test/java/com/google/cloud/pubsublite/spark/PslMicroBatchReaderTest.java?ref=e408b3e6987977c82700732492cad881a431185d",
    "patch": "@@ -16,6 +16,8 @@\n \n package com.google.cloud.pubsublite.spark;\n \n+import static com.google.cloud.pubsublite.spark.TestingUtils.createPslSourceOffset;\n+import static com.google.cloud.pubsublite.spark.TestingUtils.createSparkSourceOffset;\n import static com.google.common.truth.Truth.assertThat;\n import static org.mockito.ArgumentMatchers.eq;\n import static org.mockito.Mockito.*;\n@@ -49,31 +51,33 @@\n           headOffsetReader,\n           UnitTestExamples.exampleSubscriptionPath(),\n           OPTIONS.flowControlSettings(),\n-          MAX_MESSAGES_PER_BATCH,\n-          2);\n+          MAX_MESSAGES_PER_BATCH);\n \n-  private PslSourceOffset createPslSourceOffsetTwoPartition(long offset0, long offset1) {\n-    return PslSourceOffset.builder()\n-        .partitionOffsetMap(\n-            ImmutableMap.of(\n-                Partition.of(0L), Offset.of(offset0), Partition.of(1L), Offset.of(offset1)))\n-        .build();\n-  }\n-\n-  private SparkSourceOffset createSparkSourceOffsetTwoPartition(long offset0, long offset1) {\n-    return new SparkSourceOffset(\n-        ImmutableMap.of(\n+  @Test\n+  public void testNoCommitCursors() {\n+    when(cursorClient.listPartitionCursors(UnitTestExamples.exampleSubscriptionPath()))\n+        .thenReturn(ApiFutures.immediateFuture(ImmutableMap.of()));\n+    when(headOffsetReader.getHeadOffset()).thenReturn(createPslSourceOffset(301L, 200L));\n+    reader.setOffsetRange(Optional.empty(), Optional.empty());\n+    assertThat(((SparkSourceOffset) reader.getStartOffset()).getPartitionOffsetMap())\n+        .containsExactly(\n+            Partition.of(0L),\n+            SparkPartitionOffset.create(Partition.of(0L), -1L),\n+            Partition.of(1L),\n+            SparkPartitionOffset.create(Partition.of(1L), -1L));\n+    assertThat(((SparkSourceOffset) reader.getEndOffset()).getPartitionOffsetMap())\n+        .containsExactly(\n             Partition.of(0L),\n-            SparkPartitionOffset.create(Partition.of(0L), offset0),\n+            SparkPartitionOffset.create(Partition.of(0L), 300L),\n             Partition.of(1L),\n-            SparkPartitionOffset.create(Partition.of(1L), offset1)));\n+            SparkPartitionOffset.create(Partition.of(1L), 199L));\n   }\n \n   @Test\n   public void testEmptyOffsets() {\n     when(cursorClient.listPartitionCursors(UnitTestExamples.exampleSubscriptionPath()))\n         .thenReturn(ApiFutures.immediateFuture(ImmutableMap.of(Partition.of(0L), Offset.of(100L))));\n-    when(headOffsetReader.getHeadOffset()).thenReturn(createPslSourceOffsetTwoPartition(301L, 0L));\n+    when(headOffsetReader.getHeadOffset()).thenReturn(createPslSourceOffset(301L, 0L));\n     reader.setOffsetRange(Optional.empty(), Optional.empty());\n     assertThat(((SparkSourceOffset) reader.getStartOffset()).getPartitionOffsetMap())\n         .containsExactly(\n@@ -91,8 +95,8 @@ public void testEmptyOffsets() {\n \n   @Test\n   public void testValidOffsets() {\n-    SparkSourceOffset startOffset = createSparkSourceOffsetTwoPartition(10L, 100L);\n-    SparkSourceOffset endOffset = createSparkSourceOffsetTwoPartition(20L, 300L);\n+    SparkSourceOffset startOffset = createSparkSourceOffset(10L, 100L);\n+    SparkSourceOffset endOffset = createSparkSourceOffset(20L, 300L);\n     reader.setOffsetRange(Optional.of(startOffset), Optional.of(endOffset));\n     assertThat(reader.getStartOffset()).isEqualTo(startOffset);\n     assertThat(reader.getEndOffset()).isEqualTo(endOffset);\n@@ -108,16 +112,16 @@ public void testDeserializeOffset() {\n \n   @Test\n   public void testCommit() {\n-    SparkSourceOffset offset = createSparkSourceOffsetTwoPartition(10L, 50L);\n-    PslSourceOffset expectedCommitOffset = createPslSourceOffsetTwoPartition(11L, 51L);\n+    SparkSourceOffset offset = createSparkSourceOffset(10L, 50L);\n+    PslSourceOffset expectedCommitOffset = createPslSourceOffset(11L, 51L);\n     reader.commit(offset);\n     verify(committer, times(1)).commit(eq(expectedCommitOffset));\n   }\n \n   @Test\n   public void testPlanInputPartitionNoMessage() {\n-    SparkSourceOffset startOffset = createSparkSourceOffsetTwoPartition(10L, 100L);\n-    SparkSourceOffset endOffset = createSparkSourceOffsetTwoPartition(20L, 100L);\n+    SparkSourceOffset startOffset = createSparkSourceOffset(10L, 100L);\n+    SparkSourceOffset endOffset = createSparkSourceOffset(20L, 100L);\n     reader.setOffsetRange(Optional.of(startOffset), Optional.of(endOffset));\n     assertThat(reader.planInputPartitions()).hasSize(1);\n   }\n@@ -126,8 +130,7 @@ public void testPlanInputPartitionNoMessage() {\n   public void testMaxMessagesPerBatch() {\n     when(cursorClient.listPartitionCursors(UnitTestExamples.exampleSubscriptionPath()))\n         .thenReturn(ApiFutures.immediateFuture(ImmutableMap.of(Partition.of(0L), Offset.of(100L))));\n-    when(headOffsetReader.getHeadOffset())\n-        .thenReturn(createPslSourceOffsetTwoPartition(10000000L, 0L));\n+    when(headOffsetReader.getHeadOffset()).thenReturn(createPslSourceOffset(10000000L, 0L));\n     reader.setOffsetRange(Optional.empty(), Optional.empty());\n     assertThat(((SparkSourceOffset) reader.getEndOffset()).getPartitionOffsetMap())\n         .containsExactly(\n@@ -139,4 +142,52 @@ public void testMaxMessagesPerBatch() {\n             Partition.of(1L),\n             SparkPartitionOffset.create(Partition.of(1L), -1L));\n   }\n+\n+  @Test\n+  public void testPartitionIncreasedRetry() {\n+    SparkSourceOffset startOffset = createSparkSourceOffset(10L, 100L);\n+    SparkSourceOffset endOffset = createSparkSourceOffset(20L, 300L, 100L);\n+    reader.setOffsetRange(Optional.of(startOffset), Optional.of(endOffset));\n+    assertThat(reader.getStartOffset()).isEqualTo(startOffset);\n+    assertThat(reader.getEndOffset()).isEqualTo(endOffset);\n+    assertThat(reader.planInputPartitions()).hasSize(3);\n+  }\n+\n+  @Test\n+  public void testPartitionIncreasedNewQuery() {\n+    when(cursorClient.listPartitionCursors(UnitTestExamples.exampleSubscriptionPath()))\n+        .thenReturn(ApiFutures.immediateFuture(ImmutableMap.of(Partition.of(0L), Offset.of(100L))));\n+    SparkSourceOffset endOffset = createSparkSourceOffset(301L, 200L);\n+    when(headOffsetReader.getHeadOffset()).thenReturn(PslSparkUtils.toPslSourceOffset(endOffset));\n+    reader.setOffsetRange(Optional.empty(), Optional.empty());\n+    assertThat(reader.getStartOffset()).isEqualTo(createSparkSourceOffset(99L, -1L));\n+    assertThat(reader.getEndOffset()).isEqualTo(endOffset);\n+    assertThat(reader.planInputPartitions()).hasSize(2);\n+  }\n+\n+  @Test\n+  public void testPartitionIncreasedBeforeSetOffsets() {\n+    SparkSourceOffset endOffset = createSparkSourceOffset(301L, 200L);\n+    SparkSourceOffset startOffset = createSparkSourceOffset(100L);\n+    when(headOffsetReader.getHeadOffset()).thenReturn(PslSparkUtils.toPslSourceOffset(endOffset));\n+    reader.setOffsetRange(Optional.of(startOffset), Optional.empty());\n+    assertThat(reader.getStartOffset()).isEqualTo(startOffset);\n+    assertThat(reader.getEndOffset()).isEqualTo(endOffset);\n+    assertThat(reader.planInputPartitions()).hasSize(2);\n+  }\n+\n+  @Test\n+  public void testPartitionIncreasedBetweenSetOffsetsAndPlan() {\n+    SparkSourceOffset startOffset = createSparkSourceOffset(100L);\n+    SparkSourceOffset endOffset = createSparkSourceOffset(301L);\n+    SparkSourceOffset newEndOffset = createSparkSourceOffset(600L, 300L);\n+    when(headOffsetReader.getHeadOffset()).thenReturn(PslSparkUtils.toPslSourceOffset(endOffset));\n+    reader.setOffsetRange(Optional.of(startOffset), Optional.empty());\n+    assertThat(reader.getStartOffset()).isEqualTo(startOffset);\n+    assertThat(reader.getEndOffset()).isEqualTo(endOffset);\n+    when(headOffsetReader.getHeadOffset())\n+        .thenReturn(PslSparkUtils.toPslSourceOffset(newEndOffset));\n+    // headOffsetReader changes between setOffsets and plan should have no effect.\n+    assertThat(reader.planInputPartitions()).hasSize(1);\n+  }\n }"
  },
  {
    "sha": "43b466ce67d6a632cee263d027023019f7bd90da",
    "filename": "src/test/java/com/google/cloud/pubsublite/spark/TestingUtils.java",
    "status": "added",
    "additions": 43,
    "deletions": 0,
    "changes": 43,
    "blob_url": "https://github.com/googleapis/java-pubsublite-spark/blob/e408b3e6987977c82700732492cad881a431185d/src/test/java/com/google/cloud/pubsublite/spark/TestingUtils.java",
    "raw_url": "https://github.com/googleapis/java-pubsublite-spark/raw/e408b3e6987977c82700732492cad881a431185d/src/test/java/com/google/cloud/pubsublite/spark/TestingUtils.java",
    "contents_url": "https://api.github.com/repos/googleapis/java-pubsublite-spark/contents/src/test/java/com/google/cloud/pubsublite/spark/TestingUtils.java?ref=e408b3e6987977c82700732492cad881a431185d",
    "patch": "@@ -0,0 +1,43 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.google.cloud.pubsublite.spark;\n+\n+import com.google.cloud.pubsublite.Offset;\n+import com.google.cloud.pubsublite.Partition;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class TestingUtils {\n+  public static PslSourceOffset createPslSourceOffset(long... offsets) {\n+    Map<Partition, Offset> map = new HashMap<>();\n+    int idx = 0;\n+    for (long offset : offsets) {\n+      map.put(Partition.of(idx++), Offset.of(offset));\n+    }\n+    return PslSourceOffset.builder().partitionOffsetMap(map).build();\n+  }\n+\n+  public static SparkSourceOffset createSparkSourceOffset(long... offsets) {\n+    Map<Partition, SparkPartitionOffset> map = new HashMap<>();\n+    int idx = 0;\n+    for (long offset : offsets) {\n+      map.put(Partition.of(idx), SparkPartitionOffset.create(Partition.of(idx), offset));\n+      idx++;\n+    }\n+    return new SparkSourceOffset(map);\n+  }\n+}"
  }
]
