[
  {
    "sha": "f90f1c905571fcc512e510e123679b660836787c",
    "filename": "README.md",
    "status": "modified",
    "additions": 8,
    "deletions": 1,
    "changes": 9,
    "blob_url": "https://github.com/camunda-community-hub/kafka-connect-zeebe/blob/db9563a108ef304f071c542f3f1d64b4d67b5108/README.md",
    "raw_url": "https://github.com/camunda-community-hub/kafka-connect-zeebe/raw/db9563a108ef304f071c542f3f1d64b4d67b5108/README.md",
    "contents_url": "https://api.github.com/repos/camunda-community-hub/kafka-connect-zeebe/contents/README.md?ref=db9563a108ef304f071c542f3f1d64b4d67b5108",
    "patch": "@@ -48,7 +48,7 @@ You could simply ingest all messages from a Kafka topic and check if they correl\n \n ### Configuration\n \n-In order to communicate with the Zeebe workflow engine, the connector has to create a Zeebe client. \n+In order to communicate with the Zeebe workflow engine, the connector has to create a Zeebe client.\n \n #### Camunda Cloud Properties\n \n@@ -128,6 +128,13 @@ If this custom header is not present, then all variables in the scope will be se\n \n Kafka Connect allows you to configure what happens if a message cannot be processed. A great explanation can be found in [Kafka Connect Deep Dive â€“ Error Handling and Dead Letter Queues](https://www.confluent.io/blog/kafka-connect-deep-dive-error-handling-dead-letter-queues). This of course also applies to this connector.\n \n+# Development\n+\n+To ease with development, you can add this environment variable to kafka-connect:\n+`\"JAVA_TOOL_OPTIONS\": \"-agentlib:jdwp=transport=dt_socket,address=*:5005,server=y,suspend=n\"`\n+\n+And then use [remote debugging](https://www.jetbrains.com/help/idea/tutorial-remote-debug.html)\n+\n # Confluent Hub\n \n This project is set up to be released on Confluent Hub."
  },
  {
    "sha": "04c6c5074b347e6dd7c84bb6bc3a072ea2f9f09e",
    "filename": "docker/docker-compose-with-zeebe.yml",
    "status": "modified",
    "additions": 6,
    "deletions": 6,
    "changes": 12,
    "blob_url": "https://github.com/camunda-community-hub/kafka-connect-zeebe/blob/db9563a108ef304f071c542f3f1d64b4d67b5108/docker/docker-compose-with-zeebe.yml",
    "raw_url": "https://github.com/camunda-community-hub/kafka-connect-zeebe/raw/db9563a108ef304f071c542f3f1d64b4d67b5108/docker/docker-compose-with-zeebe.yml",
    "contents_url": "https://api.github.com/repos/camunda-community-hub/kafka-connect-zeebe/contents/docker/docker-compose-with-zeebe.yml?ref=db9563a108ef304f071c542f3f1d64b4d67b5108",
    "patch": "@@ -2,7 +2,7 @@ version: '2.1'\n \n services:\n   zeebe:\n-    image: camunda/zeebe:0.21.1\n+    image: camunda/zeebe:0.26.0\n     hostname: zeebe\n     ports:\n       - \"26500:26500\"\n@@ -20,7 +20,7 @@ services:\n       - ./operate.yml:/usr/local/operate/config/application.yml\n     depends_on:\n       - elasticsearch\n-      \n+\n   elasticsearch:\n     image: docker.elastic.co/elasticsearch/elasticsearch-oss:6.8.3\n     ports:\n@@ -41,7 +41,7 @@ services:\n       ZOO_SERVERS: server.1=zookeeper:2888:3888\n \n   kafka:\n-    image: confluentinc/cp-enterprise-kafka:5.3.0\n+    image: confluentinc/cp-enterprise-kafka:5.5.3\n     hostname: kafka\n     ports:\n       - \"9092:9092\"\n@@ -60,7 +60,7 @@ services:\n       - zookeeper\n \n   schema-registry:\n-    image: confluentinc/cp-schema-registry:5.3.0\n+    image: confluentinc/cp-schema-registry:5.5.3\n     hostname: schema-registry\n     ports:\n       - \"8081:8081\"\n@@ -73,7 +73,7 @@ services:\n       - kafka\n \n   connect:\n-    image: confluentinc/cp-kafka-connect:5.3.0\n+    image: confluentinc/cp-kafka-connect:5.5.3\n     hostname: connect\n     ports:\n       - \"8083:8083\"\n@@ -105,7 +105,7 @@ services:\n       - zeebe\n \n   control-center:\n-    image: confluentinc/cp-enterprise-control-center:5.3.0\n+    image: confluentinc/cp-enterprise-control-center:5.5.3\n     hostname: control-center\n     ports:\n       - \"9021:9021\""
  },
  {
    "sha": "4997c1dd9d0d34e7d88b12cd20a4d3852ac2b640",
    "filename": "docker/docker-compose.yml",
    "status": "modified",
    "additions": 4,
    "deletions": 4,
    "changes": 8,
    "blob_url": "https://github.com/camunda-community-hub/kafka-connect-zeebe/blob/db9563a108ef304f071c542f3f1d64b4d67b5108/docker/docker-compose.yml",
    "raw_url": "https://github.com/camunda-community-hub/kafka-connect-zeebe/raw/db9563a108ef304f071c542f3f1d64b4d67b5108/docker/docker-compose.yml",
    "contents_url": "https://api.github.com/repos/camunda-community-hub/kafka-connect-zeebe/contents/docker/docker-compose.yml?ref=db9563a108ef304f071c542f3f1d64b4d67b5108",
    "patch": "@@ -22,7 +22,7 @@ services:\n       ZOO_SERVERS: server.1=zookeeper:2888:3888\n \n   kafka:\n-    image: confluentinc/cp-enterprise-kafka:5.3.0\n+    image: confluentinc/cp-enterprise-kafka:5.5.3\n     hostname: kafka\n     ports:\n       - \"9092:9092\"\n@@ -41,7 +41,7 @@ services:\n       - zookeeper\n \n   schema-registry:\n-    image: confluentinc/cp-schema-registry:5.3.0\n+    image: confluentinc/cp-schema-registry:5.5.3\n     hostname: schema-registry\n     ports:\n       - \"8081:8081\"\n@@ -54,7 +54,7 @@ services:\n       - kafka\n \n   connect:\n-    image: confluentinc/cp-kafka-connect:5.3.0\n+    image: confluentinc/cp-kafka-connect:5.5.3\n     hostname: connect\n     ports:\n       - \"8083:8083\"\n@@ -85,7 +85,7 @@ services:\n       - kafka\n \n   control-center:\n-    image: confluentinc/cp-enterprise-control-center:5.3.0\n+    image: confluentinc/cp-enterprise-control-center:5.5.3\n     hostname: control-center\n     ports:\n       - \"9021:9021\""
  },
  {
    "sha": "986377cc9cee5bfddc696c73cfa4c5f4227ba92a",
    "filename": "src/main/java/io/zeebe/kafka/connect/sink/message/JsonRecordParser.java",
    "status": "modified",
    "additions": 21,
    "deletions": 2,
    "changes": 23,
    "blob_url": "https://github.com/camunda-community-hub/kafka-connect-zeebe/blob/db9563a108ef304f071c542f3f1d64b4d67b5108/src/main/java/io/zeebe/kafka/connect/sink/message/JsonRecordParser.java",
    "raw_url": "https://github.com/camunda-community-hub/kafka-connect-zeebe/raw/db9563a108ef304f071c542f3f1d64b4d67b5108/src/main/java/io/zeebe/kafka/connect/sink/message/JsonRecordParser.java",
    "contents_url": "https://api.github.com/repos/camunda-community-hub/kafka-connect-zeebe/contents/src/main/java/io/zeebe/kafka/connect/sink/message/JsonRecordParser.java?ref=db9563a108ef304f071c542f3f1d64b4d67b5108",
    "patch": "@@ -18,13 +18,24 @@\n import com.jayway.jsonpath.DocumentContext;\n import com.jayway.jsonpath.JsonPath;\n import com.jayway.jsonpath.JsonPathException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.json.JsonConverter;\n import org.apache.kafka.connect.sink.SinkRecord;\n+import org.apache.kafka.connect.storage.Converter;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n // Eventually extract to interface so we can swap it out for Schema or something\n public final class JsonRecordParser {\n   private static final Logger LOGGER = LoggerFactory.getLogger(JsonRecordParser.class);\n+  private static final Converter JSON_CONVERTER;\n+\n+  static {\n+    JSON_CONVERTER = new JsonConverter();\n+    JSON_CONVERTER.configure(Collections.singletonMap(\"schemas.enable\", \"false\"), false);\n+  }\n \n   private JsonPath keyPath;\n   private JsonPath namePath;\n@@ -50,7 +61,7 @@ public Message parse(final SinkRecord record) {\n     final Message.Builder builder = Message.builder();\n \n     try {\n-      final DocumentContext document = parseDocument(record.value());\n+      final DocumentContext document = parseRecordValue(record);\n       builder\n           .withId(generateId(record))\n           .withKey(document.read(keyPath).toString())\n@@ -72,9 +83,17 @@ public Message parse(final SinkRecord record) {\n   }\n \n   /**\n+   * If given a schema, use {@link JsonConverter#fromConnectData(String, Schema, Object)} to parse the value.\n    * If given a string parse the JSON document, otherwise delegate to {@link JsonPath#parse(Object)}\n    */\n-  private DocumentContext parseDocument(final Object value) {\n+  private DocumentContext parseRecordValue(final SinkRecord record) {\n+    Object value = record.value();\n+\n+    if (record.valueSchema() != null) {\n+      final byte[] bytes = JSON_CONVERTER.fromConnectData(record.topic(), record.valueSchema(), record.value());\n+      value = new String(bytes, StandardCharsets.UTF_8);\n+    }\n+\n     if (value instanceof String) {\n       return JsonPath.parse((String) value);\n     }"
  },
  {
    "sha": "47eb131fbf84ddd26531b9f0304af5d3a6018052",
    "filename": "src/main/java/io/zeebe/kafka/connect/sink/message/Message.java",
    "status": "modified",
    "additions": 11,
    "deletions": 0,
    "changes": 11,
    "blob_url": "https://github.com/camunda-community-hub/kafka-connect-zeebe/blob/db9563a108ef304f071c542f3f1d64b4d67b5108/src/main/java/io/zeebe/kafka/connect/sink/message/Message.java",
    "raw_url": "https://github.com/camunda-community-hub/kafka-connect-zeebe/raw/db9563a108ef304f071c542f3f1d64b4d67b5108/src/main/java/io/zeebe/kafka/connect/sink/message/Message.java",
    "contents_url": "https://api.github.com/repos/camunda-community-hub/kafka-connect-zeebe/contents/src/main/java/io/zeebe/kafka/connect/sink/message/Message.java?ref=db9563a108ef304f071c542f3f1d64b4d67b5108",
    "patch": "@@ -69,6 +69,17 @@ public boolean hasTimeToLive() {\n     return timeToLive != null;\n   }\n \n+  @Override\n+  public String toString() {\n+    return \"Message{\"\n+      + \"id='\" + id  + '\\''\n+      + \", name='\" + name  + '\\''\n+      + \", key='\" + key + '\\''\n+      + \", variables=\" + variables\n+      + \", timeToLive=\" + timeToLive\n+      + '}';\n+  }\n+\n   public static final class Builder {\n     private String id;\n     private String name;"
  }
]
