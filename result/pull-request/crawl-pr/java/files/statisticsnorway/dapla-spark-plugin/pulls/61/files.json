[
  {
    "sha": "3cb81e2cef8b5143b60b1b11a2c1bc6d74d98028",
    "filename": "pom.xml",
    "status": "modified",
    "additions": 15,
    "deletions": 6,
    "changes": 21,
    "blob_url": "https://github.com/statisticsnorway/dapla-spark-plugin/blob/a49999668af863f5b1c4935c58bd91747e1fb75a/pom.xml",
    "raw_url": "https://github.com/statisticsnorway/dapla-spark-plugin/raw/a49999668af863f5b1c4935c58bd91747e1fb75a/pom.xml",
    "contents_url": "https://api.github.com/repos/statisticsnorway/dapla-spark-plugin/contents/pom.xml?ref=a49999668af863f5b1c4935c58bd91747e1fb75a",
    "patch": "@@ -9,6 +9,8 @@\n \n     <properties>\n         <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n+        <maven.compiler.source>1.8</maven.compiler.source>\n+        <maven.compiler.target>1.8</maven.compiler.target>\n     </properties>\n \n     <scm>\n@@ -101,7 +103,7 @@\n         <dependency>\n             <groupId>no.ssb.dapla.catalog</groupId>\n             <artifactId>dapla-catalog-protobuf</artifactId>\n-            <version>0.10</version>\n+            <version>0.11-SNAPSHOT</version>\n             <scope>compile</scope>\n         </dependency>\n \n@@ -202,14 +204,14 @@\n         <dependency>\n             <groupId>org.apache.spark</groupId>\n             <artifactId>spark-avro_2.12</artifactId>\n-            <version>3.0.0</version>\n+            <version>3.0.1</version>\n             <scope>provided</scope>\n         </dependency>\n \n         <dependency>\n             <groupId>org.apache.spark</groupId>\n             <artifactId>spark-sql_2.12</artifactId>\n-            <version>3.0.0</version>\n+            <version>3.0.1</version>\n             <scope>provided</scope>\n             <exclusions>\n                 <exclusion>\n@@ -226,7 +228,7 @@\n         <dependency>\n             <groupId>org.apache.spark</groupId>\n             <artifactId>spark-core_2.12</artifactId>\n-            <version>3.0.0</version>\n+            <version>3.0.1</version>\n             <scope>provided</scope>\n             <exclusions>\n                 <exclusion>\n@@ -244,6 +246,13 @@\n             </exclusions>\n         </dependency>\n \n+        <dependency>\n+            <groupId>org.apache.iceberg</groupId>\n+            <artifactId>iceberg-spark3-runtime</artifactId>\n+            <version>0.11.0</version>\n+            <scope>provided</scope>\n+        </dependency>\n+\n         <dependency>\n             <groupId>org.apache.hadoop</groupId>\n             <artifactId>hadoop-common</artifactId>\n@@ -588,8 +597,8 @@\n                 <groupId>org.apache.maven.plugins</groupId>\n                 <artifactId>maven-compiler-plugin</artifactId>\n                 <configuration>\n-                    <source>8</source>\n-                    <target>8</target>\n+                    <source>9</source>\n+                    <target>9</target>\n                 </configuration>\n             </plugin>\n             <plugin>"
  },
  {
    "sha": "de6b13b20a9edda546714da4ed7e73377d8fe1ba",
    "filename": "src/main/java/no/ssb/dapla/service/CatalogClient.java",
    "status": "modified",
    "additions": 52,
    "deletions": 9,
    "changes": 61,
    "blob_url": "https://github.com/statisticsnorway/dapla-spark-plugin/blob/a49999668af863f5b1c4935c58bd91747e1fb75a/src/main/java/no/ssb/dapla/service/CatalogClient.java",
    "raw_url": "https://github.com/statisticsnorway/dapla-spark-plugin/raw/a49999668af863f5b1c4935c58bd91747e1fb75a/src/main/java/no/ssb/dapla/service/CatalogClient.java",
    "contents_url": "https://api.github.com/repos/statisticsnorway/dapla-spark-plugin/contents/src/main/java/no/ssb/dapla/service/CatalogClient.java?ref=a49999668af863f5b1c4935c58bd91747e1fb75a",
    "patch": "@@ -4,12 +4,15 @@\n import io.opentracing.contrib.okhttp3.TracingInterceptor;\n import io.opentracing.noop.NoopSpan;\n import io.opentracing.util.GlobalTracer;\n+import no.ssb.dapla.catalog.protobuf.CreateTableRequest;\n+import no.ssb.dapla.catalog.protobuf.GetTableRequest;\n+import no.ssb.dapla.catalog.protobuf.GetTableResponse;\n import no.ssb.dapla.catalog.protobuf.ListByPrefixRequest;\n import no.ssb.dapla.catalog.protobuf.ListByPrefixResponse;\n import no.ssb.dapla.catalog.protobuf.SignedDataset;\n+import no.ssb.dapla.catalog.protobuf.UpdateTableRequest;\n import no.ssb.dapla.spark.plugin.OAuth2Interceptor;\n-import no.ssb.dapla.spark.plugin.token.SparkConfStore;\n-import no.ssb.dapla.spark.plugin.token.TokenRefresher;\n+import no.ssb.dapla.spark.plugin.token.CustomAuthSupplier;\n import no.ssb.dapla.utils.ProtobufJsonUtils;\n import okhttp3.OkHttpClient;\n import okhttp3.Request;\n@@ -41,13 +44,7 @@ public CatalogClient(final SparkConf conf) {\n     public CatalogClient(final SparkConf conf, Span span) {\n         OkHttpClient.Builder builder = new OkHttpClient.Builder().callTimeout(10, TimeUnit.SECONDS);\n \n-        SparkConfStore store;\n-        if (conf != null) {\n-            store = new SparkConfStore(conf);\n-        } else {\n-            store = SparkConfStore.get();\n-        }\n-        builder.addInterceptor(new OAuth2Interceptor(new TokenRefresher(store)));\n+        builder.addInterceptor(new OAuth2Interceptor(new CustomAuthSupplier(conf)));\n         this.client = TracingInterceptor.addTracing(builder, GlobalTracer.get());\n         this.baseURL = conf.get(CONFIG_CATALOG_URL);\n         if (!this.baseURL.endsWith(\"/\")) {\n@@ -93,6 +90,52 @@ public void writeDataset(SignedDataset signedDataset) {\n         }\n     }\n \n+    public GetTableResponse getTable(GetTableRequest getTableRequest) throws NotFoundException {\n+        span.log(\"/catalog2/get \" + getTableRequest.getPath());\n+        Request request = new Request.Builder()\n+                .url(buildUrl(\"catalog2/get\"))\n+                .post(RequestBody.create(ProtobufJsonUtils.toString(getTableRequest), okhttp3.MediaType.get(MediaType.APPLICATION_JSON)))\n+                .build();\n+        try (Response response = client.newCall(request).execute()) {\n+            String json = getJson(response);\n+            handleErrorCodes(response, json);\n+            return ProtobufJsonUtils.toPojo(json, GetTableResponse.class);\n+        } catch (IOException e) {\n+            log.error(\"getTable failed\", e);\n+            throw new CatalogServiceException(e);\n+        }\n+    }\n+\n+    public void createTable(CreateTableRequest createTableRequest) {\n+        span.log(\"/catalog2/create \" + createTableRequest.getTable().getPath());\n+        Request request = new Request.Builder()\n+                .url(buildUrl(\"catalog2/create\"))\n+                .post(RequestBody.create(ProtobufJsonUtils.toString(createTableRequest), okhttp3.MediaType.get(MediaType.APPLICATION_JSON)))\n+                .build();\n+        try (Response response = client.newCall(request).execute()) {\n+            String json = getJson(response);\n+            handleErrorCodes(response, json);\n+        } catch (IOException e) {\n+            log.error(\"createTable failed\", e);\n+            throw new CatalogServiceException(e);\n+        }\n+    }\n+\n+    public void updateTable(UpdateTableRequest updateTableRequest) {\n+        span.log(\"/catalog2/update \" + updateTableRequest.getTable().getPath());\n+        Request request = new Request.Builder()\n+                .url(buildUrl(\"catalog2/update\"))\n+                .put(RequestBody.create(ProtobufJsonUtils.toString(updateTableRequest), okhttp3.MediaType.get(MediaType.APPLICATION_JSON)))\n+                .build();\n+        try (Response response = client.newCall(request).execute()) {\n+            String json = getJson(response);\n+            handleErrorCodes(response, json);\n+        } catch (IOException e) {\n+            log.error(\"updateTable failed\", e);\n+            throw new CatalogServiceException(e);\n+        }\n+    }\n+\n     private String getJson(Response response) throws IOException {\n         ResponseBody body = response.body();\n         if (body == null) return null;"
  },
  {
    "sha": "6a865a3e89660d1151cac6673f1ed1e9d4fde7aa",
    "filename": "src/main/java/no/ssb/dapla/service/DataAccessClient.java",
    "status": "modified",
    "additions": 21,
    "deletions": 9,
    "changes": 30,
    "blob_url": "https://github.com/statisticsnorway/dapla-spark-plugin/blob/a49999668af863f5b1c4935c58bd91747e1fb75a/src/main/java/no/ssb/dapla/service/DataAccessClient.java",
    "raw_url": "https://github.com/statisticsnorway/dapla-spark-plugin/raw/a49999668af863f5b1c4935c58bd91747e1fb75a/src/main/java/no/ssb/dapla/service/DataAccessClient.java",
    "contents_url": "https://api.github.com/repos/statisticsnorway/dapla-spark-plugin/contents/src/main/java/no/ssb/dapla/service/DataAccessClient.java?ref=a49999668af863f5b1c4935c58bd91747e1fb75a",
    "patch": "@@ -9,8 +9,7 @@\n import no.ssb.dapla.data.access.protobuf.WriteLocationRequest;\n import no.ssb.dapla.data.access.protobuf.WriteLocationResponse;\n import no.ssb.dapla.spark.plugin.OAuth2Interceptor;\n-import no.ssb.dapla.spark.plugin.token.SparkConfStore;\n-import no.ssb.dapla.spark.plugin.token.TokenRefresher;\n+import no.ssb.dapla.spark.plugin.token.CustomAuthSupplier;\n import no.ssb.dapla.utils.ProtobufJsonUtils;\n import okhttp3.OkHttpClient;\n import okhttp3.Request;\n@@ -41,13 +40,7 @@ public DataAccessClient(final SparkConf conf) {\n     public DataAccessClient(final SparkConf conf, Span span) {\n         okhttp3.OkHttpClient.Builder builder = new okhttp3.OkHttpClient.Builder();\n \n-        SparkConfStore store;\n-        if (conf != null) {\n-            store = new SparkConfStore(conf);\n-        } else {\n-            store = SparkConfStore.get();\n-        }\n-        builder.addInterceptor(new OAuth2Interceptor(new TokenRefresher(store)));\n+        builder.addInterceptor(new OAuth2Interceptor(new CustomAuthSupplier(conf)));\n \n         this.client = TracingInterceptor.addTracing(builder, GlobalTracer.get());\n         this.baseURL = conf.get(CONFIG_DATA_ACCESS_URL);\n@@ -80,6 +73,25 @@ public ReadLocationResponse readLocation(ReadLocationRequest readLocationRequest\n         }\n     }\n \n+    public ReadLocationResponse readLocation2(ReadLocationRequest readLocationRequest) {\n+        final String requestBody = ProtobufJsonUtils.toString(readLocationRequest);\n+        span.log(\"ReadLocationRequest\" + requestBody);\n+        Request request = new Request.Builder()\n+                .url(buildUrl(\"rpc/DataAccessService/readLocation2\"))\n+                .post(RequestBody.create(requestBody, okhttp3.MediaType.get(MediaType.APPLICATION_JSON)))\n+                .build();\n+\n+        try (Response response = client.newCall(request).execute()) {\n+            String json = getJson(response);\n+            handleErrorCodes(response, json);\n+            ReadLocationResponse readLocationResponse = ProtobufJsonUtils.toPojo(json, ReadLocationResponse.class);\n+            return readLocationResponse;\n+        } catch (IOException e) {\n+            log.error(\"readLocation failed\", e);\n+            throw new DataAccessServiceException(e);\n+        }\n+    }\n+\n     public WriteLocationResponse writeLocation(WriteLocationRequest writeLocationRequest) {\n         final String requestBody = ProtobufJsonUtils.toString(writeLocationRequest);\n         Request request = new Request.Builder()"
  },
  {
    "sha": "fce3d51e2c746e1f20a738cfe367765172f46e74",
    "filename": "src/main/java/no/ssb/dapla/service/MetadataPublisherClient.java",
    "status": "modified",
    "additions": 2,
    "deletions": 9,
    "changes": 11,
    "blob_url": "https://github.com/statisticsnorway/dapla-spark-plugin/blob/a49999668af863f5b1c4935c58bd91747e1fb75a/src/main/java/no/ssb/dapla/service/MetadataPublisherClient.java",
    "raw_url": "https://github.com/statisticsnorway/dapla-spark-plugin/raw/a49999668af863f5b1c4935c58bd91747e1fb75a/src/main/java/no/ssb/dapla/service/MetadataPublisherClient.java",
    "contents_url": "https://api.github.com/repos/statisticsnorway/dapla-spark-plugin/contents/src/main/java/no/ssb/dapla/service/MetadataPublisherClient.java?ref=a49999668af863f5b1c4935c58bd91747e1fb75a",
    "patch": "@@ -6,8 +6,7 @@\n import no.ssb.dapla.dataset.uri.DatasetUri;\n import no.ssb.dapla.metadata.distributor.protobuf.DataChangedRequest;\n import no.ssb.dapla.spark.plugin.OAuth2Interceptor;\n-import no.ssb.dapla.spark.plugin.token.SparkConfStore;\n-import no.ssb.dapla.spark.plugin.token.TokenRefresher;\n+import no.ssb.dapla.spark.plugin.token.CustomAuthSupplier;\n import no.ssb.dapla.utils.ProtobufJsonUtils;\n import okhttp3.OkHttpClient;\n import okhttp3.Request;\n@@ -37,13 +36,7 @@\n     public MetadataPublisherClient(final SparkConf conf, Span span) {\n         OkHttpClient.Builder builder = new OkHttpClient.Builder();\n \n-        SparkConfStore store;\n-        if (conf != null) {\n-            store = new SparkConfStore(conf);\n-        } else {\n-            store = SparkConfStore.get();\n-        }\n-        builder.addInterceptor(new OAuth2Interceptor(new TokenRefresher(store)));\n+        builder.addInterceptor(new OAuth2Interceptor(new CustomAuthSupplier(conf)));\n \n         this.client = TracingInterceptor.addTracing(builder, GlobalTracer.get());\n         String url = conf.get(CONFIG_METADATA_PUBLISHER_URL);"
  },
  {
    "sha": "c4d1c18c782055c679f8ff6d21b01999acc84795",
    "filename": "src/main/java/no/ssb/dapla/spark/catalog/CustomSparkCatalog.java",
    "status": "added",
    "additions": 143,
    "deletions": 0,
    "changes": 143,
    "blob_url": "https://github.com/statisticsnorway/dapla-spark-plugin/blob/a49999668af863f5b1c4935c58bd91747e1fb75a/src/main/java/no/ssb/dapla/spark/catalog/CustomSparkCatalog.java",
    "raw_url": "https://github.com/statisticsnorway/dapla-spark-plugin/raw/a49999668af863f5b1c4935c58bd91747e1fb75a/src/main/java/no/ssb/dapla/spark/catalog/CustomSparkCatalog.java",
    "contents_url": "https://api.github.com/repos/statisticsnorway/dapla-spark-plugin/contents/src/main/java/no/ssb/dapla/spark/catalog/CustomSparkCatalog.java?ref=a49999668af863f5b1c4935c58bd91747e1fb75a",
    "patch": "@@ -0,0 +1,143 @@\n+package no.ssb.dapla.spark.catalog;\n+\n+import org.apache.hadoop.conf.Configurable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.BaseMetastoreCatalog;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.SupportsNamespaces;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.exceptions.NamespaceNotEmptyException;\n+import org.apache.iceberg.exceptions.NoSuchNamespaceException;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.sql.SparkSession;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * SSB implementation of Iceberg Catalog.\n+ *\n+ * <p>\n+ * A note on namespaces: SSB namespaces are implicit and do not need to be explicitly created or deleted.\n+ * The create and delete namespace methods are no-ops for the CustomSparkCatalog. One can still list namespaces that have\n+ * objects stored in them to assist with namespace-centric catalog exploration.\n+ * </p>\n+ */\n+public class CustomSparkCatalog extends BaseMetastoreCatalog implements SupportsNamespaces, Configurable {\n+\n+    public static final String DEFAULT_CATALOG_NAME = \"ssb_catalog\";\n+\n+    // Hadoop config\n+    private Configuration config;\n+    private final SparkConf conf;\n+\n+    private static final Joiner SLASH = Joiner.on(\"/\");\n+\n+    public CustomSparkCatalog(SparkConf conf) {\n+        this.conf = conf;\n+    }\n+\n+    public CustomSparkCatalog() {\n+        this(SparkSession.active().sparkContext().getConf());\n+    }\n+\n+    @Override\n+    public void setConf(Configuration conf) {\n+        this.config = conf;\n+    }\n+\n+    @Override\n+    public Configuration getConf() {\n+        return this.config;\n+    }\n+\n+    @Override\n+    protected TableOperations newTableOps(TableIdentifier tableIdentifier) {\n+        // instantiate the CustomTableOperations\n+        return new CustomTableOperations(this.conf, this.config, tableIdentifier);\n+    }\n+\n+    @Override\n+    public Table createTable(TableIdentifier identifier, Schema schema, PartitionSpec spec, String location, Map<String, String> properties) {\n+        return super.createTable(identifier, schema, spec, location, properties);\n+    }\n+\n+    @Override\n+    protected String defaultWarehouseLocation(TableIdentifier tableIdentifier) {\n+        // Location is used for both metadata-file and data-file\n+        return TableCatalogIdentifier.fromTableIdentifier(tableIdentifier).toPath();\n+    }\n+\n+    @Override\n+    public String name() {\n+        return \"ssb-catalog\";\n+    }\n+\n+    @Override\n+    public List<TableIdentifier> listTables(Namespace namespace) {\n+        return null;\n+    }\n+\n+    @Override\n+    public boolean dropTable(TableIdentifier tableIdentifier, boolean b) {\n+        throw new UnsupportedOperationException(\"Drop table is not supported\");\n+    }\n+\n+    @Override\n+    public void renameTable(TableIdentifier tableIdentifier, TableIdentifier tableIdentifier1) {\n+        throw new UnsupportedOperationException(\"Rename table is not supported\");\n+    }\n+\n+    @Override\n+    public void createNamespace(Namespace namespace, Map<String, String> map) {\n+        // SSB namespaces are implicit and do not need to be explicitly created or deleted.\n+    }\n+\n+    @Override\n+    public List<Namespace> listNamespaces(Namespace namespace) throws NoSuchNamespaceException {\n+        String nsPath = namespace.isEmpty() ? \"/\" : SLASH.join(namespace.levels());\n+        try {\n+            // TODO: Call catalog listByPrefix\n+            return null;\n+        } catch (Exception ioe) {\n+            throw new RuntimeException(String.format(\"Failed to list namespace under: %s\", namespace), ioe);\n+        }\n+    }\n+\n+    @Override\n+    public Map<String, String> loadNamespaceMetadata(Namespace namespace) throws NoSuchNamespaceException {\n+        // Load metadata properties for a namespace.\n+        String nsPath = SLASH.join(namespace.levels());\n+        if (!namespace.isEmpty()) {\n+            // TODO: Lookup catalog\n+            return ImmutableMap.of(\"location\", nsPath);\n+        } else {\n+            throw new NoSuchNamespaceException(\"Namespace does not exist: %s\", new Object[]{namespace});\n+        }\n+    }\n+\n+    @Override\n+    public boolean dropNamespace(Namespace namespace) throws NamespaceNotEmptyException {\n+        // SSB namespaces are implicit and do not need to be explicitly created or deleted.\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean setProperties(Namespace namespace, Map<String, String> map) throws NoSuchNamespaceException {\n+        // Apply a set of metadata to a namespace in the catalog.\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean removeProperties(Namespace namespace, Set<String> set) throws NoSuchNamespaceException {\n+        // Remove a set of metadata from a namespace in the catalog.\n+        return false;\n+    }\n+}"
  },
  {
    "sha": "e8d7b8871582c1701a9a6490585e7096234bd773",
    "filename": "src/main/java/no/ssb/dapla/spark/catalog/CustomTableOperations.java",
    "status": "added",
    "additions": 83,
    "deletions": 0,
    "changes": 83,
    "blob_url": "https://github.com/statisticsnorway/dapla-spark-plugin/blob/a49999668af863f5b1c4935c58bd91747e1fb75a/src/main/java/no/ssb/dapla/spark/catalog/CustomTableOperations.java",
    "raw_url": "https://github.com/statisticsnorway/dapla-spark-plugin/raw/a49999668af863f5b1c4935c58bd91747e1fb75a/src/main/java/no/ssb/dapla/spark/catalog/CustomTableOperations.java",
    "contents_url": "https://api.github.com/repos/statisticsnorway/dapla-spark-plugin/contents/src/main/java/no/ssb/dapla/spark/catalog/CustomTableOperations.java?ref=a49999668af863f5b1c4935c58bd91747e1fb75a",
    "patch": "@@ -0,0 +1,83 @@\n+package no.ssb.dapla.spark.catalog;\n+\n+import no.ssb.dapla.catalog.protobuf.CatalogTable;\n+import no.ssb.dapla.catalog.protobuf.CreateTableRequest;\n+import no.ssb.dapla.catalog.protobuf.GetTableRequest;\n+import no.ssb.dapla.catalog.protobuf.GetTableResponse;\n+import no.ssb.dapla.catalog.protobuf.UpdateTableRequest;\n+import no.ssb.dapla.service.CatalogClient;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.spark.SparkConf;\n+\n+public class CustomTableOperations extends BaseMetastoreTableOperations {\n+    private final TableIdentifier tableIdentifier;\n+    private final Configuration conf;\n+    private final FileIO fileIO;\n+    private final CatalogClient catalogClient;\n+\n+    protected CustomTableOperations(SparkConf sparkConf, Configuration conf, TableIdentifier tableIdentifier) {\n+        this.conf = conf;\n+        this.conf.setBooleanIfUnset(\"fs.gs.impl.disable.cache\",\n+                this.conf.getBoolean(\"spark.hadoop.fs.gs.impl.disable.cache\", false));\n+        //this.fileIO = new CustomFileIO(this.conf);\n+        this.fileIO = new HadoopFileIO(this.conf);\n+        this.tableIdentifier = tableIdentifier;\n+        this.catalogClient = new CatalogClient(sparkConf);\n+    }\n+\n+    // The doRefresh method should provide implementation on how to get the metadata location\n+    @Override\n+    public void doRefresh() {\n+        try {\n+            GetTableResponse table = catalogClient.getTable(GetTableRequest.newBuilder().setPath(getPath()).build());\n+            // When updating from a metadata file location, call the helper method\n+            refreshFromMetadataLocation(table.getTable().getMetadataLocation());\n+        } catch (CatalogClient.NotFoundException e) {\n+            refreshFromMetadataLocation(null);\n+        }\n+\n+    }\n+\n+    // The doCommit method should provide implementation on how to update with metadata location atomically\n+    @Override\n+    public void doCommit(TableMetadata base, TableMetadata metadata) {\n+        // Write new metadata using helper method\n+        String newMetadataLocation = writeNewMetadata(metadata, currentVersion() + 1);\n+        if (base == null) {\n+            catalogClient.createTable(CreateTableRequest.newBuilder().build().newBuilder().setTable(\n+                    CatalogTable.newBuilder()\n+                            .setPath(getPath())\n+                            .setMetadataLocation(newMetadataLocation).build()\n+                    ).build()\n+            );\n+        } else {\n+            catalogClient.updateTable(UpdateTableRequest.newBuilder().setTable(\n+                    CatalogTable.newBuilder()\n+                            .setPath(getPath())\n+                            .setMetadataLocation(newMetadataLocation).build()\n+                    ).setPreviousMetadataLocation(base.metadataFileLocation()).build()\n+            );\n+        }\n+    }\n+\n+    private String getPath() {\n+        return TableCatalogIdentifier.fromTableIdentifier(tableIdentifier).toPath();\n+    }\n+\n+    // The io method provides a FileIO which is used to read and write the table metadata files\n+    @Override\n+    public FileIO io() {\n+        return fileIO;\n+    }\n+\n+    @Override\n+    public String metadataFileLocation(String filename) {\n+        return super.metadataFileLocation(filename);\n+    }\n+\n+}"
  },
  {
    "sha": "5a5fd4794e41d7cd0032f8764e8f2d07f6ac3968",
    "filename": "src/main/java/no/ssb/dapla/spark/catalog/TableCatalogIdentifier.java",
    "status": "added",
    "additions": 74,
    "deletions": 0,
    "changes": 74,
    "blob_url": "https://github.com/statisticsnorway/dapla-spark-plugin/blob/a49999668af863f5b1c4935c58bd91747e1fb75a/src/main/java/no/ssb/dapla/spark/catalog/TableCatalogIdentifier.java",
    "raw_url": "https://github.com/statisticsnorway/dapla-spark-plugin/raw/a49999668af863f5b1c4935c58bd91747e1fb75a/src/main/java/no/ssb/dapla/spark/catalog/TableCatalogIdentifier.java",
    "contents_url": "https://api.github.com/repos/statisticsnorway/dapla-spark-plugin/contents/src/main/java/no/ssb/dapla/spark/catalog/TableCatalogIdentifier.java?ref=a49999668af863f5b1c4935c58bd91747e1fb75a",
    "patch": "@@ -0,0 +1,74 @@\n+package no.ssb.dapla.spark.catalog;\n+\n+import com.google.common.collect.Streams;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.jetbrains.annotations.NotNull;\n+\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+public class TableCatalogIdentifier {\n+\n+    private final TableIdentifier identifier;\n+    private static final Pattern VALID_CHARS = Pattern.compile(\"([^\\\\w]|_)+\");\n+    private static final Pattern CODEPOINT = Pattern.compile(\"![0-9]{4}\");\n+    private static final Pattern INVALID_CATALOG_CHARS = Pattern.compile(\"\\\\.\");\n+\n+    private TableCatalogIdentifier(TableIdentifier identifier) {\n+        this.identifier = identifier;\n+    }\n+\n+    public static TableCatalogIdentifier fromTableIdentifier(TableIdentifier identifier) {\n+        return new TableCatalogIdentifier(identifier);\n+    }\n+    /**\n+     * Convert from path based identifier to catalog identifiers\n+     */\n+    public static TableCatalogIdentifier fromPath(String path) {\n+        if (path.startsWith(\"/\")) {\n+            path = path.substring(1);\n+        }\n+        String[] escaped = escape(path.split(\"/\")).toArray(String[]::new);\n+        return new TableCatalogIdentifier(TableIdentifier.of(escaped));\n+    }\n+\n+    public String toCatalogPath(String catalogName) {\n+        return Joiner.on(\".\").join(catalogName, unescape().map(part ->\n+                INVALID_CATALOG_CHARS.matcher(part).find() ? \"'\" + part  + \"'\" : part\n+        ).collect(Collectors.joining(\".\")));\n+    }\n+\n+    String toPath() {\n+        String path = unescape().collect(Collectors.joining(\"/\"));\n+        if (!path.startsWith(\"/\")) {\n+            path = \"/\" + path;\n+        }\n+        return path;\n+    }\n+\n+    @NotNull\n+    private static Stream<String> escape(String... values) {\n+        return Stream.of(values)\n+                .map(part -> VALID_CHARS.matcher(part).replaceAll(match -> match.group().codePoints()\n+                        .mapToObj(codePoint -> String.format(\"!%04d\", codePoint))\n+                        .collect(Collectors.joining())));\n+    }\n+\n+    @NotNull\n+    private Stream<String> unescape() {\n+        return Streams.concat(Stream.of(identifier.namespace().levels()), Stream.of(identifier.name()))\n+                .map(part -> CODEPOINT.matcher(part).replaceAll(match -> {\n+                    int point = Integer.parseInt(match.group().substring(1, 5));\n+                    return String.valueOf(Character.toChars(point));\n+                }));\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return \"TableCatalogIdentifier{\" +\n+                \"identifier=\" + identifier +\n+                '}';\n+    }\n+}"
  },
  {
    "sha": "ba50446667843b5e2e26682f4e03732f97224608",
    "filename": "src/main/java/no/ssb/dapla/spark/plugin/token/CustomAuthSupplier.java",
    "status": "added",
    "additions": 79,
    "deletions": 0,
    "changes": 79,
    "blob_url": "https://github.com/statisticsnorway/dapla-spark-plugin/blob/a49999668af863f5b1c4935c58bd91747e1fb75a/src/main/java/no/ssb/dapla/spark/plugin/token/CustomAuthSupplier.java",
    "raw_url": "https://github.com/statisticsnorway/dapla-spark-plugin/raw/a49999668af863f5b1c4935c58bd91747e1fb75a/src/main/java/no/ssb/dapla/spark/plugin/token/CustomAuthSupplier.java",
    "contents_url": "https://api.github.com/repos/statisticsnorway/dapla-spark-plugin/contents/src/main/java/no/ssb/dapla/spark/plugin/token/CustomAuthSupplier.java?ref=a49999668af863f5b1c4935c58bd91747e1fb75a",
    "patch": "@@ -0,0 +1,79 @@\n+package no.ssb.dapla.spark.plugin.token;\n+\n+import com.auth0.jwt.JWT;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import okhttp3.OkHttpClient;\n+import okhttp3.Request;\n+import okhttp3.Response;\n+import okhttp3.ResponseBody;\n+import org.apache.spark.SparkConf;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.time.temporal.ChronoUnit;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static no.ssb.dapla.spark.plugin.DaplaSparkConfig.*;\n+\n+public class CustomAuthSupplier implements TokenSupplier {\n+\n+    private static final Integer TOKEN_EXPIRY_BUFFER_SECS = 10;\n+    private static final AtomicReference<String> TOKEN = new AtomicReference<>();\n+    private static final ObjectMapper MAPPER = new ObjectMapper();\n+    private static final Logger LOG = LoggerFactory.getLogger(CustomAuthSupplier.class);\n+\n+    public CustomAuthSupplier(final SparkConf conf) {\n+        TOKEN.compareAndSet(null, conf.get(SPARK_SSB_ACCESS_TOKEN, null));\n+        if (shouldRefresh()) {\n+            refreshToken();\n+        }\n+    }\n+\n+    private boolean shouldRefresh() {\n+        if (TOKEN.get() == null) return true;\n+\n+        Instant expiresAt = JWT.decode(TOKEN.get()).getExpiresAt().toInstant();\n+        Duration timeBeforeExpiration = Duration.between(Instant.now(), expiresAt);\n+\n+        // Account for network delays etc.\n+        return timeBeforeExpiration.minus(TOKEN_EXPIRY_BUFFER_SECS, ChronoUnit.SECONDS).isNegative();\n+    }\n+\n+    private void refreshToken() {\n+        LOG.info(\"Refresh user token from \" + System.getenv(\"JUPYTERHUB_HANDLER_CUSTOM_AUTH_URL\"));\n+        try {\n+            Request request = new Request.Builder()\n+                    .url(System.getenv(\"JUPYTERHUB_HANDLER_CUSTOM_AUTH_URL\"))\n+                    .addHeader(\"Authorization\", String.format(\"token %s\", System.getenv(\"JUPYTERHUB_API_TOKEN\"))).get()\n+                    .build();\n+\n+            OkHttpClient client = new OkHttpClient().newBuilder()\n+                    .followRedirects(false)\n+                    .followSslRedirects(false)\n+                    .build();\n+            try (Response response = client.newCall(request).execute()) {\n+                if (!response.isSuccessful()) {\n+                    throw new IOException(\"authentication failed \" + response);\n+                }\n+                ResponseBody body = response.body();\n+                if (body == null) {\n+                    throw new IOException(\"empty response\");\n+                }\n+                JsonNode bodyContent = MAPPER.readTree(body.bytes());\n+                TOKEN.set(bodyContent.get(\"access_token\").asText(\"access_token\"));\n+            }\n+        } catch (IOException ioe) {\n+            LOG.error(\"Error in refreshToken\", ioe);\n+            throw new RuntimeException(ioe);\n+        }\n+    }\n+\n+    @Override\n+    public String get() {\n+        return TOKEN.get();\n+    }\n+}"
  },
  {
    "sha": "549457cee5a25a32c89acaed2ca25ed6b071d366",
    "filename": "src/main/java/no/ssb/dapla/spark/sql/TableReader.java",
    "status": "added",
    "additions": 46,
    "deletions": 0,
    "changes": 46,
    "blob_url": "https://github.com/statisticsnorway/dapla-spark-plugin/blob/a49999668af863f5b1c4935c58bd91747e1fb75a/src/main/java/no/ssb/dapla/spark/sql/TableReader.java",
    "raw_url": "https://github.com/statisticsnorway/dapla-spark-plugin/raw/a49999668af863f5b1c4935c58bd91747e1fb75a/src/main/java/no/ssb/dapla/spark/sql/TableReader.java",
    "contents_url": "https://api.github.com/repos/statisticsnorway/dapla-spark-plugin/contents/src/main/java/no/ssb/dapla/spark/sql/TableReader.java?ref=a49999668af863f5b1c4935c58bd91747e1fb75a",
    "patch": "@@ -0,0 +1,46 @@\n+package no.ssb.dapla.spark.sql;\n+\n+import no.ssb.dapla.data.access.protobuf.ReadLocationRequest;\n+import no.ssb.dapla.data.access.protobuf.ReadLocationResponse;\n+import no.ssb.dapla.service.DataAccessClient;\n+import no.ssb.dapla.spark.catalog.CustomSparkCatalog;\n+import no.ssb.dapla.spark.catalog.TableCatalogIdentifier;\n+import no.ssb.dapla.spark.plugin.SparkOptions;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+\n+public class TableReader {\n+\n+    private final SparkSession session;\n+\n+    private TableReader(SparkSession session) {\n+        this.session = session;\n+    }\n+\n+    public static TableReader forSession(SparkSession session) {\n+        return new TableReader(session);\n+    }\n+\n+    public Dataset<Row> readFrom(String path) {\n+        ReadLocationResponse readLocation = getReadLocation(session.sparkContext().conf(), path);\n+        if (!readLocation.getAccessAllowed()) {\n+            throw new RuntimeException(\"Permission denied\");\n+        }\n+        SparkSession.getActiveSession().get().conf().set(SparkOptions.ACCESS_TOKEN, readLocation.getAccessToken());\n+        SparkSession.getActiveSession().get().conf().set(SparkOptions.ACCESS_TOKEN_EXP, String.valueOf(readLocation.getExpirationTime()));\n+\n+        TableCatalogIdentifier identifier = TableCatalogIdentifier.fromPath(path);\n+        return session.sqlContext().table(identifier.toCatalogPath(CustomSparkCatalog.DEFAULT_CATALOG_NAME));\n+    }\n+\n+    private static ReadLocationResponse getReadLocation(SparkConf sparkConf, String path) {\n+        DataAccessClient dataAccessClient = new DataAccessClient(sparkConf);\n+\n+        return dataAccessClient.readLocation2(ReadLocationRequest.newBuilder()\n+                .setPath(path)\n+                .setSnapshot(0) // 0 means latest\n+                .build());\n+    }\n+}"
  },
  {
    "sha": "16247222119d6b93374b2ef715c67aab6862740d",
    "filename": "src/main/java/no/ssb/dapla/spark/sql/TableWriter.java",
    "status": "added",
    "additions": 95,
    "deletions": 0,
    "changes": 95,
    "blob_url": "https://github.com/statisticsnorway/dapla-spark-plugin/blob/a49999668af863f5b1c4935c58bd91747e1fb75a/src/main/java/no/ssb/dapla/spark/sql/TableWriter.java",
    "raw_url": "https://github.com/statisticsnorway/dapla-spark-plugin/raw/a49999668af863f5b1c4935c58bd91747e1fb75a/src/main/java/no/ssb/dapla/spark/sql/TableWriter.java",
    "contents_url": "https://api.github.com/repos/statisticsnorway/dapla-spark-plugin/contents/src/main/java/no/ssb/dapla/spark/sql/TableWriter.java?ref=a49999668af863f5b1c4935c58bd91747e1fb75a",
    "patch": "@@ -0,0 +1,95 @@\n+package no.ssb.dapla.spark.sql;\n+\n+import no.ssb.dapla.data.access.protobuf.WriteLocationRequest;\n+import no.ssb.dapla.data.access.protobuf.WriteLocationResponse;\n+import no.ssb.dapla.dataset.api.DatasetId;\n+import no.ssb.dapla.dataset.api.DatasetMeta;\n+import no.ssb.dapla.dataset.api.DatasetState;\n+import no.ssb.dapla.dataset.api.Type;\n+import no.ssb.dapla.dataset.api.Valuation;\n+import no.ssb.dapla.service.DataAccessClient;\n+import no.ssb.dapla.spark.catalog.TableCatalogIdentifier;\n+import no.ssb.dapla.spark.plugin.SparkOptions;\n+import no.ssb.dapla.utils.ProtobufJsonUtils;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.sql.CreateTableWriter;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class TableWriter {\n+\n+    private static final String METADATA_FOLDER = \".metadata\";\n+    private final Dataset<Row> dataset;\n+    private final Map<String, String> props = new HashMap<>();\n+\n+    private TableWriter(Dataset<Row> dataset) {\n+        this.dataset = dataset;\n+    }\n+\n+    public static TableWriter forDataset(Dataset<Row> dataset) {\n+        return new TableWriter(dataset);\n+    }\n+\n+    public TableWriter property(String key, String value) {\n+        this.props.put(key, value);\n+        return this;\n+    }\n+\n+    //TODO: Remove after demo\n+    public String[] getPaths(String path) {\n+        Valuation valuation = Valuation.valueOf(props.get(\"valuation\"));\n+        DatasetState state = DatasetState.valueOf(props.get(\"state\"));\n+\n+        WriteLocationResponse writeLocation = getWriteLocation(dataset.sparkSession().sparkContext().conf(), path, valuation, state);\n+        if (!writeLocation.getAccessAllowed()) {\n+            throw new RuntimeException(\"Permission denied\");\n+        }\n+        String dataPath = String.format(\"%s%s\", writeLocation.getParentUri(), path);\n+        String metadataPath = String.format(\"%s%s/%s\", writeLocation.getParentUri(), path, METADATA_FOLDER);\n+        SparkSession.getActiveSession().get().conf().set(SparkOptions.ACCESS_TOKEN, writeLocation.getAccessToken());\n+        SparkSession.getActiveSession().get().conf().set(SparkOptions.ACCESS_TOKEN_EXP, String.valueOf(writeLocation.getExpirationTime()));\n+        return new String[] {metadataPath, dataPath};\n+    }\n+\n+    public CreateTableWriter<Row> writeTo(String path) {\n+        TableCatalogIdentifier identifier = TableCatalogIdentifier.fromPath(path);\n+        Valuation valuation = Valuation.valueOf(props.get(\"valuation\"));\n+        DatasetState state = DatasetState.valueOf(props.get(\"state\"));\n+\n+        WriteLocationResponse writeLocation = getWriteLocation(dataset.sparkSession().sparkContext().conf(), path, valuation, state);\n+        if (!writeLocation.getAccessAllowed()) {\n+            throw new RuntimeException(\"Permission denied\");\n+        }\n+        String dataPath = String.format(\"%s%s\", writeLocation.getParentUri(), path);\n+        String metadataPath = String.format(\"%s%s/%s\", writeLocation.getParentUri(), path, METADATA_FOLDER);\n+        SparkSession.getActiveSession().get().conf().set(SparkOptions.ACCESS_TOKEN, writeLocation.getAccessToken());\n+        SparkSession.getActiveSession().get().conf().set(SparkOptions.ACCESS_TOKEN_EXP, String.valueOf(writeLocation.getExpirationTime()));\n+\n+        return dataset.writeTo(identifier.toCatalogPath(\"ssb_catalog\"))\n+                .using(\"iceberg\")\n+                .tableProperty(\"valuation\", valuation.name())\n+                .tableProperty(\"state\", state.name())\n+                .tableProperty(\"write.folder-storage.path\", dataPath)\n+                .tableProperty(\"write.metadata.path\", metadataPath);\n+    }\n+\n+    private static WriteLocationResponse getWriteLocation(SparkConf sparkConf, String path, Valuation valuation, DatasetState state) {\n+        DataAccessClient dataAccessClient = new DataAccessClient(sparkConf);\n+\n+        return dataAccessClient.writeLocation(WriteLocationRequest.newBuilder()\n+                .setMetadataJson(ProtobufJsonUtils.toString(DatasetMeta.newBuilder()\n+                        .setId(DatasetId.newBuilder()\n+                                .setPath(path)\n+                                .setVersion(\"0\")\n+                                .build())\n+                        .setType(Type.BOUNDED)\n+                        .setValuation(valuation)\n+                        .setState(state)\n+                        .build()))\n+                .build());\n+    }\n+}"
  },
  {
    "sha": "9560b17f0d6c2a6f8aea2c639ab9eafc4ce0d54f",
    "filename": "src/test/java/no/ssb/dapla/spark/catalog/CustomSparkCatalogTest.java",
    "status": "added",
    "additions": 324,
    "deletions": 0,
    "changes": 324,
    "blob_url": "https://github.com/statisticsnorway/dapla-spark-plugin/blob/a49999668af863f5b1c4935c58bd91747e1fb75a/src/test/java/no/ssb/dapla/spark/catalog/CustomSparkCatalogTest.java",
    "raw_url": "https://github.com/statisticsnorway/dapla-spark-plugin/raw/a49999668af863f5b1c4935c58bd91747e1fb75a/src/test/java/no/ssb/dapla/spark/catalog/CustomSparkCatalogTest.java",
    "contents_url": "https://api.github.com/repos/statisticsnorway/dapla-spark-plugin/contents/src/test/java/no/ssb/dapla/spark/catalog/CustomSparkCatalogTest.java?ref=a49999668af863f5b1c4935c58bd91747e1fb75a",
    "patch": "@@ -0,0 +1,324 @@\n+package no.ssb.dapla.spark.catalog;\n+\n+import com.auth0.jwt.JWT;\n+import com.auth0.jwt.algorithms.Algorithm;\n+import com.google.api.services.storage.StorageScopes;\n+import com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem;\n+import no.ssb.dapla.spark.sql.TableReader;\n+import no.ssb.dapla.spark.sql.TableWriter;\n+import com.google.common.collect.ImmutableMap;\n+import no.ssb.dapla.gcs.connector.GoogleHadoopFileSystemExt;\n+import no.ssb.dapla.gcs.oauth.GoogleCredentialsDetails;\n+import no.ssb.dapla.gcs.oauth.GoogleCredentialsFactory;\n+import no.ssb.dapla.gcs.token.SparkAccessTokenProvider;\n+import no.ssb.dapla.service.CatalogClient;\n+import no.ssb.dapla.service.DataAccessClient;\n+import no.ssb.dapla.spark.plugin.GsimDatasourceGCSTest;\n+import no.ssb.dapla.spark.plugin.GsimDatasourceLocalFSTest;\n+import no.ssb.dapla.spark.plugin.SparkOptions;\n+import okhttp3.HttpUrl;\n+import okhttp3.mockwebserver.MockResponse;\n+import okhttp3.mockwebserver.MockWebServer;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Transaction;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalog.Table;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.time.Instant;\n+import java.util.Date;\n+import java.util.Map;\n+\n+import static java.time.temporal.ChronoUnit.*;\n+import static org.assertj.core.api.Assertions.*;\n+import static org.hamcrest.core.IsNull.*;\n+import static org.junit.Assume.*;\n+\n+@Ignore\n+public class CustomSparkCatalogTest {\n+\n+    private MockWebServer dataAccessMockServer;\n+    private MockWebServer catalogMockServer;\n+    private SparkSession session;\n+    private GoogleCredentialsDetails credentials;\n+\n+    @Before\n+    public void setUp() throws IOException {\n+        this.catalogMockServer = new MockWebServer();\n+        this.catalogMockServer.start();\n+        HttpUrl catalogUrl = catalogMockServer.url(\"/catalog/\");\n+\n+        this.dataAccessMockServer = new MockWebServer();\n+        this.dataAccessMockServer.start();\n+        HttpUrl dataAccessUrl = dataAccessMockServer.url(\"/data-access/\");\n+\n+        this.credentials = GoogleCredentialsFactory.createCredentialsDetails(true,\n+                StorageScopes.DEVSTORAGE_READ_WRITE);\n+        // Read the unit dataset json example.\n+        session = SparkSession.builder()\n+                .appName(GsimDatasourceLocalFSTest.class.getSimpleName())\n+                .master(\"local\")\n+                .config(\"spark.ui.enabled\", false)\n+                //.config(CatalogClient.CONFIG_CATALOG_URL, catalogUrl.toString())\n+                .config(CatalogClient.CONFIG_CATALOG_URL, \"http://localhost:20110\")\n+                //.config(DataAccessClient.CONFIG_DATA_ACCESS_URL, dataAccessUrl.toString())\n+                .config(DataAccessClient.CONFIG_DATA_ACCESS_URL, \"http://localhost:20140\")\n+                .config(\"spark.hadoop.fs.gs.auth.access.token.provider.impl\", SparkAccessTokenProvider.class.getCanonicalName())\n+                .config(\"spark.hadoop.fs.gs.impl\", GoogleHadoopFileSystemExt.class.getCanonicalName())\n+                .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n+                //.config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\n+                //.config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\")\n+                .config(\"spark.sql.catalog.ssb_catalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n+                .config(\"spark.sql.catalog.ssb_catalog.catalog-impl\", CustomSparkCatalog.class.getCanonicalName())\n+                //.config(\"spark.sql.catalog.ssb_catalog.type\", \"hadoop\")\n+                //.config(\"spark.sql.catalog.ssb_catalog.uri\", \"thrift://localhost:9083\")\n+                //.config(\"iceberg.engine.hive.enabled\", \"true\")\n+                //.config(\"spark.sql.warehouse.dir\", \"/tmp/hive-warehouse\")\n+                //.config(\"hive.metastore.uris\", \"thrift://localhost:9083\")\n+                //.config(\"hive.metastore.warehouse.dir\", \"/tmp/hive-warehouse\")\n+                //.config(\"spark.sql.catalog.ssb_catalog.warehouse\", \"/tmp/spark-warehouse\")\n+                //.config(\"spark.sql.catalog.ssb_catalog.warehouse\", \"gs://ssb-dev-md-test/spark-warehouse\")\n+                //.config(\"spark.sql.catalog.ssb_catalog.io-impl\", CustomFileIO.class.getCanonicalName())\n+\n+                //.config(SparkOptions.ACCESS_TOKEN, credentials.getAccessToken())\n+                //.config(SparkOptions.ACCESS_TOKEN_EXP, credentials.getExpirationTime())\n+                //.enableHiveSupport()\n+                .config(\"spark.ssb.access\", JWT.create()\n+                        .withClaim(\"preferred_username\", \"kim.gaarder\")\n+                        .withExpiresAt(Date.from(Instant.now().plus(1, HOURS)))\n+                        .sign(Algorithm.HMAC256(\"secret\")))\n+                .getOrCreate();\n+    }\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        session.stop();\n+    }\n+\n+    static String READ_LOCATION_RESPONSE = \"{\\\"accessAllowed\\\": \\\"true\\\", \\\"accessToken\\\": \\\"myToken\\\", \\\"expirationTime\\\": \\\"1580828806046\\\", \\\"parentUri\\\": \\\"///tmp/spark-datastore\\\", \\\"version\\\": \\\"1580828806046\\\"}\";\n+    static String WRITE_LOCATION_RESPONSE = \"{\\\"accessAllowed\\\": \\\"true\\\", \\\"accessToken\\\": \\\"myToken\\\", \\\"expirationTime\\\": \\\"1580828806046\\\", \\\"parentUri\\\": \\\"///tmp/spark-datastore\\\"}\";\n+\n+    @Test\n+    public void testCreateCatalog() {\n+        dataAccessMockServer.enqueue(new MockResponse()\n+                .setBody(WRITE_LOCATION_RESPONSE)\n+                .setResponseCode(200));\n+        session.sql(\"CREATE TABLE ssb_catalog.felles.test.ssb_table (id bigint COMMENT 'unique id', data string) \\n\" +\n+                \"USING iceberg \\n\" +\n+                \"COMMENT 'Funny table'\\n\" +\n+                \"TBLPROPERTIES ('valuation' = 'INTERNAL', 'state' = 'INPUT') \\n\" +\n+                \"LOCATION '/tmp/spark-runtime/felles/test/ssb_table'\");\n+        /*\n+                \"  'write.object-storage.path'='no.ssb.dapla.spark.catalog.CustomLocationProvider'\\n\" +\n+                \"  'write.object-storage.enabled'='no.ssb.dapla.spark.catalog.CustomLocationProvider'\\n\" +\n+         */\n+        dataAccessMockServer.enqueue(new MockResponse()\n+                .setBody(WRITE_LOCATION_RESPONSE)\n+                .setResponseCode(200));\n+        dataAccessMockServer.enqueue(new MockResponse()\n+                .setBody(WRITE_LOCATION_RESPONSE)\n+                .setResponseCode(200));\n+        session.sql(\"INSERT INTO ssb_catalog.felles.test.ssb_table VALUES (1, 'a'), (2, 'b')\");\n+    }\n+\n+    @Test\n+    public void testHive() throws IOException {\n+        //session.sql(\"CREATE DATABASE IF NOT EXISTS felles LOCATION '/tmp/hive-metastore/felles.db'\");\n+        //session.sql(\"CREATE EXTERNAL TABLE IF NOT EXISTS spark_tests.s3_table_1 (key INT, value STRING) STORED AS PARQUET LOCATION 's3a://spark/s3_table_1'\");\n+\n+        File tempDirectory = Files.createTempDirectory(\"lds-gsim-spark\").toFile();\n+        InputStream parquetContent = GsimDatasourceGCSTest.class.getResourceAsStream(\"data/dataset.parquet\");\n+        Path parquetFile = tempDirectory.toPath().resolve(\"dataset.parquet\");\n+        Files.copy(parquetContent, parquetFile);\n+\n+        Dataset<Row> dataset = session.sqlContext().read()\n+                .load(parquetFile.toString());\n+\n+        dataset.write()\n+                .format(\"iceberg\")\n+                .mode(SaveMode.Overwrite)\n+                .option(\"valuation\", \"INTERNAL\")\n+                .option(\"state\", \"INPUT\")\n+                //.option(\"path\", \"gs://ssb-dev-md-test/spark-warehouse/felles/test/roblix\")\n+                .option(\"path\", \"/tmp/where/do/you/go\")\n+                .saveAsTable(\"ssb_catalog.felles.TestTable\");\n+    }\n+\n+    @Test\n+    public void testSaveAsTableLikePython() throws IOException {\n+        File tempDirectory = Files.createTempDirectory(\"lds-gsim-spark\").toFile();\n+        InputStream parquetContent = GsimDatasourceGCSTest.class.getResourceAsStream(\"data/dataset.parquet\");\n+        Path parquetFile = tempDirectory.toPath().resolve(\"dataset.parquet\");\n+        Files.copy(parquetContent, parquetFile);\n+\n+        Dataset<Row> dataset = session.sqlContext().read()\n+                .load(parquetFile.toString());\n+\n+        // Write path for Spark 3.0 (df.writeTo) is not yet available in Pyspark\n+        // Thanks to: https://projectnessie.org/tools/spark/#spark3\n+        // first instantiate the catalog\n+        HadoopCatalog catalog = new HadoopCatalog();\n+        catalog.setConf(session.sparkContext().hadoopConfiguration());\n+        catalog.initialize(\"python-catalog\", ImmutableMap.of(\"warehouse\", \"/tmp/spark-warehouse\"));\n+\n+        // Creating table by first creating a table name with namespace\n+        TableIdentifier region_name = TableIdentifier.parse(\"testing.region\");\n+\n+        // next create the schema\n+        Schema schema = SparkSchemaUtil.convert(dataset.schema());\n+\n+        // and the partition\n+        PartitionSpec region_spec = PartitionSpec.unpartitioned();\n+\n+        // finally create or replace the table\n+        Transaction transaction = catalog.newReplaceTableTransaction(region_name, schema, region_spec, ImmutableMap.of(\"valuation\", \"INTERNAL\", \"state\", \"INPUT\",\n+                \"write.folder-storage.path\", \"gs://ssb-dev-md-test/spark-warehouse/felles/testing/region\"), true);\n+        transaction.commitTransaction();\n+\n+        session.sqlContext().conf().setConfString(SparkOptions.ACCESS_TOKEN, credentials.getAccessToken());\n+        session.sqlContext().conf().setConfString(SparkOptions.ACCESS_TOKEN_EXP, Long.toString(credentials.getExpirationTime()));\n+        dataset.write().format(\"iceberg\").mode(\"overwrite\").save(\"ssb_catalog.testing.region\");\n+\n+    }\n+\n+    @Test\n+    public void testSaveAsTableLikePythonCustomImpl() throws IOException {\n+        File tempDirectory = Files.createTempDirectory(\"lds-gsim-spark\").toFile();\n+        InputStream parquetContent = GsimDatasourceGCSTest.class.getResourceAsStream(\"data/dataset.parquet\");\n+        Path parquetFile = tempDirectory.toPath().resolve(\"dataset.parquet\");\n+        Files.copy(parquetContent, parquetFile);\n+\n+        Dataset<Row> dataset = session.sqlContext().read()\n+                .load(parquetFile.toString());\n+\n+        // Write path for Spark 3.0 (df.writeTo) is not yet available in Pyspark\n+        // Thanks to: https://projectnessie.org/tools/spark/#spark3\n+        // first instantiate the catalog\n+        CustomSparkCatalog catalog = new CustomSparkCatalog(session.sparkContext().getConf());\n+        catalog.setConf(session.sparkContext().hadoopConfiguration());\n+        catalog.initialize(\"python-catalog\", ImmutableMap.of(\"warehouse\", \"/tmp/spark-warehouse\"));\n+\n+        // Creating table by first creating a table name with namespace\n+        TableIdentifier region_name = TableIdentifier.parse(\"testing.region\");\n+\n+        if (!catalog.tableExists(region_name)) {\n+            // next create the schema\n+            Schema schema = SparkSchemaUtil.convert(dataset.schema());\n+\n+            // and the partition\n+            PartitionSpec region_spec = PartitionSpec.unpartitioned();\n+            catalog.createTable(region_name, schema, region_spec, ImmutableMap.of(\"valuation\", \"INTERNAL\", \"state\", \"INPUT\",\n+                    \"write.folder-storage.path\", \"gs://ssb-dev-md-test/spark-warehouse/felles/testing/region\"));\n+        }\n+\n+        dataset.write().format(\"iceberg\").mode(\"append\").save(\"ssb_catalog.testing.region\");\n+        session.sqlContext().read().format(\"iceberg\").table(\"ssb_catalog.testing.region\").show(false);\n+        session.sqlContext().table(\"ssb_catalog/testing/region\").show(false);\n+    }\n+\n+    @Test\n+    public void testSaveAsTable() throws IOException {\n+        File tempDirectory = Files.createTempDirectory(\"lds-gsim-spark\").toFile();\n+        InputStream parquetContent = GsimDatasourceGCSTest.class.getResourceAsStream(\"data/dataset.parquet\");\n+        Path parquetFile = tempDirectory.toPath().resolve(\"dataset.parquet\");\n+        Files.copy(parquetContent, parquetFile);\n+\n+        Dataset<Row> dataset = session.sqlContext().read()\n+                .load(parquetFile.toString());\n+\n+        /*\n+        dataset.write()\n+                .format(\"iceberg\")\n+                .mode(SaveMode.Overwrite)\n+                .option(\"valuation\", \"INTERNAL\")\n+                .option(\"state\", \"INPUT\")\n+                .option(\"path\", \"gs://ssb-dev-md-test/spark-warehouse/felles/test/roblix\")\n+                .saveAsTable(\"ssb_catalog.default.roblix\");\n+         */\n+\n+        // DataFrameWriterV2 kan bruke tableProperty for  skrive til en annen path enn warehouse path\n+        /*\n+        dataset.select(\"PERSON_ID\", \"MUNICIPALITY\").writeTo(\"ssb_catalog.felles.test.roblix\")\n+                .using(\"iceberg\")\n+                .tableProperty(\"valuation\", \"INTERNAL\")\n+                .tableProperty(\"state\", \"INPUT\")\n+                .tableProperty(\"write.folder-storage.path\", \"gs://ssb-dev-md-test/spark-data/felles/test/mytable\")\n+                .tableProperty(\"write.metadata.path\", \"gs://ssb-dev-md-test/spark-data/felles/test/mytable/.catalog\")\n+                .createOrReplace();\n+\n+        session.sqlContext().table(\"ssb_catalog.felles.test.roblix\").show(false);\n+    */\n+        dataAccessMockServer.enqueue(new MockResponse()\n+                .setBody(WRITE_LOCATION_RESPONSE)\n+                .setResponseCode(200));\n+        dataAccessMockServer.enqueue(new MockResponse()\n+                .setBody(WRITE_LOCATION_RESPONSE)\n+                .setResponseCode(200));\n+\n+        TableWriter.forDataset(dataset)\n+                .property(\"valuation\", \"INTERNAL\")\n+                .property(\"state\", \"INPUT\")\n+                .writeTo(\"/skatt/test/iceberg/bjornandre\")\n+                .createOrReplace();\n+\n+        //TableReader.forSession(session).readFrom(\"/felles/test/roblix\").show();\n+\n+    }\n+    @Test\n+    public void testLoadTable() throws IOException {\n+        dataAccessMockServer.enqueue(new MockResponse()\n+                .setBody(READ_LOCATION_RESPONSE)\n+                .setResponseCode(200));\n+        //TableReader.forSession(session).readFrom(\"/felles/test/roblix\").show();\n+        //session.sqlContext().table(\"ssb_catalog.testing.region\").show(false);\n+        //session.sqlContext().sql(\"show create table test_db1.TestTable\").show(false);\n+        session.sqlContext().sql(\"DESCRIBE EXTENDED ssb_catalog.felles.test.roblix\").show(false);\n+        //session.sqlContext().sql(\"SELECT * FROM ssb_catalog.felles.test.roblix.snapshots\").show(false);\n+        //session.sqlContext().sql(\"CALL ssb_catalog.system.rollback_to_snapshot('felles.test.roblix', 2407205414572021903)\");\n+        //session.sqlContext().read().format(\"iceberg\").table(\"test_db1.TestTable\").show(false);\n+        //session.sqlContext().read().format(\"iceberg\").option(\"snapshot-id\", \"506444440131968391\").load(\"ssb_catalog.felles.test.ssb_table\").show(false);\n+        //session.sql(\"DESCRIBE HISTORY ssb_catalog.felles.test.ssb_table\").show();\n+    }\n+\n+\n+    @Test\n+    public void testListByPrefix() {\n+        catalogMockServer.enqueue(new MockResponse()\n+                .setBody(\"{\\n\" +\n+                        \"  \\\"entries\\\": [{\\n\" +\n+                        \"    \\\"path\\\": \\\"/skatt/person/test1\\\",\\n\" +\n+                        \"    \\\"timestamp\\\": \\\"1585256968006\\\"\\n\" +\n+                        \"  }, {\\n\" +\n+                        \"    \\\"path\\\": \\\"/skatt/person/test2\\\",\\n\" +\n+                        \"    \\\"timestamp\\\": \\\"1582719098762\\\"\\n\" +\n+                        \"  }]\\n\" +\n+                        \"}\\n\")\n+                .setResponseCode(200));\n+\n+        session.sql(\"CREATE TABLE ssb_catalog.felles.test.ssb_table (id bigint, data string) USING iceberg\");\n+        System.out.println(session.catalog().listDatabases());\n+        //Dataset<Database> dataset = session.catalog().listDatabases();\n+        session.catalog().createTable(\"test\", \"path\");\n+        Dataset<Table> dataset = session.catalog().listTables();\n+\n+        assertThat(dataset).isNotNull();\n+        dataset.printSchema();\n+        dataset.show();\n+        assertThat(dataset.count()).isEqualTo(1);\n+    }\n+}"
  },
  {
    "sha": "895d3fb660dc2ae0579622cba6a8a92ecb4f7214",
    "filename": "src/test/java/no/ssb/dapla/spark/catalog/TableCatalogIdentifierTest.java",
    "status": "added",
    "additions": 34,
    "deletions": 0,
    "changes": 34,
    "blob_url": "https://github.com/statisticsnorway/dapla-spark-plugin/blob/a49999668af863f5b1c4935c58bd91747e1fb75a/src/test/java/no/ssb/dapla/spark/catalog/TableCatalogIdentifierTest.java",
    "raw_url": "https://github.com/statisticsnorway/dapla-spark-plugin/raw/a49999668af863f5b1c4935c58bd91747e1fb75a/src/test/java/no/ssb/dapla/spark/catalog/TableCatalogIdentifierTest.java",
    "contents_url": "https://api.github.com/repos/statisticsnorway/dapla-spark-plugin/contents/src/test/java/no/ssb/dapla/spark/catalog/TableCatalogIdentifierTest.java?ref=a49999668af863f5b1c4935c58bd91747e1fb75a",
    "patch": "@@ -0,0 +1,34 @@\n+package no.ssb.dapla.spark.catalog;\n+\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.junit.Test;\n+\n+import java.util.List;\n+\n+import static org.assertj.core.api.Assertions.*;\n+\n+public class TableCatalogIdentifierTest {\n+\n+    @Test\n+    public void testDaplaPath() {\n+        List<String> tests = List.of(\n+                \"/some weird/p@th/to_escape\",\n+                \"/1234%&4321/_4321\",\n+                \"/path/john.doe/home\",\n+                \"/!path\",\n+                \"/with#hash\",\n+                \"/!!path\"\n+        );\n+        for (String test : tests) {\n+            assertThat(TableCatalogIdentifier.fromPath(test).toPath()).isEqualTo(test);\n+        }\n+    }\n+    @Test\n+    public void testCatalogPath() {\n+        assertThat(TableCatalogIdentifier.fromTableIdentifier(TableIdentifier.of(\"my\", \"table\")).toCatalogPath(\"catalog\"))\n+                .isEqualTo(\"catalog.my.table\");\n+        assertThat(TableCatalogIdentifier.fromPath(\"/user/john.doe/table\").toCatalogPath(\"catalog\"))\n+                .isEqualTo(\"catalog.user.'john.doe'.table\");\n+    }\n+\n+}"
  }
]
