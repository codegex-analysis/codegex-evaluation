[
  {
    "sha": "f493aabbce87760845d422c59fa2f190f9644510",
    "filename": "src/main/java/io/confluent/connect/jdbc/dialect/DatabaseDialect.java",
    "status": "modified",
    "additions": 129,
    "deletions": 12,
    "changes": 141,
    "blob_url": "https://github.com/confluentinc/kafka-connect-jdbc/blob/8bd054362c82124716562d2531e909b6697f572e/src/main/java/io/confluent/connect/jdbc/dialect/DatabaseDialect.java",
    "raw_url": "https://github.com/confluentinc/kafka-connect-jdbc/raw/8bd054362c82124716562d2531e909b6697f572e/src/main/java/io/confluent/connect/jdbc/dialect/DatabaseDialect.java",
    "contents_url": "https://api.github.com/repos/confluentinc/kafka-connect-jdbc/contents/src/main/java/io/confluent/connect/jdbc/dialect/DatabaseDialect.java?ref=8bd054362c82124716562d2531e909b6697f572e",
    "patch": "@@ -40,6 +40,7 @@\n import io.confluent.connect.jdbc.util.IdentifierRules;\n import io.confluent.connect.jdbc.util.TableDefinition;\n import io.confluent.connect.jdbc.util.TableId;\n+import io.confluent.connect.jdbc.util.UpdateDropCondition;\n \n import org.apache.kafka.connect.data.Field;\n import org.apache.kafka.connect.data.Schema;\n@@ -389,21 +390,77 @@ String buildUpdateStatement(\n    * compatibility with older versions. Subclasses that override this method do not need to\n    * override {@link #buildUpdateStatement(TableId, Collection, Collection)}.\n    *\n+   * @param table             the identifier of the table; may not be null\n+   * @param keyColumns        the identifiers of the columns in the primary/unique key; may not be\n+   *                          null but may be empty\n+   * @param nonKeyColumns     the identifiers of the other columns in the table; may not be null\n+   *                          but may be empty\n+   * @param definition        the table definition; may be null if unknown\n+   * @return the UPDATE statement; may not be null\n+   */\n+  default String buildUpdateStatement(\n+          TableId table,\n+          Collection<ColumnId> keyColumns,\n+          Collection<ColumnId> nonKeyColumns,\n+          TableDefinition definition\n+  ) {\n+    return buildUpdateStatement(table, keyColumns, nonKeyColumns);\n+  }\n+\n+  /**\n+   * Build the UPDATE prepared statement expression for the given table and its columns. Variables\n+   * for each key column should also appear in the WHERE clause of the statement.\n+   *\n+   * <p>This method is only called by the default implementation of\n+   * {@link #buildUpdateStatement(TableId, Collection, Collection, TableDefinition)}, since\n+   * many dialects implement this variant of the method. However, overriding\n+   * {@link #buildUpdateStatement(TableId, Collection, Collection, TableDefinition)} is suggested.\n+   *\n    * @param table         the identifier of the table; may not be null\n    * @param keyColumns    the identifiers of the columns in the primary/unique key; may not be null\n    *                      but may be empty\n    * @param nonKeyColumns the identifiers of the other columns in the table; may not be null but may\n    *                      be empty\n-   * @param definition    the table definition; may be null if unknown\n+   * @param updateDropConditions    the string conditions when to drop messages when updating the\n+   *                               table\n+   * @return the UPDATE statement; may not be null\n+   * @deprecated use {@link #buildUpdateStatement(TableId, Collection, Collection, TableDefinition)}\n+   */\n+  @Deprecated\n+  String buildUpdateStatement(\n+          TableId table,\n+          Collection<ColumnId> keyColumns,\n+          Collection<ColumnId> nonKeyColumns,\n+          Collection<UpdateDropCondition> updateDropConditions\n+  );\n+\n+  /**\n+   * Build the UPDATE prepared statement expression for the given table and its columns. Variables\n+   * for each key column should also appear in the WHERE clause of the statement.\n+   *\n+   * <p>By default this method calls\n+   * {@link #buildUpdateStatement(TableId, Collection, Collection)} to maintain backward\n+   * compatibility with older versions. Subclasses that override this method do not need to\n+   * override {@link #buildUpdateStatement(TableId, Collection, Collection)}.\n+   *\n+   * @param table             the identifier of the table; may not be null\n+   * @param keyColumns        the identifiers of the columns in the primary/unique key; may not\n+   *                          be null but may be empty\n+   * @param nonKeyColumns     the identifiers of the other columns in the table; may not be null\n+   *                          but may be empty\n+   * @param definition        the table definition; may be null if unknown\n+   * @param updateDropConditions    the string conditions when to drop messages when updating the\n+   *                               table\n    * @return the UPDATE statement; may not be null\n    */\n   default String buildUpdateStatement(\n       TableId table,\n       Collection<ColumnId> keyColumns,\n       Collection<ColumnId> nonKeyColumns,\n-      TableDefinition definition\n+      TableDefinition definition,\n+      Collection<UpdateDropCondition> updateDropConditions\n   ) {\n-    return buildUpdateStatement(table, keyColumns, nonKeyColumns);\n+    return buildUpdateStatement(table, keyColumns, nonKeyColumns, updateDropConditions);\n   }\n \n   /**\n@@ -418,10 +475,39 @@ default String buildUpdateStatement(\n    * is suggested.\n    *\n    * @param table         the identifier of the table; may not be null\n-   * @param keyColumns    the identifiers of the columns in the primary/unique key; may not be null\n+   * @param keyColumns    the identifiers of the columns in the primary/unique key; may not\n+   *                      but may be empty\n+   * @param nonKeyColumns the identifiers of the other columns in the table; may not be null\n    *                      but may be empty\n-   * @param nonKeyColumns the identifiers of the other columns in the table; may not be null but may\n-   *                      be empty\n+   * @return the upsert/merge statement; may not be null\n+   * @throws UnsupportedOperationException if the dialect does not support upserts\n+   * @deprecated use {@link #buildUpsertQueryStatement(TableId, Collection, Collection)}\n+   */\n+  @Deprecated\n+  String buildUpsertQueryStatement(\n+          TableId table,\n+          Collection<ColumnId> keyColumns,\n+          Collection<ColumnId> nonKeyColumns\n+  );\n+\n+  /**\n+   * Build the UPSERT or MERGE prepared statement expression to either insert a new record into the\n+   * given table or update an existing record in that table Variables for each key column should\n+   * also appear in the WHERE clause of the statement.\n+   *\n+   * <p>This method is only called by the default implementation of\n+   * {@link #buildUpsertQueryStatement(TableId, Collection, Collection, TableDefinition)}, since\n+   * many dialects implement this variant of the method. However, overriding\n+   * {@link #buildUpsertQueryStatement(TableId, Collection, Collection, TableDefinition)}\n+   * is suggested.\n+   *\n+   * @param table             the identifier of the table; may not be null\n+   * @param keyColumns        the identifiers of the columns in the primary/unique key; may not\n+   *                          be null but may be empty\n+   * @param nonKeyColumns     the identifiers of the other columns in the table; may not be null\n+   *                          but may be empty\n+   * @param updateDropConditions    the string conditions when to drop messages when updating the\n+   *                               table\n    * @return the upsert/merge statement; may not be null\n    * @throws UnsupportedOperationException if the dialect does not support upserts\n    * @deprecated use {@link #buildUpsertQueryStatement(TableId, Collection, Collection)}\n@@ -430,7 +516,8 @@ default String buildUpdateStatement(\n   String buildUpsertQueryStatement(\n       TableId table,\n       Collection<ColumnId> keyColumns,\n-      Collection<ColumnId> nonKeyColumns\n+      Collection<ColumnId> nonKeyColumns,\n+      Collection<UpdateDropCondition> updateDropConditions\n   );\n \n   /**\n@@ -444,21 +531,51 @@ String buildUpsertQueryStatement(\n    * override {@link #buildUpsertQueryStatement(TableId, Collection, Collection)}.\n    *\n    * @param table         the identifier of the table; may not be null\n-   * @param keyColumns    the identifiers of the columns in the primary/unique key; may not be null\n+   * @param keyColumns    the identifiers of the columns in the primary/unique key; may not\n+   *                      be null but may be empty\n+   * @param nonKeyColumns the identifiers of the other columns in the table; may not be null\n    *                      but may be empty\n-   * @param nonKeyColumns the identifiers of the other columns in the table; may not be null but may\n-   *                      be empty\n    * @param definition    the table definition; may be null if unknown\n    * @return the upsert/merge statement; may not be null\n    * @throws UnsupportedOperationException if the dialect does not support upserts\n    */\n+  default String buildUpsertQueryStatement(\n+          TableId table,\n+          Collection<ColumnId> keyColumns,\n+          Collection<ColumnId> nonKeyColumns,\n+          TableDefinition definition\n+  ) {\n+    return buildUpsertQueryStatement(table, keyColumns, nonKeyColumns);\n+  }\n+\n+  /**\n+   * Build the UPSERT or MERGE prepared statement expression to either insert a new record into the\n+   * given table or update an existing record in that table Variables for each key column should\n+   * also appear in the WHERE clause of the statement.\n+   *\n+   * <p>By default this method calls\n+   * {@link #buildUpsertQueryStatement(TableId, Collection, Collection)} to maintain backward\n+   * compatibility with older versions. Subclasses that override this method do not need to\n+   * override {@link #buildUpsertQueryStatement(TableId, Collection, Collection)}.\n+   *\n+   * @param table             the identifier of the table; may not be null\n+   * @param keyColumns        the identifiers of the columns in the primary/unique key; may not\n+   *                          be null but may be empty\n+   * @param nonKeyColumns     the identifiers of the other columns in the table; may not be null\n+   *                          but may be empty\n+   * @param updateDropConditions    the string conditions when to drop messages when updating the\n+   *                               table\n+   * @return the upsert/merge statement; may not be null\n+   * @throws UnsupportedOperationException if the dialect does not support upserts\n+   */\n   default String buildUpsertQueryStatement(\n       TableId table,\n       Collection<ColumnId> keyColumns,\n       Collection<ColumnId> nonKeyColumns,\n-      TableDefinition definition\n+      TableDefinition definition,\n+      Collection<UpdateDropCondition> updateDropConditions\n   ) {\n-    return buildUpsertQueryStatement(table, keyColumns, nonKeyColumns);\n+    return buildUpsertQueryStatement(table, keyColumns, nonKeyColumns, updateDropConditions);\n   }\n \n   /**"
  },
  {
    "sha": "f3f1d74efa7c6a3da533e25b686eb65a934c57b9",
    "filename": "src/main/java/io/confluent/connect/jdbc/dialect/GenericDatabaseDialect.java",
    "status": "modified",
    "additions": 51,
    "deletions": 1,
    "changes": 52,
    "blob_url": "https://github.com/confluentinc/kafka-connect-jdbc/blob/8bd054362c82124716562d2531e909b6697f572e/src/main/java/io/confluent/connect/jdbc/dialect/GenericDatabaseDialect.java",
    "raw_url": "https://github.com/confluentinc/kafka-connect-jdbc/raw/8bd054362c82124716562d2531e909b6697f572e/src/main/java/io/confluent/connect/jdbc/dialect/GenericDatabaseDialect.java",
    "contents_url": "https://api.github.com/repos/confluentinc/kafka-connect-jdbc/contents/src/main/java/io/confluent/connect/jdbc/dialect/GenericDatabaseDialect.java?ref=8bd054362c82124716562d2531e909b6697f572e",
    "patch": "@@ -90,6 +90,7 @@\n import io.confluent.connect.jdbc.util.TableDefinition;\n import io.confluent.connect.jdbc.util.TableId;\n import io.confluent.connect.jdbc.util.TableType;\n+import io.confluent.connect.jdbc.util.UpdateDropCondition;\n \n /**\n  * A {@link DatabaseDialect} implementation that provides functionality based upon JDBC and SQL.\n@@ -1488,6 +1489,17 @@ public String buildUpdateStatement(\n       TableId table,\n       Collection<ColumnId> keyColumns,\n       Collection<ColumnId> nonKeyColumns\n+  ) {\n+    return buildUpdateStatement(table, keyColumns, nonKeyColumns,\n+            (Collection<UpdateDropCondition>) null);\n+  }\n+\n+  @SuppressWarnings(\"deprecation\")\n+  public String buildUpdateStatement(\n+          TableId table,\n+          Collection<ColumnId> keyColumns,\n+          Collection<ColumnId> nonKeyColumns,\n+          Collection<UpdateDropCondition> conditions\n   ) {\n     ExpressionBuilder builder = expressionBuilder();\n     builder.append(\"UPDATE \");\n@@ -1497,8 +1509,32 @@ public String buildUpdateStatement(\n            .delimitedBy(\", \")\n            .transformedBy(ExpressionBuilder.columnNamesWith(\" = ?\"))\n            .of(nonKeyColumns);\n-    if (!keyColumns.isEmpty()) {\n+\n+    Transform<UpdateDropCondition> transformCondition = (bld, condition) -> {\n+      bld.appendColumnName(condition.field().name());\n+      bld.append(\" \");\n+      bld.append(condition.operator());\n+      bld.append(\" ?\");\n+    };\n+\n+    boolean keyColumnsEmpty = !keyColumns.isEmpty();\n+    boolean conditionsEmpty = conditions != null && !conditions.isEmpty();\n+\n+    if (keyColumnsEmpty || conditionsEmpty) {\n       builder.append(\" WHERE \");\n+    }\n+\n+    if (conditionsEmpty) {\n+      builder.appendList()\n+              .delimitedBy(\" AND \")\n+              .transformedBy(transformCondition)\n+              .of(conditions);\n+      if (keyColumnsEmpty) {\n+        builder.append(\" AND \");\n+      }\n+    }\n+\n+    if (keyColumnsEmpty) {\n       builder.appendList()\n              .delimitedBy(\" AND \")\n              .transformedBy(ExpressionBuilder.columnNamesWith(\" = ?\"))\n@@ -1517,6 +1553,20 @@ public String buildUpsertQueryStatement(\n     throw new UnsupportedOperationException();\n   }\n \n+  @Override\n+  @SuppressWarnings(\"deprecation\")\n+  public String buildUpsertQueryStatement(\n+      TableId table,\n+      Collection<ColumnId> keyColumns,\n+      Collection<ColumnId> nonKeyColumns,\n+      Collection<UpdateDropCondition> conditions\n+  ) {\n+    if (conditions == null || conditions.isEmpty()) {\n+      return buildUpsertQueryStatement(table, keyColumns, nonKeyColumns);\n+    }\n+    throw new UnsupportedOperationException();\n+  }\n+\n   @Override\n   public final String buildDeleteStatement(\n       TableId table,"
  },
  {
    "sha": "876d5ddf0300c3a8ec23ad653d2b4bca3f2375db",
    "filename": "src/main/java/io/confluent/connect/jdbc/dialect/MySqlDatabaseDialect.java",
    "status": "modified",
    "additions": 38,
    "deletions": 2,
    "changes": 40,
    "blob_url": "https://github.com/confluentinc/kafka-connect-jdbc/blob/8bd054362c82124716562d2531e909b6697f572e/src/main/java/io/confluent/connect/jdbc/dialect/MySqlDatabaseDialect.java",
    "raw_url": "https://github.com/confluentinc/kafka-connect-jdbc/raw/8bd054362c82124716562d2531e909b6697f572e/src/main/java/io/confluent/connect/jdbc/dialect/MySqlDatabaseDialect.java",
    "contents_url": "https://api.github.com/repos/confluentinc/kafka-connect-jdbc/contents/src/main/java/io/confluent/connect/jdbc/dialect/MySqlDatabaseDialect.java?ref=8bd054362c82124716562d2531e909b6697f572e",
    "patch": "@@ -34,6 +34,7 @@\n import io.confluent.connect.jdbc.util.ExpressionBuilder.Transform;\n import io.confluent.connect.jdbc.util.IdentifierRules;\n import io.confluent.connect.jdbc.util.TableId;\n+import io.confluent.connect.jdbc.util.UpdateDropCondition;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -129,11 +130,22 @@ protected String getSqlType(SinkRecordField field) {\n     }\n   }\n \n+  @Override\n+  public String buildUpsertQueryStatement(\n+          TableId table,\n+          Collection<ColumnId> keyColumns,\n+          Collection<ColumnId> nonKeyColumns\n+  ) {\n+    return buildUpsertQueryStatement(table, keyColumns, nonKeyColumns,\n+            (Collection<UpdateDropCondition>) null);\n+  }\n+\n   @Override\n   public String buildUpsertQueryStatement(\n       TableId table,\n       Collection<ColumnId> keyColumns,\n-      Collection<ColumnId> nonKeyColumns\n+      Collection<ColumnId> nonKeyColumns,\n+      Collection<UpdateDropCondition> conditions\n   ) {\n     //MySql doesn't support SQL 2003:merge so here how the upsert is handled\n     final Transform<ColumnId> transform = (builder, col) -> {\n@@ -143,6 +155,29 @@ public String buildUpsertQueryStatement(\n       builder.append(\")\");\n     };\n \n+    final Transform<UpdateDropCondition> transformCondition = (bld, con) -> {\n+      bld.appendColumnName(con.field().name())\n+         .append(\" \")\n+         .append(con.operator())\n+         .append(\" values(\")\n+         .appendColumnName(con.field().name())\n+         .append(\")\");\n+    };\n+\n+    final Transform<ColumnId> transformWithConditions = (builder, col) -> {\n+      builder.appendColumnName(col.name());\n+      builder.append(\" = IF(\");\n+      builder.appendList()\n+             .delimitedBy(\" and \")\n+             .transformedBy(transformCondition)\n+             .of(conditions);\n+      builder.append(\", values(\");\n+      builder.appendColumnName(col.name());\n+      builder.append(\"), \");\n+      builder.appendColumnName(col.name());\n+      builder.append(\")\");\n+    };\n+\n     ExpressionBuilder builder = expressionBuilder();\n     builder.append(\"insert into \");\n     builder.append(table);\n@@ -156,7 +191,8 @@ public String buildUpsertQueryStatement(\n     builder.append(\") on duplicate key update \");\n     builder.appendList()\n            .delimitedBy(\",\")\n-           .transformedBy(transform)\n+           .transformedBy(conditions != null && !conditions.isEmpty()\n+                   ? transformWithConditions : transform)\n            .of(nonKeyColumns.isEmpty() ? keyColumns : nonKeyColumns);\n     return builder.toString();\n   }"
  },
  {
    "sha": "f4fc6cec7ec935bc72268bfb879406db79238243",
    "filename": "src/main/java/io/confluent/connect/jdbc/dialect/SqlServerDatabaseDialect.java",
    "status": "modified",
    "additions": 33,
    "deletions": 3,
    "changes": 36,
    "blob_url": "https://github.com/confluentinc/kafka-connect-jdbc/blob/8bd054362c82124716562d2531e909b6697f572e/src/main/java/io/confluent/connect/jdbc/dialect/SqlServerDatabaseDialect.java",
    "raw_url": "https://github.com/confluentinc/kafka-connect-jdbc/raw/8bd054362c82124716562d2531e909b6697f572e/src/main/java/io/confluent/connect/jdbc/dialect/SqlServerDatabaseDialect.java",
    "contents_url": "https://api.github.com/repos/confluentinc/kafka-connect-jdbc/contents/src/main/java/io/confluent/connect/jdbc/dialect/SqlServerDatabaseDialect.java?ref=8bd054362c82124716562d2531e909b6697f572e",
    "patch": "@@ -30,11 +30,10 @@\n import java.sql.Types;\n import java.time.ZonedDateTime;\n import java.time.format.DateTimeFormatter;\n-import java.util.Calendar;\n import java.util.Collection;\n-import java.util.Collections;\n import java.util.List;\n import java.util.TimeZone;\n+import java.util.Collections;\n \n import io.confluent.connect.jdbc.dialect.DatabaseDialectProvider.SubprotocolBasedProvider;\n import io.confluent.connect.jdbc.sink.metadata.SinkRecordField;\n@@ -47,6 +46,7 @@\n import io.confluent.connect.jdbc.util.TableId;\n import io.confluent.connect.jdbc.util.ColumnDefinition.Mutability;\n import io.confluent.connect.jdbc.util.ColumnDefinition.Nullability;\n+import io.confluent.connect.jdbc.util.UpdateDropCondition;\n import org.apache.kafka.connect.errors.ConnectException;\n \n import static io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig.TIMESTAMP_COLUMN_NAME_CONFIG;\n@@ -311,6 +311,17 @@ public String buildUpsertQueryStatement(\n       TableId table,\n       Collection<ColumnId> keyColumns,\n       Collection<ColumnId> nonKeyColumns\n+  ) {\n+    return buildUpsertQueryStatement(table, keyColumns, nonKeyColumns,\n+            (Collection<UpdateDropCondition>) null);\n+  }\n+\n+  @Override\n+  public String buildUpsertQueryStatement(\n+      TableId table,\n+      Collection<ColumnId> keyColumns,\n+      Collection<ColumnId> nonKeyColumns,\n+      Collection<UpdateDropCondition> conditions\n   ) {\n     ExpressionBuilder builder = expressionBuilder();\n     builder.append(\"merge into \");\n@@ -327,7 +338,16 @@ public String buildUpsertQueryStatement(\n            .of(keyColumns);\n     builder.append(\")\");\n     if (nonKeyColumns != null && !nonKeyColumns.isEmpty()) {\n-      builder.append(\" when matched then update set \");\n+      builder.append(\" when matched\");\n+\n+      if (conditions != null && !conditions.isEmpty()) {\n+        builder.append(\" and \");\n+        builder.appendList()\n+               .delimitedBy(\" and \")\n+               .transformedBy(this::transformCondition)\n+               .of(conditions);\n+      }\n+      builder.append(\" then update set \");\n       builder.appendList()\n              .delimitedBy(\",\")\n              .transformedBy(this::transformUpdate)\n@@ -447,6 +467,16 @@ protected ColumnDefinition columnDefinition(\n     );\n   }\n \n+  private void transformCondition(ExpressionBuilder builder, UpdateDropCondition con) {\n+    builder.append(\"target.\")\n+           .appendColumnName(con.field().name())\n+           .append(\" \")\n+           .append(con.operator())\n+           .append(\" \")\n+           .append(\"incoming.\")\n+           .appendColumnName(con.field().name());\n+  }\n+\n   private void transformAs(ExpressionBuilder builder, ColumnId col) {\n     builder.append(\"target.\")\n            .appendColumnName(col.name())"
  },
  {
    "sha": "ef076275c2f8b335e28afbc181b7cfaf299ec33d",
    "filename": "src/main/java/io/confluent/connect/jdbc/sink/BufferedRecords.java",
    "status": "modified",
    "additions": 4,
    "deletions": 2,
    "changes": 6,
    "blob_url": "https://github.com/confluentinc/kafka-connect-jdbc/blob/8bd054362c82124716562d2531e909b6697f572e/src/main/java/io/confluent/connect/jdbc/sink/BufferedRecords.java",
    "raw_url": "https://github.com/confluentinc/kafka-connect-jdbc/raw/8bd054362c82124716562d2531e909b6697f572e/src/main/java/io/confluent/connect/jdbc/sink/BufferedRecords.java",
    "contents_url": "https://api.github.com/repos/confluentinc/kafka-connect-jdbc/contents/src/main/java/io/confluent/connect/jdbc/sink/BufferedRecords.java?ref=8bd054362c82124716562d2531e909b6697f572e",
    "patch": "@@ -286,7 +286,8 @@ private String getInsertSql() throws SQLException {\n               tableId,\n               asColumns(fieldsMetadata.keyFieldNames),\n               asColumns(fieldsMetadata.nonKeyFieldNames),\n-              dbStructure.tableDefinition(connection, tableId)\n+              dbStructure.tableDefinition(connection, tableId),\n+              config.updateDropConditions\n           );\n         } catch (UnsupportedOperationException e) {\n           throw new ConnectException(String.format(\n@@ -300,7 +301,8 @@ private String getInsertSql() throws SQLException {\n             tableId,\n             asColumns(fieldsMetadata.keyFieldNames),\n             asColumns(fieldsMetadata.nonKeyFieldNames),\n-            dbStructure.tableDefinition(connection, tableId)\n+            dbStructure.tableDefinition(connection, tableId),\n+            config.updateDropConditions\n         );\n       default:\n         throw new ConnectException(\"Invalid insert mode\");"
  },
  {
    "sha": "5bbfe338ce92c8dd66df9bf212b657c240d9104e",
    "filename": "src/main/java/io/confluent/connect/jdbc/sink/JdbcSinkConfig.java",
    "status": "modified",
    "additions": 30,
    "deletions": 0,
    "changes": 30,
    "blob_url": "https://github.com/confluentinc/kafka-connect-jdbc/blob/8bd054362c82124716562d2531e909b6697f572e/src/main/java/io/confluent/connect/jdbc/sink/JdbcSinkConfig.java",
    "raw_url": "https://github.com/confluentinc/kafka-connect-jdbc/raw/8bd054362c82124716562d2531e909b6697f572e/src/main/java/io/confluent/connect/jdbc/sink/JdbcSinkConfig.java",
    "contents_url": "https://api.github.com/repos/confluentinc/kafka-connect-jdbc/contents/src/main/java/io/confluent/connect/jdbc/sink/JdbcSinkConfig.java?ref=8bd054362c82124716562d2531e909b6697f572e",
    "patch": "@@ -38,6 +38,7 @@\n import io.confluent.connect.jdbc.util.StringUtils;\n import io.confluent.connect.jdbc.util.TableType;\n import io.confluent.connect.jdbc.util.TimeZoneValidator;\n+import io.confluent.connect.jdbc.util.UpdateDropCondition;\n import org.apache.kafka.common.config.AbstractConfig;\n import org.apache.kafka.common.config.ConfigDef;\n import org.apache.kafka.common.config.ConfigException;\n@@ -254,6 +255,20 @@\n       + \"view definition does not match the records' schemas (regardless of ``\"\n       + AUTO_EVOLVE + \"``).\";\n \n+  public static final String UPDATE_DROP_CONDITIONS = \"update.drop.if.fields.older\";\n+  private static final String UPDATE_DROP_CONDITIONS_DEFAULT = \"\";\n+  private static final String UPDATE_DROP_CONDITIONS_DOC =\n+          \"List of comma-separated timestamp fields. All of these need to be newer in the \"\n+          + \"new record to update/upsert in order for the operation to succeed.\"\n+          + \"The runtime interpretation of this config depends on the ``insert.mode``:\\n\"\n+          + \"``update``\\n\"\n+          + \"    will update only records in the table that have older timestamps.\\n\"\n+          + \"``upsert``\\n\"\n+          + \"    will update when matching the upsert condition AND having older timestamps \"\n+          + \"than the table record. If empty, no conditions are enforced\\n\"\n+          + \"NOTE: Currently only supported for MS SQL Server and MySQL\";\n+  private static final String UPDATE_DROP_CONDITIONS_DISPLAY = \"Update/Upsert Drop Conditions\";\n+\n   private static final EnumRecommender QUOTE_METHOD_RECOMMENDER =\n       EnumRecommender.in(QuoteMethod.values());\n \n@@ -377,6 +392,16 @@\n             ConfigDef.Width.MEDIUM,\n             TABLE_TYPES_DISPLAY\n         )\n+        .define(\n+            UPDATE_DROP_CONDITIONS,\n+            ConfigDef.Type.LIST,\n+            UPDATE_DROP_CONDITIONS_DEFAULT,\n+            ConfigDef.Importance.MEDIUM,\n+            UPDATE_DROP_CONDITIONS_DOC,\n+            WRITES_GROUP,\n+            5,\n+            ConfigDef.Width.LONG, UPDATE_DROP_CONDITIONS_DISPLAY\n+        )\n         // Data Mapping\n         .define(\n             TABLE_NAME_FORMAT,\n@@ -508,6 +533,7 @@\n   public final InsertMode insertMode;\n   public final PrimaryKeyMode pkMode;\n   public final List<String> pkFields;\n+  public final List<UpdateDropCondition> updateDropConditions;\n   public final Set<String> fieldsWhitelist;\n   public final String dialectName;\n   public final TimeZone timeZone;\n@@ -535,6 +561,10 @@ public JdbcSinkConfig(Map<?, ?> props) {\n     fieldsWhitelist = new HashSet<>(getList(FIELDS_WHITELIST));\n     String dbTimeZone = getString(DB_TIMEZONE_CONFIG);\n     timeZone = TimeZone.getTimeZone(ZoneId.of(dbTimeZone));\n+    updateDropConditions = getList(UPDATE_DROP_CONDITIONS)\n+            .stream()\n+            .map(c -> new UpdateDropCondition(c))\n+            .collect(Collectors.toList());\n \n     if (deleteEnabled && pkMode != PrimaryKeyMode.RECORD_KEY) {\n       throw new ConfigException("
  },
  {
    "sha": "04a74dd6b258002afe17779f622d43e5b259518b",
    "filename": "src/main/java/io/confluent/connect/jdbc/util/UpdateDropCondition.java",
    "status": "added",
    "additions": 83,
    "deletions": 0,
    "changes": 83,
    "blob_url": "https://github.com/confluentinc/kafka-connect-jdbc/blob/8bd054362c82124716562d2531e909b6697f572e/src/main/java/io/confluent/connect/jdbc/util/UpdateDropCondition.java",
    "raw_url": "https://github.com/confluentinc/kafka-connect-jdbc/raw/8bd054362c82124716562d2531e909b6697f572e/src/main/java/io/confluent/connect/jdbc/util/UpdateDropCondition.java",
    "contents_url": "https://api.github.com/repos/confluentinc/kafka-connect-jdbc/contents/src/main/java/io/confluent/connect/jdbc/util/UpdateDropCondition.java?ref=8bd054362c82124716562d2531e909b6697f572e",
    "patch": "@@ -0,0 +1,83 @@\n+/*\n+ * Copyright 2018 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.connect.jdbc.util;\n+\n+import io.confluent.connect.jdbc.util.ExpressionBuilder.Expressable;\n+\n+import java.util.Objects;\n+\n+public class UpdateDropCondition implements Expressable {\n+\n+  private final ColumnId field;\n+  private final String operator;\n+  private final int hash;\n+\n+  public UpdateDropCondition(String field) {\n+    this(new ColumnId(null, field.trim(), field.trim()));\n+  }\n+\n+  public UpdateDropCondition(ColumnId field) {\n+    this.field = field;\n+    this.operator = \"<\";\n+    this.hash = Objects.hash(this.field.name(), this.operator);\n+  }\n+\n+  public ColumnId field() {\n+    return this.field;\n+  }\n+\n+  public String operator() {\n+    return this.operator;\n+  }\n+\n+  @Override\n+  public void appendTo(ExpressionBuilder builder, boolean useQuotes) {\n+    appendTo(builder, useQuotes ? QuoteMethod.ALWAYS : QuoteMethod.NEVER);\n+  }\n+\n+  @Override\n+  public void appendTo(\n+      ExpressionBuilder builder,\n+      QuoteMethod useQuotes\n+  ) {\n+    builder.appendColumnName(this.field.name(), useQuotes);\n+    builder.append(this.operator);\n+    builder.appendColumnName(this.field.name(), useQuotes);\n+  }\n+\n+  @Override\n+  public int hashCode() {\n+    return hash;\n+  }\n+\n+  @Override\n+  public boolean equals(Object obj) {\n+    if (obj == this) {\n+      return true;\n+    }\n+    if (obj instanceof UpdateDropCondition) {\n+      UpdateDropCondition that = (UpdateDropCondition) obj;\n+      return Objects.equals(this.field.name(), that.field.name())\n+             && Objects.equals(this.operator, that.operator);\n+    }\n+    return false;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return ExpressionBuilder.create().append(this).toString();\n+  }\n+}"
  },
  {
    "sha": "e2d67f0610debfe09663f790743312dedf3e07fa",
    "filename": "src/test/java/io/confluent/connect/jdbc/dialect/GenericDatabaseDialectTest.java",
    "status": "modified",
    "additions": 14,
    "deletions": 10,
    "changes": 24,
    "blob_url": "https://github.com/confluentinc/kafka-connect-jdbc/blob/8bd054362c82124716562d2531e909b6697f572e/src/test/java/io/confluent/connect/jdbc/dialect/GenericDatabaseDialectTest.java",
    "raw_url": "https://github.com/confluentinc/kafka-connect-jdbc/raw/8bd054362c82124716562d2531e909b6697f572e/src/test/java/io/confluent/connect/jdbc/dialect/GenericDatabaseDialectTest.java",
    "contents_url": "https://api.github.com/repos/confluentinc/kafka-connect-jdbc/contents/src/test/java/io/confluent/connect/jdbc/dialect/GenericDatabaseDialectTest.java?ref=8bd054362c82124716562d2531e909b6697f572e",
    "patch": "@@ -15,6 +15,7 @@\n \n package io.confluent.connect.jdbc.dialect;\n \n+import io.confluent.connect.jdbc.util.*;\n import org.apache.kafka.common.config.AbstractConfig;\n import org.apache.kafka.connect.data.Date;\n import org.apache.kafka.connect.data.Decimal;\n@@ -45,16 +46,6 @@\n import io.confluent.connect.jdbc.sink.metadata.SinkRecordField;\n import io.confluent.connect.jdbc.source.EmbeddedDerby;\n import io.confluent.connect.jdbc.source.JdbcSourceConnectorConfig;\n-import io.confluent.connect.jdbc.util.ColumnDefinition;\n-import io.confluent.connect.jdbc.util.ColumnId;\n-import io.confluent.connect.jdbc.util.ConnectionProvider;\n-import io.confluent.connect.jdbc.util.ExpressionBuilder;\n-import io.confluent.connect.jdbc.util.IdentifierRules;\n-import io.confluent.connect.jdbc.util.QuoteMethod;\n-import io.confluent.connect.jdbc.util.StringUtils;\n-import io.confluent.connect.jdbc.util.TableDefinition;\n-import io.confluent.connect.jdbc.util.TableId;\n-import io.confluent.connect.jdbc.util.TableType;\n \n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n@@ -273,6 +264,19 @@ public void testBuildCreateTableStatement() {\n         dialect.buildInsertStatement(tableId, pkColumns, columnsAtoD));\n   }\n \n+  @Test\n+  public void testBuildUpdateStatement() {\n+    newDialectFor(TABLE_TYPES, null);\n+    Collection<UpdateDropCondition> conditions =\n+            Collections.singletonList(new UpdateDropCondition(columnA));\n+    assertEquals(\n+            \"UPDATE \\\"myTable\\\" SET \\\"columnA\\\" = ?, \\\"columnB\\\" = ?, \" +\n+            \"\\\"columnC\\\" = ?, \\\"columnD\\\" = ? WHERE \\\"columnA\\\" < ?\" +\n+            \" AND \\\"id1\\\" = ? AND \\\"id2\\\" = ?\",\n+            dialect.buildUpdateStatement(tableId, pkColumns, columnsAtoD, conditions));\n+  }\n+\n+\n   @Test\n   public void testBuildDeleteStatement() {\n     newDialectFor(TABLE_TYPES, null);"
  },
  {
    "sha": "5de7600e145b5d65c554000964ff8ffe33ba6964",
    "filename": "src/test/java/io/confluent/connect/jdbc/dialect/MySqlDatabaseDialectTest.java",
    "status": "modified",
    "additions": 18,
    "deletions": 0,
    "changes": 18,
    "blob_url": "https://github.com/confluentinc/kafka-connect-jdbc/blob/8bd054362c82124716562d2531e909b6697f572e/src/test/java/io/confluent/connect/jdbc/dialect/MySqlDatabaseDialectTest.java",
    "raw_url": "https://github.com/confluentinc/kafka-connect-jdbc/raw/8bd054362c82124716562d2531e909b6697f572e/src/test/java/io/confluent/connect/jdbc/dialect/MySqlDatabaseDialectTest.java",
    "contents_url": "https://api.github.com/repos/confluentinc/kafka-connect-jdbc/contents/src/test/java/io/confluent/connect/jdbc/dialect/MySqlDatabaseDialectTest.java?ref=8bd054362c82124716562d2531e909b6697f572e",
    "patch": "@@ -15,6 +15,7 @@\n \n package io.confluent.connect.jdbc.dialect;\n \n+import io.confluent.connect.jdbc.util.UpdateDropCondition;\n import org.apache.kafka.connect.data.Date;\n import org.apache.kafka.connect.data.Decimal;\n import org.apache.kafka.connect.data.Schema;\n@@ -23,6 +24,9 @@\n import org.apache.kafka.connect.data.Timestamp;\n import org.junit.Test;\n \n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n import java.util.List;\n \n import io.confluent.connect.jdbc.util.QuoteMethod;\n@@ -127,6 +131,20 @@ public void shouldBuildUpsertStatement() {\n     assertEquals(expected, sql);\n   }\n \n+  @Test\n+  public void shouldBuildUpsertStatementWithConditions() {\n+    Collection<UpdateDropCondition> conditions =\n+            Collections.singletonList(new UpdateDropCondition(columnA));\n+    String expected = \"insert into `myTable`(`id1`,`id2`,`columnA`,`columnB`,`columnC`\" +\n+            \",`columnD`) values(?,?,?,?,?,?) on duplicate key update \" +\n+            \"`columnA` = IF(`columnA` < values(`columnA`), values(`columnA`), `columnA`),\" +\n+            \"`columnB` = IF(`columnA` < values(`columnA`), values(`columnB`), `columnB`),\" +\n+            \"`columnC` = IF(`columnA` < values(`columnA`), values(`columnC`), `columnC`),\" +\n+            \"`columnD` = IF(`columnA` < values(`columnA`), values(`columnD`), `columnD`)\";\n+    String sql = dialect.buildUpsertQueryStatement(tableId, pkColumns, columnsAtoD, conditions);\n+    assertEquals(expected, sql);\n+  }\n+\n   @Test\n   public void createOneColNoPk() {\n     verifyCreateOneColNoPk("
  },
  {
    "sha": "d3a36b69df8da5421f9ddf22b52c250b21684102",
    "filename": "src/test/java/io/confluent/connect/jdbc/dialect/SqlServerDatabaseDialectTest.java",
    "status": "modified",
    "additions": 36,
    "deletions": 0,
    "changes": 36,
    "blob_url": "https://github.com/confluentinc/kafka-connect-jdbc/blob/8bd054362c82124716562d2531e909b6697f572e/src/test/java/io/confluent/connect/jdbc/dialect/SqlServerDatabaseDialectTest.java",
    "raw_url": "https://github.com/confluentinc/kafka-connect-jdbc/raw/8bd054362c82124716562d2531e909b6697f572e/src/test/java/io/confluent/connect/jdbc/dialect/SqlServerDatabaseDialectTest.java",
    "contents_url": "https://api.github.com/repos/confluentinc/kafka-connect-jdbc/contents/src/test/java/io/confluent/connect/jdbc/dialect/SqlServerDatabaseDialectTest.java?ref=8bd054362c82124716562d2531e909b6697f572e",
    "patch": "@@ -20,6 +20,7 @@\n import java.time.ZoneId;\n import java.time.ZonedDateTime;\n import java.util.ArrayList;\n+import java.util.Arrays;\n import java.util.List;\n import java.util.TimeZone;\n \n@@ -35,6 +36,7 @@\n \n import io.confluent.connect.jdbc.util.QuoteMethod;\n import io.confluent.connect.jdbc.util.TableId;\n+import io.confluent.connect.jdbc.util.UpdateDropCondition;\n import org.mockito.Mockito;\n \n import static org.junit.Assert.assertEquals;\n@@ -212,6 +214,40 @@ public void shouldBuildAlterTableStatement() {\n     );\n   }\n \n+  @Test\n+  public void shouldBuildUpsertStatementWithConditions() {\n+    assertEquals(\n+            \"merge into [myTable] with (HOLDLOCK) AS target using (select ? AS [id1], ?\" +\n+                    \" AS [id2], ? AS [columnA], ? AS [columnB], ? AS [columnC], ? AS [columnD])\" +\n+                    \" AS incoming on (target.[id1]=incoming.[id1] and target.[id2]=incoming\" +\n+                    \".[id2]) when matched and target.[columnA] < incoming.[columnA] then \" +\n+                    \"update set [columnA]=incoming.[columnA],\" +\n+                    \"[columnB]=incoming.[columnB],[columnC]=incoming.[columnC],\" +\n+                    \"[columnD]=incoming.[columnD] when not matched then insert ([columnA], \" +\n+                    \"[columnB], [columnC], [columnD], [id1], [id2]) values (incoming.[columnA],\" +\n+                    \"incoming.[columnB],incoming.[columnC],incoming.[columnD],incoming.[id1],\" +\n+                    \"incoming.[id2]);\",\n+            dialect.buildUpsertQueryStatement(tableId, pkColumns, columnsAtoD,\n+                    Arrays.asList(new UpdateDropCondition(columnA)))\n+    );\n+\n+    quoteIdentfiiers = QuoteMethod.NEVER;\n+    dialect = createDialect();\n+    assertEquals(\n+            \"merge into myTable with (HOLDLOCK) AS target using (select ? AS id1, ?\" +\n+                    \" AS id2, ? AS columnA, ? AS columnB, ? AS columnC, ? AS columnD)\" +\n+                    \" AS incoming on (target.id1=incoming.id1 and target.id2=incoming\" +\n+                    \".id2) when matched and target.columnA < incoming.columnA then \" +\n+                    \"update set columnA=incoming.columnA,columnB=incoming.columnB,\" +\n+                    \"columnC=incoming.columnC,columnD=incoming.columnD when not matched \" +\n+                    \"then insert (columnA, columnB, columnC, columnD, id1, id2) values \" +\n+                    \"(incoming.columnA,incoming.columnB,incoming.columnC,incoming.\" +\n+                    \"columnD,incoming.id1,incoming.id2);\",\n+            dialect.buildUpsertQueryStatement(tableId, pkColumns, columnsAtoD,\n+                    Arrays.asList(new UpdateDropCondition(columnA)))\n+    );\n+  }\n+\n   @Test\n   public void shouldBuildUpsertStatement() {\n     assertEquals("
  },
  {
    "sha": "f56176682b30c103bcaa7f4b50ae6007135f9fbf",
    "filename": "src/test/java/io/confluent/connect/jdbc/sink/JdbcSinkConfigTest.java",
    "status": "modified",
    "additions": 11,
    "deletions": 0,
    "changes": 11,
    "blob_url": "https://github.com/confluentinc/kafka-connect-jdbc/blob/8bd054362c82124716562d2531e909b6697f572e/src/test/java/io/confluent/connect/jdbc/sink/JdbcSinkConfigTest.java",
    "raw_url": "https://github.com/confluentinc/kafka-connect-jdbc/raw/8bd054362c82124716562d2531e909b6697f572e/src/test/java/io/confluent/connect/jdbc/sink/JdbcSinkConfigTest.java",
    "contents_url": "https://api.github.com/repos/confluentinc/kafka-connect-jdbc/contents/src/test/java/io/confluent/connect/jdbc/sink/JdbcSinkConfigTest.java?ref=8bd054362c82124716562d2531e909b6697f572e",
    "patch": "@@ -66,6 +66,17 @@ public void shouldCreateConfigWithAdditionalConfigs() {\n     assertTableTypes(TableType.TABLE);\n   }\n \n+  @Test\n+  public void shouldCreateConfigWithDropConditions() {\n+    props.put(\"update.drop.if.fields.older\", \"columnA,columnB\");\n+    createConfig();\n+    assertEquals(2, config.updateDropConditions.size());\n+    assertEquals(\"columnA\", config.updateDropConditions.get(0).field().name());\n+    assertEquals(\"columnB\", config.updateDropConditions.get(1).field().name());\n+    assertEquals(\"<\",config.updateDropConditions.get(0).operator());\n+    assertEquals(\"<\",config.updateDropConditions.get(1).operator());\n+  }\n+\n   @Test\n   public void shouldCreateConfigWithViewOnly() {\n     props.put(\"table.types\", \"view\");"
  }
]
