[
  {
    "sha": "68bab5b762cda91554e1486fcb03d5053493f37c",
    "filename": "documentation/src/main/doc/modules/connectors/partials/META-INF/connector/smallrye-kafka-incoming.adoc",
    "status": "modified",
    "additions": 4,
    "deletions": 0,
    "changes": 4,
    "blob_url": "https://github.com/smallrye/smallrye-reactive-messaging/blob/7d8f527bb54b3dbbfaedc55460a97a96f8f37c3f/documentation/src/main/doc/modules/connectors/partials/META-INF/connector/smallrye-kafka-incoming.adoc",
    "raw_url": "https://github.com/smallrye/smallrye-reactive-messaging/raw/7d8f527bb54b3dbbfaedc55460a97a96f8f37c3f/documentation/src/main/doc/modules/connectors/partials/META-INF/connector/smallrye-kafka-incoming.adoc",
    "contents_url": "https://api.github.com/repos/smallrye/smallrye-reactive-messaging/contents/documentation/src/main/doc/modules/connectors/partials/META-INF/connector/smallrye-kafka-incoming.adoc?ref=7d8f527bb54b3dbbfaedc55460a97a96f8f37c3f",
    "patch": "@@ -37,6 +37,10 @@ Type: _boolean_ | false | `true`\n \n Type: _boolean_ | false | `true`\n \n+| *kafka-configuration-name* | The name set in `javax.inject.Named` of a bean that provides the default Kafka consumer/producer (`Map<String, Object>`) configuration to use for this channel. The channel configuration can still override any attribute.\n+\n+Type: _string_ | false | \n+\n | *topics* | A comma-separating list of topics to be consumed. Cannot be used with the `topic` or `pattern` properties\n \n Type: _string_ | false | "
  },
  {
    "sha": "a437280e42f27d741298b3c7e4953f14966ed23b",
    "filename": "documentation/src/main/doc/modules/connectors/partials/META-INF/connector/smallrye-kafka-outgoing.adoc",
    "status": "modified",
    "additions": 4,
    "deletions": 0,
    "changes": 4,
    "blob_url": "https://github.com/smallrye/smallrye-reactive-messaging/blob/7d8f527bb54b3dbbfaedc55460a97a96f8f37c3f/documentation/src/main/doc/modules/connectors/partials/META-INF/connector/smallrye-kafka-outgoing.adoc",
    "raw_url": "https://github.com/smallrye/smallrye-reactive-messaging/raw/7d8f527bb54b3dbbfaedc55460a97a96f8f37c3f/documentation/src/main/doc/modules/connectors/partials/META-INF/connector/smallrye-kafka-outgoing.adoc",
    "contents_url": "https://api.github.com/repos/smallrye/smallrye-reactive-messaging/contents/documentation/src/main/doc/modules/connectors/partials/META-INF/connector/smallrye-kafka-outgoing.adoc?ref=7d8f527bb54b3dbbfaedc55460a97a96f8f37c3f",
    "patch": "@@ -81,6 +81,10 @@ Type: _long_ | false | `2000`\n \n Type: _boolean_ | false | `false`\n \n+| *kafka-configuration-name* | The name set in `javax.inject.Named` of a bean that provides the default Kafka consumer/producer (`Map<String, Object>`) configuration to use for this channel. The channel configuration can still override any attribute.\n+\n+Type: _string_ | false | \n+\n | *key* | A key to used when writing the record\n \n Type: _string_ | false | "
  },
  {
    "sha": "b45bc197df2f4787a75fe1d8ed15a9305fd35921",
    "filename": "documentation/src/main/doc/modules/kafka/pages/default-configuration.adoc",
    "status": "modified",
    "additions": 21,
    "deletions": 0,
    "changes": 21,
    "blob_url": "https://github.com/smallrye/smallrye-reactive-messaging/blob/7d8f527bb54b3dbbfaedc55460a97a96f8f37c3f/documentation/src/main/doc/modules/kafka/pages/default-configuration.adoc",
    "raw_url": "https://github.com/smallrye/smallrye-reactive-messaging/raw/7d8f527bb54b3dbbfaedc55460a97a96f8f37c3f/documentation/src/main/doc/modules/kafka/pages/default-configuration.adoc",
    "contents_url": "https://api.github.com/repos/smallrye/smallrye-reactive-messaging/contents/documentation/src/main/doc/modules/kafka/pages/default-configuration.adoc?ref=7d8f527bb54b3dbbfaedc55460a97a96f8f37c3f",
    "patch": "@@ -37,3 +37,24 @@ This previous example would extract all the configuration keys from MicroProfile\n ====\n Starting Quarkus 1.5, this map is automatically exposed.\n ====\n+\n+In addition to this default configuration, you can configure the name of the `Map` producer using the `kafka-configuration-name` attribute:\n+\n+[source, properties]\n+----\n+mp.messaging.incoming.my-channel.connector=smallrye-kafka\n+mp.messaging.incoming.my-channel.kafka-configuration-name=my-configuration\n+----\n+\n+In this case, the connector looks for the `Map` associated with the `my-configuration` name.\n+If `kafka-configuration-name` is not set, an optional lookup for a `Map` exposed with the channel name (`my-channel` in the previous example) is done.\n+\n+IMPORTANT: If `kafka-configuration-name` is set and no `Map` can be found, the deployment fails.\n+\n+Attribute values are resolved as follows:\n+\n+1. if the attribute is set directly on the channel configuration (`mp.messaging.incoming.my-channel.attribute=value`), this value is used\n+2. if the attribute is not set on the channel, the connector looks for a `Map` with the channel name or the configured `kafka-configuration-named` (if set) and the value is retrieved from that `Map`\n+3. If the resolved `Map` does not contain the value the default `Map` is used (exposed with the `default-kafka-broker` name)\n+\n+"
  },
  {
    "sha": "a1faefba4aa468b2936712f56cd45bd28e31e017",
    "filename": "smallrye-reactive-messaging-kafka/src/main/java/io/smallrye/reactive/messaging/kafka/KafkaConnector.java",
    "status": "modified",
    "additions": 68,
    "deletions": 15,
    "changes": 83,
    "blob_url": "https://github.com/smallrye/smallrye-reactive-messaging/blob/7d8f527bb54b3dbbfaedc55460a97a96f8f37c3f/smallrye-reactive-messaging-kafka/src/main/java/io/smallrye/reactive/messaging/kafka/KafkaConnector.java",
    "raw_url": "https://github.com/smallrye/smallrye-reactive-messaging/raw/7d8f527bb54b3dbbfaedc55460a97a96f8f37c3f/smallrye-reactive-messaging-kafka/src/main/java/io/smallrye/reactive/messaging/kafka/KafkaConnector.java",
    "contents_url": "https://api.github.com/repos/smallrye/smallrye-reactive-messaging/contents/smallrye-reactive-messaging-kafka/src/main/java/io/smallrye/reactive/messaging/kafka/KafkaConnector.java?ref=7d8f527bb54b3dbbfaedc55460a97a96f8f37c3f",
    "patch": "@@ -3,13 +3,7 @@\n import static io.smallrye.reactive.messaging.annotations.ConnectorAttribute.Direction.OUTGOING;\n import static io.smallrye.reactive.messaging.kafka.i18n.KafkaLogging.log;\n \n-import java.util.ArrayList;\n-import java.util.HashSet;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Optional;\n-import java.util.Set;\n-import java.util.UUID;\n+import java.util.*;\n import java.util.concurrent.CopyOnWriteArrayList;\n \n import javax.annotation.PostConstruct;\n@@ -19,6 +13,8 @@\n import javax.enterprise.event.Observes;\n import javax.enterprise.event.Reception;\n import javax.enterprise.inject.Instance;\n+import javax.enterprise.inject.UnsatisfiedResolutionException;\n+import javax.enterprise.inject.literal.NamedLiteral;\n import javax.inject.Inject;\n import javax.inject.Named;\n \n@@ -56,6 +52,7 @@\n @ConnectorAttribute(name = \"health-readiness-timeout\", type = \"long\", direction = Direction.INCOMING_AND_OUTGOING, description = \"During the readiness health check, the connector connects to the broker and retrieves the list of topics. This attribute specifies the maximum duration (in ms) for the retrieval. If exceeded, the channel is considered not-ready.\", defaultValue = \"2000\")\n @ConnectorAttribute(name = \"tracing-enabled\", type = \"boolean\", direction = Direction.INCOMING_AND_OUTGOING, description = \"Whether tracing is enabled (default) or disabled\", defaultValue = \"true\")\n @ConnectorAttribute(name = \"cloud-events\", type = \"boolean\", direction = Direction.INCOMING_AND_OUTGOING, description = \"Enables (default) or disables the Cloud Event support. If enabled on an _incoming_ channel, the connector analyzes the incoming records and try to create Cloud Event metadata. If enabled on an _outgoing_, the connector sends the outgoing messages as Cloud Event if the message includes Cloud Event Metadata.\", defaultValue = \"true\")\n+@ConnectorAttribute(name = \"kafka-configuration-name\", type = \"string\", direction = Direction.INCOMING_AND_OUTGOING, description = \"The name set in `javax.inject.Named` of a bean that provides the default Kafka consumer/producer (`Map<String, Object>`) configuration to use for this channel. The channel configuration can still override any attribute.\")\n \n @ConnectorAttribute(name = \"topics\", type = \"string\", direction = Direction.INCOMING, description = \"A comma-separating list of topics to be consumed. Cannot be used with the `topic` or `pattern` properties\")\n @ConnectorAttribute(name = \"pattern\", type = \"boolean\", direction = Direction.INCOMING, description = \"Indicate that the `topic` property is a regular expression. Must be used with the `topic` property. Cannot be used with the `topics` property\", defaultValue = \"false\")\n@@ -124,6 +121,9 @@\n     @Named(\"default-kafka-broker\")\n     Instance<Map<String, Object>> defaultKafkaConfiguration;\n \n+    @Inject\n+    Instance<Map<String, Object>> configurations;\n+\n     private Vertx vertx;\n \n     public void terminate(\n@@ -141,9 +141,13 @@ void init() {\n     @Override\n     public PublisherBuilder<? extends Message<?>> getPublisherBuilder(Config config) {\n         Config c = config;\n+        Map<String, Object> namedConfig = getNamedConfiguration(c);\n         if (!defaultKafkaConfiguration.isUnsatisfied()) {\n-            c = merge(config, defaultKafkaConfiguration.get());\n+            c = merge(config, namedConfig, defaultKafkaConfiguration.get());\n+        } else {\n+            c = merge(config, namedConfig, Collections.emptyMap());\n         }\n+\n         KafkaConnectorIncomingConfiguration ic = new KafkaConnectorIncomingConfiguration(c);\n         int partitions = ic.getPartitions();\n         if (partitions <= 0) {\n@@ -190,34 +194,75 @@ void init() {\n     @Override\n     public SubscriberBuilder<? extends Message<?>, Void> getSubscriberBuilder(Config config) {\n         Config c = config;\n+        Map<String, Object> namedConfig = getNamedConfiguration(c);\n         if (!defaultKafkaConfiguration.isUnsatisfied()) {\n-            c = merge(config, defaultKafkaConfiguration.get());\n+            c = merge(config, namedConfig, defaultKafkaConfiguration.get());\n+        } else {\n+            c = merge(config, namedConfig, Collections.emptyMap());\n         }\n+\n         KafkaConnectorOutgoingConfiguration oc = new KafkaConnectorOutgoingConfiguration(c);\n+\n         KafkaSink sink = new KafkaSink(vertx, oc, kafkaCDIEvents);\n         sinks.add(sink);\n         return sink.getSink();\n     }\n \n-    private Config merge(Config passedCfg, Map<String, Object> defaultKafkaCfg) {\n+    private Map<String, Object> getNamedConfiguration(Config c) {\n+        Optional<String> name = c.getOptionalValue(\"kafka-configuration-name\", String.class);\n+        Optional<String> channel = c.getOptionalValue(\"channel-name\", String.class);\n+        String cn = channel.orElse(null);\n+        Map<String, Object> namedConfig = Collections.emptyMap();\n+        if (name.isPresent()) {\n+            namedConfig = lookupForConfiguration(name.get(), false);\n+        } else if (cn != null) {\n+            namedConfig = lookupForConfiguration(cn, true);\n+        }\n+        return namedConfig;\n+    }\n+\n+    private Map<String, Object> lookupForConfiguration(String named, boolean optional) {\n+        Instance<Map<String, Object>> instance = configurations.select(NamedLiteral.of(named));\n+        if (instance.isUnsatisfied()) {\n+            if (!optional) {\n+                throw new UnsatisfiedResolutionException(\"Cannot find the configuration Kafka configuration: \" + named);\n+            } else {\n+                return Collections.emptyMap();\n+            }\n+        } else {\n+            return instance.get();\n+        }\n+    }\n+\n+    private Config merge(Config passedCfg, Map<String, Object> namedConfig,\n+            Map<String, Object> defaultKafkaCfg) {\n+        System.out.println(\"Merging config with (named) \" + namedConfig + \" and (default) \" + defaultKafkaCfg);\n         return new Config() {\n+\n             @SuppressWarnings(\"unchecked\")\n+            private <T> T getFromConfigProviders(String name) {\n+                T v = (T) namedConfig.get(name);\n+                if (v == null) {\n+                    v = (T) defaultKafkaCfg.get(name);\n+                }\n+                return v;\n+            }\n+\n             @Override\n             public <T> T getValue(String propertyName, Class<T> propertyType) {\n                 T passedCgfValue = passedCfg.getValue(propertyName, propertyType);\n                 if (passedCgfValue == null) {\n-                    return (T) defaultKafkaCfg.get(propertyName);\n+                    return getFromConfigProviders(propertyName);\n                 } else {\n                     return passedCgfValue;\n                 }\n             }\n \n-            @SuppressWarnings(\"unchecked\")\n             @Override\n             public <T> Optional<T> getOptionalValue(String propertyName, Class<T> propertyType) {\n                 Optional<T> passedCfgValue = passedCfg.getOptionalValue(propertyName, propertyType);\n                 if (!passedCfgValue.isPresent()) {\n-                    T defaultValue = (T) defaultKafkaCfg.get(propertyName);\n+                    T defaultValue = getFromConfigProviders(propertyName);\n                     return Optional.ofNullable(defaultValue);\n                 } else {\n                     return passedCfgValue;\n@@ -226,10 +271,18 @@ private Config merge(Config passedCfg, Map<String, Object> defaultKafkaCfg) {\n \n             @Override\n             public Iterable<String> getPropertyNames() {\n-                Iterable<String> names = passedCfg.getPropertyNames();\n                 Set<String> result = new HashSet<>();\n-                names.forEach(result::add);\n+\n+                // First global\n                 result.addAll(defaultKafkaCfg.keySet());\n+\n+                // Configured name\n+                result.addAll(namedConfig.keySet());\n+\n+                // Channel\n+                Iterable<String> names = passedCfg.getPropertyNames();\n+                names.forEach(result::add);\n+\n                 return result;\n             }\n "
  },
  {
    "sha": "d34ad79172413d009685780c72140910b21a206d",
    "filename": "smallrye-reactive-messaging-kafka/src/test/java/io/smallrye/reactive/messaging/kafka/DefaultConfigTest.java",
    "status": "modified",
    "additions": 462,
    "deletions": 4,
    "changes": 466,
    "blob_url": "https://github.com/smallrye/smallrye-reactive-messaging/blob/7d8f527bb54b3dbbfaedc55460a97a96f8f37c3f/smallrye-reactive-messaging-kafka/src/test/java/io/smallrye/reactive/messaging/kafka/DefaultConfigTest.java",
    "raw_url": "https://github.com/smallrye/smallrye-reactive-messaging/raw/7d8f527bb54b3dbbfaedc55460a97a96f8f37c3f/smallrye-reactive-messaging-kafka/src/test/java/io/smallrye/reactive/messaging/kafka/DefaultConfigTest.java",
    "contents_url": "https://api.github.com/repos/smallrye/smallrye-reactive-messaging/contents/smallrye-reactive-messaging-kafka/src/test/java/io/smallrye/reactive/messaging/kafka/DefaultConfigTest.java?ref=7d8f527bb54b3dbbfaedc55460a97a96f8f37c3f",
    "patch": "@@ -1,8 +1,7 @@\n package io.smallrye.reactive.messaging.kafka;\n \n import static io.smallrye.reactive.messaging.kafka.KafkaConnector.CONNECTOR_NAME;\n-import static org.assertj.core.api.Assertions.assertThat;\n-import static org.assertj.core.api.Assertions.entry;\n+import static org.assertj.core.api.Assertions.*;\n import static org.awaitility.Awaitility.await;\n \n import java.util.HashMap;\n@@ -27,6 +26,7 @@\n import org.eclipse.microprofile.reactive.messaging.Incoming;\n import org.eclipse.microprofile.reactive.messaging.Message;\n import org.eclipse.microprofile.reactive.messaging.Outgoing;\n+import org.jboss.weld.exceptions.DeploymentException;\n import org.junit.jupiter.api.Test;\n \n import io.smallrye.reactive.messaging.kafka.base.KafkaMapBasedConfig;\n@@ -37,14 +37,154 @@\n  */\n public class DefaultConfigTest extends KafkaTestBase {\n \n+    @Test\n+    public void testFromKafkaToAppToKafkaWithNamedConfig() {\n+        String topicOut = UUID.randomUUID().toString();\n+        String topicIn = UUID.randomUUID().toString();\n+        List<Map.Entry<String, String>> messages = new CopyOnWriteArrayList<>();\n+        usage.consumeStrings(topicOut, 10, 1, TimeUnit.MINUTES, null,\n+                (key, value) -> messages.add(entry(key, value)));\n+        runApplication(getKafkaConfigWithNamedConfig(topicOut, topicIn)\n+                .with(\"mp.messaging.incoming.source.kafka-configuration-name\", \"my-kafka-broker\")\n+                .with(\"mp.messaging.outgoing.kafka.kafka-configuration-name\", \"my-kafka-broker\"),\n+                MyAppProcessingDataWithNamedConfig.class);\n+\n+        AtomicInteger count = new AtomicInteger();\n+        usage.produceIntegers(10, null,\n+                () -> new ProducerRecord<>(topicIn, \"a-key\", count.getAndIncrement()));\n+\n+        await().until(() -> messages.size() >= 10);\n+        assertThat(messages).allSatisfy(entry -> {\n+            assertThat(entry.getKey()).isEqualTo(\"my-key\");\n+            assertThat(entry.getValue()).isNotNull();\n+        });\n+    }\n+\n+    @Test\n+    public void testFromKafkaToAppToKafkaWithNotMatchingNamedConfigOnIncoming() {\n+        String topicOut = UUID.randomUUID().toString();\n+        String topicIn = UUID.randomUUID().toString();\n+        try {\n+            runApplication(getKafkaConfigWithNamedConfig(topicOut, topicIn)\n+                    .with(\"mp.messaging.incoming.source.kafka-configuration-name\", \"my-kafka-broker-boom\")\n+                    .with(\"mp.messaging.outgoing.kafka.kafka-configuration-name\", \"my-kafka-broker\"),\n+                    MyAppProcessingDataWithNamedConfig.class);\n+            fail(\"Exception expected\");\n+        } catch (DeploymentException e) {\n+            assertThat(e).hasMessageContaining(\"my-kafka-broker-boom\");\n+        }\n+    }\n+\n+    @Test\n+    public void testFromKafkaToAppToKafkaWithNotMatchingNamedConfigOnOutgoing() {\n+        String topicOut = UUID.randomUUID().toString();\n+        String topicIn = UUID.randomUUID().toString();\n+        try {\n+            runApplication(getKafkaConfigWithNamedConfig(topicOut, topicIn)\n+                    .with(\"mp.messaging.incoming.source.kafka-configuration-name\", \"my-kafka-broker\")\n+                    .with(\"mp.messaging.outgoing.kafka.kafka-configuration-name\", \"my-kafka-broker-boom\"),\n+                    MyAppProcessingDataWithNamedConfig.class);\n+            fail(\"Exception expected\");\n+        } catch (DeploymentException e) {\n+            assertThat(e).hasMessageContaining(\"my-kafka-broker-boom\");\n+        }\n+    }\n+\n+    @Test\n+    public void testFromKafkaToAppToKafkaWithNamedConfigOnConnectorConfig() {\n+        String topicOut = UUID.randomUUID().toString();\n+        String topicIn = UUID.randomUUID().toString();\n+        List<Map.Entry<String, String>> messages = new CopyOnWriteArrayList<>();\n+        usage.consumeStrings(topicOut, 10, 1, TimeUnit.MINUTES, null,\n+                (key, value) -> messages.add(entry(key, value)));\n+        runApplication(getKafkaConfigWithNamedConfig(topicOut, topicIn)\n+                .with(\"mp.messaging.connector.smallrye-kafka.kafka-configuration-name\", \"my-kafka-broker\"),\n+                MyAppProcessingDataWithNamedConfig.class);\n+\n+        AtomicInteger count = new AtomicInteger();\n+        usage.produceIntegers(10, null,\n+                () -> new ProducerRecord<>(topicIn, \"a-key\", count.getAndIncrement()));\n+\n+        await().until(() -> messages.size() >= 10);\n+        assertThat(messages).allSatisfy(entry -> {\n+            assertThat(entry.getKey()).isEqualTo(\"my-key\");\n+            assertThat(entry.getValue()).isNotNull();\n+        });\n+    }\n+\n     @Test\n     public void testFromKafkaToAppToKafka() {\n         String topicOut = UUID.randomUUID().toString();\n         String topicIn = UUID.randomUUID().toString();\n         List<Map.Entry<String, String>> messages = new CopyOnWriteArrayList<>();\n         usage.consumeStrings(topicOut, 10, 1, TimeUnit.MINUTES, null,\n                 (key, value) -> messages.add(entry(key, value)));\n-        runApplication(getKafkaSinkConfigForMyAppProcessingData(topicOut, topicIn), MyAppProcessingData.class);\n+        runApplication(getKafkaConfigWithDefaultConfig(topicOut, topicIn), MyAppProcessingData.class);\n+\n+        AtomicInteger count = new AtomicInteger();\n+        usage.produceIntegers(10, null,\n+                () -> new ProducerRecord<>(topicIn, \"a-key\", count.getAndIncrement()));\n+\n+        await().until(() -> messages.size() >= 10);\n+        assertThat(messages).allSatisfy(entry -> {\n+            assertThat(entry.getKey()).isEqualTo(\"my-key\");\n+            assertThat(entry.getValue()).isNotNull();\n+        });\n+    }\n+\n+    @Test\n+    public void testFromKafkaToAppToKafkaWithNamedAndDefaultConfig() {\n+        String topicOut = UUID.randomUUID().toString();\n+        String topicIn = UUID.randomUUID().toString();\n+        List<Map.Entry<String, String>> messages = new CopyOnWriteArrayList<>();\n+        usage.consumeStrings(topicOut, 10, 1, TimeUnit.MINUTES, null,\n+                (key, value) -> messages.add(entry(key, value)));\n+        runApplication(getKafkaConfigWithNamedAndDefaultConfig(topicOut, topicIn)\n+                .with(\"mp.messaging.incoming.source.kafka-configuration-name\", \"my-kafka-broker\")\n+                .with(\"mp.messaging.outgoing.kafka.kafka-configuration-name\", \"my-kafka-broker\"),\n+                MyAppProcessingDataWithNamedAndDefaultConfig.class);\n+\n+        AtomicInteger count = new AtomicInteger();\n+        usage.produceIntegers(10, null,\n+                () -> new ProducerRecord<>(topicIn, \"a-key\", count.getAndIncrement()));\n+\n+        await().until(() -> messages.size() >= 10);\n+        assertThat(messages).allSatisfy(entry -> {\n+            assertThat(entry.getKey()).isEqualTo(\"my-key\");\n+            assertThat(entry.getValue()).isNotNull();\n+        });\n+    }\n+\n+    @Test\n+    public void testFromKafkaToAppToKafkaWithChannelAndDefaultConfig() {\n+        String topicOut = UUID.randomUUID().toString();\n+        String topicIn = UUID.randomUUID().toString();\n+        List<Map.Entry<String, String>> messages = new CopyOnWriteArrayList<>();\n+        usage.consumeStrings(topicOut, 10, 1, TimeUnit.MINUTES, null,\n+                (key, value) -> messages.add(entry(key, value)));\n+        runApplication(getKafkaConfigWithChannelAndDefaultConfig(topicOut, topicIn),\n+                MyAppProcessingDataWithChannelAndDefaultConfig.class);\n+\n+        AtomicInteger count = new AtomicInteger();\n+        usage.produceIntegers(10, null,\n+                () -> new ProducerRecord<>(topicIn, \"a-key\", count.getAndIncrement()));\n+\n+        await().until(() -> messages.size() >= 10);\n+        assertThat(messages).allSatisfy(entry -> {\n+            assertThat(entry.getKey()).isEqualTo(\"my-key\");\n+            assertThat(entry.getValue()).isNotNull();\n+        });\n+    }\n+\n+    @Test\n+    public void testFromKafkaToAppToKafkaWithChannel() {\n+        String topicOut = UUID.randomUUID().toString();\n+        String topicIn = UUID.randomUUID().toString();\n+        List<Map.Entry<String, String>> messages = new CopyOnWriteArrayList<>();\n+        usage.consumeStrings(topicOut, 10, 1, TimeUnit.MINUTES, null,\n+                (key, value) -> messages.add(entry(key, value)));\n+        runApplication(getKafkaConfigWithChannel(topicOut, topicIn),\n+                MyAppProcessingDataWithChannel.class);\n \n         AtomicInteger count = new AtomicInteger();\n         usage.produceIntegers(10, null,\n@@ -57,7 +197,7 @@ public void testFromKafkaToAppToKafka() {\n         });\n     }\n \n-    private KafkaMapBasedConfig getKafkaSinkConfigForMyAppProcessingData(String topicOut, String topicIn) {\n+    private KafkaMapBasedConfig getKafkaConfigWithDefaultConfig(String topicOut, String topicIn) {\n         KafkaMapBasedConfig.Builder builder = KafkaMapBasedConfig.builder();\n         builder.put(\"mp.messaging.outgoing.kafka.connector\", CONNECTOR_NAME);\n         builder.put(\"mp.messaging.outgoing.kafka.topic\", topicOut);\n@@ -76,6 +216,87 @@ private KafkaMapBasedConfig getKafkaSinkConfigForMyAppProcessingData(String topi\n         return builder.build();\n     }\n \n+    private KafkaMapBasedConfig getKafkaConfigWithNamedConfig(String topicOut, String topicIn) {\n+        KafkaMapBasedConfig.Builder builder = KafkaMapBasedConfig.builder();\n+        builder.put(\"mp.messaging.outgoing.kafka.connector\", CONNECTOR_NAME);\n+        builder.put(\"mp.messaging.outgoing.kafka.topic\", topicOut);\n+\n+        builder.put(\"mp.messaging.incoming.source.topic\", topicIn);\n+        builder.put(\"mp.messaging.incoming.source.graceful-shutdown\", false);\n+        builder.put(\"mp.messaging.incoming.source.connector\", CONNECTOR_NAME);\n+        builder.put(\"mp.messaging.incoming.source.auto.offset.reset\", \"earliest\");\n+        builder.put(\"mp.messaging.incoming.source.commit-strategy\", \"latest\");\n+\n+        builder.put(\"my-kafka-broker.bootstrap.servers\", getBootstrapServers());\n+        builder.put(\"my-kafka-broker.value.serializer\", StringSerializer.class.getName());\n+        builder.put(\"my-kafka-broker.value.deserializer\", IntegerDeserializer.class.getName());\n+        builder.put(\"my-kafka-broker.key.deserializer\", StringDeserializer.class.getName());\n+\n+        return builder.build();\n+    }\n+\n+    private KafkaMapBasedConfig getKafkaConfigWithNamedAndDefaultConfig(String topicOut, String topicIn) {\n+        KafkaMapBasedConfig.Builder builder = KafkaMapBasedConfig.builder();\n+        builder.put(\"mp.messaging.outgoing.kafka.connector\", CONNECTOR_NAME);\n+        builder.put(\"mp.messaging.outgoing.kafka.topic\", topicOut);\n+\n+        builder.put(\"mp.messaging.incoming.source.topic\", topicIn);\n+        builder.put(\"mp.messaging.incoming.source.graceful-shutdown\", false);\n+        builder.put(\"mp.messaging.incoming.source.connector\", CONNECTOR_NAME);\n+        builder.put(\"mp.messaging.incoming.source.auto.offset.reset\", \"earliest\");\n+        builder.put(\"mp.messaging.incoming.source.commit-strategy\", \"latest\");\n+\n+        builder.put(\"my-kafka-broker.bootstrap.servers\", getBootstrapServers());\n+        builder.put(\"my-kafka-broker.value.serializer\", StringSerializer.class.getName());\n+        builder.put(\"kafka.value.deserializer\", IntegerDeserializer.class.getName());\n+        builder.put(\"kafka.key.deserializer\", StringDeserializer.class.getName());\n+        // Overridden - illegal value on purpose\n+        builder.put(\"kafka.value.serializer\", Integer.class.getName());\n+\n+        return builder.build();\n+    }\n+\n+    private KafkaMapBasedConfig getKafkaConfigWithChannelAndDefaultConfig(String topicOut, String topicIn) {\n+        KafkaMapBasedConfig.Builder builder = KafkaMapBasedConfig.builder();\n+        builder.put(\"mp.messaging.outgoing.kafka.connector\", CONNECTOR_NAME);\n+        builder.put(\"mp.messaging.outgoing.kafka.topic\", topicOut);\n+\n+        builder.put(\"mp.messaging.incoming.source.topic\", topicIn);\n+        builder.put(\"mp.messaging.incoming.source.graceful-shutdown\", false);\n+        builder.put(\"mp.messaging.incoming.source.connector\", CONNECTOR_NAME);\n+        builder.put(\"mp.messaging.incoming.source.auto.offset.reset\", \"earliest\");\n+        builder.put(\"mp.messaging.incoming.source.commit-strategy\", \"latest\");\n+\n+        builder.put(\"kafka.bootstrap.servers\", getBootstrapServers());\n+        builder.put(\"kafka.value.serializer\", StringSerializer.class.getName());\n+        builder.put(\"source.value.deserializer\", IntegerDeserializer.class.getName());\n+        builder.put(\"kafka.key.deserializer\", StringDeserializer.class.getName());\n+        // Overridden - illegal value on purpose\n+        builder.put(\"kafka.value.deserializer\", Integer.class.getName());\n+\n+        return builder.build();\n+    }\n+\n+    private KafkaMapBasedConfig getKafkaConfigWithChannel(String topicOut, String topicIn) {\n+        KafkaMapBasedConfig.Builder builder = KafkaMapBasedConfig.builder();\n+        builder.put(\"mp.messaging.outgoing.my-kafka.connector\", CONNECTOR_NAME);\n+        builder.put(\"mp.messaging.outgoing.my-kafka.topic\", topicOut);\n+\n+        builder.put(\"mp.messaging.incoming.source.topic\", topicIn);\n+        builder.put(\"mp.messaging.incoming.source.graceful-shutdown\", false);\n+        builder.put(\"mp.messaging.incoming.source.connector\", CONNECTOR_NAME);\n+        builder.put(\"mp.messaging.incoming.source.auto.offset.reset\", \"earliest\");\n+        builder.put(\"mp.messaging.incoming.source.commit-strategy\", \"latest\");\n+\n+        builder.put(\"my-kafka.bootstrap.servers\", getBootstrapServers());\n+        builder.put(\"my-kafka.value.serializer\", StringSerializer.class.getName());\n+        builder.put(\"source.key.deserializer\", StringDeserializer.class.getName());\n+        builder.put(\"source.bootstrap.servers\", getBootstrapServers());\n+        builder.put(\"source.value.deserializer\", IntegerDeserializer.class.getName());\n+\n+        return builder.build();\n+    }\n+\n     @ApplicationScoped\n     public static class MyAppProcessingData {\n \n@@ -117,4 +338,241 @@ public String processPayload(int payload) {\n         }\n     }\n \n+    @ApplicationScoped\n+    public static class MyAppProcessingDataWithNamedConfig {\n+\n+        @Incoming(\"source\")\n+        @Outgoing(\"p1\")\n+        public Message<Integer> processMessage(Message<Integer> input) {\n+            return KafkaRecord.of(\"my-key\", input.getPayload());\n+        }\n+\n+        @Incoming(\"p1\")\n+        @Outgoing(\"kafka\")\n+        public String processPayload(int payload) {\n+            return Integer.toString(payload);\n+        }\n+\n+        @Inject\n+        Config config;\n+\n+        @Produces\n+        @ApplicationScoped\n+        @Named(\"my-kafka-broker\")\n+        public Map<String, Object> createKafkaRuntimeConfig() {\n+            Map<String, Object> properties = new HashMap<>();\n+\n+            StreamSupport\n+                    .stream(config.getPropertyNames().spliterator(), false)\n+                    .map(String::toLowerCase)\n+                    .filter(name -> name.startsWith(\"my-kafka-broker\"))\n+                    .distinct()\n+                    .sorted()\n+                    .forEach(name -> {\n+                        final String key = name.substring(\"my-kafka-broker\".length() + 1).toLowerCase().replaceAll(\"[^a-z0-9.]\",\n+                                \".\");\n+                        final String value = config.getOptionalValue(name, String.class).orElse(\"\");\n+                        properties.put(key, value);\n+                    });\n+            // Here to verify that passing a boolean does not trigger an error.\n+            properties.put(\"some-boolean\", true);\n+            return properties;\n+        }\n+    }\n+\n+    @ApplicationScoped\n+    public static class MyAppProcessingDataWithNamedAndDefaultConfig {\n+\n+        @Incoming(\"source\")\n+        @Outgoing(\"p1\")\n+        public Message<Integer> processMessage(Message<Integer> input) {\n+            return KafkaRecord.of(\"my-key\", input.getPayload());\n+        }\n+\n+        @Incoming(\"p1\")\n+        @Outgoing(\"kafka\")\n+        public String processPayload(int payload) {\n+            return Integer.toString(payload);\n+        }\n+\n+        @Inject\n+        Config config;\n+\n+        @Produces\n+        @ApplicationScoped\n+        @Named(\"my-kafka-broker\")\n+        public Map<String, Object> createKafkaRuntimeConfig() {\n+            Map<String, Object> properties = new HashMap<>();\n+\n+            StreamSupport\n+                    .stream(config.getPropertyNames().spliterator(), false)\n+                    .map(String::toLowerCase)\n+                    .filter(name -> name.startsWith(\"my-kafka-broker\"))\n+                    .distinct()\n+                    .sorted()\n+                    .forEach(name -> {\n+                        final String key = name.substring(\"my-kafka-broker\".length() + 1).toLowerCase().replaceAll(\"[^a-z0-9.]\",\n+                                \".\");\n+                        final String value = config.getOptionalValue(name, String.class).orElse(\"\");\n+                        properties.put(key, value);\n+                    });\n+            // Here to verify that passing a boolean does not trigger an error.\n+            properties.put(\"some-boolean\", true);\n+            return properties;\n+        }\n+\n+        @Produces\n+        @ApplicationScoped\n+        @Named(\"default-kafka-broker\")\n+        public Map<String, Object> createDefaultKafkaRuntimeConfig() {\n+            Map<String, Object> properties = new HashMap<>();\n+\n+            StreamSupport\n+                    .stream(config.getPropertyNames().spliterator(), false)\n+                    .map(String::toLowerCase)\n+                    .filter(name -> name.startsWith(\"kafka\"))\n+                    .distinct()\n+                    .sorted()\n+                    .forEach(name -> {\n+                        final String key = name.substring(\"kafka\".length() + 1).toLowerCase().replaceAll(\"[^a-z0-9.]\",\n+                                \".\");\n+                        final String value = config.getOptionalValue(name, String.class).orElse(\"\");\n+                        properties.put(key, value);\n+                    });\n+            // Here to verify that passing a boolean does not trigger an error.\n+            properties.put(\"some-boolean\", true);\n+            return properties;\n+        }\n+    }\n+\n+    @ApplicationScoped\n+    public static class MyAppProcessingDataWithChannelAndDefaultConfig {\n+\n+        @Incoming(\"source\")\n+        @Outgoing(\"p1\")\n+        public Message<Integer> processMessage(Message<Integer> input) {\n+            return KafkaRecord.of(\"my-key\", input.getPayload());\n+        }\n+\n+        @Incoming(\"p1\")\n+        @Outgoing(\"kafka\")\n+        public String processPayload(int payload) {\n+            return Integer.toString(payload);\n+        }\n+\n+        @Inject\n+        Config config;\n+\n+        @Produces\n+        @ApplicationScoped\n+        @Named(\"source\")\n+        public Map<String, Object> createChannelKafkaRuntimeConfig() {\n+            Map<String, Object> properties = new HashMap<>();\n+\n+            StreamSupport\n+                    .stream(config.getPropertyNames().spliterator(), false)\n+                    .map(String::toLowerCase)\n+                    .filter(name -> name.startsWith(\"source\"))\n+                    .distinct()\n+                    .sorted()\n+                    .forEach(name -> {\n+                        final String key = name.substring(\"source\".length() + 1).toLowerCase().replaceAll(\"[^a-z0-9.]\",\n+                                \".\");\n+                        final String value = config.getOptionalValue(name, String.class).orElse(\"\");\n+                        properties.put(key, value);\n+                    });\n+            // Here to verify that passing a boolean does not trigger an error.\n+            properties.put(\"some-boolean\", true);\n+            return properties;\n+        }\n+\n+        @Produces\n+        @ApplicationScoped\n+        @Named(\"default-kafka-broker\")\n+        public Map<String, Object> createDefaultKafkaRuntimeConfig() {\n+            Map<String, Object> properties = new HashMap<>();\n+\n+            StreamSupport\n+                    .stream(config.getPropertyNames().spliterator(), false)\n+                    .map(String::toLowerCase)\n+                    .filter(name -> name.startsWith(\"kafka\"))\n+                    .distinct()\n+                    .sorted()\n+                    .forEach(name -> {\n+                        final String key = name.substring(\"kafka\".length() + 1).toLowerCase().replaceAll(\"[^a-z0-9.]\",\n+                                \".\");\n+                        final String value = config.getOptionalValue(name, String.class).orElse(\"\");\n+                        properties.put(key, value);\n+                    });\n+            // Here to verify that passing a boolean does not trigger an error.\n+            properties.put(\"some-boolean\", true);\n+            return properties;\n+        }\n+    }\n+\n+    @ApplicationScoped\n+    public static class MyAppProcessingDataWithChannel {\n+\n+        @Incoming(\"source\")\n+        @Outgoing(\"p1\")\n+        public Message<Integer> processMessage(Message<Integer> input) {\n+            return KafkaRecord.of(\"my-key\", input.getPayload());\n+        }\n+\n+        @Incoming(\"p1\")\n+        @Outgoing(\"my-kafka\")\n+        public String processPayload(int payload) {\n+            return Integer.toString(payload);\n+        }\n+\n+        @Inject\n+        Config config;\n+\n+        @Produces\n+        @ApplicationScoped\n+        @Named(\"source\")\n+        public Map<String, Object> createChannelKafkaRuntimeConfig() {\n+            Map<String, Object> properties = new HashMap<>();\n+\n+            StreamSupport\n+                    .stream(config.getPropertyNames().spliterator(), false)\n+                    .map(String::toLowerCase)\n+                    .filter(name -> name.startsWith(\"source\"))\n+                    .distinct()\n+                    .sorted()\n+                    .forEach(name -> {\n+                        final String key = name.substring(\"source\".length() + 1).toLowerCase().replaceAll(\"[^a-z0-9.]\",\n+                                \".\");\n+                        final String value = config.getOptionalValue(name, String.class).orElse(\"\");\n+                        properties.put(key, value);\n+                    });\n+            // Here to verify that passing a boolean does not trigger an error.\n+            properties.put(\"some-boolean\", true);\n+            return properties;\n+        }\n+\n+        @Produces\n+        @ApplicationScoped\n+        @Named(\"my-kafka\")\n+        public Map<String, Object> createDefaultKafkaRuntimeConfig() {\n+            Map<String, Object> properties = new HashMap<>();\n+\n+            StreamSupport\n+                    .stream(config.getPropertyNames().spliterator(), false)\n+                    .map(String::toLowerCase)\n+                    .filter(name -> name.startsWith(\"my-kafka\"))\n+                    .distinct()\n+                    .sorted()\n+                    .forEach(name -> {\n+                        final String key = name.substring(\"my-kafka\".length() + 1).toLowerCase().replaceAll(\"[^a-z0-9.]\",\n+                                \".\");\n+                        final String value = config.getOptionalValue(name, String.class).orElse(\"\");\n+                        properties.put(key, value);\n+                    });\n+            // Here to verify that passing a boolean does not trigger an error.\n+            properties.put(\"some-boolean\", true);\n+            return properties;\n+        }\n+    }\n+\n }"
  },
  {
    "sha": "4ff371c7e445635b67193d7ae797d8098179d726",
    "filename": "smallrye-reactive-messaging-kafka/src/test/java/io/smallrye/reactive/messaging/kafka/KafkaSourceTest.java",
    "status": "modified",
    "additions": 2,
    "deletions": 0,
    "changes": 2,
    "blob_url": "https://github.com/smallrye/smallrye-reactive-messaging/blob/7d8f527bb54b3dbbfaedc55460a97a96f8f37c3f/smallrye-reactive-messaging-kafka/src/test/java/io/smallrye/reactive/messaging/kafka/KafkaSourceTest.java",
    "raw_url": "https://github.com/smallrye/smallrye-reactive-messaging/raw/7d8f527bb54b3dbbfaedc55460a97a96f8f37c3f/smallrye-reactive-messaging-kafka/src/test/java/io/smallrye/reactive/messaging/kafka/KafkaSourceTest.java",
    "contents_url": "https://api.github.com/repos/smallrye/smallrye-reactive-messaging/contents/smallrye-reactive-messaging-kafka/src/test/java/io/smallrye/reactive/messaging/kafka/KafkaSourceTest.java?ref=7d8f527bb54b3dbbfaedc55460a97a96f8f37c3f",
    "patch": "@@ -139,6 +139,7 @@ public void testBroadcast() {\n \n         connector = new KafkaConnector();\n         connector.executionHolder = new ExecutionHolder(vertx);\n+        connector.configurations = UnsatisfiedInstance.instance();\n         connector.defaultKafkaConfiguration = UnsatisfiedInstance.instance();\n         connector.consumerRebalanceListeners = UnsatisfiedInstance.instance();\n         connector.kafkaCDIEvents = testEvents;\n@@ -179,6 +180,7 @@ public void testBroadcastWithPartitions() {\n \n         connector = new KafkaConnector();\n         connector.executionHolder = new ExecutionHolder(vertx);\n+        connector.configurations = UnsatisfiedInstance.instance();\n         connector.defaultKafkaConfiguration = UnsatisfiedInstance.instance();\n         connector.consumerRebalanceListeners = UnsatisfiedInstance.instance();\n         connector.kafkaCDIEvents = new CountKafkaCdiEvents();"
  }
]
