[
  {
    "sha": "ea2f922c49b73782ea9db6477eda00450fb39d00",
    "filename": "pom.xml",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/folio-org/mod-inventory/blob/87eb03b5a3f0654e08a398fc3f22520c5a58dfaa/pom.xml",
    "raw_url": "https://github.com/folio-org/mod-inventory/raw/87eb03b5a3f0654e08a398fc3f22520c5a58dfaa/pom.xml",
    "contents_url": "https://api.github.com/repos/folio-org/mod-inventory/contents/pom.xml?ref=87eb03b5a3f0654e08a398fc3f22520c5a58dfaa",
    "patch": "@@ -116,7 +116,7 @@\n     <dependency>\n       <groupId>org.folio</groupId>\n       <artifactId>folio-kafka-wrapper</artifactId>\n-      <version>2.0.0</version>\n+      <version>2.1.0-SNAPSHOT</version>\n     </dependency>\n \n     <dependency>"
  },
  {
    "sha": "db99306c28e55d8df7b915f3704b6f525eb10e41",
    "filename": "src/main/java/org/folio/inventory/DataImportConsumerVerticle.java",
    "status": "modified",
    "additions": 16,
    "deletions": 1,
    "changes": 17,
    "blob_url": "https://github.com/folio-org/mod-inventory/blob/87eb03b5a3f0654e08a398fc3f22520c5a58dfaa/src/main/java/org/folio/inventory/DataImportConsumerVerticle.java",
    "raw_url": "https://github.com/folio-org/mod-inventory/raw/87eb03b5a3f0654e08a398fc3f22520c5a58dfaa/src/main/java/org/folio/inventory/DataImportConsumerVerticle.java",
    "contents_url": "https://api.github.com/repos/folio-org/mod-inventory/contents/src/main/java/org/folio/inventory/DataImportConsumerVerticle.java?ref=87eb03b5a3f0654e08a398fc3f22520c5a58dfaa",
    "patch": "@@ -6,6 +6,7 @@\n import io.vertx.core.Promise;\n import io.vertx.core.http.HttpClient;\n import io.vertx.core.json.JsonObject;\n+\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n import org.folio.DataImportEventTypes;\n@@ -17,6 +18,8 @@\n import org.folio.kafka.KafkaConsumerWrapper;\n import org.folio.kafka.KafkaTopicNameHelper;\n import org.folio.kafka.SubscriptionDefinition;\n+import org.folio.kafka.cache.KafkaInternalCache;\n+import org.folio.kafka.cache.util.CacheUtil;\n import org.folio.processing.events.EventManager;\n import org.folio.util.pubsub.PubSubClientUtils;\n \n@@ -51,6 +54,10 @@\n public class DataImportConsumerVerticle extends AbstractVerticle {\n \n   private static final Logger LOGGER = LogManager.getLogger(DataImportConsumerVerticle.class);\n+\n+  private static final long DELAY_TIME_BETWEEN_EVENTS_CLEANUP_VALUE_MILLIS = 3600000;\n+  private static final int EVENT_TIMEOUT_VALUE_HOURS = 3;\n+\n   private static final List<DataImportEventTypes> EVENT_TYPES = List.of(DI_SRS_MARC_BIB_RECORD_CREATED,\n     DI_SRS_MARC_BIB_RECORD_MODIFIED, DI_SRS_MARC_BIB_RECORD_MODIFIED_READY_FOR_POST_PROCESSING,\n     DI_SRS_MARC_BIB_RECORD_MATCHED, DI_SRS_MARC_BIB_RECORD_NOT_MATCHED,\n@@ -81,7 +88,13 @@ public void start(Promise<Void> startPromise) {\n \n     HttpClient client = vertx.createHttpClient();\n     Storage storage = Storage.basedUpon(vertx, config, client);\n-    DataImportKafkaHandler dataImportKafkaHandler = new DataImportKafkaHandler(vertx, storage, client);\n+\n+    KafkaInternalCache kafkaInternalCache = KafkaInternalCache.builder()\n+      .kafkaConfig(kafkaConfig)\n+      .build();\n+    kafkaInternalCache.initKafkaCache();\n+\n+    DataImportKafkaHandler dataImportKafkaHandler = new DataImportKafkaHandler(vertx, storage, client, kafkaInternalCache);\n \n     List<Future> futures = EVENT_TYPES.stream()\n       .map(eventType -> createKafkaConsumerWrapper(kafkaConfig, eventType, dataImportKafkaHandler))\n@@ -93,6 +106,8 @@ public void start(Promise<Void> startPromise) {\n         futures.forEach(future -> consumerWrappers.add((KafkaConsumerWrapper<String, String>) future.result()));\n         startPromise.complete();\n       });\n+\n+    CacheUtil.initCacheCleanupPeriodicTask(vertx, kafkaInternalCache, DELAY_TIME_BETWEEN_EVENTS_CLEANUP_VALUE_MILLIS, EVENT_TIMEOUT_VALUE_HOURS);\n   }\n \n   @Override"
  },
  {
    "sha": "03d7c07d06a6fc25b736328b237c8d632ef2386a",
    "filename": "src/main/java/org/folio/inventory/MarcBibInstanceHridSetConsumerVerticle.java",
    "status": "modified",
    "additions": 13,
    "deletions": 1,
    "changes": 14,
    "blob_url": "https://github.com/folio-org/mod-inventory/blob/87eb03b5a3f0654e08a398fc3f22520c5a58dfaa/src/main/java/org/folio/inventory/MarcBibInstanceHridSetConsumerVerticle.java",
    "raw_url": "https://github.com/folio-org/mod-inventory/raw/87eb03b5a3f0654e08a398fc3f22520c5a58dfaa/src/main/java/org/folio/inventory/MarcBibInstanceHridSetConsumerVerticle.java",
    "contents_url": "https://api.github.com/repos/folio-org/mod-inventory/contents/src/main/java/org/folio/inventory/MarcBibInstanceHridSetConsumerVerticle.java?ref=87eb03b5a3f0654e08a398fc3f22520c5a58dfaa",
    "patch": "@@ -4,6 +4,7 @@\n import io.vertx.core.Promise;\n import io.vertx.core.http.HttpClient;\n import io.vertx.core.json.JsonObject;\n+\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n import org.folio.inventory.dataimport.consumers.MarcBibInstanceHridSetKafkaHandler;\n@@ -14,6 +15,8 @@\n import org.folio.kafka.KafkaConsumerWrapper;\n import org.folio.kafka.KafkaTopicNameHelper;\n import org.folio.kafka.SubscriptionDefinition;\n+import org.folio.kafka.cache.KafkaInternalCache;\n+import org.folio.kafka.cache.util.CacheUtil;\n import org.folio.util.pubsub.PubSubClientUtils;\n \n import static org.folio.DataImportEventTypes.DI_SRS_MARC_BIB_INSTANCE_HRID_SET;\n@@ -27,6 +30,8 @@\n public class MarcBibInstanceHridSetConsumerVerticle extends AbstractVerticle {\n \n   private static final Logger LOGGER = LogManager.getLogger(MarcBibInstanceHridSetConsumerVerticle.class);\n+  private static final long DELAY_TIME_BETWEEN_EVENTS_CLEANUP_VALUE_MILLIS = 3600000;\n+  private static final int EVENT_TIMEOUT_VALUE_HOURS = 3;\n   private static final GlobalLoadSensor GLOBAL_LOAD_SENSOR = new GlobalLoadSensor();\n \n   private final int loadLimit = getLoadLimit();\n@@ -59,11 +64,18 @@ public void start(Promise<Void> startPromise) {\n     HttpClient client = vertx.createHttpClient();\n     Storage storage = Storage.basedUpon(vertx, config, client);\n     InstanceUpdateDelegate instanceUpdateDelegate = new InstanceUpdateDelegate(storage);\n-    MarcBibInstanceHridSetKafkaHandler marcBibInstanceHridSetKafkaHandler = new MarcBibInstanceHridSetKafkaHandler(instanceUpdateDelegate);\n+\n+    KafkaInternalCache kafkaInternalCache = KafkaInternalCache.builder()\n+      .kafkaConfig(kafkaConfig).build();\n+    kafkaInternalCache.initKafkaCache();\n+\n+    MarcBibInstanceHridSetKafkaHandler marcBibInstanceHridSetKafkaHandler = new MarcBibInstanceHridSetKafkaHandler(instanceUpdateDelegate, kafkaInternalCache);\n \n     consumerWrapper.start(marcBibInstanceHridSetKafkaHandler, PubSubClientUtils.constructModuleName())\n       .onSuccess(v -> startPromise.complete())\n       .onFailure(startPromise::fail);\n+\n+    CacheUtil.initCacheCleanupPeriodicTask(vertx, kafkaInternalCache, DELAY_TIME_BETWEEN_EVENTS_CLEANUP_VALUE_MILLIS, EVENT_TIMEOUT_VALUE_HOURS);\n   }\n \n   @Override"
  },
  {
    "sha": "6f1dca26a1ef36bd22ba6923a888d2d26c4a12b9",
    "filename": "src/main/java/org/folio/inventory/dataimport/consumers/DataImportKafkaHandler.java",
    "status": "modified",
    "additions": 25,
    "deletions": 14,
    "changes": 39,
    "blob_url": "https://github.com/folio-org/mod-inventory/blob/87eb03b5a3f0654e08a398fc3f22520c5a58dfaa/src/main/java/org/folio/inventory/dataimport/consumers/DataImportKafkaHandler.java",
    "raw_url": "https://github.com/folio-org/mod-inventory/raw/87eb03b5a3f0654e08a398fc3f22520c5a58dfaa/src/main/java/org/folio/inventory/dataimport/consumers/DataImportKafkaHandler.java",
    "contents_url": "https://api.github.com/repos/folio-org/mod-inventory/contents/src/main/java/org/folio/inventory/dataimport/consumers/DataImportKafkaHandler.java?ref=87eb03b5a3f0654e08a398fc3f22520c5a58dfaa",
    "patch": "@@ -1,15 +1,19 @@\n package org.folio.inventory.dataimport.consumers;\n \n import com.fasterxml.jackson.databind.ObjectMapper;\n+\n import io.vertx.core.Future;\n import io.vertx.core.Promise;\n import io.vertx.core.Vertx;\n import io.vertx.core.http.HttpClient;\n import io.vertx.core.json.JsonObject;\n+\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n+\n import io.vertx.ext.web.client.WebClient;\n import io.vertx.kafka.client.consumer.KafkaConsumerRecord;\n+\n import org.folio.DataImportEventPayload;\n import org.folio.dbschema.ObjectMapperTool;\n import org.folio.inventory.dataimport.HoldingWriterFactory;\n@@ -32,6 +36,7 @@\n import org.folio.inventory.dataimport.handlers.matching.loaders.ItemLoader;\n import org.folio.inventory.storage.Storage;\n import org.folio.kafka.AsyncRecordHandler;\n+import org.folio.kafka.cache.KafkaInternalCache;\n import org.folio.processing.events.EventManager;\n import org.folio.processing.events.utils.ZIPArchiver;\n import org.folio.processing.mapping.MappingManager;\n@@ -51,11 +56,13 @@\n \n   private static final Logger LOGGER = LogManager.getLogger(DataImportKafkaHandler.class);\n   private static final ObjectMapper OBJECT_MAPPER = ObjectMapperTool.getMapper();\n+  private KafkaInternalCache kafkaInternalCache;\n \n   private Vertx vertx;\n \n-  public DataImportKafkaHandler(Vertx vertx, Storage storage, HttpClient client) {\n+  public DataImportKafkaHandler(Vertx vertx, Storage storage, HttpClient client, KafkaInternalCache kafkaInternalCache) {\n     this.vertx = vertx;\n+    this.kafkaInternalCache = kafkaInternalCache;\n     registerDataImportProcessingHandlers(storage, client);\n   }\n \n@@ -64,23 +71,27 @@ public DataImportKafkaHandler(Vertx vertx, Storage storage, HttpClient client) {\n     try {\n       Promise<String> promise = Promise.promise();\n       Event event = OBJECT_MAPPER.readValue(record.value(), Event.class);\n-      DataImportEventPayload eventPayload = new JsonObject(ZIPArchiver.unzip(event.getEventPayload())).mapTo(DataImportEventPayload.class);\n-      LOGGER.info(format(\"Data import event payload has been received with event type: %s\", eventPayload.getEventType()));\n-\n-      EventManager.handleEvent(eventPayload).whenComplete((processedPayload, throwable) -> {\n-        if (throwable != null) {\n-          promise.fail(throwable);\n-        } else if (DI_ERROR.value().equals(processedPayload.getEventType())) {\n-          promise.fail(\"Failed to process data import event payload\");\n-        } else {\n-          promise.complete(record.key());\n-        }\n-      });\n-      return promise.future();\n+      if (!kafkaInternalCache.containsByKey(event.getId())) {\n+        kafkaInternalCache.putToCache(event.getId());\n+        DataImportEventPayload eventPayload = new JsonObject(ZIPArchiver.unzip(event.getEventPayload())).mapTo(DataImportEventPayload.class);\n+        LOGGER.info(format(\"Data import event payload has been received with event type: %s\", eventPayload.getEventType()));\n+\n+        EventManager.handleEvent(eventPayload).whenComplete((processedPayload, throwable) -> {\n+          if (throwable != null) {\n+            promise.fail(throwable);\n+          } else if (DI_ERROR.value().equals(processedPayload.getEventType())) {\n+            promise.fail(\"Failed to process data import event payload\");\n+          } else {\n+            promise.complete(record.key());\n+          }\n+        });\n+        return promise.future();\n+      }\n     } catch (IOException e) {\n       LOGGER.error(format(\"Failed to process data import kafka record from topic %s\", record.topic()), e);\n       return Future.failedFuture(e);\n     }\n+    return Future.succeededFuture();\n   }\n \n   private void registerDataImportProcessingHandlers(Storage storage, HttpClient client) {"
  },
  {
    "sha": "7e5310ae91984a262d993abf1e34b3c68c526fd4",
    "filename": "src/main/java/org/folio/inventory/dataimport/consumers/MarcBibInstanceHridSetKafkaHandler.java",
    "status": "modified",
    "additions": 31,
    "deletions": 20,
    "changes": 51,
    "blob_url": "https://github.com/folio-org/mod-inventory/blob/87eb03b5a3f0654e08a398fc3f22520c5a58dfaa/src/main/java/org/folio/inventory/dataimport/consumers/MarcBibInstanceHridSetKafkaHandler.java",
    "raw_url": "https://github.com/folio-org/mod-inventory/raw/87eb03b5a3f0654e08a398fc3f22520c5a58dfaa/src/main/java/org/folio/inventory/dataimport/consumers/MarcBibInstanceHridSetKafkaHandler.java",
    "contents_url": "https://api.github.com/repos/folio-org/mod-inventory/contents/src/main/java/org/folio/inventory/dataimport/consumers/MarcBibInstanceHridSetKafkaHandler.java?ref=87eb03b5a3f0654e08a398fc3f22520c5a58dfaa",
    "patch": "@@ -1,18 +1,23 @@\n package org.folio.inventory.dataimport.consumers;\n \n import com.fasterxml.jackson.databind.ObjectMapper;\n+\n import io.vertx.core.Future;\n import io.vertx.core.Promise;\n import io.vertx.core.json.JsonObject;\n+\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n+\n import io.vertx.kafka.client.consumer.KafkaConsumerRecord;\n+\n import org.folio.dbschema.ObjectMapperTool;\n import org.folio.inventory.common.Context;\n import org.folio.inventory.dataimport.handlers.actions.InstanceUpdateDelegate;\n import org.folio.inventory.dataimport.handlers.matching.util.EventHandlingUtil;\n import org.folio.kafka.AsyncRecordHandler;\n import org.folio.kafka.KafkaHeaderUtils;\n+import org.folio.kafka.cache.KafkaInternalCache;\n import org.folio.processing.events.utils.ZIPArchiver;\n import org.folio.rest.jaxrs.model.Event;\n import org.folio.rest.jaxrs.model.Record;\n@@ -36,41 +41,47 @@\n   private static final ObjectMapper OBJECT_MAPPER = ObjectMapperTool.getMapper();\n \n   private InstanceUpdateDelegate instanceUpdateDelegate;\n+  private KafkaInternalCache kafkaInternalCache;\n \n-  public MarcBibInstanceHridSetKafkaHandler(InstanceUpdateDelegate instanceUpdateDelegate) {\n+  public MarcBibInstanceHridSetKafkaHandler(InstanceUpdateDelegate instanceUpdateDelegate, KafkaInternalCache kafkaInternalCache) {\n     this.instanceUpdateDelegate = instanceUpdateDelegate;\n+    this.kafkaInternalCache = kafkaInternalCache;\n   }\n \n   @Override\n   public Future<String> handle(KafkaConsumerRecord<String, String> record) {\n     try {\n       Promise<String> promise = Promise.promise();\n       Event event = OBJECT_MAPPER.readValue(record.value(), Event.class);\n-      HashMap<String, String> eventPayload = OBJECT_MAPPER.readValue(ZIPArchiver.unzip(event.getEventPayload()), HashMap.class);\n-      LOGGER.info(format(\"Event payload has been received with event type: %s\", event.getEventType()));\n+      if (!kafkaInternalCache.containsByKey(event.getId())) {\n+        kafkaInternalCache.putToCache(event.getId());\n+        HashMap<String, String> eventPayload = OBJECT_MAPPER.readValue(ZIPArchiver.unzip(event.getEventPayload()), HashMap.class);\n+        LOGGER.info(format(\"Event payload has been received with event type: %s\", event.getEventType()));\n \n-      if (isAnyEmpty(eventPayload.get(MARC_KEY), eventPayload.get(MAPPING_RULES_KEY), eventPayload.get(MAPPING_PARAMS_KEY))) {\n-        String message = \"Event payload does not contain required data to update Instance\";\n-        LOGGER.error(message);\n-        return Future.failedFuture(message);\n-      }\n+        if (isAnyEmpty(eventPayload.get(MARC_KEY), eventPayload.get(MAPPING_RULES_KEY), eventPayload.get(MAPPING_PARAMS_KEY))) {\n+          String message = \"Event payload does not contain required data to update Instance\";\n+          LOGGER.error(message);\n+          return Future.failedFuture(message);\n+        }\n \n-      Map<String, String> headersMap = KafkaHeaderUtils.kafkaHeadersToMap(record.headers());\n-      Context context = EventHandlingUtil.constructContext(headersMap.get(OKAPI_TENANT_HEADER), headersMap.get(OKAPI_TOKEN_HEADER), headersMap.get(OKAPI_URL_HEADER));\n-      Record marcRecord = new JsonObject(eventPayload.get(MARC.value())).mapTo(Record.class);\n+        Map<String, String> headersMap = KafkaHeaderUtils.kafkaHeadersToMap(record.headers());\n+        Context context = EventHandlingUtil.constructContext(headersMap.get(OKAPI_TENANT_HEADER), headersMap.get(OKAPI_TOKEN_HEADER), headersMap.get(OKAPI_URL_HEADER));\n+        Record marcRecord = new JsonObject(eventPayload.get(MARC.value())).mapTo(Record.class);\n \n-      instanceUpdateDelegate.handle(eventPayload, marcRecord, context).onComplete(ar -> {\n-        if (ar.succeeded()) {\n-          promise.complete(record.key());\n-        } else {\n-          LOGGER.error(\"Failed to process data import event payload\", ar.cause());\n-          promise.fail(ar.cause());\n-        }\n-      });\n-      return promise.future();\n+        instanceUpdateDelegate.handle(eventPayload, marcRecord, context).onComplete(ar -> {\n+          if (ar.succeeded()) {\n+            promise.complete(record.key());\n+          } else {\n+            LOGGER.error(\"Failed to process data import event payload\", ar.cause());\n+            promise.fail(ar.cause());\n+          }\n+        });\n+        return promise.future();\n+      }\n     } catch (Exception e) {\n       LOGGER.error(format(\"Failed to process data import kafka record from topic %s\", record.topic()), e);\n       return Future.failedFuture(e);\n     }\n+    return Future.succeededFuture();\n   }\n }"
  },
  {
    "sha": "c9817a4a889dcc8e47695a2ab8900153a75825a9",
    "filename": "src/test/java/org/folio/inventory/dataimport/consumers/DataImportConsumerVerticleTest.java",
    "status": "modified",
    "additions": 9,
    "deletions": 1,
    "changes": 10,
    "blob_url": "https://github.com/folio-org/mod-inventory/blob/87eb03b5a3f0654e08a398fc3f22520c5a58dfaa/src/test/java/org/folio/inventory/dataimport/consumers/DataImportConsumerVerticleTest.java",
    "raw_url": "https://github.com/folio-org/mod-inventory/raw/87eb03b5a3f0654e08a398fc3f22520c5a58dfaa/src/test/java/org/folio/inventory/dataimport/consumers/DataImportConsumerVerticleTest.java",
    "contents_url": "https://api.github.com/repos/folio-org/mod-inventory/contents/src/test/java/org/folio/inventory/dataimport/consumers/DataImportConsumerVerticleTest.java?ref=87eb03b5a3f0654e08a398fc3f22520c5a58dfaa",
    "patch": "@@ -18,6 +18,7 @@\n import org.folio.inventory.DataImportConsumerVerticle;\n import org.folio.kafka.KafkaConfig;\n import org.folio.kafka.KafkaTopicNameHelper;\n+import org.folio.kafka.cache.KafkaInternalCache;\n import org.folio.processing.events.EventManager;\n import org.folio.processing.events.services.handler.EventHandler;\n import org.folio.processing.events.utils.ZIPArchiver;\n@@ -30,6 +31,7 @@\n import org.junit.Test;\n import org.junit.runner.RunWith;\n import org.mockito.Mock;\n+import org.mockito.Mockito;\n import org.mockito.MockitoAnnotations;\n \n import java.io.IOException;\n@@ -72,6 +74,10 @@\n   @Mock\n   private EventHandler mockedEventHandler;\n \n+  @Mock\n+  private KafkaInternalCache kafkaInternalCache;\n+\n+\n   private JobProfile jobProfile = new JobProfile()\n     .withId(UUID.randomUUID().toString())\n     .withName(\"Create instance\")\n@@ -152,10 +158,12 @@ public void shouldSendEventWithProcessedEventPayloadWhenProcessingCoreHandlerSuc\n       .withProfileSnapshot(profileSnapshotWrapper);\n \n     String topic = KafkaTopicNameHelper.formatTopicName(KAFKA_ENV_NAME, getDefaultNameSpace(), TENANT_ID, dataImportEventPayload.getEventType());\n-    Event event = new Event().withEventPayload(ZIPArchiver.zip(Json.encode(dataImportEventPayload)));\n+    Event event = new Event().withId(\"01\").withEventPayload(ZIPArchiver.zip(Json.encode(dataImportEventPayload)));\n     KeyValue<String, String> record = new KeyValue<>(\"test-key\", Json.encode(event));\n     SendKeyValues<String, String> request = SendKeyValues.to(topic, Collections.singletonList(record)).useDefaults();\n \n+    Mockito.when(kafkaInternalCache.containsByKey(\"01\")).thenReturn(false);\n+\n     // when\n     cluster.send(request);\n "
  },
  {
    "sha": "72c3e6e6ab51dd2cc9002a3ce75f86f262dd09a5",
    "filename": "src/test/java/org/folio/inventory/dataimport/consumers/DataImportKafkaHandlerTest.java",
    "status": "modified",
    "additions": 49,
    "deletions": 3,
    "changes": 52,
    "blob_url": "https://github.com/folio-org/mod-inventory/blob/87eb03b5a3f0654e08a398fc3f22520c5a58dfaa/src/test/java/org/folio/inventory/dataimport/consumers/DataImportKafkaHandlerTest.java",
    "raw_url": "https://github.com/folio-org/mod-inventory/raw/87eb03b5a3f0654e08a398fc3f22520c5a58dfaa/src/test/java/org/folio/inventory/dataimport/consumers/DataImportKafkaHandlerTest.java",
    "contents_url": "https://api.github.com/repos/folio-org/mod-inventory/contents/src/test/java/org/folio/inventory/dataimport/consumers/DataImportKafkaHandlerTest.java?ref=87eb03b5a3f0654e08a398fc3f22520c5a58dfaa",
    "patch": "@@ -16,6 +16,7 @@\n import org.folio.MappingProfile;\n import org.folio.inventory.storage.Storage;\n import org.folio.kafka.KafkaConfig;\n+import org.folio.kafka.cache.KafkaInternalCache;\n import org.folio.processing.events.EventManager;\n import org.folio.processing.events.services.handler.EventHandler;\n import org.folio.processing.events.utils.ZIPArchiver;\n@@ -26,6 +27,7 @@\n import org.junit.Test;\n import org.junit.runner.RunWith;\n import org.mockito.Mock;\n+import org.mockito.Mockito;\n import org.mockito.MockitoAnnotations;\n \n import java.io.IOException;\n@@ -44,6 +46,7 @@\n import static org.folio.rest.jaxrs.model.ProfileSnapshotWrapper.ContentType.JOB_PROFILE;\n import static org.folio.rest.jaxrs.model.ProfileSnapshotWrapper.ContentType.MAPPING_PROFILE;\n import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.ArgumentMatchers.anyString;\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.when;\n \n@@ -57,6 +60,10 @@\n \n   @Mock\n   private Storage mockedStorage;\n+\n+  @Mock\n+  private KafkaInternalCache kafkaInternalCache;\n+\n   @Mock\n   private KafkaConsumerRecord<String, String> kafkaRecord;\n \n@@ -106,7 +113,7 @@ public void setUp() {\n       .build();\n \n     HttpClient client = vertx.createHttpClient();\n-    dataImportKafkaHandler = new DataImportKafkaHandler(vertx, mockedStorage, client);\n+    dataImportKafkaHandler = new DataImportKafkaHandler(vertx, mockedStorage, client, kafkaInternalCache);\n     EventManager.clearEventHandlers();\n     EventManager.registerKafkaEventPublisher(kafkaConfig, vertx, 1);\n   }\n@@ -122,7 +129,7 @@ public void shouldReturnSucceededFutureWhenProcessingCoreHandlerSucceeded(TestCo\n       .withContext(new HashMap<>())\n       .withProfileSnapshot(profileSnapshotWrapper);\n \n-    Event event = new Event().withEventPayload(ZIPArchiver.zip(Json.encode(dataImportEventPayload)));\n+    Event event = new Event().withId(\"01\").withEventPayload(ZIPArchiver.zip(Json.encode(dataImportEventPayload)));\n     String expectedKafkaRecordKey = \"test_key\";\n     when(kafkaRecord.key()).thenReturn(expectedKafkaRecordKey);\n     when(kafkaRecord.value()).thenReturn(Json.encode(event));\n@@ -133,6 +140,8 @@ public void shouldReturnSucceededFutureWhenProcessingCoreHandlerSucceeded(TestCo\n       .thenReturn(CompletableFuture.completedFuture(new DataImportEventPayload()));\n     EventManager.registerEventHandler(mockedEventHandler);\n \n+    Mockito.when(kafkaInternalCache.containsByKey(\"01\")).thenReturn(false);\n+\n     // when\n     Future<String> future = dataImportKafkaHandler.handle(kafkaRecord);\n \n@@ -156,7 +165,7 @@ public void shouldReturnFailedFutureWhenProcessingCoreHandlerFailed(TestContext\n       .withContext(new HashMap<>())\n       .withProfileSnapshot(profileSnapshotWrapper);\n \n-    Event event = new Event().withEventPayload(ZIPArchiver.zip(Json.encode(dataImportEventPayload)));\n+    Event event = new Event().withId(\"01\").withEventPayload(ZIPArchiver.zip(Json.encode(dataImportEventPayload)));\n     when(kafkaRecord.value()).thenReturn(Json.encode(event));\n \n     EventHandler mockedEventHandler = mock(EventHandler.class);\n@@ -165,6 +174,8 @@ public void shouldReturnFailedFutureWhenProcessingCoreHandlerFailed(TestContext\n       .thenReturn(CompletableFuture.failedFuture(new RuntimeException()));\n     EventManager.registerEventHandler(mockedEventHandler);\n \n+    Mockito.when(kafkaInternalCache.containsByKey(\"01\")).thenReturn(false);\n+\n     // when\n     Future<String> future = dataImportKafkaHandler.handle(kafkaRecord);\n \n@@ -175,5 +186,40 @@ public void shouldReturnFailedFutureWhenProcessingCoreHandlerFailed(TestContext\n     });\n   }\n \n+  @Test\n+  public void shouldNotHandleIfCacheAlreadyContainsThisEvent(TestContext context) throws IOException {\n+    // given\n+    Async async = context.async();\n+    DataImportEventPayload dataImportEventPayload = new DataImportEventPayload()\n+      .withTenant(TENANT_ID)\n+      .withOkapiUrl(\"localhost\")\n+      .withToken(\"test-token\")\n+      .withContext(new HashMap<>())\n+      .withProfileSnapshot(profileSnapshotWrapper);\n+\n+    Event event = new Event().withId(\"01\").withEventPayload(ZIPArchiver.zip(Json.encode(dataImportEventPayload)));\n+    String expectedKafkaRecordKey = \"test_key\";\n+    when(kafkaRecord.key()).thenReturn(expectedKafkaRecordKey);\n+    when(kafkaRecord.value()).thenReturn(Json.encode(event));\n+\n+    EventHandler mockedEventHandler = mock(EventHandler.class);\n+    when(mockedEventHandler.isEligible(any(DataImportEventPayload.class))).thenReturn(true);\n+    when(mockedEventHandler.handle(any(DataImportEventPayload.class)))\n+      .thenReturn(CompletableFuture.completedFuture(new DataImportEventPayload()));\n+    EventManager.registerEventHandler(mockedEventHandler);\n+\n+    Mockito.when(kafkaInternalCache.containsByKey(\"01\")).thenReturn(true);\n+\n+    // when\n+    Future<String> future = dataImportKafkaHandler.handle(kafkaRecord);\n+\n+    // then\n+    future.onComplete(ar -> {\n+      context.assertTrue(ar.succeeded());\n+      context.assertNotEquals(expectedKafkaRecordKey, ar.result());\n+      async.complete();\n+    });\n+  }\n+\n }\n "
  },
  {
    "sha": "e454c9d1ea781424d0e3e55f3688820a5b512909",
    "filename": "src/test/java/org/folio/inventory/dataimport/consumers/MarcBibInstanceHridSetKafkaHandlerTest.java",
    "status": "modified",
    "additions": 40,
    "deletions": 4,
    "changes": 44,
    "blob_url": "https://github.com/folio-org/mod-inventory/blob/87eb03b5a3f0654e08a398fc3f22520c5a58dfaa/src/test/java/org/folio/inventory/dataimport/consumers/MarcBibInstanceHridSetKafkaHandlerTest.java",
    "raw_url": "https://github.com/folio-org/mod-inventory/raw/87eb03b5a3f0654e08a398fc3f22520c5a58dfaa/src/test/java/org/folio/inventory/dataimport/consumers/MarcBibInstanceHridSetKafkaHandlerTest.java",
    "contents_url": "https://api.github.com/repos/folio-org/mod-inventory/contents/src/test/java/org/folio/inventory/dataimport/consumers/MarcBibInstanceHridSetKafkaHandlerTest.java?ref=87eb03b5a3f0654e08a398fc3f22520c5a58dfaa",
    "patch": "@@ -17,6 +17,7 @@\n import org.folio.inventory.domain.instances.InstanceCollection;\n import org.folio.inventory.storage.Storage;\n import org.folio.inventory.support.InstanceUtil;\n+import org.folio.kafka.cache.KafkaInternalCache;\n import org.folio.processing.events.utils.ZIPArchiver;\n import org.folio.rest.jaxrs.model.Event;\n import org.folio.rest.jaxrs.model.MappingDetail;\n@@ -58,6 +59,8 @@\n   private InstanceCollection mockedInstanceCollection;\n   @Mock\n   private KafkaConsumerRecord<String, String> kafkaRecord;\n+  @Mock\n+  private KafkaInternalCache kafkaInternalCache;\n \n   private ActionProfile actionProfile = new ActionProfile()\n     .withId(UUID.randomUUID().toString())\n@@ -110,7 +113,7 @@ public void setUp() throws IOException {\n       return null;\n     }).when(mockedInstanceCollection).update(any(Instance.class), any(Consumer.class), any(Consumer.class));\n \n-    marcBibInstanceHridSetKafkaHandler = new MarcBibInstanceHridSetKafkaHandler(new InstanceUpdateDelegate(mockedStorage));\n+    marcBibInstanceHridSetKafkaHandler = new MarcBibInstanceHridSetKafkaHandler(new InstanceUpdateDelegate(mockedStorage), kafkaInternalCache);\n   }\n \n   @Test\n@@ -122,11 +125,13 @@ public void shouldReturnSucceededFutureWithObtainedRecordKey(TestContext context\n     payload.put(\"MAPPING_RULES\", mappingRules.encode());\n     payload.put(\"MAPPING_PARAMS\", new JsonObject().encode());\n \n-    Event event = new Event().withEventPayload(ZIPArchiver.zip(Json.encode(payload)));\n+    Event event = new Event().withId(\"01\").withEventPayload(ZIPArchiver.zip(Json.encode(payload)));\n     String expectedKafkaRecordKey = \"test_key\";\n     when(kafkaRecord.key()).thenReturn(expectedKafkaRecordKey);\n     when(kafkaRecord.value()).thenReturn(Json.encode(event));\n \n+    Mockito.when(kafkaInternalCache.containsByKey(\"01\")).thenReturn(false);\n+\n     // when\n     Future<String> future = marcBibInstanceHridSetKafkaHandler.handle(kafkaRecord);\n \n@@ -146,9 +151,11 @@ public void shouldReturnFailedFutureWhenPayloadHasNoMarcRecord(TestContext conte\n     payload.put(\"MAPPING_RULES\", mappingRules.encode());\n     payload.put(\"MAPPING_PARAMS\", new JsonObject().encode());\n \n-    Event event = new Event().withEventPayload(ZIPArchiver.zip(Json.encode(payload)));\n+    Event event = new Event().withId(\"01\").withEventPayload(ZIPArchiver.zip(Json.encode(payload)));\n     when(kafkaRecord.value()).thenReturn(Json.encode(event));\n \n+    Mockito.when(kafkaInternalCache.containsByKey(\"01\")).thenReturn(false);\n+\n     // when\n     Future<String> future = marcBibInstanceHridSetKafkaHandler.handle(kafkaRecord);\n \n@@ -163,9 +170,11 @@ public void shouldReturnFailedFutureWhenPayloadHasNoMarcRecord(TestContext conte\n   public void shouldReturnFailedFutureWhenPayloadCanNotBeMapped(TestContext context) {\n     // given\n     Async async = context.async();\n-    Event event = new Event().withEventPayload(null);\n+    Event event = new Event().withId(\"01\").withEventPayload(null);\n     when(kafkaRecord.value()).thenReturn(Json.encode(event));\n \n+    Mockito.when(kafkaInternalCache.containsByKey(\"01\")).thenReturn(false);\n+\n     // when\n     Future<String> future = marcBibInstanceHridSetKafkaHandler.handle(kafkaRecord);\n \n@@ -175,4 +184,31 @@ public void shouldReturnFailedFutureWhenPayloadCanNotBeMapped(TestContext contex\n       async.complete();\n     });\n   }\n+\n+  @Test\n+  public void shouldNotHandleIfCacheAlreadyContainsThisEvent(TestContext context) throws IOException {\n+    // given\n+    Async async = context.async();\n+    Map<String, String> payload = new HashMap<>();\n+    payload.put(Record.RecordType.MARC.value(), Json.encode(record));\n+    payload.put(\"MAPPING_RULES\", mappingRules.encode());\n+    payload.put(\"MAPPING_PARAMS\", new JsonObject().encode());\n+\n+    Event event = new Event().withId(\"01\").withEventPayload(ZIPArchiver.zip(Json.encode(payload)));\n+    String expectedKafkaRecordKey = \"test_key\";\n+    when(kafkaRecord.key()).thenReturn(expectedKafkaRecordKey);\n+    when(kafkaRecord.value()).thenReturn(Json.encode(event));\n+\n+    Mockito.when(kafkaInternalCache.containsByKey(\"01\")).thenReturn(true);\n+\n+    // when\n+    Future<String> future = marcBibInstanceHridSetKafkaHandler.handle(kafkaRecord);\n+\n+    // then\n+    future.onComplete(ar -> {\n+      context.assertTrue(ar.succeeded());\n+      context.assertNotEquals(expectedKafkaRecordKey, ar.result());\n+      async.complete();\n+    });\n+  }\n }"
  }
]
