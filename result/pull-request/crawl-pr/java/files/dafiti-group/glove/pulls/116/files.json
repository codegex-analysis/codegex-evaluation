[
  {
    "sha": "fd0f8cd7c609206c8c92d5dcfec08a43041a3957",
    "filename": "README.md",
    "status": "modified",
    "additions": 12,
    "deletions": 12,
    "changes": 24,
    "blob_url": "https://github.com/dafiti-group/glove/blob/a48f645baddccbb151b22a180658af1f8d3689c9/README.md",
    "raw_url": "https://github.com/dafiti-group/glove/raw/a48f645baddccbb151b22a180658af1f8d3689c9/README.md",
    "contents_url": "https://api.github.com/repos/dafiti-group/glove/contents/README.md?ref=a48f645baddccbb151b22a180658af1f8d3689c9",
    "patch": "@@ -318,7 +318,7 @@ O metadado possui os seguintes atributos obrigatórios:\n | Tecnologia         | Parâmetro                  | Descrição                                                                                                                                                                                                                                                                                                                                         |\n |--------------------|----------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n |                    | MODULE                     | Identifica o módulo que será utilizado                                                                                                                                                                                                                                                                                                            |\n-|SPECTRUM| ALLOW_DUPLICATED           | Identifica se campos com custom primary key duplicadas serão inseridas na tabela.                                                                                                                                                                                                                                                                 |\n+| SPECTRUM\t\t\t | ALLOW_DUPLICATED           | Identifica se campos com custom primary key duplicadas serão inseridas na tabela.                                                                                                                                                                                                                                                                 |\n | SPECTRUM           | ALLOW_RECREATE             | Identifica se a tabela destino pode ser recriada quando ocorre alteração da estrutura da tabela origem.                                                                                                                                                                                                                                           |\n |                    | CONNECTION_NAME            | Identifica o nome de uma conexão configurada no arquivo de conexões. O arquivo de conexões pode ser encontrado em ~/.kettle/connections.properties                                                                                                                                                                                                |\n |                    | CUSTOM_PRIMARY_KEY         | Identifica a chave primária da tabela origem. Este parâmetro deve ser informado apenas se a tabela de origem não possuir chave primária. Caso contrário o GLOVE fará o preenchimento automaticamente.                                                                                                                                             |\n@@ -328,24 +328,24 @@ O metadado possui os seguintes atributos obrigatórios:\n |                    | DELTA_VALUE                | Identifica o valor do delta.                                                                                                                                                                                                                                                                                                                      |\n | REDSHIFT           | DISTKEY                    | Identifica a chave de distribuição da tabela. (http://docs.aws.amazon.com/pt_br/redshift/latest/dg/c_best-practices-best-dist-key.html)                                                                                                                                                                                                           |\n | REDSHIFT           | DISTSTYLE                  | Identifica o estilo de distribuição da tabela. (http://docs.aws.amazon.com/pt_br/redshift/latest/dg/c_best-practices-sort-key.html)                                                                                                                                                                                                               |\n-|SPECTRUM| EXPORT_BUCKET_DEFAULT      | Identifica o bucket para exportação de dados. Quando o export é originado de uma named query com escopo FULL, o nome do arquivo de saída pode ser incluído. Caso seja originado de uma named query com escopo PARTITIONED, somente o diretório de saída deve ser informado, uma vez que pode ser gerado um ou mais arquivos no mesmo caminho.                                                                                                                                                                                                                                                                                                     |\n+| SPECTRUM\t\t\t | EXPORT_BUCKET_DEFAULT      | Identifica um ou mais buckets, separados por vírgula, para exportação de dados. Quando o export é originado de uma named query com escopo FULL, o nome do arquivo de saída pode ser incluído. Caso seja originado de uma named query com escopo PARTITIONED, somente o diretório de saída deve ser informado, uma vez que seja gerado um arquivo para cada partição identificada.                                                                                                                                                                                                                                                                                                     |\n |                    | GENERIC_PARAMETER          | Identifica os parâmetros que serão passados para uma named query. Parâmetros de data podem utilizar macros (#), no seguinte formato: <parameter>:#D-1, <parameter>:#D-1,yyyyMMdd                                                                                                                                                                  |\n-|SPECTRUM| IS_EXPORT                  | Identifica se será gerada exportação do resultset para o bucket informado no parâmetro STORAGE_BUCKET.                                                                                                                                                                                                                                                |\n-|SPECTRUM| ONLY_EXPORT                | Identifica se o processo é apenas de exportação, sem que nenhuma tabela seja materializada.                                                                                                                                                                                                                                                       |\n+| SPECTRUM\t\t\t | IS_EXPORT                  | Identifica se será gerada exportação do resultset para o bucket informado no parâmetro STORAGE_BUCKET.                                                                                                                                                                                                                                                |\n+| SPECTRUM\t\t\t | ONLY_EXPORT                | Identifica se o processo é apenas de exportação, sem que nenhuma tabela seja materializada.                                                                                                                                                                                                                                                       |\n |                    | IS_RECREATE                | Identifica se a tabela de destino deve ser recriada e todos os dados recarregados. Sempre que o IS_RECREATE é utilizado, uma cópia dos dados da tabela é copiada para o bucket de disaster recovery. [0: Inativo, 1: Ativo], 0 é o padrão                                                                                                         |\n |                    | IS_RELOAD                  | Identifica se todos os dados devem ser recarregados sem que a tabela de destino seja recriada. Não é criado disaster recovery.                                                                                                                                                                                                                    |\n |                    | NAMED_QUERY                | Identifica o diretório contendo osstepsde umanamed query.                                                                                                                                                                                                                                                                                         |\n |                    | NAMED_QUERY_DIRECTORY      | Identifica o diretório de origem dasnamed queries. Por padrão, asnamed queriessão procuradas no diretório /home/etl/named_query                                                                                                                                                                                                                   |\n |                    | NAMED_QUERY_IGNORE_STEP    | Identifica os passos que devem ser ignorados na execução de uma named query, caso haja mais de um step separar vírgula, exemplo: 1,2,3                                                                                                                                                                                                            |\n |                    | NAMED_QUERY_INCLUDE_STEP   | Identifica os passos que devem ser considerados na execução de umanamed query, caso haja mais de um step separar vírgula, exemplo: 1,2,3                                                                                                                                                                                                          |\n-|SPECTRUM| OUTPUT_FORMAT              | Identifica o formato de arquivo de saída.                                                                                                                                                                                                                                                                                                         |\n-|SPECTRUM| OUTPUT_COMPRESSION         | Identifica o tipo de compressão dos arquivos de saída, sendo suportado gzip e snappy.                                                                                                                                                                                                                                                             |\n+| SPECTRUM\t\t\t | OUTPUT_FORMAT              | Identifica o formato de arquivo de saída.                                                                                                                                                                                                                                                                                                         |\n+| SPECTRUM\t\t\t | OUTPUT_COMPRESSION         | Identifica o tipo de compressão dos arquivos de saída, sendo suportado gzip e snappy.                                                                                                                                                                                                                                                             |\n |                    | WHERE_CONDITION_TO_DELTA   | Condição para carga delta.                                                                                                                                                                                                                                                                                                                        |\n |                    | PARTITION_FIELD            | Identifica o campo que será utilizado para particionamento dos dados.                                                                                                                                                                                                                                                                             |\n |                    | PARTITION_TYPE             | Tipo do campo utilizado como partição, podem ser utilizados os tipostimestamp,datee id.                                                                                                                                                                                                                                                           |\n |                    | PARTITION_FORMAT           | Formato da partição quando o particionamento for feito por data, podem ser utilizado os seguintes formatos YYYY,  YYYYMM, YYYYMMDD e YYYYWW.                                                                                                                                                                                                      |\n |                    | PARTITION_LENGTH           | Quantidade de registros de cada partição quando o particionamento for feito por ID. É importante que as mesma recomendações aplicadas para o PARTITION_FORMAT sejam seguidas para a escolha do PARTITION_LENGTH.                                                                                                                                  |\n-|SPECTRUM| PARTITION_MODE             | Tipo de particionamento que será utilizado. Quando utilizado o tipo virtual, os arquivos serão particionados dentro do diretório do S3, mas não será criada o particionamento no catálogo do Athena. Quando utilizado o particionamento real, os arquivos são particionados no S3 e é criado o registro do particionamento no catálogo do Athena. |\n+| SPECTRUM\t\t\t | PARTITION_MODE             | Tipo de particionamento que será utilizado. Quando utilizado o tipo virtual, os arquivos serão particionados dentro do diretório do S3, mas não será criada o particionamento no catálogo do Athena. Quando utilizado o particionamento real, os arquivos são particionados no S3 e é criado o registro do particionamento no catálogo do Athena. |\n |                    | PARTITION_HAS_PREFIX       | Identifica se o campo de partição da tabela do SAP utiliza o caracter 0 como prefixo.                                                                                                                                                                                                                                                             |\n |                    | PARTITION_LAZY             | Identifica se deve particionar os dados apenas no momento da carga para o storage. Caso o valor do parâmetro seja definido como 1, os dados serão particionados no momento em que são extraídos da fonte.                                                                                                                                         |\n | SPECTRUM           | PARTITION_MERGE            | Identifica se deve ser realizadomergedos dados da partição. Quando este parâmetro recebe o valor 0, a partição processada é substituída.                                                                                                                                                                                                          |\n@@ -373,9 +373,9 @@ O metadado possui os seguintes atributos obrigatórios:\n | SPECTRUM           | SPLIT_STRATEGY             | Identifica a estratégia utilizada para o particionamento dos dados sendo: FAST, os dados sendo processados são confiáveis e não contém caracteres especiais ou quebra de linas. SECURE, os dados não são confiáveis. Default: SECURE.                                                                                                             |\n | REDSHIFT           | QUOTE                      | Identifica se o unload de dados do Redshift devem conter aspas duplas, sendo 0 para não e 1 para sim.                                                                                                                                                                                                                                             |\n | REDSHIFT           | ENCODE                     | Identifica o tipo de compressão aplicado nas colunas de tabelas do redshift, sendo o valor padrão ZSTD.                                                                                                                                                                                                                                           |\n-|SPECTRUM| EXPORT_SPREADSHEET_DEFAULT | Identifica o Id da Google spreadsheet.                                                                                                                                                                                                                                                                                                            |\n-|SPECTRUM| EXPORT_SHEET_DEFAULT       | Identifica o nome da página (guia).                                                                                                                                                                                                                                                                                                               |\n-|SPECTRUM| EXPORT_SHEETS_METHOD       | Identifica o tipo de atualização da planilha [0: FULL, 1:APPEND], 0 é o padrão.                                                                                                                                                                                                                                                                   |\n+| SPECTRUM\t\t\t | EXPORT_SPREADSHEET_DEFAULT | Identifica o Id da Google spreadsheet.                                                                                                                                                                                                                                                                                                            |\n+| SPECTRUM\t\t\t | EXPORT_SHEET_DEFAULT       | Identifica o nome da página (guia).                                                                                                                                                                                                                                                                                                               |\n+| SPECTRUM\t\t\t | EXPORT_SHEETS_METHOD       | Identifica o tipo de atualização da planilha [0: FULL, 1:APPEND], 0 é o padrão.                                                                                                                                                                                                                                                                   |\n |                    | MANIFEST                   | Identifica se o manifest será passado por parâmetro.                                                                                                                                                                                                                                                                                              |\n |                    | METADATA_BLACKLIST         | Identifica a lista de campos a serem excluídos (apenas para database module).                                                                                                                                                                                                                                                                     |\n |                    | PARTITION_EAGER            | Identifica se deve extrair todo o dado, quando 1, ou uma partição por vez, quando 0. Padrão é 1.                                                                                                                                                                                                                                                  |\n@@ -385,8 +385,8 @@ O metadado possui os seguintes atributos obrigatórios:\n | REDSHIFT           | SORTKEY                    | Identifica a chave de distribuição do Redshift.                                                                                                                                                                                                                                                                                                   |\n |                    | STORAGE_BUCKET_BACKUP      | Identifica o bucket do storage que será utilizado para backup de arquivos.                                                                                                                                                                                                                                                                        |\n |                    | WHERE_CONDITION_TO_RECOVER | Identifica a condição para recuperação de desastres.                                                                                                                                                                                                                                                                                              |\n-| SPECTRUM | EXPORT_PROFILE | Identifica qual [profile](https://docs.aws.amazon.com/credref/latest/refdocs/creds-config-files.html) de configuração do S3 deve ser usado no processo de exportação. Default: default                                                                                                                                                                                                                                                                                              |\n-| SPECTRUM | EXPORT_TYPE | Identifica o tipo de arquivo que será gerado no processo de exportação. Onde: gz para gzipe e csv para csv. Default: gz                                                                                                                                                                                                                                                                                              |\n+| SPECTRUM \t\t\t | EXPORT_PROFILE \t\t\t  | Identifica qual [profile](https://docs.aws.amazon.com/credref/latest/refdocs/creds-config-files.html) de configuração do S3 deve ser usado no processo de exportação. Default: default                                                                                                                                                                                                                                                                                              |\n+| SPECTRUM \t\t\t | EXPORT_TYPE\t\t\t\t  | Identifica o tipo de arquivo que será gerado no processo de exportação. Onde: gz para gzip, zip para zip e csv para csv. Default: gz                                                                                                                                                                                                                                                                                              |\n \n ## Contributing, Bugs, Questions\n Contributions are more than welcome! If you want to propose new changes, fix bugs or improve something feel free to fork the repository and send us a Pull Request. You can also open new `Issues` for reporting bugs and general problems."
  },
  {
    "sha": "c6595408eced0140c139c92eee687ba0f18c98b0",
    "filename": "api/converter/pom.xml",
    "status": "modified",
    "additions": 2,
    "deletions": 2,
    "changes": 4,
    "blob_url": "https://github.com/dafiti-group/glove/blob/a48f645baddccbb151b22a180658af1f8d3689c9/api/converter/pom.xml",
    "raw_url": "https://github.com/dafiti-group/glove/raw/a48f645baddccbb151b22a180658af1f8d3689c9/api/converter/pom.xml",
    "contents_url": "https://api.github.com/repos/dafiti-group/glove/contents/api/converter/pom.xml?ref=a48f645baddccbb151b22a180658af1f8d3689c9",
    "patch": "@@ -76,7 +76,7 @@\n         <dependency>\n             <groupId>com.univocity</groupId>\n             <artifactId>univocity-parsers</artifactId>\n-            <version>2.9.0</version>\n+            <version>2.9.1</version>\n         </dependency>\n         \n         <dependency>\n@@ -88,7 +88,7 @@\n         <dependency>\n             <groupId>com.amazonaws</groupId>\n             <artifactId>aws-java-sdk-s3</artifactId>\n-            <version>1.11.442</version>\n+            <version>1.11.907</version>\n         </dependency>\n         \n         <dependency>"
  },
  {
    "sha": "845f2b0a65d5eba65eb0dce5d01a5a04e6d38b25",
    "filename": "api/converter/src/main/java/br/com/dafiti/converter/Converter.java",
    "status": "modified",
    "additions": 32,
    "deletions": 26,
    "changes": 58,
    "blob_url": "https://github.com/dafiti-group/glove/blob/a48f645baddccbb151b22a180658af1f8d3689c9/api/converter/src/main/java/br/com/dafiti/converter/Converter.java",
    "raw_url": "https://github.com/dafiti-group/glove/raw/a48f645baddccbb151b22a180658af1f8d3689c9/api/converter/src/main/java/br/com/dafiti/converter/Converter.java",
    "contents_url": "https://api.github.com/repos/dafiti-group/glove/contents/api/converter/src/main/java/br/com/dafiti/converter/Converter.java?ref=a48f645baddccbb151b22a180658af1f8d3689c9",
    "patch": "@@ -73,28 +73,29 @@ public Converter() {\n         options.addOption(\"f\", \"filename\", true, \"Filename, with wildcard if necessary, to be converted\");\n         options.addOption(\"s\", \"schema\", true, \"Avro schema to be used on conversion\");\n         options.addOption(\"D\", \"delimiter\", true, \"Delimiter ofd csv files\");\n-        options.addOption(\"T\", \"target\", true, \"Identify the target format\");\n-        options.addOption(\"r\", \"replace\", false, \"Identify if csv files will be replaced to parquet files\");\n+        options.addOption(\"T\", \"target\", true, \"Identifies the target format\");\n+        options.addOption(\"r\", \"replace\", false, \"Identifies if csv files will be replaced to parquet files\");\n         options.addOption(\"d\", \"debug\", true, \"Show full log messages\");\n         options.addOption(\"t\", \"thread\", true, \"Limit of thread\");\n-        options.addOption(\"o\", \"output\", true, \"Identify the output path\");\n-        options.addOption(\"q\", \"quote\", true, \"Identify the quote character\");\n-        options.addOption(\"e\", \"escape\", true, \"Identify the quote escape character\");\n-        options.addOption(\"H\", \"header\", false, \"Identify the csv file has a header\");\n-        options.addOption(\"c\", \"field\", true, \"Identify the header fields of a csv file\");\n+        options.addOption(\"o\", \"output\", true, \"Identifies the output path\");\n+        options.addOption(\"q\", \"quote\", true, \"Identifies the quote character\");\n+        options.addOption(\"e\", \"escape\", true, \"Identifies the quote escape character\");\n+        options.addOption(\"H\", \"header\", false, \"Identifies the csv file has a header\");\n+        options.addOption(\"c\", \"field\", true, \"Identifies the header fields of a csv file\");\n         options.addOption(\"S\", \"sample\", true, \"Define the data sample to be analized at metadata extraction process\");\n-        options.addOption(\"i\", \"metadata\", true, \"Identify the csv field metadata\");\n-        options.addOption(\"C\", \"compression\", true, \"Identify the compression to be applied\");\n-        options.addOption(\"y\", \"dialect\", true, \"Identify the metadata dialect\");\n+        options.addOption(\"i\", \"metadata\", true, \"Identifies the csv field metadata\");\n+        options.addOption(\"C\", \"compression\", true, \"Identifies the compression to be applied\");\n+        options.addOption(\"y\", \"dialect\", true, \"Identifies the metadata dialect\");\n         options.addOption(\"h\", \"help\", false, \"Show help and usage message\");\n         options.addOption(\"p\", \"partition\", true, \"Partition column\");\n         options.addOption(\"k\", \"fieldkey\", true, \"Unique key field\");\n-        options.addOption(\"d\", \"merge\", true, \"Identify if should merge existing files\");\n-        options.addOption(\"z\", \"duplicated\", true, \"Identify if duplicated is allowed\");\n-        options.addOption(\"b\", \"bucket\", true, \"Identify the storage bucket\");\n-        options.addOption(\"m\", \"mode\", true, \"Identify the partition mode\");\n-        options.addOption(\"w\", \"reservedWords\", true, \"Identify the reserved words file list\");\n-        options.addOption(\"ps\", \"splitStrategy\", true, \"Identify the split strategy\");\n+        options.addOption(\"d\", \"merge\", true, \"Identifies if should merge existing files\");\n+        options.addOption(\"z\", \"duplicated\", true, \"Identifies if duplicated is allowed\");\n+        options.addOption(\"b\", \"bucket\", true, \"Identifies the storage bucket\");\n+        options.addOption(\"m\", \"mode\", true, \"Identifies the partition mode\");\n+        options.addOption(\"w\", \"reservedWords\", true, \"Identifies the reserved words file list\");\n+        options.addOption(\"ps\", \"splitStrategy\", true, \"Identifies the split strategy\");\n+        options.addOption(\"nm\", \"readable\", false, \"Identifies if partition name should be readable at runtime\");\n     }\n \n     /**\n@@ -136,6 +137,7 @@ public static void main(String[] args) {\n         boolean merge = false;\n         boolean duplicated = false;\n         boolean debug = false;\n+        boolean readable = false;\n         int thread = 1;\n         int fieldKeyPos = -1;\n         int fieldPartitionPos = 0;\n@@ -228,7 +230,7 @@ public static void main(String[] args) {\n                 sample = Integer.valueOf(line.getOptionValue(\"sample\"));\n             }\n \n-            //Identify the log level.\n+            //Identifies the log level.\n             if (line.hasOption(\"debug\")) {\n                 debug = (Integer.valueOf(line.getOptionValue(\"debug\")) == 1);\n \n@@ -239,27 +241,27 @@ public static void main(String[] args) {\n                 }\n             }\n \n-            //Identify how many files should be converted simultaneously. \n+            //Identifies how many files should be converted simultaneously. \n             if (line.hasOption(\"thread\")) {\n                 thread = Integer.valueOf(line.getOptionValue(\"thread\"));\n             }\n \n-            //Identify the csv partition column. \n+            //Identifies the csv partition column. \n             if (line.hasOption(\"partition\")) {\n                 fieldPartitionPos = Integer.valueOf(line.getOptionValue(\"partition\"));\n             }\n \n-            //Identify the csv partition column. \n+            //Identifies the csv partition column. \n             if (line.hasOption(\"fieldkey\")) {\n                 fieldKeyPos = Integer.valueOf(line.getOptionValue(\"fieldkey\"));\n             }\n \n-            //Identify if duplicated is allowed. \n+            //Identifies if duplicated is allowed. \n             if (line.hasOption(\"duplicated\")) {\n                 duplicated = (Integer.valueOf(line.getOptionValue(\"duplicated\")) == 1);\n             }\n \n-            //Identify if should merge existing files. \n+            //Identifies if should merge existing files. \n             if (line.hasOption(\"merge\")) {\n                 merge = (Integer.valueOf(line.getOptionValue(\"merge\")) == 1);\n             }\n@@ -279,18 +281,21 @@ public static void main(String[] args) {\n                 splitStrategy = line.getOptionValue(\"splitStrategy\").toUpperCase();\n             }\n \n-            //Identify if should remove the csv file. \n+            //Identifies if should remove the csv file. \n             replace = line.hasOption(\"replace\");\n \n-            //Identify if the csv file has header. \n+            //Identifies if the csv file has header. \n             header = line.hasOption(\"header\");\n \n-            //Identify if should show help or convert files.\n+            //Identifies if partition name should be readable at runtime.\n+            readable = line.hasOption(\"readable\");\n+\n+            //Identifies if should show help or convert files.\n             if (line.hasOption(\"help\") && args.length == 1) {\n                 converter.help();\n             }\n \n-            //Identify if there are files to process.\n+            //Identifies if there are files to process.\n             if (files != null) {\n                 //Define the number of threads.\n                 ExecutorService executor = Executors.newFixedThreadPool(thread);\n@@ -365,6 +370,7 @@ public static void main(String[] args) {\n                                                 quoteEscape,\n                                                 header,\n                                                 replace,\n+                                                readable,\n                                                 splitStrategy));\n                                 break;\n                             default:"
  },
  {
    "sha": "2367c9af913e8b7ea366f90fbc694fe8c284b151",
    "filename": "api/converter/src/main/java/br/com/dafiti/csv/CSVSplitter.java",
    "status": "modified",
    "additions": 50,
    "deletions": 7,
    "changes": 57,
    "blob_url": "https://github.com/dafiti-group/glove/blob/a48f645baddccbb151b22a180658af1f8d3689c9/api/converter/src/main/java/br/com/dafiti/csv/CSVSplitter.java",
    "raw_url": "https://github.com/dafiti-group/glove/raw/a48f645baddccbb151b22a180658af1f8d3689c9/api/converter/src/main/java/br/com/dafiti/csv/CSVSplitter.java",
    "contents_url": "https://api.github.com/repos/dafiti-group/glove/contents/api/converter/src/main/java/br/com/dafiti/csv/CSVSplitter.java?ref=a48f645baddccbb151b22a180658af1f8d3689c9",
    "patch": "@@ -53,6 +53,7 @@\n     private final Character quoteEscape;\n     private final boolean header;\n     private final boolean replace;\n+    private final boolean readable;\n     private final String splitStrategy;\n \n     /**\n@@ -65,6 +66,8 @@\n      * @param quoteEscape File escape.\n      * @param header Identify if the file has header.\n      * @param replace Identify if should replace the orignal file.\n+     * @param readable Identifies if partition name should be readable at\n+     * runtime.\n      * @param splitStrategy Identify if should use the fastest strategy to\n      * partitioning.\n      */\n@@ -76,6 +79,7 @@ public CSVSplitter(\n             Character quoteEscape,\n             boolean header,\n             boolean replace,\n+            boolean readable,\n             String splitStrategy) {\n \n         this.csvFile = csvFile;\n@@ -85,7 +89,9 @@ public CSVSplitter(\n         this.quoteEscape = quoteEscape;\n         this.replace = replace;\n         this.header = header;\n+        this.readable = readable;\n         this.splitStrategy = splitStrategy;\n+\n     }\n \n     /**\n@@ -119,6 +125,7 @@ public void run() {\n      */\n     private void fastSplit() throws IOException {\n         String part = \"\";\n+        String fileHeader = \"\";\n         int lineNumber = 0;\n         HashMap<String, BufferedWriter> partitions = new HashMap<>();\n         LineIterator lineIterator = FileUtils.lineIterator(csvFile, \"UTF-8\");\n@@ -127,7 +134,7 @@ private void fastSplit() throws IOException {\n             while (lineIterator.hasNext()) {\n                 String line = lineIterator.nextLine();\n \n-                if (!(lineNumber == 0 && header)) {\n+                if (!(lineNumber == 0 && this.header)) {\n                     String[] split = line.split(delimiter.toString());\n \n                     if (split.length != 0) {\n@@ -144,17 +151,35 @@ private void fastSplit() throws IOException {\n                         if (part.isEmpty()) {\n                             String partition = split[partitionColumn].replaceAll(\"\\\\W\", \"\");\n \n+                            if (partition.isEmpty()) {\n+                                partition = \"UNDEFINED\";\n+                            }\n+\n                             if (!partitions.containsKey(partition)) {\n-                                String partitionPath = csvFile.getParent() + \"/\" + partition;\n-                                Files.createDirectories(Paths.get(partitionPath));\n-                                BufferedWriter bufferedWriter = new BufferedWriter(new FileWriter(partitionPath + \"/\" + UUID.randomUUID() + \".csv\"));\n+                                String partitionPath;\n+                                BufferedWriter bufferedWriter;\n+\n+                                if (readable) {\n+                                    partitionPath = csvFile.getParent();\n+                                    bufferedWriter = new BufferedWriter(new FileWriter(partitionPath + \"/\" + partition + \".csv\"));\n+\n+                                    if (header) {\n+                                        bufferedWriter.append(fileHeader + \"\\r\\n\");\n+                                    }\n+                                } else {\n+                                    partitionPath = csvFile.getParent() + \"/\" + partition;\n+                                    Files.createDirectories(Paths.get(partitionPath));\n+                                    bufferedWriter = new BufferedWriter(new FileWriter(partitionPath + \"/\" + UUID.randomUUID() + \".csv\"));\n+                                }\n \n                                 partitions.put(partition, bufferedWriter);\n                             }\n \n                             partitions.get(partition).append(line + \"\\r\\n\");\n                         }\n                     }\n+                } else {\n+                    fileHeader = line;\n                 }\n \n                 lineNumber++;\n@@ -182,6 +207,7 @@ private void fastSplit() throws IOException {\n      */\n     private void secureSplit() throws IOException {\n         int lineNumber = 0;\n+        String[] fileHeader = null;\n         HashMap<String, CsvWriter> partitions = new HashMap<>();\n \n         //Writer. \n@@ -210,13 +236,30 @@ private void secureSplit() throws IOException {\n             if (!(lineNumber == 0 && header)) {\n                 String partition = record[partitionColumn].replaceAll(\"\\\\W\", \"\");\n \n+                if (partition.isEmpty()) {\n+                    partition = \"UNDEFINED\";\n+                }\n+\n                 if (!partitions.containsKey(partition)) {\n-                    String partitionPath = csvFile.getParent() + \"/\" + partition;\n-                    Files.createDirectories(Paths.get(partitionPath));\n-                    partitions.put(partition, new CsvWriter(new FileWriter(partitionPath + \"/\" + UUID.randomUUID() + \".csv\"), writerSettings));\n+                    String partitionPath;\n+\n+                    if (readable) {\n+                        partitionPath = csvFile.getParent();\n+                        partitions.put(partition, new CsvWriter(new FileWriter(partitionPath + \"/\" + partition + \".csv\"), writerSettings));\n+\n+                        if (header) {\n+                            partitions.get(partition).writeRow(fileHeader);\n+                        }\n+                    } else {\n+                        partitionPath = csvFile.getParent() + \"/\" + partition;\n+                        Files.createDirectories(Paths.get(partitionPath));\n+                        partitions.put(partition, new CsvWriter(new FileWriter(partitionPath + \"/\" + UUID.randomUUID() + \".csv\"), writerSettings));\n+                    }\n                 }\n \n                 partitions.get(partition).writeRow(record);\n+            } else {\n+                fileHeader = record;\n             }\n \n             lineNumber++;"
  },
  {
    "sha": "0725a6bb43b2f9c827c6be46e0b7f70116834001",
    "filename": "extractor/lib/converter.jar",
    "status": "modified",
    "additions": 0,
    "deletions": 0,
    "changes": 0,
    "blob_url": "https://github.com/dafiti-group/glove/blob/a48f645baddccbb151b22a180658af1f8d3689c9/extractor/lib/converter.jar",
    "raw_url": "https://github.com/dafiti-group/glove/raw/a48f645baddccbb151b22a180658af1f8d3689c9/extractor/lib/converter.jar",
    "contents_url": "https://api.github.com/repos/dafiti-group/glove/contents/extractor/lib/converter.jar?ref=a48f645baddccbb151b22a180658af1f8d3689c9"
  },
  {
    "sha": "79d9f7fe12a387fd60c1afccb7705f29fb9973d3",
    "filename": "extractor/shell/spectrum.sh",
    "status": "modified",
    "additions": 74,
    "deletions": 21,
    "changes": 95,
    "blob_url": "https://github.com/dafiti-group/glove/blob/a48f645baddccbb151b22a180658af1f8d3689c9/extractor/shell/spectrum.sh",
    "raw_url": "https://github.com/dafiti-group/glove/raw/a48f645baddccbb151b22a180658af1f8d3689c9/extractor/shell/spectrum.sh",
    "contents_url": "https://api.github.com/repos/dafiti-group/glove/contents/extractor/shell/spectrum.sh?ref=a48f645baddccbb151b22a180658af1f8d3689c9",
    "patch": "@@ -627,33 +627,86 @@ if [ ${QUEUE_FILE_COUNT} -gt 0 ]; then\n \n \t\t\t# Define o storage de exportação.\n \t\t\tif [ \"${#EXPORT_BUCKET}\" -gt \"0\" ]; then\n-\t\t\t\t# Envia o arquivo para o storage.\n-\t\t\t\techo \"Exporting resultset to ${EXPORT_BUCKET} using profile ${EXPORT_PROFILE}!\"\n+\t\t\t\t# Particiona o arquivo de entrada. \n+\t\t\t\tif [ \"${#PARTITION_FIELD}\" -gt \"0\" ]; then\n+\t\t\t\t\tFILE_INDEX=0\n+\n+\t\t\t\t\techo \"Merging files!\"\n+\n+\t\t\t\t\t# Une os dados em um único arquivo.  \n+\t\t\t\t\tfor i in `ls ${RAWFILE_QUEUE_PATH}*`\n+\t\t\t\t\tdo\n+\t\t\t\t\t\tif [ ${FILE_INDEX} = 0 ]; then\t\n+\t\t\t\t\t\t\tcat ${i}\t>> ${RAWFILE_QUEUE_PATH}merged.csv\n+\t\t\t\t\t\t\terror_check\n+\t\t\t\t\t\telse\n+\t\t\t\t\t\t\tsed '1d' ${i} >> ${RAWFILE_QUEUE_PATH}merged.csv\n+\t\t\t\t\t\t\terror_check\t\n+\t\t\t\t\t\tfi\n+\n+\t\t\t\t\t\tFILE_INDEX=$(( $FILE_INDEX + 1 ))\n+\t\t\t\t\tdone\n+\n+\t\t\t\t\techo \"Partitioning data file delimited by ${DELIMITER}!\"\n+\n+\t\t\t\t\t# Particiona o arquivo em single thread (thread=1) para preservar os dados e nome das partições.  \n+\t\t\t\t\t# TODO - A geração de um único arquivo de saída deve ser suportada pelo conversor de dados nativamente sem a necessidade do merge anterior. \n+\t\t\t\t\tjava -jar ${GLOVE_HOME}/extractor/lib/converter.jar \\\n+\t\t\t\t\t\t--folder=${RAWFILE_QUEUE_PATH} \\\n+\t\t\t\t\t\t--filename=merged.csv \\\n+\t\t\t\t\t\t--delimiter=${DELIMITER} \\\n+\t\t\t\t\t\t--target=csv \\\n+\t\t\t\t\t\t--splitStrategy=${SPLIT_STRATEGY} \\\n+\t\t\t\t\t\t--partition=0 \\\n+\t\t\t\t\t\t--thread=1 \\\n+\t\t\t\t\t\t--escape=${QUOTE_ESCAPE} \\\n+\t\t\t\t\t\t--header \\\n+\t\t\t\t\t\t--readable \\\n+\t\t\t\t\t\t--replace \\\n+\t\t\t\t\t\t--debug=${DEBUG}\n+\t\t\t\t\terror_check\n+\t\t\t\tfi\n+\n+\t\t\t\t# Identifica cada bucket para o qual o export deve ser enviado.\n+\t\t\t\tBUCKETS=(`echo ${EXPORT_BUCKET} | tr -d ' ' | tr ',' ' '`)\n \n \t\t\t\t# Identifica se deve compactar o arquivo a ser exportado.\n-\t\t\t\tif [ ${EXPORT_TYPE} == \"gz\" ]; then\n-\t\t\t\t\t# Compacta o arquivo csv e mantém o arquivo original.\n-\t\t\t\t\tpigz -k ${RAWFILE_QUEUE_PATH}*${DATA_FILE}*\n+\t\t\t\tif [ ${EXPORT_TYPE} == \"gz\" ]  || [ ${EXPORT_TYPE} == \"zip\" ]; then\n+\t\t\t\t\techo \"Compacting files at ${RAWFILE_QUEUE_PATH} to ${EXPORT_TYPE}!\"\n \n-\t\t\t\t\t# Envia o arquivo compactado para o bucket de destino.\n-\t\t\t\t\tif [ \"${#PARTITION_FIELD}\" -gt \"0\" ]; then\n-\t\t\t\t\t\taws s3 cp ${RAWFILE_QUEUE_PATH} ${EXPORT_BUCKET} --profile ${EXPORT_PROFILE} --recursive --exclude \"*\" --include \"*.gz\" --only-show-errors --acl bucket-owner-full-control\n+\t\t\t\t\tif [ ${EXPORT_TYPE} == \"gz\" ]; then\n+\t\t\t\t\t\tpigz -k ${RAWFILE_QUEUE_PATH}*\n \t\t\t\t\telse\n-\t\t\t\t\t\taws s3 cp ${RAWFILE_QUEUE_FILE}.gz ${EXPORT_BUCKET} --profile ${EXPORT_PROFILE} --only-show-errors --acl bucket-owner-full-control\n-\t\t\t\t\tfi\n-\t\t\t\t\terror_check\n-\n-\t\t\t\t\t# Remove o arquivo compactado do diretório.\n-\t\t\t\t\trm -f ${RAWFILE_QUEUE_PATH}*.gz\n+\t\t\t\t\t\tfind ${RAWFILE_QUEUE_PATH} -type f -not -name '${DATA_FILE}*' -execdir zip '{}.zip' '{}' \\;\n+\t\t\t\t\tfi \t\n+\n+\t\t\t\t\tfor index in \"${!BUCKETS[@]}\"\n+\t\t\t\t\tdo\n+\t\t\t\t\t\techo \"Exporting resultset to ${BUCKETS[index]} using profile ${EXPORT_PROFILE}!\"\n+\n+\t\t\t\t\t\tif [ \"${#PARTITION_FIELD}\" -gt \"0\" ]; then\n+\t\t\t\t\t\t\taws s3 cp ${RAWFILE_QUEUE_PATH} ${BUCKETS[index]} --profile ${EXPORT_PROFILE} --recursive --exclude \"${DATA_FILE}*\" --exclude \"*.csv\" --only-show-errors --acl bucket-owner-full-control\n+\t\t\t\t\t\telse\n+\t\t\t\t\t\t\taws s3 cp ${RAWFILE_QUEUE_FILE}.${EXPORT_TYPE} ${BUCKETS[index]} --profile ${EXPORT_PROFILE} --only-show-errors --acl bucket-owner-full-control\n+\t\t\t\t\t\tfi\n+\t\t\t\t\t\terror_check\n+\t\t\t\t\tdone\n \t\t\t\telse\n-\t\t\t\t\t# Envia o arquivo para o bucket de destino.\n-\t\t\t\t\tif [ \"${#PARTITION_FIELD}\" -gt \"0\" ]; then\n-\t\t\t\t\t\taws s3 cp ${RAWFILE_QUEUE_PATH} ${EXPORT_BUCKET} --profile ${EXPORT_PROFILE} --recursive --exclude \"*\" --include \"${DATA_FILE}*\" --only-show-errors --acl bucket-owner-full-control\n-\t\t\t\t\telse\n-\t\t\t\t\t\taws s3 cp ${RAWFILE_QUEUE_FILE} ${EXPORT_BUCKET} --profile ${EXPORT_PROFILE} --only-show-errors --acl bucket-owner-full-control\n-\t\t\t\t\tfi\n-\t\t\t\t\terror_check\n+\t\t\t\t\tfor index in \"${!BUCKETS[@]}\"\n+\t\t\t\t\tdo\n+\t\t\t\t\t\techo \"Exporting resultset to ${BUCKETS[index]} using profile ${EXPORT_PROFILE}!\"\n+\t\t\t\t\t\t\n+\t\t\t\t\t\tif [ \"${#PARTITION_FIELD}\" -gt \"0\" ]; then\n+\t\t\t\t\t\t\taws s3 cp ${RAWFILE_QUEUE_PATH} ${BUCKETS[index]} --profile ${EXPORT_PROFILE} --recursive --exclude \"${DATA_FILE}*\" --only-show-errors --acl bucket-owner-full-control\n+\t\t\t\t\t\telse\n+\t\t\t\t\t\t\taws s3 cp ${RAWFILE_QUEUE_FILE} ${BUCKETS[index]} --profile ${EXPORT_PROFILE} --only-show-errors --acl bucket-owner-full-control\n+\t\t\t\t\t\tfi\n+\t\t\t\t\t\terror_check\n+\t\t\t\t\tdone\n \t\t\t\tfi\n+\n+\t\t\t\t# Remove os arquivos temporários. \t\t\t\t\t\t\n+\t\t\t\tfind ${RAWFILE_QUEUE_PATH} -not -name '${DATA_FILE}*.csv' -delete\n \t\t\telif [ \"${#EXPORT_SPREADSHEET}\" -gt \"0\" ]; then\n \t\t\t\tif [ ${DEBUG} = 1 ] ; then\n \t\t\t\t\techo \"DEBUG:java -jar ${GLOVE_HOME}/extractor/lib/google-sheets-export.jar \\"
  },
  {
    "sha": "dcd809d11564fa462903b3cba38dcdbe4c179fc6",
    "filename": "extractor/sql/metadata/spectrum/mysql.sql",
    "status": "modified",
    "additions": 34,
    "deletions": 27,
    "changes": 61,
    "blob_url": "https://github.com/dafiti-group/glove/blob/a48f645baddccbb151b22a180658af1f8d3689c9/extractor/sql/metadata/spectrum/mysql.sql",
    "raw_url": "https://github.com/dafiti-group/glove/raw/a48f645baddccbb151b22a180658af1f8d3689c9/extractor/sql/metadata/spectrum/mysql.sql",
    "contents_url": "https://api.github.com/repos/dafiti-group/glove/contents/extractor/sql/metadata/spectrum/mysql.sql?ref=a48f645baddccbb151b22a180658af1f8d3689c9",
    "patch": "@@ -6,21 +6,21 @@ SELECT * FROM (\n \t        WHEN '${PARTITION_TYPE}' = 'date' OR '${PARTITION_TYPE}' = 'timestamp' THEN \n \t\t\t\tCONCAT('COALESCE( DATE_FORMAT( IF( WEEKDAY(',column_name,') IS NULL, ', \n \t\t\t\t\tCASE '${PARTITION_FORMAT}' \n-\t\t\t\t\t\tWHEN 'YYYY' THEN '''1900''' \n-\t\t\t\t\t\tWHEN 'YYYYMM' THEN '''190001''' \n-\t\t\t\t\t\tWHEN 'YYYYWW' THEN '''190001''' \n+\t\t\t\t\t\tWHEN 'YYYY' \tTHEN '''1900''' \n+\t\t\t\t\t\tWHEN 'YYYYMM' \tTHEN '''190001''' \n+\t\t\t\t\t\tWHEN 'YYYYWW' \tTHEN '''190001''' \n \t\t\t\t\t\tWHEN 'YYYYMMDD' THEN '''19000101''' \n \t\t\t\t\tEND, ', ',column_name,' ),', \n \t\t\t\t\tCASE '${PARTITION_FORMAT}' \n-\t\t\t\t\t\tWHEN 'YYYY' THEN '''%Y''' \n-\t\t\t\t\t\tWHEN 'YYYYMM' THEN '''%Y%m''' \n-\t\t\t\t\t\tWHEN 'YYYYWW' THEN '''%Y%v''' \n+\t\t\t\t\t\tWHEN 'YYYY' \tTHEN '''%Y''' \n+\t\t\t\t\t\tWHEN 'YYYYMM' \tTHEN '''%Y%m''' \n+\t\t\t\t\t\tWHEN 'YYYYWW' \tTHEN '''%Y%v''' \n \t\t\t\t\t\tWHEN 'YYYYMMDD' THEN '''%Y%m%d''' \n \t\t\t\t\tEND,'), ',\n \t\t\t\t\tCASE '${PARTITION_FORMAT}' \n-\t\t\t\t\t\tWHEN 'YYYY' THEN '''1900''' \n-\t\t\t\t\t\tWHEN 'YYYYMM' THEN '''190001''' \n-\t\t\t\t\t\tWHEN 'YYYYWW' THEN '''190001''' \n+\t\t\t\t\t\tWHEN 'YYYY' \tTHEN '''1900''' \n+\t\t\t\t\t\tWHEN 'YYYYMM' \tTHEN '''190001''' \n+\t\t\t\t\t\tWHEN 'YYYYWW' \tTHEN '''190001''' \n \t\t\t\t\t\tWHEN 'YYYYMMDD' THEN '''19000101'''\n \t\t\t\t\t\tELSE '''190001''' \n \t\t\t\tEND,') AS partition_field')\n@@ -30,21 +30,21 @@ SELECT * FROM (\n \t        WHEN '${PARTITION_TYPE}' = 'date' OR '${PARTITION_TYPE}' = 'timestamp' THEN \n \t\t\t\tCONCAT('COALESCE( DATE_FORMAT( IF( WEEKDAY(',column_name,') IS NULL, ', \n \t\t\t\t\tCASE '${PARTITION_FORMAT}' \n-\t\t\t\t\t\tWHEN 'YYYY' THEN '''1900''' \n-\t\t\t\t\t\tWHEN 'YYYYMM' THEN '''190001''' \n-\t\t\t\t\t\tWHEN 'YYYYWW' THEN '''190001''' \n+\t\t\t\t\t\tWHEN 'YYYY' \tTHEN '''1900''' \n+\t\t\t\t\t\tWHEN 'YYYYMM' \tTHEN '''190001''' \n+\t\t\t\t\t\tWHEN 'YYYYWW' \tTHEN '''190001''' \n \t\t\t\t\t\tWHEN 'YYYYMMDD' THEN '''19000101''' \n \t\t\t\t\tEND, ', ',column_name,' ),', \n \t\t\t\t\tCASE '${PARTITION_FORMAT}' \n-\t\t\t\t\t\tWHEN 'YYYY' THEN '''%Y''' \n-\t\t\t\t\t\tWHEN 'YYYYMM' THEN '''%Y%m''' \n-\t\t\t\t\t\tWHEN 'YYYYWW' THEN '''%Y%v''' \n+\t\t\t\t\t\tWHEN 'YYYY' \tTHEN '''%Y''' \n+\t\t\t\t\t\tWHEN 'YYYYMM' \tTHEN '''%Y%m''' \n+\t\t\t\t\t\tWHEN 'YYYYWW' \tTHEN '''%Y%v''' \n \t\t\t\t\t\tWHEN 'YYYYMMDD' THEN '''%Y%m%d''' \n \t\t\t\t\tEND,'), ',\n \t\t\t\t\tCASE '${PARTITION_FORMAT}' \n-\t\t\t\t\t\tWHEN 'YYYY' THEN '''1900''' \n-\t\t\t\t\t\tWHEN 'YYYYMM' THEN '''190001''' \n-\t\t\t\t\t\tWHEN 'YYYYWW' THEN '''190001''' \n+\t\t\t\t\t\tWHEN 'YYYY' \tTHEN '''1900''' \n+\t\t\t\t\t\tWHEN 'YYYYMM' \tTHEN '''190001''' \n+\t\t\t\t\t\tWHEN 'YYYYWW' \tTHEN '''190001''' \n \t\t\t\t\t\tWHEN 'YYYYMMDD' THEN '''19000101'''\n \t\t\t\t\t\tELSE '''190001''' \n \t\t\t\tEND,')')\n@@ -90,6 +90,7 @@ SELECT * FROM (\n \n \tUNION ALL\n \n+\n     SELECT DISTINCT\n         ordinal_position,\n         CASE\n@@ -111,15 +112,15 @@ SELECT * FROM (\n \t\t\tWHEN 'tinyint'      THEN 'int'\n \t\t\tWHEN 'smallint'     THEN 'int'\n \t\t\tWHEN 'mediumint'    THEN 'int'\n-          \tWHEN 'int'          THEN 'int'\n+          \tWHEN 'int'          THEN IF(column_type LIKE '%unsigned%', 'bigint', 'int') \n             WHEN 'bigint'       THEN 'bigint'\n            \tWHEN 'tinytext'     THEN 'varchar(65535)'\n            \tWHEN 'mediumtext'   THEN 'varchar(65535)'\n            \tWHEN 'text'         THEN 'varchar(65535)'\n            \tWHEN 'longtext'     THEN 'varchar(65535)'\n             WHEN 'blob'         THEN 'varchar(65535)'\n             WHEN 'mediumblob'   THEN 'varchar(65535)'\n-\t        WHEN 'longblob'   THEN 'varchar(65535)'\t\n+\t        WHEN 'longblob'   \tTHEN 'varchar(65535)'\t\n             WHEN 'date'         THEN 'varchar(10)'\n             WHEN 'datetime'     THEN 'varchar(19)'\n             WHEN 'time'         THEN 'varchar(17)'\n@@ -132,16 +133,21 @@ SELECT * FROM (\n             WHEN 'char'         THEN CONCAT('varchar','(', CHARACTER_MAXIMUM_LENGTH + ROUND( ( CHARACTER_MAXIMUM_LENGTH - 1 ) / 2 ),')')\n             WHEN 'varchar'      THEN CONCAT('varchar','(', CHARACTER_MAXIMUM_LENGTH + ROUND( ( CHARACTER_MAXIMUM_LENGTH - 1 ) / 2 ),')')\n \t\t\tWHEN 'boolean' \t\tTHEN 'boolean'\n+\t\t\tELSE 'varchar(255)'\n         END AS field_type,\n \t\tCONCAT('{\"name\": \"', LOWER( REPLACE(column_name,' ','_') ), '\",\"type\":', \n-\t\t\tIF( data_type IN (\"tinyint\",\"smallint\",\"mediumint\", \"int\", \"bit\"), '[\"null\", \"int\"]', \n+\t\t\tIF( data_type IN (\"tinyint\",\"smallint\",\"mediumint\", \"bit\"), '[\"null\", \"int\"]', \n+\t\t\tIF( data_type IN (\"int\") AND column_type LIKE '%unsigned%', '[\"null\", \"long\"]', \n+\t\t\tIF( data_type IN (\"int\") AND column_type NOT LIKE '%unsigned%', '[\"null\", \"int\"]', \n \t\t\tIF( data_type IN (\"bigint\"), '[\"null\", \"long\"]', \n \t\t\tIF( data_type IN (\"float\",\"double\"), '[\"null\", \"double\"]', \n \t\t\tIF( data_type IN (\"decimal\"), CONCAT( '[\"null\", {\"type\":\"fixed\", \"name\": \"', LOWER( REPLACE(column_name,' ','_') ) , '\", \"size\":' , CAST( ROUND( IF( NUMERIC_PRECISION > 38, 38, NUMERIC_PRECISION ) ) / 2 AS SIGNED ) , ', \"logicalType\": \"decimal\", \"precision\":' , ROUND( IF( NUMERIC_PRECISION > 38, 38, NUMERIC_PRECISION ) ) , ', \"scale\":' , NUMERIC_SCALE , '}]' ), \n-\t\t\tIF( data_type = \"timestamp\",'[\"null\", \"string\"]', IF( data_type=\"datetime\",'[\"null\", \"string\"]', \n+\t\t\tIF( data_type = \"timestamp\",'[\"null\", \"string\"]', \n+\t\t\tIF( data_type = \"datetime\",'[\"null\", \"string\"]', \n \t\t\tIF( data_type = \"boolean\",'[\"null\", \"boolean\"]', \n \t\t\tIF( data_type = \"date\",'[\"null\", \"string\"]', \n-\t\t\tIF( data_type = \"time\",'[\"null\", \"string\"]','[\"null\", \"string\"]' ))))))))), ' , \"default\": null}'\n+\t\t\tIF( data_type = \"time\",'[\"null\", \"string\"]','[\"null\", \"string\"]' ))))))))))\n+\t\t\t), ' , \"default\": null}'\n \t\t) AS json,\n \t\tLOWER( REPLACE(column_name,' ','_') ) AS column_name,\n         0 AS column_key,\n@@ -152,7 +158,8 @@ SELECT * FROM (\n \t\tLOWER( c.table_schema ) = LOWER('${INPUT_TABLE_SCHEMA}')\n    \t\tAND\n \t\tLOWER( c.table_name ) = LOWER('${INPUT_TABLE_NAME}')\n-\t\tAND UPPER(COLUMN_NAME) NOT IN (${METADATA_BLACKLIST})\n+\t\tAND \n+\t\tUPPER(COLUMN_NAME) NOT IN (${METADATA_BLACKLIST})\n \n     UNION ALL\n \n@@ -162,8 +169,8 @@ SELECT * FROM (\n \t\tCONCAT('CONCAT(','DATE_FORMAT(now(),','''','%Y-%m-%d %T', '''','),', '''' ,' ${TIMEZONE_OFFSET}', '''',')') AS casting,\n         'varchar(19)' AS field_type,\n   \t\t'{\"name\": \"etl_load_date\",\"type\":[\"null\", \"string\"], \"default\": null}' AS json,\n-        'etl_load_date' \t\t\t\t\t\t\tAS column_name,\n-        0 \t\t\t\t\t\t\t\t\t\t\tAS column_key,\n-\t\t'' \t\t\t\t\t\t\t\t\t\t\tAS encoding\n+        'etl_load_date' AS column_name,\n+        0 AS column_key,\n+\t\t'' AS encoding\n ) x\n ORDER BY x.ordinal_position"
  }
]
