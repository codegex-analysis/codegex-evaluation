[
  {
    "sha": "747d9dd656a5d60c058ab7549426045adccc300d",
    "filename": "logisland-components/logisland-services/logisland-service-elasticsearch/logisland-service-elasticsearch-api/src/main/java/com/hurence/logisland/service/elasticsearch/ElasticsearchClientService.java",
    "status": "modified",
    "additions": 6,
    "deletions": 1,
    "changes": 7,
    "blob_url": "https://github.com/Hurence/logisland/blob/22a433ab75df3ef948d8a0d765964c19b44b354b/logisland-components/logisland-services/logisland-service-elasticsearch/logisland-service-elasticsearch-api/src/main/java/com/hurence/logisland/service/elasticsearch/ElasticsearchClientService.java",
    "raw_url": "https://github.com/Hurence/logisland/raw/22a433ab75df3ef948d8a0d765964c19b44b354b/logisland-components/logisland-services/logisland-service-elasticsearch/logisland-service-elasticsearch-api/src/main/java/com/hurence/logisland/service/elasticsearch/ElasticsearchClientService.java",
    "contents_url": "https://api.github.com/repos/Hurence/logisland/contents/logisland-components/logisland-services/logisland-service-elasticsearch/logisland-service-elasticsearch-api/src/main/java/com/hurence/logisland/service/elasticsearch/ElasticsearchClientService.java?ref=22a433ab75df3ef948d8a0d765964c19b44b354b",
    "patch": "@@ -204,7 +204,12 @@ public ValidationResult validate(final String subject, final String input) {\n             .addValidator(StandardValidators.CHARACTER_SET_VALIDATOR)\n             .build();\n \n-\n+    PropertyDescriptor GEOLOCATION_FIELD_LABEL = new PropertyDescriptor.Builder()\n+            .name(\"geolocation.output.field.name\")\n+            .description(\"Label used to name the output record field for geolocation properties\")\n+            .required(false)\n+            .addValidator(StandardValidators.NON_EMPTY_VALIDATOR)\n+            .build();\n \n     /**\n      * Put a given document in elasticsearch bulk processor."
  },
  {
    "sha": "8b65d0319f1ced39c08138e0ffe2d7e9c7080d6b",
    "filename": "logisland-components/logisland-services/logisland-service-elasticsearch/logisland-service-elasticsearch_7_x-client/src/main/java/com/hurence/logisland/service/elasticsearch/ElasticsearchRecordConverter.java",
    "status": "modified",
    "additions": 13,
    "deletions": 1,
    "changes": 14,
    "blob_url": "https://github.com/Hurence/logisland/blob/22a433ab75df3ef948d8a0d765964c19b44b354b/logisland-components/logisland-services/logisland-service-elasticsearch/logisland-service-elasticsearch_7_x-client/src/main/java/com/hurence/logisland/service/elasticsearch/ElasticsearchRecordConverter.java",
    "raw_url": "https://github.com/Hurence/logisland/raw/22a433ab75df3ef948d8a0d765964c19b44b354b/logisland-components/logisland-services/logisland-service-elasticsearch/logisland-service-elasticsearch_7_x-client/src/main/java/com/hurence/logisland/service/elasticsearch/ElasticsearchRecordConverter.java",
    "contents_url": "https://api.github.com/repos/Hurence/logisland/contents/logisland-components/logisland-services/logisland-service-elasticsearch/logisland-service-elasticsearch_7_x-client/src/main/java/com/hurence/logisland/service/elasticsearch/ElasticsearchRecordConverter.java?ref=22a433ab75df3ef948d8a0d765964c19b44b354b",
    "patch": "@@ -41,6 +41,18 @@\n      * @return the json converted record\n      */\n     static String convertToString(Record record) {\n+      return convertToString(record, new String(\"location\"));\n+    }\n+    \n+    /**\n+     * Converts an Event into an Elasticsearch document\n+     * to be indexed later\n+     *e\n+     * @param record to convert\n+     * @param geolocationFieldLabel is the label for the geolocation field\n+     * @return the json converted record\n+     */\n+    static String convertToString(Record record, String geolocationFieldLabel) {\n         logger.trace(record.toString());\n         try {\n             SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\");\n@@ -117,7 +129,7 @@ static String convertToString(Record record) {\n \n \n             if((geolocation[0] != 0) && (geolocation[1] != 0)) {\n-                document.latlon(\"geolocation\", geolocation[0], geolocation[1]);\n+                document.latlon(geolocationFieldLabel, geolocation[0], geolocation[1]);\n             }\n \n "
  },
  {
    "sha": "ceb7a11ec7fca9341e6e108dd3a39776e71bc424",
    "filename": "logisland-components/logisland-services/logisland-service-elasticsearch/logisland-service-elasticsearch_7_x-client/src/main/java/com/hurence/logisland/service/elasticsearch/Elasticsearch_7_x_ClientService.java",
    "status": "modified",
    "additions": 7,
    "deletions": 1,
    "changes": 8,
    "blob_url": "https://github.com/Hurence/logisland/blob/22a433ab75df3ef948d8a0d765964c19b44b354b/logisland-components/logisland-services/logisland-service-elasticsearch/logisland-service-elasticsearch_7_x-client/src/main/java/com/hurence/logisland/service/elasticsearch/Elasticsearch_7_x_ClientService.java",
    "raw_url": "https://github.com/Hurence/logisland/raw/22a433ab75df3ef948d8a0d765964c19b44b354b/logisland-components/logisland-services/logisland-service-elasticsearch/logisland-service-elasticsearch_7_x-client/src/main/java/com/hurence/logisland/service/elasticsearch/Elasticsearch_7_x_ClientService.java",
    "contents_url": "https://api.github.com/repos/Hurence/logisland/contents/logisland-components/logisland-services/logisland-service-elasticsearch/logisland-service-elasticsearch_7_x-client/src/main/java/com/hurence/logisland/service/elasticsearch/Elasticsearch_7_x_ClientService.java?ref=22a433ab75df3ef948d8a0d765964c19b44b354b",
    "patch": "@@ -108,6 +108,7 @@\n     private volatile String authToken;\n     protected volatile transient BulkProcessor bulkProcessor;\n     protected volatile Map<String/*id*/, String/*errors*/> errors = new HashMap<>();\n+    private String geolocationFieldLabel;\n \n     @Override\n     public List<PropertyDescriptor> getSupportedPropertyDescriptors() {\n@@ -129,6 +130,7 @@\n         props.add(HOSTS);\n         props.add(PROP_SSL_CONTEXT_SERVICE);\n         props.add(CHARSET);\n+        props.add(GEOLOCATION_FIELD_LABEL);\n \n         return Collections.unmodifiableList(props);\n     }\n@@ -162,6 +164,7 @@ protected void createElasticsearchClient(ControllerServiceInitializationContext\n             final String password = context.getPropertyValue(PASSWORD).asString();\n             final String hosts = context.getPropertyValue(HOSTS).asString();\n             final boolean enableSsl = context.getPropertyValue(ENABLE_SSL).asBoolean();\n+            geolocationFieldLabel = context.getPropertyValue(GEOLOCATION_FIELD_LABEL).asString();\n \n             esHosts = getEsHosts(hosts, enableSsl);\n \n@@ -799,7 +802,7 @@ public void bulkPut(String indexName, Record record) throws DatastoreClientServi\n \n         }\n \n-        bulkPut(indexName, null, ElasticsearchRecordConverter.convertToString(record), Optional.of(record.getId()));\n+        bulkPut(indexName, null, convertRecordToString(record), Optional.of(record.getId()));\n     }\n \n     @Override\n@@ -829,6 +832,9 @@ public long queryCount(String query) {\n \n     @Override\n     public String convertRecordToString(Record record) {\n+        if (geolocationFieldLabel != null) {\n+          return ElasticsearchRecordConverter.convertToString(record, geolocationFieldLabel);\n+        }\n         return ElasticsearchRecordConverter.convertToString(record);\n     }\n "
  },
  {
    "sha": "b8c5d1450e210d748ef7cbc637c860c4d747a002",
    "filename": "logisland-core/logisland-engines/logisland-engine-spark_2_X/logisland-engine-spark_2_common/src/main/scala/com/hurence/logisland/engine/spark/KafkaStreamProcessingEngine.scala",
    "status": "modified",
    "additions": 33,
    "deletions": 14,
    "changes": 47,
    "blob_url": "https://github.com/Hurence/logisland/blob/22a433ab75df3ef948d8a0d765964c19b44b354b/logisland-core/logisland-engines/logisland-engine-spark_2_X/logisland-engine-spark_2_common/src/main/scala/com/hurence/logisland/engine/spark/KafkaStreamProcessingEngine.scala",
    "raw_url": "https://github.com/Hurence/logisland/raw/22a433ab75df3ef948d8a0d765964c19b44b354b/logisland-core/logisland-engines/logisland-engine-spark_2_X/logisland-engine-spark_2_common/src/main/scala/com/hurence/logisland/engine/spark/KafkaStreamProcessingEngine.scala",
    "contents_url": "https://api.github.com/repos/Hurence/logisland/contents/logisland-core/logisland-engines/logisland-engine-spark_2_X/logisland-engine-spark_2_common/src/main/scala/com/hurence/logisland/engine/spark/KafkaStreamProcessingEngine.scala?ref=22a433ab75df3ef948d8a0d765964c19b44b354b",
    "patch": "@@ -84,7 +84,7 @@ object KafkaStreamProcessingEngine {\n           \"local(\\\\[([0-9]+|\\\\*)(,[0-9]+)?\\\\])?|\" +\n           \"spark:\\\\/\\\\/[a-z0-9\\\\.\\\\-]+(:[0-9]+)?(,[a-z0-9\\\\.\\\\-]+(:[0-9]+)?)*|\" +\n           \"mesos:\\\\/\\\\/((zk:\\\\/\\\\/[a-z0-9\\\\.\\\\-]+:[0-9]+(,[a-z0-9\\\\.\\\\-]+:[0-9]+)*\\\\/mesos)|(([0-9]+\\\\.[0-9]+\\\\.[0-9]+\\\\.[0-9]+|[a-z][a-z0-9\\\\.\\\\-]+)(:[0-9]+)?))|\" +\n-          \"k8s://.+)$\")))\n+          \"k8s:\\\\/\\\\/.+)$\")))\n         .defaultValue(\"local[2]\")\n         .build\n \n@@ -375,6 +375,19 @@ object KafkaStreamProcessingEngine {\n       .required(false)\n       .addValidator(StandardValidators.BOOLEAN_VALIDATOR)\n       .build\n+\n+    val OVERWRITE_EXISTING = new AllowableValue(\"overwrite_existing\", \"overwrite existing field\", \"if field already exist\")\n+\n+    val KEEP_OLD_FIELD = new AllowableValue(\"keep_old_field\", \"keep only old field value\", \"keep only old field\")\n+\n+    val SPARK_CONF_POLICY = new PropertyDescriptor.Builder()\n+            .name(\"spark.conf.properties.policy\")\n+            .description(\"What to do when a field with the same name already exists ?\")\n+            .required(false)\n+            .defaultValue(OVERWRITE_EXISTING.getValue())\n+            .allowableValues(OVERWRITE_EXISTING, KEEP_OLD_FIELD)\n+            .build();\n+\n }\n \n class KafkaStreamProcessingEngine extends AbstractProcessingEngine {\n@@ -392,24 +405,27 @@ class KafkaStreamProcessingEngine extends AbstractProcessingEngine {\n     override def init(context: EngineContext): Unit  = {\n         super.init(context)\n         val engineContext = context.asInstanceOf[EngineContext]\n-        val sparkMaster = engineContext.getPropertyValue(KafkaStreamProcessingEngine.SPARK_MASTER).asString\n-        val appName = engineContext.getPropertyValue(KafkaStreamProcessingEngine.SPARK_APP_NAME).asString\n         batchDurationMs = engineContext.getPropertyValue(KafkaStreamProcessingEngine.SPARK_STREAMING_BATCH_DURATION).asInteger().intValue()\n         sparkStreamingTimeout = engineContext.getPropertyValue(KafkaStreamProcessingEngine.SPARK_STREAMING_TIMEOUT).asInteger().toInt\n-        /**\n-          * job configuration\n-          */\n-        conf.setAppName(appName)\n-        conf.setMaster(sparkMaster)\n+        val conflictPolicy = engineContext.getPropertyValue(KafkaStreamProcessingEngine.SPARK_CONF_POLICY).asString\n \n         def setConfProperty(conf: SparkConf, engineContext: EngineContext, propertyDescriptor: PropertyDescriptor) = {\n-\n             // Need to check if the properties are set because those properties are not \"requires\"\n             if (engineContext.getPropertyValue(propertyDescriptor).isSet) {\n-                conf.set(propertyDescriptor.getName, engineContext.getPropertyValue(propertyDescriptor).asString)\n+                if (conf.contains(propertyDescriptor.getName)) {\n+                    logger.warn(\"Property \" + propertyDescriptor.getName + \" already present in the sparkConfiguration with value \" + conf.get(propertyDescriptor.getName))\n+                }\n+\n+                if (conflictPolicy.equals(KafkaStreamProcessingEngine.OVERWRITE_EXISTING.getValue)) {\n+                    conf.set(propertyDescriptor.getName, engineContext.getPropertyValue(propertyDescriptor).asString)\n+                    logger.info(\"Property \" + propertyDescriptor.getName + \" set in the sparkConfiguration with value \" + conf.get(propertyDescriptor.getName))\n+                }\n             }\n         }\n \n+        setConfProperty(conf, engineContext, KafkaStreamProcessingEngine.SPARK_APP_NAME)\n+        setConfProperty(conf, engineContext, KafkaStreamProcessingEngine.SPARK_MASTER)\n+\n         setConfProperty(conf, engineContext, KafkaStreamProcessingEngine.SPARK_STREAMING_UI_RETAINED_BATCHES)\n         setConfProperty(conf, engineContext, KafkaStreamProcessingEngine.SPARK_STREAMING_RECEIVER_WAL_ENABLE)\n         setConfProperty(conf, engineContext, KafkaStreamProcessingEngine.SPARK_STREAMING_KAFKA_MAXRETRIES)\n@@ -436,19 +452,19 @@ class KafkaStreamProcessingEngine extends AbstractProcessingEngine {\n \n         conf.set(\"spark.kryo.registrator\", \"com.hurence.logisland.util.spark.ProtoBufRegistrator\")\n \n-        if (sparkMaster startsWith \"yarn\") {\n+        if (conf.get(KafkaStreamProcessingEngine.SPARK_MASTER.getName) startsWith \"yarn\") {\n             // Note that SPARK_YARN_DEPLOYMODE is not used by spark itself but only by spark-submit CLI\n             // That's why we do not need to propagate it here\n             setConfProperty(conf, engineContext, KafkaStreamProcessingEngine.SPARK_YARN_QUEUE)\n         }\n \n+        logger.info(\"Configuration from logisland main\")\n+        logger.info(conf.toDebugString)\n+        \n         val sparkContext = getCurrentSparkContext()\n \n         UserMetricsSystem.initialize(sparkContext, \"LogislandMetrics\")\n \n-\n-\n-\n         /**\n           * shutdown context gracefully\n           */\n@@ -495,6 +511,8 @@ class KafkaStreamProcessingEngine extends AbstractProcessingEngine {\n             }\n         })\n \n+        val sparkMaster = conf.get(KafkaStreamProcessingEngine.SPARK_MASTER.getName)\n+        val appName = conf.get(KafkaStreamProcessingEngine.SPARK_APP_NAME.getName)\n         logger.info(s\"spark context initialized with master:$sparkMaster, \" +\n             s\"appName:$appName, \" +\n             s\"batchDuration:$batchDurationMs \")\n@@ -538,6 +556,7 @@ class KafkaStreamProcessingEngine extends AbstractProcessingEngine {\n         descriptors.add(KafkaStreamProcessingEngine.SPARK_MESOS_CORE_MAX)\n         descriptors.add(KafkaStreamProcessingEngine.SPARK_TOTAL_EXECUTOR_CORES)\n         descriptors.add(KafkaStreamProcessingEngine.SPARK_SUPERVISE)\n+        descriptors.add(KafkaStreamProcessingEngine.SPARK_CONF_POLICY)\n         Collections.unmodifiableList(descriptors)\n     }\n "
  },
  {
    "sha": "d738c9a238944b8fb9d6454c9860a84c73dac0b9",
    "filename": "logisland-core/logisland-framework/logisland-utils/pom.xml",
    "status": "modified",
    "additions": 1,
    "deletions": 10,
    "changes": 11,
    "blob_url": "https://github.com/Hurence/logisland/blob/22a433ab75df3ef948d8a0d765964c19b44b354b/logisland-core/logisland-framework/logisland-utils/pom.xml",
    "raw_url": "https://github.com/Hurence/logisland/raw/22a433ab75df3ef948d8a0d765964c19b44b354b/logisland-core/logisland-framework/logisland-utils/pom.xml",
    "contents_url": "https://api.github.com/repos/Hurence/logisland/contents/logisland-core/logisland-framework/logisland-utils/pom.xml?ref=22a433ab75df3ef948d8a0d765964c19b44b354b",
    "patch": "@@ -156,6 +156,7 @@\n                             <artifactSet>\n                                 <includes>\n                                     <include>org.apache.avro:avro</include>\n+                                    <include>io.confluent</include>\n                                 </includes>\n                             </artifactSet>\n                             <filters>\n@@ -183,16 +184,6 @@\n                                 <transformer\n                                         implementation=\"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\"/>\n                             </transformers>\n-<!--\n-                            <relocations>\n-                                <relocation>\n-                                    <pattern>org.apache.avro</pattern>\n-                                    <shadedPattern>logisland.repackaged.org.apache.avro</shadedPattern>\n-                                </relocation>\n-\n-                            </relocations>\n--->\n-\n                         </configuration>\n                     </execution>\n                 </executions>"
  },
  {
    "sha": "1793cbc9405684a757ff10474aa4294d63faddb5",
    "filename": "logisland-docker/spark8s/Dockerfile",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/Hurence/logisland/blob/22a433ab75df3ef948d8a0d765964c19b44b354b/logisland-docker/spark8s/Dockerfile",
    "raw_url": "https://github.com/Hurence/logisland/raw/22a433ab75df3ef948d8a0d765964c19b44b354b/logisland-docker/spark8s/Dockerfile",
    "contents_url": "https://api.github.com/repos/Hurence/logisland/contents/logisland-docker/spark8s/Dockerfile?ref=22a433ab75df3ef948d8a0d765964c19b44b354b",
    "patch": "@@ -1,4 +1,4 @@\n-FROM gcr.io/spark-operator/spark:v2.4.0\n+FROM gcr.io/spark-operator/spark:v2.4.5\n \n LABEL maintainer=\"support@hurence.com\"\n "
  }
]
