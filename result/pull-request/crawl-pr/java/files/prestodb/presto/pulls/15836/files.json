[
  {
    "sha": "db25773f6611df3ecf4e98fcf72c478ab11e736e",
    "filename": "pom.xml",
    "status": "modified",
    "additions": 14,
    "deletions": 1,
    "changes": 15,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/pom.xml",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/pom.xml",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/pom.xml?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -61,7 +61,7 @@\n         <dep.testng.version>6.10</dep.testng.version>\n         <dep.assertj-core.version>3.8.0</dep.assertj-core.version>\n         <dep.logback.version>1.2.3</dep.logback.version>\n-        <dep.parquet.version>1.10.0</dep.parquet.version>\n+        <dep.parquet.version>1.11.0</dep.parquet.version>\n         <dep.nexus-staging-plugin.version>1.6.8</dep.nexus-staging-plugin.version>\n         <dep.asm.version>9.0</dep.asm.version>\n         <dep.gcs.version>1.9.17</dep.gcs.version>\n@@ -105,6 +105,7 @@\n         <module>presto-hive-common</module>\n         <module>presto-hive-hadoop2</module>\n         <module>presto-hive-metastore</module>\n+        <module>presto-iceberg</module>\n         <module>presto-i18n-functions</module>\n         <module>presto-teradata-functions</module>\n         <module>presto-example-http</module>\n@@ -282,6 +283,18 @@\n                 <version>${project.version}</version>\n             </dependency>\n \n+            <dependency>\n+                <groupId>com.facebook.presto</groupId>\n+                <artifactId>presto-iceberg</artifactId>\n+                <version>${project.version}</version>\n+            </dependency>\n+\n+            <dependency>\n+                <groupId>javax.annotation</groupId>\n+                <artifactId>javax.annotation-api</artifactId>\n+                <version>1.3.2</version>\n+            </dependency>\n+\n             <dependency>\n                 <groupId>com.facebook.presto</groupId>\n                 <artifactId>presto-example-http</artifactId>"
  },
  {
    "sha": "78003cb3c84bd0fe4e98dcace8fe11dfd5a7a460",
    "filename": "presto-geospatial/pom.xml",
    "status": "modified",
    "additions": 4,
    "deletions": 0,
    "changes": 4,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-geospatial/pom.xml",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-geospatial/pom.xml",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-geospatial/pom.xml?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -195,6 +195,10 @@\n                         <ignoredResourcePattern>parquet.thrift</ignoredResourcePattern>\n                         <ignoredResourcePattern>about.html</ignoredResourcePattern>\n                     </ignoredResourcePatterns>\n+                    <ignoredClassPatterns>\n+                        <ignoredClassPattern>shaded.parquet.it.unimi.dsi.fastutil.*</ignoredClassPattern>\n+                        <ignoredClassPattern>module-info</ignoredClassPattern>\n+                    </ignoredClassPatterns>\n                 </configuration>\n             </plugin>\n         </plugins>"
  },
  {
    "sha": "fb996d5ac64f51a17012d91864850a47ed99928c",
    "filename": "presto-hive-hadoop2/pom.xml",
    "status": "modified",
    "additions": 4,
    "deletions": 0,
    "changes": 4,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-hive-hadoop2/pom.xml",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-hive-hadoop2/pom.xml",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-hive-hadoop2/pom.xml?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -161,6 +161,10 @@\n                                 <ignoredResourcePattern>parquet.thrift</ignoredResourcePattern>\n                                 <ignoredResourcePattern>about.html</ignoredResourcePattern>\n                             </ignoredResourcePatterns>\n+                            <ignoredClassPatterns>\n+                                <ignoredClassPattern>shaded.parquet.it.unimi.dsi.fastutil.*</ignoredClassPattern>\n+                                <ignoredClassPattern>module-info</ignoredClassPattern>\n+                            </ignoredClassPatterns>\n                         </configuration>\n                     </plugin>\n                 </plugins>"
  },
  {
    "sha": "2d7126a86400ba4aa20a2274c157966cab62cd7f",
    "filename": "presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/file/FileHiveMetastore.java",
    "status": "modified",
    "additions": 13,
    "deletions": 2,
    "changes": 15,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/file/FileHiveMetastore.java",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/file/FileHiveMetastore.java",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/file/FileHiveMetastore.java?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -110,6 +110,8 @@\n     private static final String ADMIN_ROLE_NAME = \"admin\";\n     private static final String PRESTO_SCHEMA_FILE_NAME = \".prestoSchema\";\n     private static final String PRESTO_PERMISSIONS_DIRECTORY_NAME = \".prestoPermissions\";\n+    private static final String ICEBERG_TABLE_TYPE_NAME = \"table_type\";\n+    private static final String ICEBERG_TABLE_TYPE_VALUE = \"iceberg\";\n     // todo there should be a way to manage the admins list\n     private static final Set<String> ADMIN_USERS = ImmutableSet.of(\"admin\", \"hive\", \"hdfs\");\n \n@@ -248,7 +250,7 @@ else if (table.getTableType().equals(EXTERNAL_TABLE)) {\n                 if (!externalFileSystem.isDirectory(externalLocation)) {\n                     throw new PrestoException(HIVE_METASTORE_ERROR, \"External table location does not exist\");\n                 }\n-                if (isChildDirectory(catalogDirectory, externalLocation)) {\n+                if (isChildDirectory(catalogDirectory, externalLocation) && !isIcebergTable(table.getParameters())) {\n                     throw new PrestoException(HIVE_METASTORE_ERROR, \"External table location can not be inside the system metadata directory\");\n                 }\n             }\n@@ -458,9 +460,13 @@ public synchronized void renameTable(String databaseName, String tableName, Stri\n         requireNonNull(newDatabaseName, \"newDatabaseName is null\");\n         requireNonNull(newTableName, \"newTableName is null\");\n \n-        getRequiredTable(databaseName, tableName);\n+        Table table = getRequiredTable(databaseName, tableName);\n         getRequiredDatabase(newDatabaseName);\n \n+        if (isIcebergTable(table.getParameters())) {\n+            throw new PrestoException(NOT_SUPPORTED, \"Rename not supported for Iceberg tables\");\n+        }\n+\n         // verify new table does not exist\n         verifyTableNotExists(newDatabaseName, newTableName);\n \n@@ -474,6 +480,11 @@ public synchronized void renameTable(String databaseName, String tableName, Stri\n         }\n     }\n \n+    private static boolean isIcebergTable(Map<String, String> parameters)\n+    {\n+        return ICEBERG_TABLE_TYPE_VALUE.equalsIgnoreCase(parameters.get(ICEBERG_TABLE_TYPE_NAME));\n+    }\n+\n     @Override\n     public synchronized void addColumn(String databaseName, String tableName, String columnName, HiveType columnType, String columnComment)\n     {"
  },
  {
    "sha": "80998c12af9a327bf5c35128cfa214500a34bec1",
    "filename": "presto-hive/pom.xml",
    "status": "modified",
    "additions": 4,
    "deletions": 0,
    "changes": 4,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-hive/pom.xml",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-hive/pom.xml",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-hive/pom.xml?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -398,6 +398,10 @@\n                         <ignoredResourcePattern>parquet.thrift</ignoredResourcePattern>\n                         <ignoredResourcePattern>about.html</ignoredResourcePattern>\n                     </ignoredResourcePatterns>\n+                    <ignoredClassPatterns>\n+                        <ignoredClassPattern>shaded.parquet.it.unimi.dsi.fastutil.*</ignoredClassPattern>\n+                        <ignoredClassPattern>module-info</ignoredClassPattern>\n+                    </ignoredClassPatterns>\n                 </configuration>\n             </plugin>\n "
  },
  {
    "sha": "dd9aa039a7d988b2154d582d9d75c4c4bad68d48",
    "filename": "presto-hive/src/main/java/com/facebook/presto/hive/HiveMetadata.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-hive/src/main/java/com/facebook/presto/hive/HiveMetadata.java",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-hive/src/main/java/com/facebook/presto/hive/HiveMetadata.java",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-hive/src/main/java/com/facebook/presto/hive/HiveMetadata.java?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -1330,7 +1330,7 @@ private static Table buildTableObject(\n         return tableBuilder.build();\n     }\n \n-    private static PrincipalPrivileges buildInitialPrivilegeSet(String tableOwner)\n+    public static PrincipalPrivileges buildInitialPrivilegeSet(String tableOwner)\n     {\n         PrestoPrincipal owner = new PrestoPrincipal(USER, tableOwner);\n         return new PrincipalPrivileges("
  },
  {
    "sha": "46ba878244a6f6c9821eb22fe42e81ede601c2d7",
    "filename": "presto-iceberg/pom.xml",
    "status": "added",
    "additions": 425,
    "deletions": 0,
    "changes": 425,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/pom.xml",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/pom.xml",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-iceberg/pom.xml?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -0,0 +1,425 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+    <modelVersion>4.0.0</modelVersion>\n+    <parent>\n+        <groupId>com.facebook.presto</groupId>\n+        <artifactId>presto-root</artifactId>\n+        <version>0.250-SNAPSHOT</version>\n+    </parent>\n+    <artifactId>presto-iceberg</artifactId>\n+    <description>Presto - Iceberg Connector</description>\n+    <packaging>presto-plugin</packaging>\n+\n+    <properties>\n+        <air.main.basedir>${project.parent.basedir}</air.main.basedir>\n+        <dep.iceberg.version>0.9.0</dep.iceberg.version>\n+    </properties>\n+\n+    <dependencies>\n+        <dependency>\n+            <groupId>com.facebook.airlift</groupId>\n+            <artifactId>concurrent</artifactId>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.facebook.presto</groupId>\n+            <artifactId>presto-client</artifactId>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.facebook.presto</groupId>\n+            <artifactId>presto-hive-common</artifactId>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.apache.avro</groupId>\n+            <artifactId>avro</artifactId>\n+            <version>1.9.2</version>\n+            <scope>runtime</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.apache.parquet</groupId>\n+            <artifactId>parquet-format-structures</artifactId>\n+            <version>${dep.parquet.version}</version>\n+            <scope>runtime</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.apache.parquet</groupId>\n+            <artifactId>parquet-common</artifactId>\n+            <version>${dep.parquet.version}</version>\n+            <exclusions>\n+                <exclusion>\n+                    <groupId>org.apache.parquet</groupId>\n+                    <artifactId>parquet-format</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.slf4j</groupId>\n+                    <artifactId>slf4j-api</artifactId>\n+                </exclusion>\n+            </exclusions>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.apache.parquet</groupId>\n+            <artifactId>parquet-column</artifactId>\n+            <version>${dep.parquet.version}</version>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.facebook.presto</groupId>\n+            <artifactId>presto-hive</artifactId>\n+            <exclusions>\n+                <exclusion>\n+                    <groupId>org.apache.parquet</groupId>\n+                    <artifactId>parquet-common</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.codehaus.mojo</groupId>\n+                    <artifactId>animal-sniffer-annotations</artifactId>\n+                </exclusion>\n+            </exclusions>\n+            <scope>compile</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.facebook.presto</groupId>\n+            <artifactId>presto-hive-metastore</artifactId>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.facebook.presto</groupId>\n+            <artifactId>presto-memory-context</artifactId>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.facebook.presto</groupId>\n+            <artifactId>presto-parquet</artifactId>\n+            <exclusions>\n+                <exclusion>\n+                    <groupId>org.apache.parquet</groupId>\n+                    <artifactId>parquet-format</artifactId>\n+                </exclusion>\n+            </exclusions>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.facebook.presto</groupId>\n+            <artifactId>presto-plugin-toolkit</artifactId>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.facebook.presto.hadoop</groupId>\n+            <artifactId>hadoop-apache2</artifactId>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.facebook.presto.hive</groupId>\n+            <artifactId>hive-apache</artifactId>\n+            <exclusions>\n+                <exclusion>\n+                    <groupId>org.apache.parquet</groupId>\n+                    <artifactId>parquet-format-structures</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.apache.parquet</groupId>\n+                    <artifactId>parquet-common</artifactId>\n+                </exclusion>\n+            </exclusions>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.facebook.airlift</groupId>\n+            <artifactId>bootstrap</artifactId>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.facebook.airlift</groupId>\n+            <artifactId>configuration</artifactId>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.facebook.airlift</groupId>\n+            <artifactId>event</artifactId>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.facebook.airlift</groupId>\n+            <artifactId>json</artifactId>\n+            <scope>compile</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.facebook.airlift</groupId>\n+            <artifactId>log</artifactId>\n+            <scope>compile</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>io.airlift</groupId>\n+            <artifactId>units</artifactId>\n+            <scope>provided</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.facebook.drift</groupId>\n+            <artifactId>drift-api</artifactId>\n+            <scope>provided</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.fasterxml.jackson.core</groupId>\n+            <artifactId>jackson-core</artifactId>\n+            <version>2.10.2</version>\n+            <scope>compile</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.fasterxml.jackson.core</groupId>\n+            <artifactId>jackson-databind</artifactId>\n+            <version>2.10.2</version>\n+            <scope>compile</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.google.code.findbugs</groupId>\n+            <artifactId>jsr305</artifactId>\n+            <optional>true</optional>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.google.guava</groupId>\n+            <artifactId>guava</artifactId>\n+            <exclusions>\n+                <exclusion>\n+                    <groupId>org.codehaus.mojo</groupId>\n+                    <artifactId>animal-sniffer-annotations</artifactId>\n+                </exclusion>\n+            </exclusions>\n+            <scope>compile</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.google.inject</groupId>\n+            <artifactId>guice</artifactId>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>javax.inject</groupId>\n+            <artifactId>javax.inject</artifactId>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>javax.validation</groupId>\n+            <artifactId>validation-api</artifactId>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>joda-time</groupId>\n+            <artifactId>joda-time</artifactId>\n+        </dependency>\n+\n+        <!-- Iceberg -->\n+        <dependency>\n+            <groupId>org.apache.iceberg</groupId>\n+            <artifactId>iceberg-bundled-guava</artifactId>\n+            <version>${dep.iceberg.version}</version>\n+            <scope>runtime</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.apache.iceberg</groupId>\n+            <artifactId>iceberg-api</artifactId>\n+            <version>${dep.iceberg.version}</version>\n+            <exclusions>\n+                <exclusion>\n+                    <groupId>org.apache.iceberg</groupId>\n+                    <artifactId>iceberg-bundled-guava</artifactId>\n+                </exclusion>\n+            </exclusions>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.apache.iceberg</groupId>\n+            <artifactId>iceberg-core</artifactId>\n+            <version>${dep.iceberg.version}</version>\n+            <exclusions>\n+                <exclusion>\n+                    <groupId>org.apache.avro</groupId>\n+                    <artifactId>avro</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>com.google.errorprone</groupId>\n+                    <artifactId>error_prone_annotations</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.checkerframework</groupId>\n+                    <artifactId>checker-qual</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>com.fasterxml.jackson.core</groupId>\n+                    <artifactId>jackson-core</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>com.fasterxml.jackson.core</groupId>\n+                    <artifactId>jackson-databind</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>org.apache.iceberg</groupId>\n+                    <artifactId>iceberg-bundled-guava</artifactId>\n+                </exclusion>\n+            </exclusions>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.apache.iceberg</groupId>\n+            <artifactId>iceberg-hive</artifactId>\n+            <version>${dep.iceberg.version}</version>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.apache.iceberg</groupId>\n+            <artifactId>iceberg-parquet</artifactId>\n+            <version>${dep.iceberg.version}</version>\n+            <exclusions>\n+                <exclusion>\n+                    <groupId>org.apache.parquet</groupId>\n+                    <artifactId>parquet-avro</artifactId>\n+                </exclusion>\n+            </exclusions>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.weakref</groupId>\n+            <artifactId>jmxutils</artifactId>\n+            <scope>compile</scope>\n+        </dependency>\n+\n+        <!-- used by tests but also needed transitively -->\n+        <dependency>\n+            <groupId>com.facebook.airlift</groupId>\n+            <artifactId>log-manager</artifactId>\n+            <scope>runtime</scope>\n+        </dependency>\n+\n+        <!-- Presto SPI -->\n+        <dependency>\n+            <groupId>com.facebook.presto</groupId>\n+            <artifactId>presto-spi</artifactId>\n+            <scope>provided</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.facebook.presto</groupId>\n+            <artifactId>presto-common</artifactId>\n+            <scope>provided</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>io.airlift</groupId>\n+            <artifactId>slice</artifactId>\n+            <scope>provided</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.fasterxml.jackson.core</groupId>\n+            <artifactId>jackson-annotations</artifactId>\n+            <version>2.10.2</version>\n+            <scope>provided</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.openjdk.jol</groupId>\n+            <artifactId>jol-core</artifactId>\n+            <scope>provided</scope>\n+        </dependency>\n+\n+        <!-- for testing -->\n+        <dependency>\n+            <groupId>com.facebook.presto</groupId>\n+            <artifactId>presto-cache</artifactId>\n+            <scope>compile</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.facebook.presto</groupId>\n+            <artifactId>presto-main</artifactId>\n+            <scope>test</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.facebook.presto</groupId>\n+            <artifactId>presto-tests</artifactId>\n+            <scope>test</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>com.facebook.presto</groupId>\n+            <artifactId>presto-tpch</artifactId>\n+            <scope>test</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>io.airlift.tpch</groupId>\n+            <artifactId>tpch</artifactId>\n+            <scope>test</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.assertj</groupId>\n+            <artifactId>assertj-core</artifactId>\n+            <scope>test</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.jetbrains</groupId>\n+            <artifactId>annotations</artifactId>\n+            <scope>test</scope>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.testng</groupId>\n+            <artifactId>testng</artifactId>\n+            <scope>test</scope>\n+        </dependency>\n+    </dependencies>\n+\n+    <build>\n+        <pluginManagement>\n+            <plugins>\n+                <plugin>\n+                    <groupId>org.basepom.maven</groupId>\n+                    <artifactId>duplicate-finder-maven-plugin</artifactId>\n+                    <configuration>\n+                        <ignoredResourcePatterns>\n+                            <ignoredResourcePattern>parquet.thrift</ignoredResourcePattern>\n+                            <ignoredResourcePattern>about.html</ignoredResourcePattern>\n+                            <ignoredResourcePattern>org.apache.avro.data/Json.avsc</ignoredResourcePattern>\n+                        </ignoredResourcePatterns>\n+                        <ignoredClassPatterns>\n+                            <ignoredClassPattern>shaded.parquet.it.unimi.dsi.fastutil.*</ignoredClassPattern>\n+                            <ignoredClassPattern>module-info</ignoredClassPattern>\n+                            <ignoredClassPattern>org.apache.avro.*</ignoredClassPattern>\n+                            <ignoredClassPattern>org.apache.parquet.*</ignoredClassPattern>\n+                        </ignoredClassPatterns>\n+                    </configuration>\n+                </plugin>\n+                <plugin>\n+                    <groupId>org.apache.maven.plugins</groupId>\n+                    <artifactId>maven-enforcer-plugin</artifactId>\n+                    <configuration>\n+                        <rules>\n+                            <requireUpperBoundDeps>\n+                                <excludes combine.children=\"append\">\n+                                    <exclude>com.google.guava:guava</exclude>\n+                                </excludes>\n+                            </requireUpperBoundDeps>\n+                        </rules>\n+                    </configuration>\n+                </plugin>\n+\n+            </plugins>\n+        </pluginManagement>\n+    </build>\n+</project>"
  },
  {
    "sha": "bc3f31d0728e989abe867f21daec796b059ad47c",
    "filename": "presto-iceberg/src/main/java/com/facebook/presto/iceberg/CommitTaskData.java",
    "status": "added",
    "additions": 57,
    "deletions": 0,
    "changes": 57,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/CommitTaskData.java",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/CommitTaskData.java",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-iceberg/src/main/java/com/facebook/presto/iceberg/CommitTaskData.java?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import java.util.Optional;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class CommitTaskData\n+{\n+    private final String path;\n+    private final MetricsWrapper metrics;\n+    private final Optional<String> partitionDataJson;\n+\n+    @JsonCreator\n+    public CommitTaskData(\n+            @JsonProperty(\"path\") String path,\n+            @JsonProperty(\"metrics\") MetricsWrapper metrics,\n+            @JsonProperty(\"partitionDataJson\") Optional<String> partitionDataJson)\n+    {\n+        this.path = requireNonNull(path, \"path is null\");\n+        this.metrics = requireNonNull(metrics, \"metrics is null\");\n+        this.partitionDataJson = requireNonNull(partitionDataJson, \"partitionDataJson is null\");\n+    }\n+\n+    @JsonProperty\n+    public String getPath()\n+    {\n+        return path;\n+    }\n+\n+    @JsonProperty\n+    public MetricsWrapper getMetrics()\n+    {\n+        return metrics;\n+    }\n+\n+    @JsonProperty\n+    public Optional<String> getPartitionDataJson()\n+    {\n+        return partitionDataJson;\n+    }\n+}"
  },
  {
    "sha": "7a7a0b887c1f504816cfc8d1ceae5f74b222d820",
    "filename": "presto-iceberg/src/main/java/com/facebook/presto/iceberg/ExpressionConverter.java",
    "status": "added",
    "additions": 203,
    "deletions": 0,
    "changes": 203,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/ExpressionConverter.java",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/ExpressionConverter.java",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-iceberg/src/main/java/com/facebook/presto/iceberg/ExpressionConverter.java?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -0,0 +1,203 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg;\n+\n+import com.facebook.presto.common.predicate.Domain;\n+import com.facebook.presto.common.predicate.Marker;\n+import com.facebook.presto.common.predicate.Range;\n+import com.facebook.presto.common.predicate.SortedRangeSet;\n+import com.facebook.presto.common.predicate.TupleDomain;\n+import com.facebook.presto.common.predicate.ValueSet;\n+import com.facebook.presto.common.type.ArrayType;\n+import com.facebook.presto.common.type.DateType;\n+import com.facebook.presto.common.type.DecimalType;\n+import com.facebook.presto.common.type.Decimals;\n+import com.facebook.presto.common.type.IntegerType;\n+import com.facebook.presto.common.type.MapType;\n+import com.facebook.presto.common.type.RealType;\n+import com.facebook.presto.common.type.RowType;\n+import com.facebook.presto.common.type.Type;\n+import com.facebook.presto.common.type.VarbinaryType;\n+import com.facebook.presto.common.type.VarcharType;\n+import com.google.common.base.VerifyException;\n+import io.airlift.slice.Slice;\n+import org.apache.iceberg.expressions.Expression;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static com.facebook.presto.common.predicate.Marker.Bound.ABOVE;\n+import static com.facebook.presto.common.predicate.Marker.Bound.BELOW;\n+import static com.facebook.presto.common.predicate.Marker.Bound.EXACTLY;\n+import static com.google.common.base.MoreObjects.firstNonNull;\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.util.Objects.requireNonNull;\n+import static org.apache.iceberg.expressions.Expressions.alwaysFalse;\n+import static org.apache.iceberg.expressions.Expressions.alwaysTrue;\n+import static org.apache.iceberg.expressions.Expressions.and;\n+import static org.apache.iceberg.expressions.Expressions.equal;\n+import static org.apache.iceberg.expressions.Expressions.greaterThan;\n+import static org.apache.iceberg.expressions.Expressions.greaterThanOrEqual;\n+import static org.apache.iceberg.expressions.Expressions.isNull;\n+import static org.apache.iceberg.expressions.Expressions.lessThan;\n+import static org.apache.iceberg.expressions.Expressions.lessThanOrEqual;\n+import static org.apache.iceberg.expressions.Expressions.not;\n+import static org.apache.iceberg.expressions.Expressions.or;\n+\n+public final class ExpressionConverter\n+{\n+    private ExpressionConverter() {}\n+\n+    public static Expression toIcebergExpression(TupleDomain<IcebergColumnHandle> tupleDomain)\n+    {\n+        if (tupleDomain.isAll()) {\n+            return alwaysTrue();\n+        }\n+        if (!tupleDomain.getDomains().isPresent()) {\n+            return alwaysFalse();\n+        }\n+        Map<IcebergColumnHandle, Domain> domainMap = tupleDomain.getDomains().get();\n+        Expression expression = alwaysTrue();\n+        for (Map.Entry<IcebergColumnHandle, Domain> entry : domainMap.entrySet()) {\n+            IcebergColumnHandle columnHandle = entry.getKey();\n+            Domain domain = entry.getValue();\n+            expression = and(expression, toIcebergExpression(columnHandle.getName(), columnHandle.getType(), domain));\n+        }\n+        return expression;\n+    }\n+\n+    private static Expression toIcebergExpression(String columnName, Type type, Domain domain)\n+    {\n+        if (domain.isAll()) {\n+            return alwaysTrue();\n+        }\n+        if (domain.getValues().isNone()) {\n+            return domain.isNullAllowed() ? isNull(columnName) : alwaysFalse();\n+        }\n+\n+        if (domain.getValues().isAll()) {\n+            return domain.isNullAllowed() ? alwaysTrue() : not(isNull(columnName));\n+        }\n+\n+        // Skip structural types. TODO: Evaluate Apache Iceberg's support for predicate on structural types\n+        if (type instanceof ArrayType || type instanceof MapType || type instanceof RowType) {\n+            return alwaysTrue();\n+        }\n+\n+        ValueSet domainValues = domain.getValues();\n+        Expression expression = null;\n+        if (domain.isNullAllowed()) {\n+            expression = isNull(columnName);\n+        }\n+\n+        if (domainValues instanceof SortedRangeSet) {\n+            List<Range> orderedRanges = ((SortedRangeSet) domainValues).getOrderedRanges();\n+            expression = firstNonNull(expression, alwaysFalse());\n+\n+            for (Range range : orderedRanges) {\n+                Marker low = range.getLow();\n+                Marker high = range.getHigh();\n+                Marker.Bound lowBound = low.getBound();\n+                Marker.Bound highBound = high.getBound();\n+\n+                // case col <> 'val' is represented as (col < 'val' or col > 'val')\n+                if (lowBound == EXACTLY && highBound == EXACTLY) {\n+                    // case ==\n+                    if (getIcebergLiteralValue(type, low).equals(getIcebergLiteralValue(type, high))) {\n+                        expression = or(expression, equal(columnName, getIcebergLiteralValue(type, low)));\n+                    }\n+                    else { // case between\n+                        Expression between = and(\n+                                greaterThanOrEqual(columnName, getIcebergLiteralValue(type, low)),\n+                                lessThanOrEqual(columnName, getIcebergLiteralValue(type, high)));\n+                        expression = or(expression, between);\n+                    }\n+                }\n+                else {\n+                    if (lowBound == EXACTLY && low.getValueBlock().isPresent()) {\n+                        // case >=\n+                        expression = or(expression, greaterThanOrEqual(columnName, getIcebergLiteralValue(type, low)));\n+                    }\n+                    else if (lowBound == ABOVE && low.getValueBlock().isPresent()) {\n+                        // case >\n+                        expression = or(expression, greaterThan(columnName, getIcebergLiteralValue(type, low)));\n+                    }\n+\n+                    if (highBound == EXACTLY && high.getValueBlock().isPresent()) {\n+                        // case <=\n+                        if (low.getValueBlock().isPresent()) {\n+                            expression = and(expression, lessThanOrEqual(columnName, getIcebergLiteralValue(type, high)));\n+                        }\n+                        else {\n+                            expression = or(expression, lessThanOrEqual(columnName, getIcebergLiteralValue(type, high)));\n+                        }\n+                    }\n+                    else if (highBound == BELOW && high.getValueBlock().isPresent()) {\n+                        // case <\n+                        if (low.getValueBlock().isPresent()) {\n+                            expression = and(expression, lessThan(columnName, getIcebergLiteralValue(type, high)));\n+                        }\n+                        else {\n+                            expression = or(expression, lessThan(columnName, getIcebergLiteralValue(type, high)));\n+                        }\n+                    }\n+                }\n+            }\n+            return expression;\n+        }\n+\n+        throw new VerifyException(\"Did not expect a domain value set other than SortedRangeSet but got \" + domainValues.getClass().getSimpleName());\n+    }\n+\n+    private static Object getIcebergLiteralValue(Type type, Marker marker)\n+    {\n+        if (type instanceof IntegerType) {\n+            return toIntExact((long) marker.getValue());\n+        }\n+\n+        if (type instanceof RealType) {\n+            return intBitsToFloat(toIntExact((long) marker.getValue()));\n+        }\n+\n+        // TODO: Remove this conversion once we move to next iceberg version\n+        if (type instanceof DateType) {\n+            return toIntExact(((Long) marker.getValue()));\n+        }\n+\n+        if (type instanceof VarcharType) {\n+            return ((Slice) marker.getValue()).toStringUtf8();\n+        }\n+\n+        if (type instanceof VarbinaryType) {\n+            return ByteBuffer.wrap(((Slice) marker.getValue()).getBytes());\n+        }\n+\n+        if (type instanceof DecimalType) {\n+            DecimalType decimalType = (DecimalType) type;\n+            Object value = requireNonNull(marker.getValue(), \"The value of the marker must be non-null\");\n+            if (Decimals.isShortDecimal(decimalType)) {\n+                checkArgument(value instanceof Long, \"A short decimal should be represented by a Long value but was %s\", value.getClass().getName());\n+                return BigDecimal.valueOf((long) value).movePointLeft(decimalType.getScale());\n+            }\n+            checkArgument(value instanceof Slice, \"A long decimal should be represented by a Slice value but was %s\", value.getClass().getName());\n+            return new BigDecimal(Decimals.decodeUnscaledValue((Slice) value), decimalType.getScale());\n+        }\n+\n+        return marker.getValue();\n+    }\n+}"
  },
  {
    "sha": "1753d21a438ff07ff14158fb11c423ebc26738b4",
    "filename": "presto-iceberg/src/main/java/com/facebook/presto/iceberg/HdfsFileIo.java",
    "status": "added",
    "additions": 64,
    "deletions": 0,
    "changes": 64,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/HdfsFileIo.java",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/HdfsFileIo.java",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-iceberg/src/main/java/com/facebook/presto/iceberg/HdfsFileIo.java?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg;\n+\n+import com.facebook.presto.hive.HdfsContext;\n+import com.facebook.presto.hive.HdfsEnvironment;\n+import com.facebook.presto.spi.PrestoException;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.io.OutputFile;\n+\n+import java.io.IOException;\n+\n+import static com.facebook.presto.iceberg.IcebergErrorCode.ICEBERG_FILESYSTEM_ERROR;\n+import static java.util.Objects.requireNonNull;\n+\n+public class HdfsFileIo\n+        implements FileIO\n+{\n+    private final HdfsEnvironment environment;\n+    private final HdfsContext context;\n+\n+    public HdfsFileIo(HdfsEnvironment environment, HdfsContext context)\n+    {\n+        this.environment = requireNonNull(environment, \"environment is null\");\n+        this.context = requireNonNull(context, \"context is null\");\n+    }\n+\n+    @Override\n+    public InputFile newInputFile(String path)\n+    {\n+        return new HdfsInputFile(new Path(path), environment, context);\n+    }\n+\n+    @Override\n+    public OutputFile newOutputFile(String path)\n+    {\n+        return new HdfsOutputFile(new Path(path), environment, context);\n+    }\n+\n+    @Override\n+    public void deleteFile(String pathString)\n+    {\n+        Path path = new Path(pathString);\n+        try {\n+            environment.doAs(context.getIdentity().getUser(), () -> environment.getFileSystem(context, path).delete(path, false));\n+        }\n+        catch (IOException e) {\n+            throw new PrestoException(ICEBERG_FILESYSTEM_ERROR, \"Failed to delete file: \" + path, e);\n+        }\n+    }\n+}"
  },
  {
    "sha": "c85ede8ee6f1a83ccaee00535797a39eb2878c39",
    "filename": "presto-iceberg/src/main/java/com/facebook/presto/iceberg/HdfsInputFile.java",
    "status": "added",
    "additions": 73,
    "deletions": 0,
    "changes": 73,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/HdfsInputFile.java",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/HdfsInputFile.java",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-iceberg/src/main/java/com/facebook/presto/iceberg/HdfsInputFile.java?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg;\n+\n+import com.facebook.presto.hive.HdfsContext;\n+import com.facebook.presto.hive.HdfsEnvironment;\n+import com.facebook.presto.spi.PrestoException;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.io.SeekableInputStream;\n+\n+import java.io.IOException;\n+\n+import static com.facebook.presto.iceberg.IcebergErrorCode.ICEBERG_FILESYSTEM_ERROR;\n+import static java.util.Objects.requireNonNull;\n+\n+public class HdfsInputFile\n+        implements InputFile\n+{\n+    private final InputFile delegate;\n+    private final HdfsEnvironment environment;\n+    private final String user;\n+\n+    public HdfsInputFile(Path path, HdfsEnvironment environment, HdfsContext context)\n+    {\n+        requireNonNull(path, \"path is null\");\n+        this.environment = requireNonNull(environment, \"environment is null\");\n+        requireNonNull(context, \"context is null\");\n+        try {\n+            this.delegate = HadoopInputFile.fromPath(path, environment.getFileSystem(context, path), environment.getConfiguration(context, path));\n+        }\n+        catch (IOException e) {\n+            throw new PrestoException(ICEBERG_FILESYSTEM_ERROR, \"Failed to create input file: \" + path, e);\n+        }\n+        this.user = context.getIdentity().getUser();\n+    }\n+\n+    @Override\n+    public long getLength()\n+    {\n+        return environment.doAs(user, delegate::getLength);\n+    }\n+\n+    @Override\n+    public SeekableInputStream newStream()\n+    {\n+        return environment.doAs(user, delegate::newStream);\n+    }\n+\n+    @Override\n+    public String location()\n+    {\n+        return delegate.location();\n+    }\n+\n+    @Override\n+    public boolean exists()\n+    {\n+        return environment.doAs(user, delegate::exists);\n+    }\n+}"
  },
  {
    "sha": "77c84a93f00e8dbf83eda43722c243a6800ae4ac",
    "filename": "presto-iceberg/src/main/java/com/facebook/presto/iceberg/HdfsOutputFile.java",
    "status": "added",
    "additions": 76,
    "deletions": 0,
    "changes": 76,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/HdfsOutputFile.java",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/HdfsOutputFile.java",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-iceberg/src/main/java/com/facebook/presto/iceberg/HdfsOutputFile.java?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg;\n+\n+import com.facebook.presto.hive.HdfsContext;\n+import com.facebook.presto.hive.HdfsEnvironment;\n+import com.facebook.presto.spi.PrestoException;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.hadoop.HadoopOutputFile;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.io.PositionOutputStream;\n+\n+import java.io.IOException;\n+\n+import static com.facebook.presto.iceberg.IcebergErrorCode.ICEBERG_FILESYSTEM_ERROR;\n+import static java.util.Objects.requireNonNull;\n+\n+public class HdfsOutputFile\n+        implements OutputFile\n+{\n+    private final OutputFile delegate;\n+    private final Path path;\n+    private final HdfsEnvironment environment;\n+    private final HdfsContext context;\n+    private final String user;\n+\n+    public HdfsOutputFile(Path path, HdfsEnvironment environment, HdfsContext context)\n+    {\n+        this.path = requireNonNull(path, \"path is null\");\n+        this.environment = requireNonNull(environment, \"environment is null\");\n+        this.context = requireNonNull(context, \"context is null\");\n+        try {\n+            this.delegate = HadoopOutputFile.fromPath(path, environment.getFileSystem(context, path), environment.getConfiguration(context, path));\n+        }\n+        catch (IOException e) {\n+            throw new PrestoException(ICEBERG_FILESYSTEM_ERROR, \"Failed to create output file: \" + path.toString(), e);\n+        }\n+        this.user = context.getIdentity().getUser();\n+    }\n+\n+    @Override\n+    public PositionOutputStream create()\n+    {\n+        return environment.doAs(user, delegate::create);\n+    }\n+\n+    @Override\n+    public PositionOutputStream createOrOverwrite()\n+    {\n+        return environment.doAs(user, delegate::createOrOverwrite);\n+    }\n+\n+    @Override\n+    public String location()\n+    {\n+        return delegate.location();\n+    }\n+\n+    @Override\n+    public InputFile toInputFile()\n+    {\n+        return new HdfsInputFile(path, environment, context);\n+    }\n+}"
  },
  {
    "sha": "ebd7332556827e8d0b1a209d89d3a4e200d6e355",
    "filename": "presto-iceberg/src/main/java/com/facebook/presto/iceberg/HistoryTable.java",
    "status": "added",
    "additions": 91,
    "deletions": 0,
    "changes": 91,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/HistoryTable.java",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/HistoryTable.java",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-iceberg/src/main/java/com/facebook/presto/iceberg/HistoryTable.java?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg;\n+\n+import com.facebook.presto.common.predicate.TupleDomain;\n+import com.facebook.presto.spi.ColumnMetadata;\n+import com.facebook.presto.spi.ConnectorSession;\n+import com.facebook.presto.spi.ConnectorTableMetadata;\n+import com.facebook.presto.spi.InMemoryRecordSet;\n+import com.facebook.presto.spi.RecordCursor;\n+import com.facebook.presto.spi.SchemaTableName;\n+import com.facebook.presto.spi.SystemTable;\n+import com.facebook.presto.spi.connector.ConnectorTransactionHandle;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.HistoryEntry;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.util.SnapshotUtil;\n+\n+import java.util.List;\n+import java.util.Set;\n+\n+import static com.facebook.presto.common.type.BigintType.BIGINT;\n+import static com.facebook.presto.common.type.BooleanType.BOOLEAN;\n+import static com.facebook.presto.common.type.DateTimeEncoding.packDateTimeWithZone;\n+import static com.facebook.presto.common.type.TimestampWithTimeZoneType.TIMESTAMP_WITH_TIME_ZONE;\n+import static java.util.Objects.requireNonNull;\n+\n+public class HistoryTable\n+        implements SystemTable\n+{\n+    private final ConnectorTableMetadata tableMetadata;\n+    private final Table icebergTable;\n+\n+    private static final List<ColumnMetadata> COLUMNS = ImmutableList.<ColumnMetadata>builder()\n+            .add(new ColumnMetadata(\"made_current_at\", TIMESTAMP_WITH_TIME_ZONE))\n+            .add(new ColumnMetadata(\"snapshot_id\", BIGINT))\n+            .add(new ColumnMetadata(\"parent_id\", BIGINT))\n+            .add(new ColumnMetadata(\"is_current_ancestor\", BOOLEAN))\n+            .build();\n+\n+    public HistoryTable(SchemaTableName tableName, Table icebergTable)\n+    {\n+        tableMetadata = new ConnectorTableMetadata(requireNonNull(tableName, \"tableName is null\"), COLUMNS);\n+        this.icebergTable = requireNonNull(icebergTable, \"icebergTable is null\");\n+    }\n+\n+    @Override\n+    public Distribution getDistribution()\n+    {\n+        return Distribution.SINGLE_COORDINATOR;\n+    }\n+\n+    @Override\n+    public ConnectorTableMetadata getTableMetadata()\n+    {\n+        return tableMetadata;\n+    }\n+\n+    @Override\n+    public RecordCursor cursor(ConnectorTransactionHandle transactionHandle, ConnectorSession session, TupleDomain<Integer> constraint)\n+    {\n+        InMemoryRecordSet.Builder table = InMemoryRecordSet.builder(COLUMNS);\n+\n+        Set<Long> ancestorIds = ImmutableSet.copyOf(SnapshotUtil.currentAncestors(icebergTable));\n+        for (HistoryEntry historyEntry : icebergTable.history()) {\n+            long snapshotId = historyEntry.snapshotId();\n+            Snapshot snapshot = icebergTable.snapshot(snapshotId);\n+\n+            table.addRow(\n+                    packDateTimeWithZone(historyEntry.timestampMillis(), session.getSqlFunctionProperties().getTimeZoneKey()),\n+                    snapshotId,\n+                    snapshot != null ? snapshot.parentId() : null,\n+                    ancestorIds.contains(snapshotId));\n+        }\n+\n+        return table.build().cursor();\n+    }\n+}"
  },
  {
    "sha": "036bdb313fdf9af9dedf7ed93efe75b1034d37f0",
    "filename": "presto-iceberg/src/main/java/com/facebook/presto/iceberg/HiveTableOperations.java",
    "status": "added",
    "additions": 365,
    "deletions": 0,
    "changes": 365,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/HiveTableOperations.java",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/HiveTableOperations.java",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-iceberg/src/main/java/com/facebook/presto/iceberg/HiveTableOperations.java?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -0,0 +1,365 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg;\n+\n+import com.facebook.airlift.log.Logger;\n+import com.facebook.presto.hive.HdfsContext;\n+import com.facebook.presto.hive.HdfsEnvironment;\n+import com.facebook.presto.hive.HiveType;\n+import com.facebook.presto.hive.metastore.Column;\n+import com.facebook.presto.hive.metastore.ExtendedHiveMetastore;\n+import com.facebook.presto.hive.metastore.HivePrivilegeInfo;\n+import com.facebook.presto.hive.metastore.HivePrivilegeInfo.HivePrivilege;\n+import com.facebook.presto.hive.metastore.PrestoTableType;\n+import com.facebook.presto.hive.metastore.PrincipalPrivileges;\n+import com.facebook.presto.hive.metastore.StorageFormat;\n+import com.facebook.presto.hive.metastore.Table;\n+import com.facebook.presto.spi.PrestoException;\n+import com.facebook.presto.spi.SchemaTableName;\n+import com.facebook.presto.spi.TableNotFoundException;\n+import com.facebook.presto.spi.security.PrestoPrincipal;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableMultimap;\n+import org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe;\n+import org.apache.hadoop.mapred.FileInputFormat;\n+import org.apache.hadoop.mapred.FileOutputFormat;\n+import org.apache.iceberg.LocationProviders;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableMetadataParser;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.CommitFailedException;\n+import org.apache.iceberg.hive.HiveTypeConverter;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.types.Types.NestedField;\n+import org.apache.iceberg.util.Tasks;\n+\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.NotThreadSafe;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static com.facebook.presto.hive.HiveMetadata.TABLE_COMMENT;\n+import static com.facebook.presto.iceberg.IcebergErrorCode.ICEBERG_INVALID_METADATA;\n+import static com.facebook.presto.iceberg.IcebergUtil.isIcebergTable;\n+import static com.facebook.presto.spi.security.PrincipalType.USER;\n+import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static java.lang.Integer.parseInt;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.UUID.randomUUID;\n+import static org.apache.iceberg.BaseMetastoreTableOperations.ICEBERG_TABLE_TYPE_VALUE;\n+import static org.apache.iceberg.BaseMetastoreTableOperations.TABLE_TYPE_PROP;\n+import static org.apache.iceberg.TableMetadataParser.getFileExtension;\n+import static org.apache.iceberg.TableProperties.METADATA_COMPRESSION;\n+import static org.apache.iceberg.TableProperties.METADATA_COMPRESSION_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_METADATA_LOCATION;\n+\n+@NotThreadSafe\n+public class HiveTableOperations\n+        implements TableOperations\n+{\n+    private static final Logger log = Logger.get(HiveTableOperations.class);\n+\n+    public static final String METADATA_LOCATION = \"metadata_location\";\n+    public static final String PREVIOUS_METADATA_LOCATION = \"previous_metadata_location\";\n+    private static final String METADATA_FOLDER_NAME = \"metadata\";\n+\n+    private static final StorageFormat STORAGE_FORMAT = StorageFormat.create(\n+            LazySimpleSerDe.class.getName(),\n+            FileInputFormat.class.getName(),\n+            FileOutputFormat.class.getName());\n+\n+    private final ExtendedHiveMetastore metastore;\n+    private final String database;\n+    private final String tableName;\n+    private final Optional<String> owner;\n+    private final Optional<String> location;\n+    private final FileIO fileIo;\n+\n+    private TableMetadata currentMetadata;\n+    private String currentMetadataLocation;\n+    private boolean shouldRefresh = true;\n+    private int version = -1;\n+\n+    public HiveTableOperations(ExtendedHiveMetastore metastore, HdfsEnvironment hdfsEnvironment, HdfsContext hdfsContext, String database, String table)\n+    {\n+        this(new HdfsFileIo(hdfsEnvironment, hdfsContext), metastore, database, table, Optional.empty(), Optional.empty());\n+    }\n+\n+    public HiveTableOperations(ExtendedHiveMetastore metastore, HdfsEnvironment hdfsEnvironment, HdfsContext hdfsContext, String database, String table, String owner, String location)\n+    {\n+        this(new HdfsFileIo(hdfsEnvironment, hdfsContext),\n+                metastore,\n+                database,\n+                table,\n+                Optional.of(requireNonNull(owner, \"owner is null\")),\n+                Optional.of(requireNonNull(location, \"location is null\")));\n+    }\n+\n+    private HiveTableOperations(FileIO fileIo, ExtendedHiveMetastore metastore, String database, String table, Optional<String> owner, Optional<String> location)\n+    {\n+        this.fileIo = requireNonNull(fileIo, \"fileIo is null\");\n+        this.metastore = requireNonNull(metastore, \"metastore is null\");\n+        this.database = requireNonNull(database, \"database is null\");\n+        this.tableName = requireNonNull(table, \"table is null\");\n+        this.owner = requireNonNull(owner, \"owner is null\");\n+        this.location = requireNonNull(location, \"location is null\");\n+    }\n+\n+    @Override\n+    public TableMetadata current()\n+    {\n+        if (shouldRefresh) {\n+            return refresh();\n+        }\n+        return currentMetadata;\n+    }\n+\n+    @Override\n+    public TableMetadata refresh()\n+    {\n+        if (location.isPresent()) {\n+            refreshFromMetadataLocation(null);\n+            return currentMetadata;\n+        }\n+\n+        Table table = getTable();\n+\n+        if (!isIcebergTable(table)) {\n+            throw new UnknownTableTypeException(getSchemaTableName());\n+        }\n+\n+        String metadataLocation = table.getParameters().get(METADATA_LOCATION);\n+        if (metadataLocation == null) {\n+            throw new PrestoException(ICEBERG_INVALID_METADATA, format(\"Table is missing [%s] property: %s\", METADATA_LOCATION, getSchemaTableName()));\n+        }\n+\n+        refreshFromMetadataLocation(metadataLocation);\n+\n+        return currentMetadata;\n+    }\n+\n+    @Override\n+    public void commit(@Nullable TableMetadata base, TableMetadata metadata)\n+    {\n+        requireNonNull(metadata, \"metadata is null\");\n+\n+        // if the metadata is already out of date, reject it\n+        if (!Objects.equals(base, current())) {\n+            throw new CommitFailedException(\"Cannot commit: stale table metadata for %s\", getSchemaTableName());\n+        }\n+\n+        // if the metadata is not changed, return early\n+        if (Objects.equals(base, metadata)) {\n+            return;\n+        }\n+\n+        String newMetadataLocation = writeNewMetadata(metadata, version + 1);\n+\n+        // TODO: use metastore locking\n+\n+        Table table;\n+        try {\n+            if (base == null) {\n+                String tableComment = metadata.properties().get(TABLE_COMMENT);\n+                Map<String, String> parameters = new HashMap<>();\n+                parameters.put(\"EXTERNAL\", \"TRUE\");\n+                parameters.put(TABLE_TYPE_PROP, ICEBERG_TABLE_TYPE_VALUE);\n+                parameters.put(METADATA_LOCATION, newMetadataLocation);\n+                if (tableComment != null) {\n+                    parameters.put(TABLE_COMMENT, tableComment);\n+                }\n+                Table.Builder builder = Table.builder()\n+                        .setDatabaseName(database)\n+                        .setTableName(tableName)\n+                        .setOwner(owner.orElseThrow(() -> new IllegalStateException(\"Owner not set\")))\n+                        .setTableType(PrestoTableType.EXTERNAL_TABLE)\n+                        .setDataColumns(toHiveColumns(metadata.schema().columns()))\n+                        .withStorage(storage -> storage.setLocation(metadata.location()))\n+                        .withStorage(storage -> storage.setStorageFormat(STORAGE_FORMAT))\n+                        .setParameters(parameters);\n+                table = builder.build();\n+            }\n+            else {\n+                Table currentTable = getTable();\n+                checkState(currentMetadataLocation != null, \"No current metadata location for existing table\");\n+                String metadataLocation = currentTable.getParameters().get(METADATA_LOCATION);\n+                if (!currentMetadataLocation.equals(metadataLocation)) {\n+                    throw new CommitFailedException(\"Metadata location [%s] is not same as table metadata location [%s] for %s\", currentMetadataLocation, metadataLocation, getSchemaTableName());\n+                }\n+                table = Table.builder(currentTable)\n+                        .setDataColumns(toHiveColumns(metadata.schema().columns()))\n+                        .withStorage(storage -> storage.setLocation(metadata.location()))\n+                        .setParameters(ImmutableMap.of(\n+                                METADATA_LOCATION, newMetadataLocation,\n+                                PREVIOUS_METADATA_LOCATION, currentMetadataLocation))\n+                        .build();\n+            }\n+        }\n+        catch (RuntimeException e) {\n+            try {\n+                io().deleteFile(newMetadataLocation);\n+            }\n+            catch (RuntimeException ex) {\n+                e.addSuppressed(ex);\n+            }\n+            throw e;\n+        }\n+\n+        PrestoPrincipal owner = new PrestoPrincipal(USER, table.getOwner());\n+        PrincipalPrivileges privileges = new PrincipalPrivileges(\n+                ImmutableMultimap.<String, HivePrivilegeInfo>builder()\n+                        .put(table.getOwner(), new HivePrivilegeInfo(HivePrivilege.SELECT, true, owner, owner))\n+                        .put(table.getOwner(), new HivePrivilegeInfo(HivePrivilege.INSERT, true, owner, owner))\n+                        .put(table.getOwner(), new HivePrivilegeInfo(HivePrivilege.UPDATE, true, owner, owner))\n+                        .put(table.getOwner(), new HivePrivilegeInfo(HivePrivilege.DELETE, true, owner, owner))\n+                        .build(),\n+                ImmutableMultimap.of());\n+        if (base == null) {\n+            metastore.createTable(table, privileges);\n+        }\n+        else {\n+            metastore.replaceTable(database, tableName, table, privileges);\n+        }\n+\n+        shouldRefresh = true;\n+    }\n+\n+    @Override\n+    public FileIO io()\n+    {\n+        return fileIo;\n+    }\n+\n+    @Override\n+    public String metadataFileLocation(String filename)\n+    {\n+        TableMetadata metadata = current();\n+        String location;\n+        if (metadata != null) {\n+            String writeLocation = metadata.properties().get(WRITE_METADATA_LOCATION);\n+            if (writeLocation != null) {\n+                return format(\"%s/%s\", writeLocation, filename);\n+            }\n+            location = metadata.location();\n+        }\n+        else {\n+            location = this.location.orElseThrow(() -> new IllegalStateException(\"Location not set\"));\n+        }\n+        return format(\"%s/%s/%s\", location, METADATA_FOLDER_NAME, filename);\n+    }\n+\n+    @Override\n+    public LocationProvider locationProvider()\n+    {\n+        TableMetadata metadata = current();\n+        return LocationProviders.locationsFor(metadata.location(), metadata.properties());\n+    }\n+\n+    private Table getTable()\n+    {\n+        return metastore.getTable(database, tableName)\n+                .orElseThrow(() -> new TableNotFoundException(getSchemaTableName()));\n+    }\n+\n+    private SchemaTableName getSchemaTableName()\n+    {\n+        return new SchemaTableName(database, tableName);\n+    }\n+\n+    private String writeNewMetadata(TableMetadata metadata, int newVersion)\n+    {\n+        String newTableMetadataFilePath = newTableMetadataFilePath(metadata, newVersion);\n+        OutputFile newMetadataLocation = fileIo.newOutputFile(newTableMetadataFilePath);\n+\n+        // write the new metadata\n+        TableMetadataParser.write(metadata, newMetadataLocation);\n+\n+        return newTableMetadataFilePath;\n+    }\n+\n+    private void refreshFromMetadataLocation(String newLocation)\n+    {\n+        // use null-safe equality check because new tables have a null metadata location\n+        if (Objects.equals(currentMetadataLocation, newLocation)) {\n+            shouldRefresh = false;\n+            return;\n+        }\n+\n+        AtomicReference<TableMetadata> newMetadata = new AtomicReference<>();\n+        Tasks.foreach(newLocation)\n+                .retry(20)\n+                .exponentialBackoff(100, 5000, 600000, 4.0)\n+                .suppressFailureWhenFinished()\n+                .run(metadataLocation -> newMetadata.set(\n+                        TableMetadataParser.read(this, io().newInputFile(metadataLocation))));\n+\n+        String newUUID = newMetadata.get().uuid();\n+        if (currentMetadata != null) {\n+            checkState(newUUID == null || newUUID.equals(currentMetadata.uuid()),\n+                    \"Table UUID does not match: current=%s != refreshed=%s\", currentMetadata.uuid(), newUUID);\n+        }\n+\n+        currentMetadata = newMetadata.get();\n+        currentMetadataLocation = newLocation;\n+        version = parseVersion(newLocation);\n+        shouldRefresh = false;\n+    }\n+\n+    private static String newTableMetadataFilePath(TableMetadata meta, int newVersion)\n+    {\n+        String codec = meta.property(METADATA_COMPRESSION, METADATA_COMPRESSION_DEFAULT);\n+        return metadataFileLocation(meta, format(\"%05d-%s%s\", newVersion, randomUUID(), getFileExtension(codec)));\n+    }\n+\n+    private static String metadataFileLocation(TableMetadata metadata, String filename)\n+    {\n+        String location = metadata.properties().get(WRITE_METADATA_LOCATION);\n+        if (location != null) {\n+            return format(\"%s/%s\", location, filename);\n+        }\n+        return format(\"%s/%s/%s\", metadata.location(), METADATA_FOLDER_NAME, filename);\n+    }\n+\n+    private static int parseVersion(String metadataLocation)\n+    {\n+        int versionStart = metadataLocation.lastIndexOf('/') + 1; // if '/' isn't found, this will be 0\n+        int versionEnd = metadataLocation.indexOf('-', versionStart);\n+        try {\n+            return parseInt(metadataLocation.substring(versionStart, versionEnd));\n+        }\n+        catch (NumberFormatException | IndexOutOfBoundsException e) {\n+            log.warn(e, \"Unable to parse version from metadata location: %s\", metadataLocation);\n+            return -1;\n+        }\n+    }\n+\n+    private static List<Column> toHiveColumns(List<NestedField> columns)\n+    {\n+        return columns.stream()\n+                .map(column -> new Column(\n+                        column.name(),\n+                        HiveType.valueOf(HiveTypeConverter.convert(column.type())),\n+                        Optional.empty()))\n+                .collect(toImmutableList());\n+    }\n+}"
  },
  {
    "sha": "af0e2cf52582c78555ae7474d9973ce0f8586ef1",
    "filename": "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergColumnHandle.java",
    "status": "added",
    "additions": 98,
    "deletions": 0,
    "changes": 98,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergColumnHandle.java",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergColumnHandle.java",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergColumnHandle.java?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg;\n+\n+import com.facebook.presto.common.type.Type;\n+import com.facebook.presto.spi.ColumnHandle;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import java.util.Objects;\n+import java.util.Optional;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergColumnHandle\n+        implements ColumnHandle\n+{\n+    private final int id;\n+    private final String name;\n+    private final Type type;\n+    private final Optional<String> comment;\n+\n+    @JsonCreator\n+    public IcebergColumnHandle(\n+            @JsonProperty(\"id\") int id,\n+            @JsonProperty(\"name\") String name,\n+            @JsonProperty(\"type\") Type type,\n+            @JsonProperty(\"comment\") Optional<String> comment)\n+    {\n+        this.id = id;\n+        this.name = requireNonNull(name, \"name is null\");\n+        this.type = requireNonNull(type, \"type is null\");\n+        this.comment = requireNonNull(comment, \"comment is null\");\n+    }\n+\n+    @JsonProperty\n+    public int getId()\n+    {\n+        return id;\n+    }\n+\n+    @JsonProperty\n+    public String getName()\n+    {\n+        return name;\n+    }\n+\n+    @JsonProperty\n+    public Type getType()\n+    {\n+        return type;\n+    }\n+\n+    @JsonProperty\n+    public Optional<String> getComment()\n+    {\n+        return comment;\n+    }\n+\n+    @Override\n+    public int hashCode()\n+    {\n+        return Objects.hash(id, name, type, comment);\n+    }\n+\n+    @Override\n+    public boolean equals(Object obj)\n+    {\n+        if (this == obj) {\n+            return true;\n+        }\n+        if (obj == null || getClass() != obj.getClass()) {\n+            return false;\n+        }\n+        IcebergColumnHandle other = (IcebergColumnHandle) obj;\n+        return this.id == other.id &&\n+                Objects.equals(this.name, other.name) &&\n+                Objects.equals(this.type, other.type) &&\n+                Objects.equals(this.comment, other.comment);\n+    }\n+\n+    @Override\n+    public String toString()\n+    {\n+        return id + \":\" + name + \":\" + type.getDisplayName();\n+    }\n+}"
  },
  {
    "sha": "3179caeb5c7c2566d4a01362ae109e1ddd03aacc",
    "filename": "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergConfig.java",
    "status": "added",
    "additions": 55,
    "deletions": 0,
    "changes": 55,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergConfig.java",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergConfig.java",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergConfig.java?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -0,0 +1,55 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg;\n+\n+import com.facebook.airlift.configuration.Config;\n+import com.facebook.presto.hive.HiveCompressionCodec;\n+import org.apache.iceberg.FileFormat;\n+\n+import javax.validation.constraints.NotNull;\n+\n+import static com.facebook.presto.hive.HiveCompressionCodec.GZIP;\n+import static com.facebook.presto.iceberg.IcebergFileFormat.PARQUET;\n+\n+public class IcebergConfig\n+{\n+    private IcebergFileFormat fileFormat = PARQUET;\n+    private HiveCompressionCodec compressionCodec = GZIP;\n+\n+    @NotNull\n+    public FileFormat getFileFormat()\n+    {\n+        return FileFormat.valueOf(fileFormat.name());\n+    }\n+\n+    @Config(\"iceberg.file-format\")\n+    public IcebergConfig setFileFormat(IcebergFileFormat fileFormat)\n+    {\n+        this.fileFormat = fileFormat;\n+        return this;\n+    }\n+\n+    @NotNull\n+    public HiveCompressionCodec getCompressionCodec()\n+    {\n+        return compressionCodec;\n+    }\n+\n+    @Config(\"iceberg.compression-codec\")\n+    public IcebergConfig setCompressionCodec(HiveCompressionCodec compressionCodec)\n+    {\n+        this.compressionCodec = compressionCodec;\n+        return this;\n+    }\n+}"
  },
  {
    "sha": "44051b77cd1589dfac3da32466281eff5b45baea",
    "filename": "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergConnector.java",
    "status": "added",
    "additions": 202,
    "deletions": 0,
    "changes": 202,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergConnector.java",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergConnector.java",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergConnector.java?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg;\n+\n+import com.facebook.airlift.bootstrap.LifeCycleManager;\n+import com.facebook.presto.hive.HiveTransactionHandle;\n+import com.facebook.presto.spi.SystemTable;\n+import com.facebook.presto.spi.classloader.ThreadContextClassLoader;\n+import com.facebook.presto.spi.connector.Connector;\n+import com.facebook.presto.spi.connector.ConnectorAccessControl;\n+import com.facebook.presto.spi.connector.ConnectorCapabilities;\n+import com.facebook.presto.spi.connector.ConnectorMetadata;\n+import com.facebook.presto.spi.connector.ConnectorNodePartitioningProvider;\n+import com.facebook.presto.spi.connector.ConnectorPageSinkProvider;\n+import com.facebook.presto.spi.connector.ConnectorPageSourceProvider;\n+import com.facebook.presto.spi.connector.ConnectorSplitManager;\n+import com.facebook.presto.spi.connector.ConnectorTransactionHandle;\n+import com.facebook.presto.spi.connector.classloader.ClassLoaderSafeConnectorMetadata;\n+import com.facebook.presto.spi.procedure.Procedure;\n+import com.facebook.presto.spi.session.PropertyMetadata;\n+import com.facebook.presto.spi.transaction.IsolationLevel;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+\n+import java.util.List;\n+import java.util.Set;\n+\n+import static com.facebook.presto.spi.connector.ConnectorCapabilities.NOT_NULL_COLUMN_CONSTRAINT;\n+import static com.facebook.presto.spi.transaction.IsolationLevel.SERIALIZABLE;\n+import static com.facebook.presto.spi.transaction.IsolationLevel.checkConnectorSupports;\n+import static com.google.common.collect.Sets.immutableEnumSet;\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergConnector\n+        implements Connector\n+{\n+    private final LifeCycleManager lifeCycleManager;\n+    private final IcebergTransactionManager transactionManager;\n+    private final IcebergMetadataFactory metadataFactory;\n+    private final ConnectorSplitManager splitManager;\n+    private final ConnectorPageSourceProvider pageSourceProvider;\n+    private final ConnectorPageSinkProvider pageSinkProvider;\n+    private final ConnectorNodePartitioningProvider nodePartitioningProvider;\n+    private final Set<SystemTable> systemTables;\n+    private final List<PropertyMetadata<?>> sessionProperties;\n+    private final List<PropertyMetadata<?>> schemaProperties;\n+    private final List<PropertyMetadata<?>> tableProperties;\n+    private final ConnectorAccessControl accessControl;\n+    private final Set<Procedure> procedures;\n+\n+    public IcebergConnector(\n+            LifeCycleManager lifeCycleManager,\n+            IcebergTransactionManager transactionManager,\n+            IcebergMetadataFactory metadataFactory,\n+            ConnectorSplitManager splitManager,\n+            ConnectorPageSourceProvider pageSourceProvider,\n+            ConnectorPageSinkProvider pageSinkProvider,\n+            ConnectorNodePartitioningProvider nodePartitioningProvider,\n+            Set<SystemTable> systemTables,\n+            List<PropertyMetadata<?>> sessionProperties,\n+            List<PropertyMetadata<?>> schemaProperties,\n+            List<PropertyMetadata<?>> tableProperties,\n+            ConnectorAccessControl accessControl,\n+            Set<Procedure> procedures)\n+    {\n+        this.lifeCycleManager = requireNonNull(lifeCycleManager, \"lifeCycleManager is null\");\n+        this.transactionManager = requireNonNull(transactionManager, \"transactionManager is null\");\n+        this.metadataFactory = requireNonNull(metadataFactory, \"metadataFactory is null\");\n+        this.splitManager = requireNonNull(splitManager, \"splitManager is null\");\n+        this.pageSourceProvider = requireNonNull(pageSourceProvider, \"pageSourceProvider is null\");\n+        this.pageSinkProvider = requireNonNull(pageSinkProvider, \"pageSinkProvider is null\");\n+        this.nodePartitioningProvider = requireNonNull(nodePartitioningProvider, \"nodePartitioningProvider is null\");\n+        this.systemTables = ImmutableSet.copyOf(requireNonNull(systemTables, \"systemTables is null\"));\n+        this.sessionProperties = ImmutableList.copyOf(requireNonNull(sessionProperties, \"sessionProperties is null\"));\n+        this.schemaProperties = ImmutableList.copyOf(requireNonNull(schemaProperties, \"schemaProperties is null\"));\n+        this.tableProperties = ImmutableList.copyOf(requireNonNull(tableProperties, \"tableProperties is null\"));\n+        this.accessControl = requireNonNull(accessControl, \"accessControl is null\");\n+        this.procedures = requireNonNull(procedures, \"procedures is null\");\n+    }\n+\n+    @Override\n+    public boolean isSingleStatementWritesOnly()\n+    {\n+        return true;\n+    }\n+\n+    @Override\n+    public Set<ConnectorCapabilities> getCapabilities()\n+    {\n+        return immutableEnumSet(NOT_NULL_COLUMN_CONSTRAINT);\n+    }\n+\n+    @Override\n+    public ConnectorMetadata getMetadata(ConnectorTransactionHandle transaction)\n+    {\n+        ConnectorMetadata metadata = transactionManager.get(transaction);\n+        return new ClassLoaderSafeConnectorMetadata(metadata, getClass().getClassLoader());\n+    }\n+\n+    @Override\n+    public ConnectorSplitManager getSplitManager()\n+    {\n+        return splitManager;\n+    }\n+\n+    @Override\n+    public ConnectorPageSourceProvider getPageSourceProvider()\n+    {\n+        return pageSourceProvider;\n+    }\n+\n+    @Override\n+    public ConnectorPageSinkProvider getPageSinkProvider()\n+    {\n+        return pageSinkProvider;\n+    }\n+\n+    @Override\n+    public ConnectorNodePartitioningProvider getNodePartitioningProvider()\n+    {\n+        return nodePartitioningProvider;\n+    }\n+\n+    @Override\n+    public Set<SystemTable> getSystemTables()\n+    {\n+        return systemTables;\n+    }\n+\n+    @Override\n+    public Set<Procedure> getProcedures()\n+    {\n+        return procedures;\n+    }\n+\n+    @Override\n+    public List<PropertyMetadata<?>> getSessionProperties()\n+    {\n+        return sessionProperties;\n+    }\n+\n+    @Override\n+    public List<PropertyMetadata<?>> getSchemaProperties()\n+    {\n+        return schemaProperties;\n+    }\n+\n+    @Override\n+    public List<PropertyMetadata<?>> getTableProperties()\n+    {\n+        return tableProperties;\n+    }\n+\n+    @Override\n+    public ConnectorAccessControl getAccessControl()\n+    {\n+        return accessControl;\n+    }\n+\n+    @Override\n+    public ConnectorTransactionHandle beginTransaction(IsolationLevel isolationLevel, boolean readOnly)\n+    {\n+        checkConnectorSupports(SERIALIZABLE, isolationLevel);\n+        ConnectorTransactionHandle transaction = new HiveTransactionHandle();\n+        try (ThreadContextClassLoader ignored = new ThreadContextClassLoader(getClass().getClassLoader())) {\n+            transactionManager.put(transaction, metadataFactory.create());\n+        }\n+        return transaction;\n+    }\n+\n+    @Override\n+    public void commit(ConnectorTransactionHandle transaction)\n+    {\n+        transactionManager.remove(transaction);\n+    }\n+\n+    @Override\n+    public void rollback(ConnectorTransactionHandle transaction)\n+    {\n+        IcebergMetadata metadata = transactionManager.remove(transaction);\n+        try (ThreadContextClassLoader ignored = new ThreadContextClassLoader(getClass().getClassLoader())) {\n+            metadata.rollback();\n+        }\n+    }\n+\n+    @Override\n+    public final void shutdown()\n+    {\n+        lifeCycleManager.stop();\n+    }\n+}"
  },
  {
    "sha": "edf470043fac38c3ce6fd244b820e0e61f0cc062",
    "filename": "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergConnectorFactory.java",
    "status": "added",
    "additions": 60,
    "deletions": 0,
    "changes": 60,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergConnectorFactory.java",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergConnectorFactory.java",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergConnectorFactory.java?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -0,0 +1,60 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg;\n+\n+import com.facebook.presto.spi.ConnectorHandleResolver;\n+import com.facebook.presto.spi.connector.Connector;\n+import com.facebook.presto.spi.connector.ConnectorContext;\n+import com.facebook.presto.spi.connector.ConnectorFactory;\n+\n+import java.lang.reflect.InvocationTargetException;\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import static com.google.common.base.Throwables.throwIfUnchecked;\n+\n+public class IcebergConnectorFactory\n+        implements ConnectorFactory\n+{\n+    @Override\n+    public String getName()\n+    {\n+        return \"iceberg\";\n+    }\n+\n+    @Override\n+    public ConnectorHandleResolver getHandleResolver()\n+    {\n+        return new IcebergHandleResolver();\n+    }\n+\n+    @Override\n+    public Connector create(String catalogName, Map<String, String> config, ConnectorContext context)\n+    {\n+        ClassLoader classLoader = IcebergConnectorFactory.class.getClassLoader();\n+        try {\n+            return (Connector) classLoader.loadClass(InternalIcebergConnectorFactory.class.getName())\n+                    .getMethod(\"createConnector\", String.class, Map.class, ConnectorContext.class, Optional.class)\n+                    .invoke(null, catalogName, config, context, Optional.empty());\n+        }\n+        catch (InvocationTargetException e) {\n+            Throwable targetException = e.getTargetException();\n+            throwIfUnchecked(targetException);\n+            throw new RuntimeException(targetException);\n+        }\n+        catch (ReflectiveOperationException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+}"
  },
  {
    "sha": "274fd1adbad17ee0238c994d44966d44a43c6462",
    "filename": "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergErrorCode.java",
    "status": "added",
    "additions": 53,
    "deletions": 0,
    "changes": 53,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergErrorCode.java",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergErrorCode.java",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergErrorCode.java?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -0,0 +1,53 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg;\n+\n+import com.facebook.presto.spi.ErrorCode;\n+import com.facebook.presto.spi.ErrorCodeSupplier;\n+import com.facebook.presto.spi.ErrorType;\n+\n+import static com.facebook.presto.spi.ErrorType.EXTERNAL;\n+import static com.facebook.presto.spi.ErrorType.INTERNAL_ERROR;\n+import static com.facebook.presto.spi.ErrorType.USER_ERROR;\n+\n+public enum IcebergErrorCode\n+        implements ErrorCodeSupplier\n+{\n+    ICEBERG_UNKNOWN_TABLE_TYPE(0, EXTERNAL),\n+    ICEBERG_INVALID_METADATA(1, EXTERNAL),\n+    ICEBERG_TOO_MANY_OPEN_PARTITIONS(2, USER_ERROR),\n+    ICEBERG_INVALID_PARTITION_VALUE(3, EXTERNAL),\n+    ICEBERG_BAD_DATA(4, EXTERNAL),\n+    ICEBERG_MISSING_DATA(5, EXTERNAL),\n+    ICEBERG_CANNOT_OPEN_SPLIT(6, EXTERNAL),\n+    ICEBERG_WRITER_OPEN_ERROR(7, EXTERNAL),\n+    ICEBERG_FILESYSTEM_ERROR(8, EXTERNAL),\n+    ICEBERG_CURSOR_ERROR(9, EXTERNAL),\n+    ICEBERG_WRITE_VALIDATION_FAILED(10, INTERNAL_ERROR),\n+    ICEBERG_INVALID_SNAPSHOT_ID(11, USER_ERROR),\n+    /**/;\n+\n+    private final ErrorCode errorCode;\n+\n+    IcebergErrorCode(int code, ErrorType type)\n+    {\n+        errorCode = new ErrorCode(code + 0x0504_0000, name(), type);\n+    }\n+\n+    @Override\n+    public ErrorCode toErrorCode()\n+    {\n+        return errorCode;\n+    }\n+}"
  },
  {
    "sha": "1889d344493dc7023b98d654a474bd738484b913",
    "filename": "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergFileFormat.java",
    "status": "added",
    "additions": 20,
    "deletions": 0,
    "changes": 20,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergFileFormat.java",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergFileFormat.java",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergFileFormat.java?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -0,0 +1,20 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg;\n+\n+public enum IcebergFileFormat\n+{\n+    ORC,\n+    PARQUET,\n+}"
  },
  {
    "sha": "b9ecd12fac8218f804992cddf3ba26a1817955a8",
    "filename": "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergFileWriter.java",
    "status": "added",
    "additions": 23,
    "deletions": 0,
    "changes": 23,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergFileWriter.java",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergFileWriter.java",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergFileWriter.java?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg;\n+\n+import com.facebook.presto.hive.HiveFileWriter;\n+import org.apache.iceberg.Metrics;\n+\n+public interface IcebergFileWriter\n+        extends HiveFileWriter\n+{\n+    Metrics getMetrics();\n+}"
  },
  {
    "sha": "10c78108078d5636adacda57b7a6e2687099e4ed",
    "filename": "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergFileWriterFactory.java",
    "status": "added",
    "additions": 127,
    "deletions": 0,
    "changes": 127,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergFileWriterFactory.java",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergFileWriterFactory.java",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergFileWriterFactory.java?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg;\n+\n+import com.facebook.presto.common.type.Type;\n+import com.facebook.presto.common.type.TypeManager;\n+import com.facebook.presto.hive.FileFormatDataSourceStats;\n+import com.facebook.presto.hive.HdfsContext;\n+import com.facebook.presto.hive.HdfsEnvironment;\n+import com.facebook.presto.parquet.writer.ParquetWriterOptions;\n+import com.facebook.presto.spi.ConnectorSession;\n+import com.facebook.presto.spi.PrestoException;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Types;\n+\n+import javax.inject.Inject;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.stream.IntStream;\n+\n+import static com.facebook.presto.hive.HiveSessionProperties.getParquetWriterBlockSize;\n+import static com.facebook.presto.hive.HiveSessionProperties.getParquetWriterPageSize;\n+import static com.facebook.presto.iceberg.IcebergErrorCode.ICEBERG_WRITER_OPEN_ERROR;\n+import static com.facebook.presto.iceberg.IcebergSessionProperties.getCompressionCodec;\n+import static com.facebook.presto.iceberg.TypeConverter.toPrestoType;\n+import static com.facebook.presto.iceberg.util.PrimitiveTypeMapBuilder.makeTypeMap;\n+import static com.facebook.presto.spi.StandardErrorCode.NOT_SUPPORTED;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static java.util.Objects.requireNonNull;\n+import static org.apache.iceberg.parquet.ParquetSchemaUtil.convert;\n+\n+public class IcebergFileWriterFactory\n+{\n+    private final HdfsEnvironment hdfsEnvironment;\n+    private final TypeManager typeManager;\n+    private final FileFormatDataSourceStats readStats;\n+\n+    @Inject\n+    public IcebergFileWriterFactory(\n+            HdfsEnvironment hdfsEnvironment,\n+            TypeManager typeManager,\n+            FileFormatDataSourceStats readStats)\n+    {\n+        this.hdfsEnvironment = requireNonNull(hdfsEnvironment, \"hdfsEnvironment is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.readStats = requireNonNull(readStats, \"readStats is null\");\n+    }\n+\n+    public IcebergFileWriter createFileWriter(\n+            Path outputPath,\n+            Schema icebergSchema,\n+            JobConf jobConf,\n+            ConnectorSession session,\n+            HdfsContext hdfsContext,\n+            FileFormat fileFormat)\n+    {\n+        switch (fileFormat) {\n+            // TODO: support ORC\n+            case PARQUET:\n+                return createParquetWriter(outputPath, icebergSchema, jobConf, session, hdfsContext);\n+        }\n+        throw new PrestoException(NOT_SUPPORTED, \"File format not supported for Iceberg: \" + fileFormat);\n+    }\n+\n+    private IcebergFileWriter createParquetWriter(\n+            Path outputPath,\n+            Schema icebergSchema,\n+            JobConf jobConf,\n+            ConnectorSession session,\n+            HdfsContext hdfsContext)\n+    {\n+        List<String> fileColumnNames = icebergSchema.columns().stream()\n+                .map(Types.NestedField::name)\n+                .collect(toImmutableList());\n+        List<Type> fileColumnTypes = icebergSchema.columns().stream()\n+                .map(column -> toPrestoType(column.type(), typeManager))\n+                .collect(toImmutableList());\n+\n+        try {\n+            FileSystem fileSystem = hdfsEnvironment.getFileSystem(session.getUser(), outputPath, jobConf);\n+\n+            Callable<Void> rollbackAction = () -> {\n+                fileSystem.delete(outputPath, false);\n+                return null;\n+            };\n+\n+            ParquetWriterOptions parquetWriterOptions = ParquetWriterOptions.builder()\n+                    .setMaxPageSize(getParquetWriterPageSize(session))\n+                    .setMaxPageSize(getParquetWriterBlockSize(session))\n+                    .build();\n+\n+            return new IcebergParquetFileWriter(\n+                    hdfsEnvironment.doAs(session.getUser(), () -> fileSystem.create(outputPath)),\n+                    rollbackAction,\n+                    fileColumnNames,\n+                    fileColumnTypes,\n+                    convert(icebergSchema, \"table\"),\n+                    makeTypeMap(fileColumnTypes, fileColumnNames),\n+                    parquetWriterOptions,\n+                    IntStream.range(0, fileColumnNames.size()).toArray(),\n+                    getCompressionCodec(session).getParquetCompressionCodec().get(),\n+                    outputPath,\n+                    hdfsEnvironment,\n+                    hdfsContext);\n+        }\n+        catch (IOException e) {\n+            throw new PrestoException(ICEBERG_WRITER_OPEN_ERROR, \"Error creating Parquet file\", e);\n+        }\n+    }\n+}"
  },
  {
    "sha": "312c94d8cb0aa965076b4b86df0350163ec2a0d9",
    "filename": "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHandleResolver.java",
    "status": "added",
    "additions": 70,
    "deletions": 0,
    "changes": 70,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHandleResolver.java",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHandleResolver.java",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergHandleResolver.java?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg;\n+\n+import com.facebook.presto.hive.HiveTransactionHandle;\n+import com.facebook.presto.spi.ColumnHandle;\n+import com.facebook.presto.spi.ConnectorHandleResolver;\n+import com.facebook.presto.spi.ConnectorInsertTableHandle;\n+import com.facebook.presto.spi.ConnectorOutputTableHandle;\n+import com.facebook.presto.spi.ConnectorSplit;\n+import com.facebook.presto.spi.ConnectorTableHandle;\n+import com.facebook.presto.spi.ConnectorTableLayoutHandle;\n+import com.facebook.presto.spi.connector.ConnectorTransactionHandle;\n+\n+public class IcebergHandleResolver\n+        implements ConnectorHandleResolver\n+{\n+    @Override\n+    public Class<? extends ConnectorTableHandle> getTableHandleClass()\n+    {\n+        return IcebergTableHandle.class;\n+    }\n+\n+    @Override\n+    public Class<? extends ConnectorTableLayoutHandle> getTableLayoutHandleClass()\n+    {\n+        return IcebergTableLayoutHandle.class;\n+    }\n+\n+    @Override\n+    public Class<? extends ColumnHandle> getColumnHandleClass()\n+    {\n+        return IcebergColumnHandle.class;\n+    }\n+\n+    @Override\n+    public Class<? extends ConnectorSplit> getSplitClass()\n+    {\n+        return IcebergSplit.class;\n+    }\n+\n+    @Override\n+    public Class<? extends ConnectorOutputTableHandle> getOutputTableHandleClass()\n+    {\n+        return IcebergWritableTableHandle.class;\n+    }\n+\n+    @Override\n+    public Class<? extends ConnectorInsertTableHandle> getInsertTableHandleClass()\n+    {\n+        return IcebergWritableTableHandle.class;\n+    }\n+\n+    @Override\n+    public Class<? extends ConnectorTransactionHandle> getTransactionHandleClass()\n+    {\n+        return HiveTransactionHandle.class;\n+    }\n+}"
  },
  {
    "sha": "048954ad7a2898a775a29cefa04bb5392f5a561e",
    "filename": "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergInputInfo.java",
    "status": "added",
    "additions": 37,
    "deletions": 0,
    "changes": 37,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergInputInfo.java",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergInputInfo.java",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergInputInfo.java?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg;\n+\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+\n+import java.util.Optional;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergInputInfo\n+{\n+    private final Optional<Long> snapshotId;\n+\n+    public IcebergInputInfo(\n+            @JsonProperty(\"snapshotId\") Optional<Long> snapshotId)\n+    {\n+        this.snapshotId = requireNonNull(snapshotId, \"snapshotId is null\");\n+    }\n+\n+    @JsonProperty\n+    public Optional<Long> getSnapshotId()\n+    {\n+        return snapshotId;\n+    }\n+}"
  },
  {
    "sha": "1764a869d3547d4f9890a0d2bde46a68d47037ac",
    "filename": "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergMetadata.java",
    "status": "added",
    "additions": 559,
    "deletions": 0,
    "changes": 559,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergMetadata.java",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergMetadata.java",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergMetadata.java?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -0,0 +1,559 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg;\n+\n+import com.facebook.airlift.json.JsonCodec;\n+import com.facebook.presto.common.predicate.TupleDomain;\n+import com.facebook.presto.common.type.TypeManager;\n+import com.facebook.presto.hive.HdfsContext;\n+import com.facebook.presto.hive.HdfsEnvironment;\n+import com.facebook.presto.hive.HiveWrittenPartitions;\n+import com.facebook.presto.hive.TableAlreadyExistsException;\n+import com.facebook.presto.hive.metastore.Database;\n+import com.facebook.presto.hive.metastore.ExtendedHiveMetastore;\n+import com.facebook.presto.hive.metastore.Table;\n+import com.facebook.presto.spi.ColumnHandle;\n+import com.facebook.presto.spi.ColumnMetadata;\n+import com.facebook.presto.spi.ConnectorInsertTableHandle;\n+import com.facebook.presto.spi.ConnectorNewTableLayout;\n+import com.facebook.presto.spi.ConnectorOutputTableHandle;\n+import com.facebook.presto.spi.ConnectorSession;\n+import com.facebook.presto.spi.ConnectorTableHandle;\n+import com.facebook.presto.spi.ConnectorTableLayout;\n+import com.facebook.presto.spi.ConnectorTableLayoutHandle;\n+import com.facebook.presto.spi.ConnectorTableLayoutResult;\n+import com.facebook.presto.spi.ConnectorTableMetadata;\n+import com.facebook.presto.spi.Constraint;\n+import com.facebook.presto.spi.PrestoException;\n+import com.facebook.presto.spi.SchemaNotFoundException;\n+import com.facebook.presto.spi.SchemaTableName;\n+import com.facebook.presto.spi.SchemaTablePrefix;\n+import com.facebook.presto.spi.SystemTable;\n+import com.facebook.presto.spi.TableNotFoundException;\n+import com.facebook.presto.spi.connector.ConnectorMetadata;\n+import com.facebook.presto.spi.connector.ConnectorOutputMetadata;\n+import com.facebook.presto.spi.statistics.ComputedStatistics;\n+import com.facebook.presto.spi.statistics.TableStatistics;\n+import com.google.common.base.VerifyException;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import io.airlift.slice.Slice;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.Transaction;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.types.Types.NestedField;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import static com.facebook.presto.common.type.BigintType.BIGINT;\n+import static com.facebook.presto.hive.HiveMetadata.TABLE_COMMENT;\n+import static com.facebook.presto.iceberg.IcebergSchemaProperties.getSchemaLocation;\n+import static com.facebook.presto.iceberg.IcebergTableProperties.FILE_FORMAT_PROPERTY;\n+import static com.facebook.presto.iceberg.IcebergTableProperties.PARTITIONING_PROPERTY;\n+import static com.facebook.presto.iceberg.IcebergTableProperties.getFileFormat;\n+import static com.facebook.presto.iceberg.IcebergTableProperties.getPartitioning;\n+import static com.facebook.presto.iceberg.IcebergTableProperties.getTableLocation;\n+import static com.facebook.presto.iceberg.IcebergUtil.getColumns;\n+import static com.facebook.presto.iceberg.IcebergUtil.getDataPath;\n+import static com.facebook.presto.iceberg.IcebergUtil.getFileFormat;\n+import static com.facebook.presto.iceberg.IcebergUtil.getIcebergTable;\n+import static com.facebook.presto.iceberg.IcebergUtil.getTableComment;\n+import static com.facebook.presto.iceberg.IcebergUtil.isIcebergTable;\n+import static com.facebook.presto.iceberg.PartitionFields.parsePartitionFields;\n+import static com.facebook.presto.iceberg.PartitionFields.toPartitionFields;\n+import static com.facebook.presto.iceberg.TableType.DATA;\n+import static com.facebook.presto.iceberg.TypeConverter.toIcebergType;\n+import static com.facebook.presto.iceberg.TypeConverter.toPrestoType;\n+import static com.facebook.presto.spi.StandardErrorCode.INVALID_SCHEMA_PROPERTY;\n+import static com.facebook.presto.spi.StandardErrorCode.NOT_SUPPORTED;\n+import static com.facebook.presto.spi.StandardErrorCode.SCHEMA_NOT_EMPTY;\n+import static com.facebook.presto.spi.security.PrincipalType.USER;\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static com.google.common.collect.ImmutableMap.toImmutableMap;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.function.Function.identity;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.iceberg.TableMetadata.newTableMetadata;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.Transactions.createTableTransaction;\n+\n+public class IcebergMetadata\n+        implements ConnectorMetadata\n+{\n+    private final ExtendedHiveMetastore metastore;\n+    private final HdfsEnvironment hdfsEnvironment;\n+    private final TypeManager typeManager;\n+    private final JsonCodec<CommitTaskData> commitTaskCodec;\n+\n+    private final Map<String, Optional<Long>> snapshotIds = new ConcurrentHashMap<>();\n+\n+    private Transaction transaction;\n+\n+    public IcebergMetadata(\n+            ExtendedHiveMetastore metastore,\n+            HdfsEnvironment hdfsEnvironment,\n+            TypeManager typeManager,\n+            JsonCodec<CommitTaskData> commitTaskCodec)\n+    {\n+        this.metastore = requireNonNull(metastore, \"metastore is null\");\n+        this.hdfsEnvironment = requireNonNull(hdfsEnvironment, \"hdfsEnvironment is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.commitTaskCodec = requireNonNull(commitTaskCodec, \"commitTaskCodec is null\");\n+    }\n+\n+    @Override\n+    public List<String> listSchemaNames(ConnectorSession session)\n+    {\n+        return metastore.getAllDatabases();\n+    }\n+\n+    @Override\n+    public IcebergTableHandle getTableHandle(ConnectorSession session, SchemaTableName tableName)\n+    {\n+        IcebergTableName name = IcebergTableName.from(tableName.getTableName());\n+        verify(name.getTableType() == DATA, \"Wrong table type: \" + name.getTableType());\n+\n+        Optional<Table> hiveTable = metastore.getTable(tableName.getSchemaName(), name.getTableName());\n+        if (!hiveTable.isPresent()) {\n+            return null;\n+        }\n+        if (!isIcebergTable(hiveTable.get())) {\n+            throw new UnknownTableTypeException(tableName);\n+        }\n+\n+        org.apache.iceberg.Table table = getIcebergTable(metastore, hdfsEnvironment, session, tableName);\n+        Optional<Long> snapshotId = getSnapshotId(table, name.getSnapshotId());\n+\n+        return new IcebergTableHandle(\n+                tableName.getSchemaName(),\n+                name.getTableName(),\n+                name.getTableType(),\n+                snapshotId,\n+                TupleDomain.all());\n+    }\n+\n+    @Override\n+    public List<ConnectorTableLayoutResult> getTableLayouts(ConnectorSession session, ConnectorTableHandle table, Constraint<ColumnHandle> constraint, Optional<Set<ColumnHandle>> desiredColumns)\n+    {\n+        IcebergTableHandle handle = (IcebergTableHandle) table;\n+        ConnectorTableLayout layout = new ConnectorTableLayout(new IcebergTableLayoutHandle(handle, constraint.getSummary()));\n+        return ImmutableList.of(new ConnectorTableLayoutResult(layout, constraint.getSummary()));\n+    }\n+\n+    @Override\n+    public ConnectorTableLayout getTableLayout(ConnectorSession session, ConnectorTableLayoutHandle handle)\n+    {\n+        return new ConnectorTableLayout(handle);\n+    }\n+\n+    @Override\n+    public Optional<SystemTable> getSystemTable(ConnectorSession session, SchemaTableName tableName)\n+    {\n+        return getRawSystemTable(session, tableName);\n+    }\n+\n+    private Optional<SystemTable> getRawSystemTable(ConnectorSession session, SchemaTableName tableName)\n+    {\n+        IcebergTableName name = IcebergTableName.from(tableName.getTableName());\n+\n+        Optional<Table> hiveTable = metastore.getTable(tableName.getSchemaName(), name.getTableName());\n+        if (!hiveTable.isPresent() || !isIcebergTable(hiveTable.get())) {\n+            return Optional.empty();\n+        }\n+\n+        org.apache.iceberg.Table table = getIcebergTable(metastore, hdfsEnvironment, session, tableName);\n+\n+        SchemaTableName systemTableName = new SchemaTableName(tableName.getSchemaName(), name.getTableNameWithType());\n+        switch (name.getTableType()) {\n+            case DATA:\n+                break;\n+            case HISTORY:\n+                if (name.getSnapshotId().isPresent()) {\n+                    throw new PrestoException(NOT_SUPPORTED, \"Snapshot ID not supported for history table: \" + systemTableName);\n+                }\n+                return Optional.of(new HistoryTable(systemTableName, table));\n+            case SNAPSHOTS:\n+                if (name.getSnapshotId().isPresent()) {\n+                    throw new PrestoException(NOT_SUPPORTED, \"Snapshot ID not supported for snapshots table: \" + systemTableName);\n+                }\n+                return Optional.of(new SnapshotsTable(systemTableName, typeManager, table));\n+            case PARTITIONS:\n+                return Optional.of(new PartitionTable(systemTableName, typeManager, table, getSnapshotId(table, name.getSnapshotId())));\n+            case MANIFESTS:\n+                return Optional.of(new ManifestsTable(systemTableName, table, getSnapshotId(table, name.getSnapshotId())));\n+        }\n+        return Optional.empty();\n+    }\n+\n+    @Override\n+    public ConnectorTableMetadata getTableMetadata(ConnectorSession session, ConnectorTableHandle table)\n+    {\n+        return getTableMetadata(session, ((IcebergTableHandle) table).getSchemaTableName());\n+    }\n+\n+    @Override\n+    public List<SchemaTableName> listTables(ConnectorSession session, Optional<String> schemaName)\n+    {\n+        return metastore.getAllTables(schemaName.get()).orElseGet(metastore::getAllDatabases).stream()\n+                .map(table -> new SchemaTableName(schemaName.get(), table))\n+                .collect(toList());\n+    }\n+\n+    @Override\n+    public Map<String, ColumnHandle> getColumnHandles(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+        return getColumns(icebergTable.schema(), typeManager).stream()\n+                .collect(toImmutableMap(IcebergColumnHandle::getName, identity()));\n+    }\n+\n+    @Override\n+    public ColumnMetadata getColumnMetadata(ConnectorSession session, ConnectorTableHandle tableHandle, ColumnHandle columnHandle)\n+    {\n+        IcebergColumnHandle column = (IcebergColumnHandle) columnHandle;\n+        return new ColumnMetadata(column.getName(), column.getType(), column.getComment().orElse(\"\"), false);\n+    }\n+\n+    @Override\n+    public Map<SchemaTableName, List<ColumnMetadata>> listTableColumns(ConnectorSession session, SchemaTablePrefix prefix)\n+    {\n+        List<SchemaTableName> tables = listTables(session, Optional.of(prefix.getSchemaName()));\n+\n+        ImmutableMap.Builder<SchemaTableName, List<ColumnMetadata>> columns = ImmutableMap.builder();\n+        for (SchemaTableName table : tables) {\n+            try {\n+                columns.put(table, getTableMetadata(session, table).getColumns());\n+            }\n+            catch (TableNotFoundException e) {\n+                // table disappeared during listing operation\n+            }\n+            catch (UnknownTableTypeException e) {\n+                // ignore table of unknown type\n+            }\n+        }\n+        return columns.build();\n+    }\n+\n+    @Override\n+    public void createSchema(ConnectorSession session, String schemaName, Map<String, Object> properties)\n+    {\n+        Optional<String> location = getSchemaLocation(properties).map(uri -> {\n+            try {\n+                hdfsEnvironment.getFileSystem(new HdfsContext(session, schemaName), new Path(uri));\n+            }\n+            catch (IOException | IllegalArgumentException e) {\n+                throw new PrestoException(INVALID_SCHEMA_PROPERTY, \"Invalid location URI: \" + uri, e);\n+            }\n+            return uri;\n+        });\n+\n+        Database database = Database.builder()\n+                .setDatabaseName(schemaName)\n+                .setLocation(location)\n+                .setOwnerType(USER)\n+                .setOwnerName(session.getUser())\n+                .build();\n+\n+        metastore.createDatabase(database);\n+    }\n+\n+    @Override\n+    public void dropSchema(ConnectorSession session, String schemaName)\n+    {\n+        // basic sanity check to provide a better error message\n+        if (!listTables(session, Optional.of(schemaName)).isEmpty() ||\n+                !listViews(session, Optional.of(schemaName)).isEmpty()) {\n+            throw new PrestoException(SCHEMA_NOT_EMPTY, \"Schema not empty: \" + schemaName);\n+        }\n+        metastore.dropDatabase(schemaName);\n+    }\n+\n+    @Override\n+    public void renameSchema(ConnectorSession session, String source, String target)\n+    {\n+        metastore.renameDatabase(source, target);\n+    }\n+\n+    @Override\n+    public void createTable(ConnectorSession session, ConnectorTableMetadata tableMetadata, boolean ignoreExisting)\n+    {\n+        Optional<ConnectorNewTableLayout> layout = getNewTableLayout(session, tableMetadata);\n+        finishCreateTable(session, beginCreateTable(session, tableMetadata, layout), ImmutableList.of(), ImmutableList.of());\n+    }\n+\n+    @Override\n+    public ConnectorOutputTableHandle beginCreateTable(ConnectorSession session, ConnectorTableMetadata tableMetadata, Optional<ConnectorNewTableLayout> layout)\n+    {\n+        SchemaTableName schemaTableName = tableMetadata.getTable();\n+        String schemaName = schemaTableName.getSchemaName();\n+        String tableName = schemaTableName.getTableName();\n+\n+        Schema schema = toIcebergSchema(tableMetadata.getColumns());\n+\n+        PartitionSpec partitionSpec = parsePartitionFields(schema, getPartitioning(tableMetadata.getProperties()));\n+\n+        Database database = metastore.getDatabase(schemaName)\n+                .orElseThrow(() -> new SchemaNotFoundException(schemaName));\n+\n+        HdfsContext hdfsContext = new HdfsContext(session, schemaName, tableName);\n+        String targetPath = getTableLocation(tableMetadata.getProperties());\n+        if (targetPath == null) {\n+            Optional<String> location = database.getLocation();\n+            if (!location.isPresent() || location.get().isEmpty()) {\n+                throw new PrestoException(NOT_SUPPORTED, \"Database \" + schemaName + \" location is not set\");\n+            }\n+\n+            Path databasePath = new Path(location.get());\n+            Path resultPath = new Path(databasePath, tableName);\n+            targetPath = resultPath.toString();\n+        }\n+\n+        TableOperations operations = new HiveTableOperations(metastore, hdfsEnvironment, hdfsContext, schemaName, tableName, session.getUser(), targetPath);\n+        if (operations.current() != null) {\n+            throw new TableAlreadyExistsException(schemaTableName);\n+        }\n+\n+        ImmutableMap.Builder<String, String> propertiesBuilder = ImmutableMap.builderWithExpectedSize(2);\n+        FileFormat fileFormat = getFileFormat(tableMetadata.getProperties());\n+        propertiesBuilder.put(DEFAULT_FILE_FORMAT, fileFormat.toString());\n+        if (tableMetadata.getComment().isPresent()) {\n+            propertiesBuilder.put(TABLE_COMMENT, tableMetadata.getComment().get());\n+        }\n+\n+        TableMetadata metadata = newTableMetadata(schema, partitionSpec, targetPath, propertiesBuilder.build());\n+\n+        transaction = createTableTransaction(tableName, operations, metadata);\n+\n+        return new IcebergWritableTableHandle(\n+                schemaName,\n+                tableName,\n+                SchemaParser.toJson(metadata.schema()),\n+                PartitionSpecParser.toJson(metadata.spec()),\n+                getColumns(metadata.schema(), typeManager),\n+                targetPath,\n+                fileFormat);\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishCreateTable(ConnectorSession session, ConnectorOutputTableHandle tableHandle, Collection<Slice> fragments, Collection<ComputedStatistics> computedStatistics)\n+    {\n+        return finishInsert(session, (IcebergWritableTableHandle) tableHandle, fragments, computedStatistics);\n+    }\n+\n+    @Override\n+    public ConnectorInsertTableHandle beginInsert(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        IcebergTableHandle table = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table.getSchemaTableName());\n+\n+        transaction = icebergTable.newTransaction();\n+\n+        return new IcebergWritableTableHandle(\n+                table.getSchemaName(),\n+                table.getTableName(),\n+                SchemaParser.toJson(icebergTable.schema()),\n+                PartitionSpecParser.toJson(icebergTable.spec()),\n+                getColumns(icebergTable.schema(), typeManager),\n+                getDataPath(icebergTable.location()),\n+                getFileFormat(icebergTable));\n+    }\n+\n+    @Override\n+    public Optional<ConnectorOutputMetadata> finishInsert(ConnectorSession session, ConnectorInsertTableHandle insertHandle, Collection<Slice> fragments, Collection<ComputedStatistics> computedStatistics)\n+    {\n+        IcebergWritableTableHandle table = (IcebergWritableTableHandle) insertHandle;\n+        org.apache.iceberg.Table icebergTable = transaction.table();\n+\n+        List<CommitTaskData> commitTasks = fragments.stream()\n+                .map(slice -> commitTaskCodec.fromJson(slice.getBytes()))\n+                .collect(toImmutableList());\n+\n+        Type[] partitionColumnTypes = icebergTable.spec().fields().stream()\n+                .map(field -> field.transform().getResultType(\n+                        icebergTable.schema().findType(field.sourceId())))\n+                .toArray(Type[]::new);\n+\n+        AppendFiles appendFiles = transaction.newFastAppend();\n+        for (CommitTaskData task : commitTasks) {\n+            HdfsContext context = new HdfsContext(session, table.getSchemaName(), table.getTableName());\n+\n+            DataFiles.Builder builder = DataFiles.builder(icebergTable.spec())\n+                    .withInputFile(new HdfsInputFile(new Path(task.getPath()), hdfsEnvironment, context))\n+                    .withFormat(table.getFileFormat())\n+                    .withMetrics(task.getMetrics().metrics());\n+\n+            if (!icebergTable.spec().fields().isEmpty()) {\n+                String partitionDataJson = task.getPartitionDataJson()\n+                        .orElseThrow(() -> new VerifyException(\"No partition data for partitioned table\"));\n+                builder.withPartition(PartitionData.fromJson(partitionDataJson, partitionColumnTypes));\n+            }\n+\n+            appendFiles.appendFile(builder.build());\n+        }\n+\n+        appendFiles.commit();\n+        transaction.commitTransaction();\n+\n+        return Optional.of(new HiveWrittenPartitions(commitTasks.stream()\n+                .map(CommitTaskData::getPath)\n+                .collect(toImmutableList())));\n+    }\n+\n+    @Override\n+    public ColumnHandle getUpdateRowIdColumnHandle(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        return new IcebergColumnHandle(0, \"$row_id\", BIGINT, Optional.empty());\n+    }\n+\n+    @Override\n+    public void dropTable(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n+        metastore.dropTable(handle.getSchemaName(), handle.getTableName(), true);\n+    }\n+\n+    @Override\n+    public void renameTable(ConnectorSession session, ConnectorTableHandle tableHandle, SchemaTableName newTable)\n+    {\n+        IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n+        metastore.renameTable(handle.getSchemaName(), handle.getTableName(), newTable.getSchemaName(), newTable.getTableName());\n+    }\n+\n+    @Override\n+    public void addColumn(ConnectorSession session, ConnectorTableHandle tableHandle, ColumnMetadata column)\n+    {\n+        IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n+        icebergTable.updateSchema().addColumn(column.getName(), toIcebergType(column.getType())).commit();\n+    }\n+\n+    @Override\n+    public void dropColumn(ConnectorSession session, ConnectorTableHandle tableHandle, ColumnHandle column)\n+    {\n+        IcebergTableHandle icebergTableHandle = (IcebergTableHandle) tableHandle;\n+        IcebergColumnHandle handle = (IcebergColumnHandle) column;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, icebergTableHandle.getSchemaTableName());\n+        icebergTable.updateSchema().deleteColumn(handle.getName()).commit();\n+    }\n+\n+    @Override\n+    public void renameColumn(ConnectorSession session, ConnectorTableHandle tableHandle, ColumnHandle source, String target)\n+    {\n+        IcebergTableHandle icebergTableHandle = (IcebergTableHandle) tableHandle;\n+        IcebergColumnHandle columnHandle = (IcebergColumnHandle) source;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, icebergTableHandle.getSchemaTableName());\n+        icebergTable.updateSchema().renameColumn(columnHandle.getName(), target).commit();\n+    }\n+\n+    private ConnectorTableMetadata getTableMetadata(ConnectorSession session, SchemaTableName table)\n+    {\n+        if (!metastore.getTable(table.getSchemaName(), table.getTableName()).isPresent()) {\n+            throw new TableNotFoundException(table);\n+        }\n+\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, table);\n+\n+        List<ColumnMetadata> columns = getColumnMetadatas(icebergTable);\n+\n+        ImmutableMap.Builder<String, Object> properties = ImmutableMap.builder();\n+        properties.put(FILE_FORMAT_PROPERTY, getFileFormat(icebergTable));\n+        if (!icebergTable.spec().fields().isEmpty()) {\n+            properties.put(PARTITIONING_PROPERTY, toPartitionFields(icebergTable.spec()));\n+        }\n+\n+        return new ConnectorTableMetadata(table, columns, properties.build(), getTableComment(icebergTable));\n+    }\n+\n+    private List<ColumnMetadata> getColumnMetadatas(org.apache.iceberg.Table table)\n+    {\n+        return table.schema().columns().stream()\n+                .map(column -> {\n+                    return new ColumnMetadata(column.name(), toPrestoType(column.type(), typeManager), column.doc(), false);\n+                })\n+                .collect(toImmutableList());\n+    }\n+\n+    private static Schema toIcebergSchema(List<ColumnMetadata> columns)\n+    {\n+        List<NestedField> icebergColumns = new ArrayList<>();\n+        for (ColumnMetadata column : columns) {\n+            if (!column.isHidden()) {\n+                int index = icebergColumns.size();\n+                Type type = toIcebergType(column.getType());\n+                NestedField field = column.isNullable()\n+                        ? NestedField.optional(index, column.getName(), type, column.getComment())\n+                        : NestedField.required(index, column.getName(), type, column.getComment());\n+                icebergColumns.add(field);\n+            }\n+        }\n+        Type icebergSchema = Types.StructType.of(icebergColumns);\n+        AtomicInteger nextFieldId = new AtomicInteger(1);\n+        icebergSchema = TypeUtil.assignFreshIds(icebergSchema, nextFieldId::getAndIncrement);\n+        return new Schema(icebergSchema.asStructType().fields());\n+    }\n+\n+    @Override\n+    public ConnectorTableHandle beginDelete(ConnectorSession session, ConnectorTableHandle tableHandle)\n+    {\n+        throw new PrestoException(NOT_SUPPORTED, \"This connector only supports delete where one or more partitions are deleted entirely\");\n+    }\n+\n+    public ExtendedHiveMetastore getMetastore()\n+    {\n+        return metastore;\n+    }\n+\n+    public void rollback()\n+    {\n+        // TODO: cleanup open transaction\n+    }\n+\n+    @Override\n+    public TableStatistics getTableStatistics(ConnectorSession session, ConnectorTableHandle tableHandle, Optional<ConnectorTableLayoutHandle> tableLayoutHandle, List<ColumnHandle> columnHandles, Constraint<ColumnHandle> constraint)\n+    {\n+        IcebergTableHandle handle = (IcebergTableHandle) tableHandle;\n+        org.apache.iceberg.Table icebergTable = getIcebergTable(metastore, hdfsEnvironment, session, handle.getSchemaTableName());\n+        return TableStatisticsMaker.getTableStatistics(typeManager, constraint, handle, icebergTable);\n+    }\n+\n+    private Optional<Long> getSnapshotId(org.apache.iceberg.Table table, Optional<Long> snapshotId)\n+    {\n+        if (snapshotId.isPresent()) {\n+            return Optional.of(IcebergUtil.resolveSnapshotId(table, snapshotId.get()));\n+        }\n+        return Optional.ofNullable(table.currentSnapshot()).map(Snapshot::snapshotId);\n+    }\n+}"
  },
  {
    "sha": "e66c570503f8dab04da5f725b9e7286b8b42f986",
    "filename": "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergMetadataFactory.java",
    "status": "added",
    "additions": 59,
    "deletions": 0,
    "changes": 59,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergMetadataFactory.java",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergMetadataFactory.java",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergMetadataFactory.java?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg;\n+\n+import com.facebook.airlift.json.JsonCodec;\n+import com.facebook.presto.common.type.TypeManager;\n+import com.facebook.presto.hive.HdfsEnvironment;\n+import com.facebook.presto.hive.metastore.ExtendedHiveMetastore;\n+\n+import javax.inject.Inject;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergMetadataFactory\n+{\n+    private final ExtendedHiveMetastore metastore;\n+    private final HdfsEnvironment hdfsEnvironment;\n+    private final TypeManager typeManager;\n+    private final JsonCodec<CommitTaskData> commitTaskCodec;\n+\n+    @Inject\n+    public IcebergMetadataFactory(\n+            IcebergConfig config,\n+            ExtendedHiveMetastore metastore,\n+            HdfsEnvironment hdfsEnvironment,\n+            TypeManager typeManager,\n+            JsonCodec<CommitTaskData> commitTaskDataJsonCodec)\n+    {\n+        this(metastore, hdfsEnvironment, typeManager, commitTaskDataJsonCodec);\n+    }\n+\n+    public IcebergMetadataFactory(\n+            ExtendedHiveMetastore metastore,\n+            HdfsEnvironment hdfsEnvironment,\n+            TypeManager typeManager,\n+            JsonCodec<CommitTaskData> commitTaskCodec)\n+    {\n+        this.metastore = requireNonNull(metastore, \"metastore is null\");\n+        this.hdfsEnvironment = requireNonNull(hdfsEnvironment, \"hdfsEnvironment is null\");\n+        this.typeManager = requireNonNull(typeManager, \"typeManager is null\");\n+        this.commitTaskCodec = requireNonNull(commitTaskCodec, \"commitTaskCodec is null\");\n+    }\n+\n+    public IcebergMetadata create()\n+    {\n+        return new IcebergMetadata(metastore, hdfsEnvironment, typeManager, commitTaskCodec);\n+    }\n+}"
  },
  {
    "sha": "82bce9760b66d3f9c480b413a091df4923eabb77",
    "filename": "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergMetastoreModule.java",
    "status": "added",
    "additions": 38,
    "deletions": 0,
    "changes": 38,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergMetastoreModule.java",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergMetastoreModule.java",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergMetastoreModule.java?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg;\n+\n+import com.facebook.presto.hive.metastore.ExtendedHiveMetastore;\n+import com.google.inject.Binder;\n+import com.google.inject.Module;\n+\n+import javax.inject.Inject;\n+\n+public class IcebergMetastoreModule\n+        implements Module\n+{\n+    @Override\n+    public void configure(Binder binder)\n+    {\n+        binder.bind(MetastoreValidator.class).asEagerSingleton();\n+    }\n+\n+    public static class MetastoreValidator\n+    {\n+        @Inject\n+        public MetastoreValidator(ExtendedHiveMetastore metastore)\n+        {\n+        }\n+    }\n+}"
  },
  {
    "sha": "4dda164e8587439a587bd40fa7219f2660c3cc7a",
    "filename": "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergModule.java",
    "status": "added",
    "additions": 146,
    "deletions": 0,
    "changes": 146,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergModule.java",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergModule.java",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergModule.java?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg;\n+\n+import com.facebook.presto.cache.CacheConfig;\n+import com.facebook.presto.cache.CacheFactory;\n+import com.facebook.presto.cache.CacheManager;\n+import com.facebook.presto.cache.CacheStats;\n+import com.facebook.presto.cache.ForCachingFileSystem;\n+import com.facebook.presto.cache.NoOpCacheManager;\n+import com.facebook.presto.cache.filemerge.FileMergeCacheConfig;\n+import com.facebook.presto.cache.filemerge.FileMergeCacheManager;\n+import com.facebook.presto.hive.DynamicConfigurationProvider;\n+import com.facebook.presto.hive.FileFormatDataSourceStats;\n+import com.facebook.presto.hive.ForCachingHiveMetastore;\n+import com.facebook.presto.hive.ForMetastoreHdfsEnvironment;\n+import com.facebook.presto.hive.HdfsConfiguration;\n+import com.facebook.presto.hive.HdfsConfigurationInitializer;\n+import com.facebook.presto.hive.HdfsEnvironment;\n+import com.facebook.presto.hive.HiveClientConfig;\n+import com.facebook.presto.hive.HiveHdfsConfiguration;\n+import com.facebook.presto.hive.HiveNodePartitioningProvider;\n+import com.facebook.presto.hive.MetastoreClientConfig;\n+import com.facebook.presto.hive.ParquetFileWriterConfig;\n+import com.facebook.presto.hive.cache.HiveCachingHdfsConfiguration;\n+import com.facebook.presto.hive.gcs.GcsConfigurationInitializer;\n+import com.facebook.presto.hive.gcs.HiveGcsConfig;\n+import com.facebook.presto.hive.gcs.HiveGcsConfigurationInitializer;\n+import com.facebook.presto.hive.metastore.CachingHiveMetastore;\n+import com.facebook.presto.hive.metastore.ExtendedHiveMetastore;\n+import com.facebook.presto.hive.metastore.MetastoreConfig;\n+import com.facebook.presto.spi.connector.ConnectorNodePartitioningProvider;\n+import com.facebook.presto.spi.connector.ConnectorPageSinkProvider;\n+import com.facebook.presto.spi.connector.ConnectorPageSourceProvider;\n+import com.facebook.presto.spi.connector.ConnectorSplitManager;\n+import com.facebook.presto.spi.procedure.Procedure;\n+import com.google.inject.Binder;\n+import com.google.inject.Module;\n+import com.google.inject.Provides;\n+import com.google.inject.Scopes;\n+import com.google.inject.multibindings.Multibinder;\n+import org.weakref.jmx.testing.TestingMBeanServer;\n+\n+import javax.inject.Singleton;\n+import javax.management.MBeanServer;\n+\n+import java.util.concurrent.ExecutorService;\n+\n+import static com.facebook.airlift.concurrent.Threads.daemonThreadsNamed;\n+import static com.facebook.airlift.configuration.ConfigBinder.configBinder;\n+import static com.facebook.airlift.json.JsonCodecBinder.jsonCodecBinder;\n+import static com.facebook.presto.cache.CacheType.FILE_MERGE;\n+import static com.google.inject.multibindings.Multibinder.newSetBinder;\n+import static java.util.concurrent.Executors.newFixedThreadPool;\n+import static java.util.concurrent.Executors.newScheduledThreadPool;\n+import static org.weakref.jmx.guice.ExportBinder.newExporter;\n+\n+public class IcebergModule\n+        implements Module\n+{\n+    @Override\n+    public void configure(Binder binder)\n+    {\n+        binder.bind(HdfsEnvironment.class).in(Scopes.SINGLETON);\n+        configBinder(binder).bindConfig(CacheConfig.class);\n+        configBinder(binder).bindConfig(FileMergeCacheConfig.class);\n+        binder.bind(CacheStats.class).in(Scopes.SINGLETON);\n+        configBinder(binder).bindConfig(MetastoreClientConfig.class);\n+        configBinder(binder).bindConfig(HiveGcsConfig.class);\n+        binder.bind(GcsConfigurationInitializer.class).to(HiveGcsConfigurationInitializer.class).in(Scopes.SINGLETON);\n+        binder.bind(HdfsConfiguration.class).annotatedWith(ForMetastoreHdfsEnvironment.class).to(HiveCachingHdfsConfiguration.class).in(Scopes.SINGLETON);\n+        binder.bind(HdfsConfiguration.class).annotatedWith(ForCachingFileSystem.class).to(HiveHdfsConfiguration.class).in(Scopes.SINGLETON);\n+        binder.bind(ExtendedHiveMetastore.class).to(CachingHiveMetastore.class).in(Scopes.SINGLETON);\n+        binder.bind(MBeanServer.class).toInstance(new TestingMBeanServer());\n+\n+        binder.bind(HdfsConfigurationInitializer.class).in(Scopes.SINGLETON);\n+        newSetBinder(binder, DynamicConfigurationProvider.class);\n+\n+        binder.bind(CacheFactory.class).in(Scopes.SINGLETON);\n+        binder.bind(IcebergTransactionManager.class).in(Scopes.SINGLETON);\n+\n+        configBinder(binder).bindConfig(HiveClientConfig.class);\n+        configBinder(binder).bindConfig(IcebergConfig.class);\n+        configBinder(binder).bindConfig(MetastoreConfig.class);\n+\n+        binder.bind(IcebergSessionProperties.class).in(Scopes.SINGLETON);\n+        binder.bind(IcebergTableProperties.class).in(Scopes.SINGLETON);\n+\n+        binder.bind(ConnectorSplitManager.class).to(IcebergSplitManager.class).in(Scopes.SINGLETON);\n+        binder.bind(ConnectorPageSourceProvider.class).to(IcebergPageSourceProvider.class).in(Scopes.SINGLETON);\n+        binder.bind(ConnectorPageSinkProvider.class).to(IcebergPageSinkProvider.class).in(Scopes.SINGLETON);\n+        binder.bind(ConnectorNodePartitioningProvider.class).to(HiveNodePartitioningProvider.class).in(Scopes.SINGLETON);\n+\n+        configBinder(binder).bindConfig(ParquetFileWriterConfig.class);\n+\n+        binder.bind(IcebergMetadataFactory.class).in(Scopes.SINGLETON);\n+\n+        jsonCodecBinder(binder).bindJsonCodec(CommitTaskData.class);\n+\n+        binder.bind(FileFormatDataSourceStats.class).in(Scopes.SINGLETON);\n+        newExporter(binder).export(FileFormatDataSourceStats.class).withGeneratedName();\n+\n+        binder.bind(IcebergFileWriterFactory.class).in(Scopes.SINGLETON);\n+        newExporter(binder).export(IcebergFileWriterFactory.class).withGeneratedName();\n+\n+        Multibinder<Procedure> procedures = newSetBinder(binder, Procedure.class);\n+        procedures.addBinding().toProvider(RollbackToSnapshotProcedure.class).in(Scopes.SINGLETON);\n+    }\n+\n+    @ForCachingHiveMetastore\n+    @Singleton\n+    @Provides\n+    public ExecutorService createCachingHiveMetastoreExecutor(MetastoreClientConfig metastoreClientConfig)\n+    {\n+        return newFixedThreadPool(\n+                metastoreClientConfig.getMaxMetastoreRefreshThreads(),\n+                daemonThreadsNamed(\"hive-metastore-iceberg-%s\"));\n+    }\n+\n+    @Singleton\n+    @Provides\n+    public CacheManager createCacheManager(CacheConfig cacheConfig, FileMergeCacheConfig fileMergeCacheConfig, CacheStats cacheStats)\n+    {\n+        if (cacheConfig.isCachingEnabled() && cacheConfig.getCacheType() == FILE_MERGE) {\n+            return new FileMergeCacheManager(\n+                    cacheConfig,\n+                    fileMergeCacheConfig,\n+                    cacheStats,\n+                    newScheduledThreadPool(5, daemonThreadsNamed(\"iceberg-cache-flusher-%s\")),\n+                    newScheduledThreadPool(1, daemonThreadsNamed(\"iceberg-cache-remover-%s\")),\n+                    newScheduledThreadPool(1, daemonThreadsNamed(\"hive-cache-size-calculator-%s\")));\n+        }\n+        return new NoOpCacheManager();\n+    }\n+}"
  },
  {
    "sha": "3d0dc80cff26374044a041a628a850c018723a1b",
    "filename": "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergPageSink.java",
    "status": "added",
    "additions": 495,
    "deletions": 0,
    "changes": 495,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergPageSink.java",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergPageSink.java",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergPageSink.java?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -0,0 +1,495 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg;\n+\n+import com.facebook.airlift.json.JsonCodec;\n+import com.facebook.presto.common.Page;\n+import com.facebook.presto.common.block.Block;\n+import com.facebook.presto.common.type.BigintType;\n+import com.facebook.presto.common.type.BooleanType;\n+import com.facebook.presto.common.type.DateType;\n+import com.facebook.presto.common.type.DecimalType;\n+import com.facebook.presto.common.type.DoubleType;\n+import com.facebook.presto.common.type.IntegerType;\n+import com.facebook.presto.common.type.RealType;\n+import com.facebook.presto.common.type.SmallintType;\n+import com.facebook.presto.common.type.TinyintType;\n+import com.facebook.presto.common.type.Type;\n+import com.facebook.presto.common.type.VarbinaryType;\n+import com.facebook.presto.common.type.VarcharType;\n+import com.facebook.presto.hive.HdfsContext;\n+import com.facebook.presto.hive.HdfsEnvironment;\n+import com.facebook.presto.iceberg.PartitionTransforms.ColumnTransform;\n+import com.facebook.presto.spi.ConnectorPageSink;\n+import com.facebook.presto.spi.ConnectorSession;\n+import com.facebook.presto.spi.PageIndexer;\n+import com.facebook.presto.spi.PageIndexerFactory;\n+import com.facebook.presto.spi.PrestoException;\n+import com.google.common.collect.ImmutableList;\n+import io.airlift.slice.Slice;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.transforms.Transform;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.function.Function;\n+\n+import static com.facebook.presto.common.type.Decimals.readBigDecimal;\n+import static com.facebook.presto.hive.util.ConfigurationUtils.toJobConf;\n+import static com.facebook.presto.iceberg.IcebergErrorCode.ICEBERG_TOO_MANY_OPEN_PARTITIONS;\n+import static com.facebook.presto.iceberg.PartitionTransforms.getColumnTransform;\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Verify.verify;\n+import static com.google.common.collect.ImmutableList.toImmutableList;\n+import static io.airlift.slice.Slices.wrappedBuffer;\n+import static java.lang.Float.intBitsToFloat;\n+import static java.lang.Math.toIntExact;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+import static java.util.UUID.randomUUID;\n+import static java.util.concurrent.CompletableFuture.completedFuture;\n+\n+public class IcebergPageSink\n+        implements ConnectorPageSink\n+{\n+    private static final int MAX_PAGE_POSITIONS = 4096;\n+\n+    @SuppressWarnings({\"FieldCanBeLocal\", \"FieldMayBeStatic\"})\n+    private final int maxOpenWriters = 100;  // TODO: make this configurable\n+    private final Schema outputSchema;\n+    private final PartitionSpec partitionSpec;\n+    private final String outputPath;\n+    private final IcebergFileWriterFactory fileWriterFactory;\n+    private final HdfsEnvironment hdfsEnvironment;\n+    private final HdfsContext hdfsContext;\n+    private final JobConf jobConf;\n+    private final JsonCodec<CommitTaskData> jsonCodec;\n+    private final ConnectorSession session;\n+    private final FileFormat fileFormat;\n+    private final PagePartitioner pagePartitioner;\n+\n+    private final List<WriteContext> writers = new ArrayList<>();\n+\n+    private long writtenBytes;\n+    private long systemMemoryUsage;\n+    private long validationCpuNanos;\n+\n+    public IcebergPageSink(\n+            Schema outputSchema,\n+            PartitionSpec partitionSpec,\n+            String outputPath,\n+            IcebergFileWriterFactory fileWriterFactory,\n+            PageIndexerFactory pageIndexerFactory,\n+            HdfsEnvironment hdfsEnvironment,\n+            HdfsContext hdfsContext,\n+            List<IcebergColumnHandle> inputColumns,\n+            JsonCodec<CommitTaskData> jsonCodec,\n+            ConnectorSession session,\n+            FileFormat fileFormat)\n+    {\n+        requireNonNull(inputColumns, \"inputColumns is null\");\n+        this.outputSchema = requireNonNull(outputSchema, \"outputSchema is null\");\n+        this.partitionSpec = requireNonNull(partitionSpec, \"partitionSpec is null\");\n+        this.outputPath = requireNonNull(outputPath, \"outputPath is null\");\n+        this.fileWriterFactory = requireNonNull(fileWriterFactory, \"fileWriterFactory is null\");\n+        this.hdfsEnvironment = requireNonNull(hdfsEnvironment, \"hdfsEnvironment is null\");\n+        this.hdfsContext = requireNonNull(hdfsContext, \"hdfsContext is null\");\n+        this.jobConf = toJobConf(hdfsEnvironment.getConfiguration(hdfsContext, new Path(outputPath)));\n+        this.jsonCodec = requireNonNull(jsonCodec, \"jsonCodec is null\");\n+        this.session = requireNonNull(session, \"session is null\");\n+        this.fileFormat = requireNonNull(fileFormat, \"fileFormat is null\");\n+        this.pagePartitioner = new PagePartitioner(pageIndexerFactory, toPartitionColumns(inputColumns, partitionSpec));\n+    }\n+\n+    @Override\n+    public long getCompletedBytes()\n+    {\n+        return writtenBytes;\n+    }\n+\n+    @Override\n+    public long getSystemMemoryUsage()\n+    {\n+        return systemMemoryUsage;\n+    }\n+\n+    @Override\n+    public long getValidationCpuNanos()\n+    {\n+        return validationCpuNanos;\n+    }\n+\n+    @Override\n+    public CompletableFuture<?> appendPage(Page page)\n+    {\n+        hdfsEnvironment.doAs(session.getUser(), () -> doAppend(page));\n+\n+        return NOT_BLOCKED;\n+    }\n+\n+    @Override\n+    public CompletableFuture<Collection<Slice>> finish()\n+    {\n+        Collection<Slice> commitTasks = new ArrayList<>();\n+\n+        for (WriteContext context : writers) {\n+            context.getWriter().commit();\n+\n+            CommitTaskData task = new CommitTaskData(\n+                    context.getPath().toString(),\n+                    new MetricsWrapper(context.writer.getMetrics()),\n+                    context.getPartitionData().map(PartitionData::toJson));\n+\n+            commitTasks.add(wrappedBuffer(jsonCodec.toJsonBytes(task)));\n+        }\n+\n+        writtenBytes = writers.stream()\n+                .mapToLong(writer -> writer.getWriter().getWrittenBytes())\n+                .sum();\n+        validationCpuNanos = writers.stream()\n+                .mapToLong(writer -> writer.getWriter().getValidationCpuNanos())\n+                .sum();\n+\n+        return completedFuture(commitTasks);\n+    }\n+\n+    @Override\n+    public void abort()\n+    {\n+        RuntimeException error = null;\n+        for (WriteContext context : writers) {\n+            try {\n+                if (context != null) {\n+                    context.getWriter().rollback();\n+                }\n+            }\n+            catch (Throwable t) {\n+                if (error == null) {\n+                    error = new RuntimeException(\"Exception during rollback\");\n+                }\n+                error.addSuppressed(t);\n+            }\n+        }\n+        if (error != null) {\n+            throw error;\n+        }\n+    }\n+\n+    private void doAppend(Page page)\n+    {\n+        while (page.getPositionCount() > MAX_PAGE_POSITIONS) {\n+            Page chunk = page.getRegion(0, MAX_PAGE_POSITIONS);\n+            page = page.getRegion(MAX_PAGE_POSITIONS, page.getPositionCount() - MAX_PAGE_POSITIONS);\n+            writePage(chunk);\n+        }\n+\n+        writePage(page);\n+    }\n+\n+    private void writePage(Page page)\n+    {\n+        int[] writerIndexes = getWriterIndexes(page);\n+\n+        // position count for each writer\n+        int[] sizes = new int[writers.size()];\n+        for (int index : writerIndexes) {\n+            sizes[index]++;\n+        }\n+\n+        // record which positions are used by which writer\n+        int[][] writerPositions = new int[writers.size()][];\n+        int[] counts = new int[writers.size()];\n+\n+        for (int position = 0; position < page.getPositionCount(); position++) {\n+            int index = writerIndexes[position];\n+\n+            int count = counts[index];\n+            if (count == 0) {\n+                writerPositions[index] = new int[sizes[index]];\n+            }\n+            writerPositions[index][count] = position;\n+            counts[index]++;\n+        }\n+\n+        // invoke the writers\n+        for (int index = 0; index < writerPositions.length; index++) {\n+            int[] positions = writerPositions[index];\n+            if (positions == null) {\n+                continue;\n+            }\n+\n+            // if write is partitioned across multiple writers, filter page using dictionary blocks\n+            Page pageForWriter = page;\n+            if (positions.length != page.getPositionCount()) {\n+                verify(positions.length == counts[index]);\n+                pageForWriter = pageForWriter.getPositions(positions, 0, positions.length);\n+            }\n+\n+            IcebergFileWriter writer = writers.get(index).getWriter();\n+\n+            long currentWritten = writer.getWrittenBytes();\n+            long currentMemory = writer.getSystemMemoryUsage();\n+\n+            writer.appendRows(pageForWriter);\n+\n+            writtenBytes += (writer.getWrittenBytes() - currentWritten);\n+            systemMemoryUsage += (writer.getSystemMemoryUsage() - currentMemory);\n+        }\n+    }\n+\n+    private int[] getWriterIndexes(Page page)\n+    {\n+        int[] writerIndexes = pagePartitioner.partitionPage(page);\n+\n+        if (pagePartitioner.getMaxIndex() >= maxOpenWriters) {\n+            throw new PrestoException(ICEBERG_TOO_MANY_OPEN_PARTITIONS, format(\"Exceeded limit of %s open writers for partitions\", maxOpenWriters));\n+        }\n+\n+        // expand writers list to new size\n+        while (writers.size() <= pagePartitioner.getMaxIndex()) {\n+            writers.add(null);\n+        }\n+\n+        // create missing writers\n+        for (int position = 0; position < page.getPositionCount(); position++) {\n+            int writerIndex = writerIndexes[position];\n+            if (writers.get(writerIndex) != null) {\n+                continue;\n+            }\n+\n+            Optional<PartitionData> partitionData = getPartitionData(pagePartitioner.getColumns(), page, position);\n+            Optional<String> partitionPath = partitionData.map(partitionSpec::partitionToPath);\n+\n+            WriteContext writer = createWriter(partitionPath, partitionData);\n+\n+            writers.set(writerIndex, writer);\n+        }\n+        verify(writers.size() == pagePartitioner.getMaxIndex() + 1);\n+        verify(!writers.contains(null));\n+\n+        return writerIndexes;\n+    }\n+\n+    private WriteContext createWriter(Optional<String> partitionPath, Optional<PartitionData> partitionData)\n+    {\n+        Path outputPath = new Path(this.outputPath);\n+        if (partitionPath.isPresent()) {\n+            outputPath = new Path(outputPath, partitionPath.get());\n+        }\n+        outputPath = new Path(outputPath, randomUUID().toString());\n+        outputPath = new Path(fileFormat.addExtension(outputPath.toString()));\n+\n+        IcebergFileWriter writer = fileWriterFactory.createFileWriter(\n+                outputPath,\n+                outputSchema,\n+                jobConf,\n+                session,\n+                hdfsContext,\n+                fileFormat);\n+\n+        return new WriteContext(writer, outputPath, partitionData);\n+    }\n+\n+    private static Optional<PartitionData> getPartitionData(List<PartitionColumn> columns, Page page, int position)\n+    {\n+        if (columns.isEmpty()) {\n+            return Optional.empty();\n+        }\n+\n+        Object[] values = new Object[columns.size()];\n+        for (int i = 0; i < columns.size(); i++) {\n+            PartitionColumn column = columns.get(i);\n+            Block block = page.getBlock(column.getSourceChannel());\n+            Type type = column.getSourceType();\n+            Object value = getIcebergValue(block, position, type);\n+            values[i] = applyTransform(column.getField().transform(), value);\n+        }\n+        return Optional.of(new PartitionData(values));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private static Object applyTransform(Transform<?, ?> transform, Object value)\n+    {\n+        return ((Transform<Object, Object>) transform).apply(value);\n+    }\n+\n+    public static Object getIcebergValue(Block block, int position, Type type)\n+    {\n+        if (block.isNull(position)) {\n+            return null;\n+        }\n+        if (type instanceof BigintType) {\n+            return type.getLong(block, position);\n+        }\n+        if (type instanceof IntegerType || type instanceof SmallintType || type instanceof TinyintType || type instanceof DateType) {\n+            return toIntExact(type.getLong(block, position));\n+        }\n+        if (type instanceof BooleanType) {\n+            return type.getBoolean(block, position);\n+        }\n+        if (type instanceof DecimalType) {\n+            return readBigDecimal((DecimalType) type, block, position);\n+        }\n+        if (type instanceof RealType) {\n+            return intBitsToFloat(toIntExact(type.getLong(block, position)));\n+        }\n+        if (type instanceof DoubleType) {\n+            return type.getDouble(block, position);\n+        }\n+        if (type instanceof VarbinaryType) {\n+            return type.getSlice(block, position).getBytes();\n+        }\n+        if (type instanceof VarcharType) {\n+            return type.getSlice(block, position).toStringUtf8();\n+        }\n+        throw new UnsupportedOperationException(\"Type not supported as partition column: \" + type.getDisplayName());\n+    }\n+\n+    private static List<PartitionColumn> toPartitionColumns(List<IcebergColumnHandle> handles, PartitionSpec partitionSpec)\n+    {\n+        Map<Integer, Integer> idChannels = new HashMap<>();\n+        for (int i = 0; i < handles.size(); i++) {\n+            idChannels.put(handles.get(i).getId(), i);\n+        }\n+\n+        return partitionSpec.fields().stream()\n+                .map(field -> {\n+                    Integer channel = idChannels.get(field.sourceId());\n+                    checkArgument(channel != null, \"partition field not found: %s\", field);\n+                    Type inputType = handles.get(channel).getType();\n+                    ColumnTransform transform = getColumnTransform(field, inputType);\n+                    return new PartitionColumn(field, channel, inputType, transform.getType(), transform.getTransform());\n+                })\n+                .collect(toImmutableList());\n+    }\n+\n+    private static class WriteContext\n+    {\n+        private final IcebergFileWriter writer;\n+        private final Path path;\n+        private final Optional<PartitionData> partitionData;\n+\n+        public WriteContext(IcebergFileWriter writer, Path path, Optional<PartitionData> partitionData)\n+        {\n+            this.writer = requireNonNull(writer, \"writer is null\");\n+            this.path = requireNonNull(path, \"path is null\");\n+            this.partitionData = requireNonNull(partitionData, \"partitionData is null\");\n+        }\n+\n+        public IcebergFileWriter getWriter()\n+        {\n+            return writer;\n+        }\n+\n+        public Path getPath()\n+        {\n+            return path;\n+        }\n+\n+        public Optional<PartitionData> getPartitionData()\n+        {\n+            return partitionData;\n+        }\n+    }\n+\n+    private static class PagePartitioner\n+    {\n+        private final PageIndexer pageIndexer;\n+        private final List<PartitionColumn> columns;\n+\n+        public PagePartitioner(PageIndexerFactory pageIndexerFactory, List<PartitionColumn> columns)\n+        {\n+            this.pageIndexer = pageIndexerFactory.createPageIndexer(columns.stream()\n+                    .map(PartitionColumn::getResultType)\n+                    .collect(toImmutableList()));\n+            this.columns = ImmutableList.copyOf(columns);\n+        }\n+\n+        public int[] partitionPage(Page page)\n+        {\n+            Block[] blocks = new Block[columns.size()];\n+            for (int i = 0; i < columns.size(); i++) {\n+                PartitionColumn column = columns.get(i);\n+                Block block = page.getBlock(column.getSourceChannel());\n+                blocks[i] = column.getBlockTransform().apply(block);\n+            }\n+            Page transformed = new Page(page.getPositionCount(), blocks);\n+\n+            return pageIndexer.indexPage(transformed);\n+        }\n+\n+        public int getMaxIndex()\n+        {\n+            return pageIndexer.getMaxIndex();\n+        }\n+\n+        public List<PartitionColumn> getColumns()\n+        {\n+            return columns;\n+        }\n+    }\n+\n+    private static class PartitionColumn\n+    {\n+        private final PartitionField field;\n+        private final int sourceChannel;\n+        private final Type sourceType;\n+        private final Type resultType;\n+        private final Function<Block, Block> blockTransform;\n+\n+        public PartitionColumn(PartitionField field, int sourceChannel, Type sourceType, Type resultType, Function<Block, Block> blockTransform)\n+        {\n+            this.field = requireNonNull(field, \"field is null\");\n+            this.sourceChannel = sourceChannel;\n+            this.sourceType = requireNonNull(sourceType, \"sourceType is null\");\n+            this.resultType = requireNonNull(resultType, \"resultType is null\");\n+            this.blockTransform = requireNonNull(blockTransform, \"blockTransform is null\");\n+        }\n+\n+        public PartitionField getField()\n+        {\n+            return field;\n+        }\n+\n+        public int getSourceChannel()\n+        {\n+            return sourceChannel;\n+        }\n+\n+        public Type getSourceType()\n+        {\n+            return sourceType;\n+        }\n+\n+        public Type getResultType()\n+        {\n+            return resultType;\n+        }\n+\n+        public Function<Block, Block> getBlockTransform()\n+        {\n+            return blockTransform;\n+        }\n+    }\n+}"
  },
  {
    "sha": "5b018cdd74ba54a67d63bd2b97d746fef94769d5",
    "filename": "presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergPageSinkProvider.java",
    "status": "added",
    "additions": 87,
    "deletions": 0,
    "changes": 87,
    "blob_url": "https://github.com/prestodb/presto/blob/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergPageSinkProvider.java",
    "raw_url": "https://github.com/prestodb/presto/raw/0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergPageSinkProvider.java",
    "contents_url": "https://api.github.com/repos/prestodb/presto/contents/presto-iceberg/src/main/java/com/facebook/presto/iceberg/IcebergPageSinkProvider.java?ref=0ecc61dfc6f0f081f7cd85fd317ae60e1d28d247",
    "patch": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.iceberg;\n+\n+import com.facebook.airlift.json.JsonCodec;\n+import com.facebook.presto.hive.HdfsContext;\n+import com.facebook.presto.hive.HdfsEnvironment;\n+import com.facebook.presto.spi.ConnectorInsertTableHandle;\n+import com.facebook.presto.spi.ConnectorOutputTableHandle;\n+import com.facebook.presto.spi.ConnectorPageSink;\n+import com.facebook.presto.spi.ConnectorSession;\n+import com.facebook.presto.spi.PageIndexerFactory;\n+import com.facebook.presto.spi.PageSinkContext;\n+import com.facebook.presto.spi.connector.ConnectorPageSinkProvider;\n+import com.facebook.presto.spi.connector.ConnectorTransactionHandle;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+\n+import javax.inject.Inject;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class IcebergPageSinkProvider\n+        implements ConnectorPageSinkProvider\n+{\n+    private final HdfsEnvironment hdfsEnvironment;\n+    private final JsonCodec<CommitTaskData> jsonCodec;\n+    private final IcebergFileWriterFactory fileWriterFactory;\n+    private final PageIndexerFactory pageIndexerFactory;\n+\n+    @Inject\n+    public IcebergPageSinkProvider(\n+            HdfsEnvironment hdfsEnvironment,\n+            JsonCodec<CommitTaskData> jsonCodec,\n+            IcebergFileWriterFactory fileWriterFactory,\n+            PageIndexerFactory pageIndexerFactory)\n+    {\n+        this.hdfsEnvironment = requireNonNull(hdfsEnvironment, \"hdfsEnvironment is null\");\n+        this.jsonCodec = requireNonNull(jsonCodec, \"jsonCodec is null\");\n+        this.fileWriterFactory = requireNonNull(fileWriterFactory, \"fileWriterFactory is null\");\n+        this.pageIndexerFactory = requireNonNull(pageIndexerFactory, \"pageIndexerFactory is null\");\n+    }\n+\n+    @Override\n+    public ConnectorPageSink createPageSink(ConnectorTransactionHandle transactionHandle, ConnectorSession session, ConnectorOutputTableHandle outputTableHandle, PageSinkContext pageSinkContext)\n+    {\n+        return createPageSink(session, (IcebergWritableTableHandle) outputTableHandle);\n+    }\n+\n+    @Override\n+    public ConnectorPageSink createPageSink(ConnectorTransactionHandle transactionHandle, ConnectorSession session, ConnectorInsertTableHandle insertTableHandle, PageSinkContext pageSinkContext)\n+    {\n+        return createPageSink(session, (IcebergWritableTableHandle) insertTableHandle);\n+    }\n+\n+    private ConnectorPageSink createPageSink(ConnectorSession session, IcebergWritableTableHandle tableHandle)\n+    {\n+        HdfsContext hdfsContext = new HdfsContext(session, tableHandle.getSchemaName(), tableHandle.getTableName());\n+        Schema schema = SchemaParser.fromJson(tableHandle.getSchemaAsJson());\n+        PartitionSpec partitionSpec = PartitionSpecParser.fromJson(schema, tableHandle.getPartitionSpecAsJson());\n+        return new IcebergPageSink(\n+                schema,\n+                partitionSpec,\n+                tableHandle.getOutputPath(),\n+                fileWriterFactory,\n+                pageIndexerFactory,\n+                hdfsEnvironment,\n+                hdfsContext,\n+                tableHandle.getInputColumns(),\n+                jsonCodec,\n+                session,\n+                tableHandle.getFileFormat());\n+    }\n+}"
  }
]
