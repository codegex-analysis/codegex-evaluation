[
  {
    "sha": "c65399f15bb49ec3b56c861831721c1fb81cbab8",
    "filename": ".gitmodules",
    "status": "added",
    "additions": 4,
    "deletions": 0,
    "changes": 4,
    "blob_url": "https://github.com/acoli-repo/conll-rdf/blob/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/.gitmodules",
    "raw_url": "https://github.com/acoli-repo/conll-rdf/raw/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/.gitmodules",
    "contents_url": "https://api.github.com/repos/acoli-repo/conll-rdf/contents/.gitmodules?ref=4f49b0dd21f0cfe1898b860851c18fcd891fdf13",
    "patch": "@@ -0,0 +1,4 @@\n+[submodule \"src/test/java/org/acoli/conll/rdf\"]\n+\tpath = src/test/java/org/acoli/conll/rdf\n+\turl = https://github.com/leogott/conll-rdf\n+\tbranch = test"
  },
  {
    "sha": "dc48acbd9392405b2fafa86c0d3e727e767569dd",
    "filename": "src/main/java/org/acoli/conll/rdf/CoNLL2RDF.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/acoli-repo/conll-rdf/blob/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/CoNLL2RDF.java",
    "raw_url": "https://github.com/acoli-repo/conll-rdf/raw/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/CoNLL2RDF.java",
    "contents_url": "https://api.github.com/repos/acoli-repo/conll-rdf/contents/src/main/java/org/acoli/conll/rdf/CoNLL2RDF.java?ref=4f49b0dd21f0cfe1898b860851c18fcd891fdf13",
    "patch": "@@ -48,7 +48,7 @@ public CoNLL2RDF(String baseURI, String[] fields) throws IOException {\n \t}\r\n \r\n \t\r\n-\tprivate static Logger LOG = Logger.getLogger(CoNLL2RDF.class.getName());\r\n+\tprivate static Logger LOG = Logger.getLogger(CoNLL2RDF.class);\r\n \t\r\n \t/** @param argv baseURI field1 field2 ... (see variable <code>help</code> and method <code>conll2ttl</code>) */\r\n \tpublic static void main(String[] argv) throws Exception {\t\t\r"
  },
  {
    "sha": "4e080134df18cda697c40d89a3b0bab88468deed",
    "filename": "src/main/java/org/acoli/conll/rdf/CoNLLBrackets2RDF.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/acoli-repo/conll-rdf/blob/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/CoNLLBrackets2RDF.java",
    "raw_url": "https://github.com/acoli-repo/conll-rdf/raw/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/CoNLLBrackets2RDF.java",
    "contents_url": "https://api.github.com/repos/acoli-repo/conll-rdf/contents/src/main/java/org/acoli/conll/rdf/CoNLLBrackets2RDF.java?ref=4f49b0dd21f0cfe1898b860851c18fcd891fdf13",
    "patch": "@@ -13,7 +13,7 @@\n  *  */\r\n abstract class CoNLLBrackets2RDF extends Format2RDF {\r\n \r\n-\tprivate static Logger LOG = Logger.getLogger(Format2RDF.class.getName());\r\n+\tprivate static Logger LOG = Logger.getLogger(Format2RDF.class);\r\n \t\r\n \t/** marks columns that contain brackets, judging from the first sentence that  */\r\n \tprotected final Boolean[] col2bracket;\r"
  },
  {
    "sha": "a69be00c1136512996867b2ea30db7ce030af7bf",
    "filename": "src/main/java/org/acoli/conll/rdf/CoNLLRDFAnnotator.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/acoli-repo/conll-rdf/blob/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/CoNLLRDFAnnotator.java",
    "raw_url": "https://github.com/acoli-repo/conll-rdf/raw/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/CoNLLRDFAnnotator.java",
    "contents_url": "https://api.github.com/repos/acoli-repo/conll-rdf/contents/src/main/java/org/acoli/conll/rdf/CoNLLRDFAnnotator.java?ref=4f49b0dd21f0cfe1898b860851c18fcd891fdf13",
    "patch": "@@ -33,7 +33,7 @@\n  */\n public class CoNLLRDFAnnotator extends CoNLLRDFFormatter {\n \t\n-\t\tprivate static Logger LOG = Logger.getLogger(CoNLLRDFAnnotator.class.getName());\n+\t\tprivate static Logger LOG = Logger.getLogger(CoNLLRDFAnnotator.class);\n \n \t\tpublic static void main(String[] argv) throws IOException {\n \t\t\tLOG.info(\"synopsis: CoNLLRDFAnnotator file.ttl \\n\""
  },
  {
    "sha": "208fa6863d7ba89fc5a07578e6f21d4474efd444",
    "filename": "src/main/java/org/acoli/conll/rdf/CoNLLRDFCommandLine.java",
    "status": "added",
    "additions": 286,
    "deletions": 0,
    "changes": 286,
    "blob_url": "https://github.com/acoli-repo/conll-rdf/blob/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/CoNLLRDFCommandLine.java",
    "raw_url": "https://github.com/acoli-repo/conll-rdf/raw/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/CoNLLRDFCommandLine.java",
    "contents_url": "https://api.github.com/repos/acoli-repo/conll-rdf/contents/src/main/java/org/acoli/conll/rdf/CoNLLRDFCommandLine.java?ref=4f49b0dd21f0cfe1898b860851c18fcd891fdf13",
    "patch": "@@ -0,0 +1,286 @@\n+package org.acoli.conll.rdf;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.io.OutputStreamWriter;\n+import java.io.PrintWriter;\n+import java.io.StringWriter;\n+import java.io.Writer;\n+import java.net.MalformedURLException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.nio.file.InvalidPathException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.UUID;\n+import java.util.zip.GZIPInputStream;\n+\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.commons.cli.DefaultParser;\n+import org.apache.commons.cli.HelpFormatter;\n+import org.apache.commons.cli.Option;\n+import org.apache.commons.cli.Options;\n+import org.apache.commons.cli.ParseException;\n+import org.apache.commons.lang3.tuple.ImmutablePair;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.apache.jena.query.Query;\n+import org.apache.jena.query.QueryException;\n+import org.apache.jena.query.QueryFactory;\n+import org.apache.jena.rdf.model.Model;\n+import org.apache.jena.rdf.model.ModelFactory;\n+import org.apache.jena.update.UpdateFactory;\n+import org.apache.jena.update.UpdateRequest;\n+import org.apache.log4j.Level;\n+import org.apache.log4j.Logger;\n+\n+class CoNLLRDFCommandLine {\n+\tprivate static final Logger LOG = Logger.getLogger(CoNLLRDFCommandLine.class);\n+\tprivate final Logger logger;\n+\tprivate final String syntax;\n+\tprivate final String header;\n+\tprivate final String footer;\n+\tprivate final Options options;\n+\tprivate final HelpFormatter helpFormatter = new HelpFormatter();\n+\n+\t/**\n+\t * Command Line Handler, parses args[] and prints usage help\n+\t * \n+\t * @param syntax      e.g. CoNLLStreamExtractor baseURI [ FIELD ... ]\n+\t * @param description e.g. reads CoNLL from stdin, splits sentences, creates\n+\t *                    CoNLL RDF, applies SPARQL queries\n+\t * @param options     the command line options\n+\t * @param logger      the calling class' Log4j Logger\n+\t */\n+\tCoNLLRDFCommandLine(String syntax, String description, Iterator<Option> options, Logger logger) {\n+\t\tthis.logger = logger;\n+\t\tthis.syntax = syntax;\n+\t\tthis.header = description;\n+\t\tthis.footer = null;\n+\t\tthis.options = new Options()\n+\t\t\t.addOption(\"help\", false, \"print this help message and exit\")\n+\t\t\t.addOption(\"silent\", false, \"supress help message and logging of info messages\");\n+\n+\t\twhile (options.hasNext()) {\n+\t\t\tthis.options.addOption(options.next());\n+\t\t}\n+\n+\t\thelpFormatter.setOptionComparator(null); // don't sort cli-options in help message\n+\t\thelpFormatter.setSyntaxPrefix(\"synopsis: \");\n+\t}\n+\tCoNLLRDFCommandLine(String syntax, String description, Options options, Logger logger) {\n+\t\tthis(syntax, description, options.getOptions().iterator(), logger);\n+\t}\n+\tCoNLLRDFCommandLine(String syntax, String description, Option[] options, Logger logger) {\n+\t\tthis(syntax, description, Arrays.asList(options).iterator(), logger);\n+\t}\n+\n+\t/**\n+\t * \n+\t * @param args\n+\t * @return the parsed Arguments as CommandLine\n+\t * @throws ParseException\n+\t */\n+\tpublic CommandLine parseArgs(String[] args) throws ParseException {\n+\t\tfinal CommandLine cmd;\n+\t\ttry {\n+\t\t\tcmd = new DefaultParser().parse(this.options, args);\n+\t\t} catch (ParseException e) {\n+\t\t\tlogUsageHelp();\n+\t\t\tthrow e;\n+\t\t}\n+\n+\t\tLOG.debug(\"String[] args = \" + Arrays.toString(args));\n+\t\tLOG.debug(\"Option[] opts = \" + Arrays.deepToString(cmd.getOptions()));\n+\t\tLOG.debug(\"Unparsed args = \" + cmd.getArgList().toString());\n+\n+\t\t// print help and exit\n+\t\tif (cmd.hasOption(\"help\")) {\n+\t\t\tprintUsageHelp();\n+\t\t\tSystem.exit(0);\n+\t\t}\n+\t\tif (cmd.hasOption(\"silent\")) {\n+\t\t\tlogger.setLevel(Level.WARN);\n+\t\t}\n+\n+\t\t// READ LOGLEVEL\n+\t\tif (cmd.hasOption(\"loglevel\")) {\n+\t\t\tlogger.setLevel(Level.toLevel(cmd.getOptionValue(\"loglevel\")));\n+\t\t}\n+\t\treturn cmd;\n+\t}\n+\n+\t/**\n+\t * Print usage help to stderr\n+\t */\n+\tprivate void printUsageHelp() {\n+\t\thelpFormatter.printHelp(syntax, header, options, footer);\n+\t}\n+\t/**\n+\t * Log usage help with LOG.info\n+\t */\n+\tprivate void logUsageHelp() {\n+\t\tfinal StringWriter info = new StringWriter();\n+\t\tfinal PrintWriter pw = new PrintWriter(info);\n+\n+\t\thelpFormatter.printHelp(pw, 80, syntax, header, options, helpFormatter.getLeftPadding(),\n+\t\t\t\thelpFormatter.getDescPadding(), footer);\n+\t\tpw.flush();\n+\n+\t\t// add new line after logger's own prefix\n+\t\tlogger.info(\"\\n\" + info.toString());\n+\t}\n+\n+\t/**\n+\t * Read an entire file to String, encoded as UTF-8. Intended to load small files\n+\t * like sparql queries into memory. Implementation of\n+\t * {@Code Files.readString(Path path)} available in Java 11\n+\t * \n+\t * @param path the path to the file\n+\t * @return a String containing the content read from the file\n+\t * @throws IOException              if the read fails\n+\t * @throws IllegalArgumentException if the file is reported as larger than 2GB\n+\t */\n+\tpublic static String readString(Path path) throws IOException {\n+\t\t// Files.readString(path, StandardCharsets.UTF_8); // requires Java 11\n+\t\tif (path.toFile().length() > 2000000000) {\n+\t\t\tthrow new IllegalArgumentException(\n+\t\t\t\t\t\"The file \" + path + \" is too large. \" + path.toFile().length() + \" bytes\");\n+\t\t}\n+\t\tbyte[] buffer = new byte[(int) path.toFile().length()];\n+\t\tbuffer = Files.readAllBytes(path);\n+\t\treturn new String(buffer, StandardCharsets.UTF_8);\n+\t}\n+\n+\tpublic static String readUrl(URL url) throws IOException {\n+\t\tBufferedReader reader;\n+\t\tif (url.toString().endsWith(\".gz\")) {\n+\t\t\treader = new BufferedReader(new InputStreamReader(new GZIPInputStream(url.openStream()), StandardCharsets.UTF_8));\n+\t\t} else {\n+\t\t\treader = new BufferedReader(new InputStreamReader(url.openStream(), StandardCharsets.UTF_8));\n+\t\t}\n+\t\tStringBuilder out = new StringBuilder();\n+\t\tfor (String line = reader.readLine(); line != null; line = reader.readLine())\n+\t\t\tout.append(line + \"\\n\");\n+\t\treturn out.toString();\n+\t}\n+\n+\tpublic static Query selectQueryFromLiteral(String arg) {\n+\t\tfinal Query query = QueryFactory.create(arg);\n+\t\tif (query.isSelectType()) {\n+\t\t\treturn query;\n+\t\t} else {\n+\t\t\tthrow new IllegalArgumentException(\"The provided query is not a select query: \" + arg);\n+\t\t}\n+\t}\n+\tpublic static UpdateRequest updateRequestFromLiteral(String arg) {\n+\t\treturn UpdateFactory.create(arg);\n+\t}\n+\n+\tpublic static String readSparqlFile(String arg) throws IOException {\n+\t\tPath path = Paths.get(arg);\n+\t\tFile file = path.toFile();\n+\n+\t\tif (file.isFile() && file.exists()) { // can be read from a file\n+\t\t\tLOG.debug(\"Found File \" + arg + \" with length \" + file.length());\n+\t\t}\n+\t\tif ( ! arg.endsWith(\".sparql\")) {\n+\t\t\tthrow new IllegalArgumentException(\"File \" + arg + \" has an unexpected extension. expected '.sparql'\");\n+\t\t}\n+\t\treturn readString(path);\n+\t}\n+\n+\tpublic static Query readSparqlSelect(String arg) throws IOException {\n+\t\t// arg = arg.strip();\n+\t\ttry {\n+\t\t\tif (new File(arg).exists()) {\n+\t\t\t\treturn selectQueryFromLiteral(readString(Paths.get(arg)));\n+\t\t\t}\n+\t\t} catch (InvalidPathException e) {\n+\t\t\tLOG.debug(e);\n+\t\t}\n+\t\ttry {\n+\t\t\treturn selectQueryFromLiteral(readUri(new URI(arg)));\n+\t\t} catch (URISyntaxException e) {\n+\t\t\tLOG.debug(e);\n+\t\t}\n+\t\t// read as literal\n+\t\ttry {\n+\t\t\treturn selectQueryFromLiteral(arg);\n+\t\t} catch (QueryException e) {\n+\t\t\tLOG.debug(e);\n+\t\t}\n+\t\tthrow new IllegalArgumentException(\"arg could not be read as sparql\");\n+\t}\n+\tpublic static Model readModel(URI uri) throws IOException {\n+\t\tModel m = ModelFactory.createDefaultModel();\n+\t\treturn m.read(readUri(uri));\n+\t}\n+\n+\t/**\n+\t * Tries to read from a specific URI.\n+\t * Tries to read content directly or from GZIP\n+\t * Validates content against UTF-8.\n+\t * @param uri\n+\t * \t\tthe URI to be read\n+\t * @return\n+\t * \t\tthe text content\n+\t * @throws MalformedURLException\n+\t * @throws IOException\n+\t */\n+\tpublic static String readUri(URI uri) throws MalformedURLException, IOException {\n+\t\tif (uri.isAbsolute()){\n+\t\t\treturn readUrl(uri.toURL());\n+\t\t} else {\n+\t\t\treturn readString(Paths.get(uri));\n+\t\t}\n+\t}\n+\t\n+\tpublic static Writer newFileWriter(File outputDir, String updateName, String sentence_id, int update_id, int iteration, int step, String fileExtension) throws FileNotFoundException {\n+\t\tif (updateName != null && !updateName.isEmpty()) {\n+\t\t\tupdateName = updateName.replace(\".sparql\", \"\");\n+\t\t} else {\n+\t\t\tupdateName = UUID.randomUUID().toString();\n+\t\t}\n+\t\tFile file = new File(outputDir,\n+\t\t\tString.format(\n+\t\t\t\t\"%s__U%03d_I%04d_S%03d__%s%s\",\n+\t\t\t\tsentence_id, update_id, iteration, step, updateName, fileExtension)\n+\t\t\t);\n+\t\treturn new OutputStreamWriter(new FileOutputStream(file), StandardCharsets.UTF_8);\n+\t}\n+\n+\tpublic static Pair<String, String> parseUpdate(String arg) throws IOException, ParseException {\n+\t\tfinal String updateRaw = arg.replaceFirst(\"\\\\{[0-9u*]+\\\\}$\", \"\");\n+\t\tString freq = arg.replaceFirst(\".*\\\\{([0-9u*]+)\\\\}$\", \"$1\");\n+\t\tif (arg.equals(freq)) {\n+\t\t\tfreq = \"1\";\n+\t\t} else if (freq.equals(\"u\")) {\n+\t\t\tfreq = \"*\";\n+\t}\n+\t\treturn new ImmutablePair<>(updateRaw, freq);\n+\t}\n+\n+\tpublic static File parseDir(String arg) throws IOException {\n+\t\t// arg = arg.toLowerCase(); // FIXME why?\n+\t\tFile dir = new File(arg);\n+\t\tif (dir.exists() || dir.mkdirs()) {\n+\t\t\tif (! dir.isDirectory()) {\n+\t\t\t\tdir = null;\n+\t\t\t\tthrow new IOException(\"Error: Given directory is not valid: \" + arg);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tdir = null;\n+\t\t\tthrow new IOException(\"Error: Failed to create given directory: \" + arg);\n+\t\t}\n+\t\treturn dir;\n+\t}\n+}"
  },
  {
    "sha": "8ab57a9c8fa51ffc84cfee5b1a7390dc82ff6388",
    "filename": "src/main/java/org/acoli/conll/rdf/CoNLLRDFComponent.java",
    "status": "modified",
    "additions": 31,
    "deletions": 15,
    "changes": 46,
    "blob_url": "https://github.com/acoli-repo/conll-rdf/blob/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/CoNLLRDFComponent.java",
    "raw_url": "https://github.com/acoli-repo/conll-rdf/raw/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/CoNLLRDFComponent.java",
    "contents_url": "https://api.github.com/repos/acoli-repo/conll-rdf/contents/src/main/java/org/acoli/conll/rdf/CoNLLRDFComponent.java?ref=4f49b0dd21f0cfe1898b860851c18fcd891fdf13",
    "patch": "@@ -1,34 +1,50 @@\n package org.acoli.conll.rdf;\r\n \r\n import java.io.BufferedReader;\r\n+import java.io.IOException;\r\n+import java.io.InputStreamReader;\r\n import java.io.PrintStream;\r\n+import java.util.Arrays;\r\n+import java.util.List;\r\n \r\n public abstract class CoNLLRDFComponent implements Runnable {\r\n+\tstatic final List<Integer> CHECKINTERVAL = Arrays.asList(3, 10, 25, 50, 100, 200, 500);\r\n+\tstatic final String DEFAULTUPDATENAME = \"DIRECTUPDATE\";\r\n+\t// maximal update iterations allowed until the update loop is canceled and an error msg is thrown\r\n+\t// (to prevent faulty update scripts running in an endless loop)\r\n+\tstatic final int MAXITERATE = 999;\r\n \r\n-\tprivate BufferedReader inputStream;\r\n-\tprivate PrintStream outputStream;\r\n-\t\r\n-\t\r\n+\tprivate BufferedReader inputStream = new BufferedReader(new InputStreamReader(System.in));\r\n+\tprivate PrintStream outputStream = System.out;\r\n \r\n+\t//TODO remove all references to configureFromCommandLine(String[] args)\r\n \r\n-\tpublic BufferedReader getInputStream() {\r\n+\tprotected abstract void processSentenceStream() throws IOException;\r\n+\r\n+\tpublic final BufferedReader getInputStream() {\r\n \t\treturn inputStream;\r\n \t}\r\n-\r\n-\tpublic void setInputStream(BufferedReader inputStream) {\r\n+\tpublic final void setInputStream(BufferedReader inputStream) {\r\n \t\tthis.inputStream = inputStream;\r\n \t}\r\n-\r\n-\tpublic PrintStream getOutputStream() {\r\n+\tpublic final PrintStream getOutputStream() {\r\n \t\treturn outputStream;\r\n \t}\r\n-\r\n-\tpublic void setOutputStream(PrintStream outputStream) {\r\n+\tpublic final void setOutputStream(PrintStream outputStream) {\r\n \t\tthis.outputStream = outputStream;\r\n-\t\r\n \t}\r\n \r\n-\tpublic abstract void start();\r\n-\t\r\n-\t\r\n+\t@Override\r\n+\tpublic final void run() {\r\n+\t\ttry {\r\n+\t\t\tprocessSentenceStream();\r\n+\t\t} catch (IOException e) {\r\n+\t\t\te.printStackTrace();\r\n+\t\t\tSystem.exit(1);\r\n+\t\t}\r\n+\t}\r\n+\t// TODO is this method used anywhere?\r\n+\tpublic final void start() {\r\n+\t\trun();\r\n+\t}\r\n }\r"
  },
  {
    "sha": "37ab1add73a93f361ed71bc177550ab85eccfeab",
    "filename": "src/main/java/org/acoli/conll/rdf/CoNLLRDFFormatter.java",
    "status": "modified",
    "additions": 761,
    "deletions": 730,
    "changes": 1491,
    "blob_url": "https://github.com/acoli-repo/conll-rdf/blob/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/CoNLLRDFFormatter.java",
    "raw_url": "https://github.com/acoli-repo/conll-rdf/raw/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/CoNLLRDFFormatter.java",
    "contents_url": "https://api.github.com/repos/acoli-repo/conll-rdf/contents/src/main/java/org/acoli/conll/rdf/CoNLLRDFFormatter.java?ref=4f49b0dd21f0cfe1898b860851c18fcd891fdf13",
    "patch": "@@ -1,12 +1,12 @@\n /*\n  * Copyright [2017] [ACoLi Lab, Prof. Dr. Chiarcos, Goethe University Frankfurt]\n- * \n+ *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n  * You may obtain a copy of the License at\n- * \n+ *\n  *     http://www.apache.org/licenses/LICENSE-2.0\n- * \n+ *\n  * Unless required by applicable law or agreed to in writing, software\n  * distributed under the License is distributed on an \"AS IS\" BASIS,\n  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n@@ -20,851 +20,882 @@\n import java.util.*;\n import org.apache.jena.rdf.model.*;\t\t// Jena 2.x\n import org.apache.log4j.Logger;\n+import org.apache.commons.cli.ParseException;\n import org.apache.jena.query.*;\n \n-\n-/** reads CoNLL-RDF from stdin, writes it formatted to stdout (requires a Un*x shell)<br>\n- *  this is basically for diagnostic purposes \n- *  @author Christian Chiarcos {@literal chiarcos@informatik.uni-frankfurt.de}\n- *  @author Christian Faeth {@literal faeth@em.uni-frankfurt.de}\n+/**\n+ * reads CoNLL-RDF from stdin, writes it formatted to stdout (requires a Un*x\n+ * shell)<br>\n+ * this is basically for diagnostic purposes\n+ * \n+ * @author Christian Chiarcos {@literal chiarcos@informatik.uni-frankfurt.de}\n+ * @author Christian Faeth {@literal faeth@em.uni-frankfurt.de}\n  */\n public class CoNLLRDFFormatter extends CoNLLRDFComponent {\n-\t\n-\tprotected static Logger LOG = Logger.getLogger(CoNLLRDFFormatter.class.getName());\n-\t\t\n-\tpublic static final String ANSI_RESET    = \"\\u001B[0m\";\n+\n+\tprotected static Logger LOG = Logger.getLogger(CoNLLRDFFormatter.class);\n+\n+\tpublic static final String ANSI_RESET = \"\\u001B[0m\";\n \tpublic static final String ANSI_BRIGHTER = \"\\u001B[1m\";\n-\tpublic static final String ANSI_ULINE    = \"\\u001B[4m\";\n-\tpublic static final String ANSI_FLASH\t = \"\\u001B[5m\";\n-\tpublic static final String ANSI_BLACK    = \"\\u001B[30m\";\n-\tpublic static final String ANSI_RED      = \"\\u001B[31m\";\n-\tpublic static final String ANSI_GREEN    = \"\\u001B[32m\";\n-\tpublic static final String ANSI_YELLOW   = \"\\u001B[33m\";\n-\tpublic static final String ANSI_BLUE     = \"\\u001B[34m\";\n-\tpublic static final String ANSI_PURPLE   = \"\\u001B[35m\";\n-\tpublic static final String ANSI_CYAN     = \"\\u001B[36m\";\n-\tpublic static final String ANSI_WHITE    = \"\\u001B[37m\";\n+\tpublic static final String ANSI_ULINE = \"\\u001B[4m\";\n+\tpublic static final String ANSI_FLASH = \"\\u001B[5m\";\n+\tpublic static final String ANSI_BLACK = \"\\u001B[30m\";\n+\tpublic static final String ANSI_RED = \"\\u001B[31m\";\n+\tpublic static final String ANSI_GREEN = \"\\u001B[32m\";\n+\tpublic static final String ANSI_YELLOW = \"\\u001B[33m\";\n+\tpublic static final String ANSI_BLUE = \"\\u001B[34m\";\n+\tpublic static final String ANSI_PURPLE = \"\\u001B[35m\";\n+\tpublic static final String ANSI_CYAN = \"\\u001B[36m\";\n+\tpublic static final String ANSI_WHITE = \"\\u001B[37m\";\n \tpublic static final String ANSI_BLACK_BK = \"\\u001B[40m\";\n-\tpublic static final String ANSI_RED_BK   = \"\\u001B[41m\";\n+\tpublic static final String ANSI_RED_BK = \"\\u001B[41m\";\n \tpublic static final String ANSI_GREEN_BK = \"\\u001B[42m\";\n-\tpublic static final String ANSI_YLW_BK   = \"\\u001B[43m\";\n-\tpublic static final String ANSI_BLUE_BK  = \"\\u001B[44m\";\n-\tpublic static final String ANSI_PPL_BK   = \"\\u001B[45m\";\n-\tpublic static final String ANSI_CYAN_BK  = \"\\u001B[46m\";\n+\tpublic static final String ANSI_YLW_BK = \"\\u001B[43m\";\n+\tpublic static final String ANSI_BLUE_BK = \"\\u001B[44m\";\n+\tpublic static final String ANSI_PPL_BK = \"\\u001B[45m\";\n+\tpublic static final String ANSI_CYAN_BK = \"\\u001B[46m\";\n \tpublic static final String ANSI_WHITE_BK = \"\\u001B[47m\";\n-\t\n+\n \tpublic static class Module {\n \t\tprivate Mode mode = Mode.CONLLRDF;\n \t\tprivate List<String> cols = new ArrayList<String>();\n \t\tString select = \"\";\n \t\tprivate PrintStream outputStream;\n-\t\t\n+\n \t\tpublic Mode getMode() {\n \t\t\treturn mode;\n \t\t}\n+\n \t\tpublic void setMode(Mode mode) {\n \t\t\tthis.mode = mode;\n \t\t}\n+\n \t\tpublic List<String> getCols() {\n \t\t\treturn cols;\n \t\t}\n+\n \t\tpublic void setCols(List<String> cols) {\n \t\t\tthis.cols = cols;\n \t\t}\n+\n \t\tpublic String getSelect() {\n \t\t\treturn select;\n \t\t}\n+\n \t\tpublic void setSelect(String select) {\n \t\t\tthis.select = select;\n \t\t}\n+\n \t\tpublic PrintStream getOutputStream() {\n \t\t\treturn outputStream;\n \t\t}\n+\n \t\tpublic void setOutputStream(PrintStream outputStream) {\n \t\t\tthis.outputStream = outputStream;\n \t\t}\n \t}\n-\t\n+\n \tpublic static enum Mode {\n \t\tCONLLRDF, CONLL, DEBUG, SPARQLTSV, GRAMMAR, SEMANTICS, GRAMMAR_SEMANTICS\n \t}\n \n \tprivate List<Module> modules = new ArrayList<Module>();\n-\t\n+\n \tpublic List<Module> getModules() {\n \t\treturn modules;\n \t}\n \n-\t\t/** do some highlighting, but provide the full TTL data*/\n-\t\tpublic String colorTTL(String buffer) {\n-\t\t\treturn buffer.replaceAll(\"(terms:[^ ]*)\",ANSI_YLW_BK+\"$1\"+ANSI_RESET)\n-\t\t\t\t\t\t.replaceAll(\"(rdfs:label +)(\\\"[^\\\"]*\\\")\",\"$1\"+ANSI_CYAN+\"$2\"+ANSI_RESET)\n-\t\t\t\t\t\t.replaceAll(\"(nif:[^ ]*)\",ANSI_YELLOW+\"$1\"+ANSI_RESET)\n-\t\t\t\t\t\t.replaceAll(\"(conll:[^ \\n]*)([^;\\n]*[;]?)\",ANSI_CYAN_BK+ANSI_BRIGHTER+ANSI_BLUE+\"$1\"+ANSI_RESET+ANSI_CYAN_BK+ANSI_BRIGHTER+\"$2\"+ANSI_RESET);\n-\t\t}\n-\t\t\n-\t\t/** default: do not return type assignments */\n-\t\tprotected static String extractCoNLLGraph(String buffer) {\n-\t\t\treturn extractCoNLLGraph(buffer,false);\n+\t/** do some highlighting, but provide the full TTL data */\n+\tpublic String colorTTL(String buffer) {\n+\t\treturn buffer.replaceAll(\"(terms:[^ ]*)\", ANSI_YLW_BK + \"$1\" + ANSI_RESET)\n+\t\t\t\t.replaceAll(\"(rdfs:label +)(\\\"[^\\\"]*\\\")\", \"$1\" + ANSI_CYAN + \"$2\" + ANSI_RESET)\n+\t\t\t\t.replaceAll(\"(nif:[^ ]*)\", ANSI_YELLOW + \"$1\" + ANSI_RESET)\n+\t\t\t\t.replaceAll(\"(conll:[^ \\n]*)([^;\\n]*[;]?)\", ANSI_CYAN_BK + ANSI_BRIGHTER + ANSI_BLUE + \"$1\" + ANSI_RESET\n+\t\t\t\t\t\t+ ANSI_CYAN_BK + ANSI_BRIGHTER + \"$2\" + ANSI_RESET);\n+\t}\n+\n+\t/** default: do not return type assignments */\n+\tprotected static String extractCoNLLGraph(String buffer) {\n+\t\treturn extractCoNLLGraph(buffer, false);\n+\t}\n+\n+\t/**\n+\t * buffer must be valid turtle, produces an extra column for terms: type\n+\t * assignments\n+\t */\n+\tprotected static String extractCoNLLGraph(String buffer, boolean includeTermConcepts) {\n+\t\tModel m = null;\n+\t\ttry {\n+\t\t\tm = ModelFactory.createDefaultModel().read(new StringReader(buffer), null, \"TTL\");\n+\t\t} catch (org.apache.jena.riot.RiotException e) {\n+\t\t\te.printStackTrace();\n+\t\t\tLOG.error(\"while reading:\\n\" + buffer);\n \t\t}\n+\t\tVector<String> ids = new Vector<String>();\n+\t\tVector<String> words = new Vector<String>();\n+\t\tVector<String> annos = new Vector<String>();\n+\t\tVector<Integer> depth = new Vector<Integer>();\n+\t\tVector<String> edges = new Vector<String>();\n+\t\tVector<String> headDir = new Vector<String>();\n+\t\tVector<String> terms = new Vector<String>();\n+\t\tInteger maxDepth = 0;\n+\t\tInteger maxEdgeLength = 0;\n+\t\tInteger maxIdLength = 0;\n+\t\tInteger maxWordLength = 0;\n+\t\tInteger maxTermLength = 0;\n \n-\t\t/** buffer must be valid turtle, produces an extra column for terms: type assignments */\n-\t\tprotected static String extractCoNLLGraph(String buffer, boolean includeTermConcepts) {\n-\t\t\tModel m = null;\n-\t\t\ttry {\n-\t\t\t\tm = ModelFactory.createDefaultModel().read(new StringReader(buffer),null, \"TTL\");\n-\t\t\t} catch (org.apache.jena.riot.RiotException e) {\n-\t\t\t\te.printStackTrace();\n-\t\t\t\tLOG.error(\"while reading:\\n\"+buffer);\n-\t\t\t}\n-\t\t\tVector<String> ids = new Vector<String>();\n-\t\t\tVector<String> words = new Vector<String>();\n-\t\t\tVector<String> annos = new Vector<String>();\n-\t\t\tVector<Integer> depth = new Vector<Integer>();\n-\t\t\tVector<String> edges = new Vector<String>();\n-\t\t\tVector<String> headDir = new Vector<String>();\n-\t\t\tVector<String> terms = new Vector<String>();\n- \t\t\tInteger maxDepth = 0;\n- \t\t\tInteger maxEdgeLength = 0;\n- \t\t\tInteger maxIdLength = 0;\n- \t\t\tInteger maxWordLength = 0;\n- \t\t\tInteger maxTermLength = 0;\n-\n-\t\t\tString word = null;\n-\t\t\ttry {\n-\t\t\t\tword = QueryExecutionFactory.create(\n-\t\t\t\t\t\"PREFIX nif: <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#>\\n\"+\n-\t\t\t\t\t\"SELECT ?first WHERE { ?first a nif:Word. FILTER(NOT EXISTS{ [] nif:nextWord ?first })} LIMIT 1\",\n-\t\t\t\t\tm).execSelect().next().get(\"?first\").toString();\n-\t\t\t\twhile(true) {\n-\t\t\t\t\tids.add(word.replaceAll(\".*[\\\\\\\\/#:]\", \"\"));\n-\t\t\t\t\tmaxIdLength=Math.max(maxIdLength, ids.get(ids.size()-1).length());\n-\t\t\t\t\ttry {\n-\t\t\t\t\t\twords.add(\n-\t\t\t\t\t\t\t\tQueryExecutionFactory.create(\n-\t\t\t\t\t\t\t\t\t\t\"PREFIX conll: <http://ufal.mff.cuni.cz/conll2009-st/task-description.html#>\\n\"+\n-\t\t\t\t\t\t\t\t\t\t\"SELECT ?word WHERE { <\"+word+\"> conll:WORD ?word } LIMIT 1\",\n-\t\t\t\t\t\t\t\t\t\tm).execSelect().next().get(\"?word\").toString());\n-\t\t\t\t\t} catch (NoSuchElementException e) {\n-\t\t\t\t\t\tLOG.warn(\"Warning: no conll:WORD (WORD column) found\");\n-\t\t\t\t\t\twords.add(\"\");\n-\t\t\t\t\t}\n-\t\t\t\t\tmaxWordLength=Math.max(maxWordLength, words.get(words.size()-1).length());\n-\t\t\t\t\tString anno = \"\";\n-\t\t\t\t\tResultSet annos_raw = QueryExecutionFactory.create(\n-\t\t\t\t\t\t\t\"PREFIX conll: <http://ufal.mff.cuni.cz/conll2009-st/task-description.html#>\\n\"+\n-\t\t\t\t\t\t\t\"SELECT ?rel ?val WHERE { <\"+word+\"> ?rel ?val \\n\"\n-\t\t\t\t\t\t\t\t\t+ \"FILTER(contains(str(?rel),'http://ufal.mff.cuni.cz/conll2009-st/task-description.html#'))\\n\"\n-\t\t\t\t\t\t\t\t\t+ \"FILTER(?rel!=conll:HEAD && ?rel!=conll:EDGE && ?rel!=conll:WORD) } ORDER BY ASC(?rel)\",\n-\t\t\t\t\t\t\tm).execSelect();\n-\t\t\t\t\tString rel = \"\";\n-\t\t\t\t\twhile(annos_raw.hasNext()) {\n-\t\t\t\t\t\tQuerySolution next = annos_raw.next();\n-\t\t\t\t\t\tString nextRel = next.get(\"?rel\").toString().replaceFirst(\".*#\",\"\");\n-\t\t\t\t\t\tif(!rel.equals(nextRel)) \n-\t\t\t\t\t\t\tanno=anno+\n-\t\t\t\t\t\t\t\tANSI_BLUE+ANSI_ULINE+\n-\t\t\t\t\t\t\t\tnextRel+\n-\t\t\t\t\t\t\t\tANSI_RESET+\" \";\n-\t\t\t\t\t\trel=nextRel;\n-\t\t\t\t\t\tanno=anno+\n-\t\t\t\t\t\t\t\tnext.get(\"?val\").toString().\n-\t\t\t\t\t\t\t\t replaceFirst(\"^http://purl.org/acoli/open-ie/(.*)$\",ANSI_YLW_BK+\"$1\"+ANSI_RESET).\n-\t\t\t\t\t\t\t\t replaceFirst(\".*#\",\"\")+\n-\t\t\t\t\t\t\t\t\" \";\n-\t\t\t\t\t}\n-\t\t\t\t\t\n-\t\t\t\t\t// we append OLiA annotations to CoNLL annotations\n-\t\t\t\t\tResultSet olia_types= QueryExecutionFactory.create(\n-\t\t\t\t\t\t\t\"PREFIX conll: <http://ufal.mff.cuni.cz/conll2009-st/task-description.html#>\\n\"+\n-\t\t\t\t\t\t\t\"SELECT ?concept WHERE { <\"+word+\"> a ?concept \\n\"\n-\t\t\t\t\t\t\t\t\t+ \"FILTER(contains(str(?concept),'http://purl.org/olia'))\\n\"\n-\t\t\t\t\t\t\t\t\t+ \"} ORDER BY ASC(?val)\",\n-\t\t\t\t\t\t\tm).execSelect();\n-\t\t\t\t\twhile(olia_types.hasNext())\n-\t\t\t\t\t\t\tanno=anno+\n-\t\t\t\t\t\t\t\tANSI_RED+\n-\t\t\t\t\t\t\t\tolia_types.next().get(\"?concept\").toString().replaceFirst(\"^.*/([^/]*)\\\\.(owl|rdf)[#/]\",\"$1:\")+\n-\t\t\t\t\t\t\t\tANSI_RESET+\" \";\n-\n-\t\t\t\t\t// append OLiA features\n-\t\t\t\t\tResultSet olia_feats= QueryExecutionFactory.create(\n-\t\t\t\t\t\t\t\"PREFIX conll: <http://ufal.mff.cuni.cz/conll2009-st/task-description.html#>\\n\"+\n-\t\t\t\t\t\t\t\"SELECT ?rel ?concept WHERE { <\"+word+\"> ?rel ?val. ?val a ?concept.\\n\"\n-\t\t\t\t\t\t\t\t\t+ \"FILTER(contains(str(?rel),'http://purl.org/olia'))\\n\"\n-\t\t\t\t\t\t\t\t\t+ \"FILTER(contains(str(?concept),'http://purl.org/olia'))\\n\"\n-\t\t\t\t\t\t\t\t\t+ \"} ORDER BY ASC(?rel)\",\n-\t\t\t\t\t\t\tm).execSelect();\n-\t\t\t\t\twhile(olia_feats.hasNext()) {\n-\t\t\t\t\t\tQuerySolution next = olia_feats.next();\n-\t\t\t\t\t\tanno = anno+\n-\t\t\t\t\t\t\t\tANSI_RED+ANSI_ULINE+\n-\t\t\t\t\t\t\t\tnext.get(\"?rel\").toString().replaceFirst(\"^.*/([^/]*)\\\\.(owl|rdf)[#/]\",\"$1:\")+\n-\t\t\t\t\t\t\t\tANSI_RESET+\".\"+ANSI_RED+\n-\t\t\t\t\t\t\t\tnext.get(\"?concept\").toString().replaceFirst(\"^.*/([^/]*)\\\\.(owl|rdf)[#/]\",\"$1:\")+\n-\t\t\t\t\t\t\t\tANSI_RESET+\" \";\n-\t\t\t\t\t}\t\t\t\t\t\n-\t\t\t\t\t\n-\t\t\t\t\tannos.add(anno);\n-\t\t\t\t\t\n-\t\t\t\t\tString head = \"\";\n-\t\t\t\t\ttry {\n-\t\t\t\t\t\thead = \n-\t\t\t\t\t\t\t\tQueryExecutionFactory.create(\n-\t\t\t\t\t\t\t\t\t\t\"PREFIX conll: <http://ufal.mff.cuni.cz/conll2009-st/task-description.html#>\\n\"+\n-\t\t\t\t\t\t\t\t\t\t\"SELECT ?head WHERE { <\"+word+\"> conll:HEAD ?head} LIMIT 1\",\n-\t\t\t\t\t\t\t\t\t\tm).execSelect().next().get(\"?head\").toString();\n-\t\t\t\t\t\tif(Integer.parseInt(head.replaceAll(\"[^0-9]\",\"\")) < Integer.parseInt(word.replaceAll(\"[^0-9]\",\"\")))\n-\t\t\t\t\t\t\theadDir.add(\" \\\\ \"); \n-\t\t\t\t\t\telse \n-\t\t\t\t\t\t\theadDir.add(\" / \");\n-\t\t\t\t\t} catch (NumberFormatException e) {\n-\t\t\t\t\t\te.printStackTrace();\n-\t\t\t\t\t\tif(head.compareTo(word)<1) headDir.add(\" \\\\ \"); else headDir.add(\" / \");\n-\t\t\t\t\t} catch (NoSuchElementException e) {\n-\t\t\t\t\t\theadDir.add(\"   \");\n-\t\t\t\t\t}\n-\t\t\t\t\t\n-\t\t\t\t\ttry {\n-\t\t\t\t\t\tdepth.add(\n-\t\t\t\t\t\t\t\tInteger.parseInt(QueryExecutionFactory.create(\n-\t\t\t\t\t\t\t\t\"PREFIX conll: <http://ufal.mff.cuni.cz/conll2009-st/task-description.html#>\\n\"+\n-\t\t\t\t\t\t\t\t\"SELECT (COUNT(DISTINCT ?head) AS ?depth) WHERE { <\"+word+\"> conll:HEAD+ ?head }\",\n-\t\t\t\t\t\t\t\tm).execSelect().next().get(\"?depth\").toString().replaceFirst(\"^\\\"?([0-9]+)[\\\\^\\\"].*\",\"$1\")));\n-\t\t\t\t\t} catch(NoSuchElementException e) {\n-\t\t\t\t\t\tif(depth.size()==0) depth.add(1);\n-\t\t\t\t\t\telse depth.add(depth.get(depth.size()-1));\n-\t\t\t\t\t}\n-\t\t\t\t\tmaxDepth=Math.max(maxDepth, depth.get(depth.size()-1));\n-\n-\n-\t\t\t\t\ttry { // return the longest edge\n-\t\t\t\t\t\tedges.add(\n-\t\t\t\t\t\t\t\tQueryExecutionFactory.create(\n-\t\t\t\t\t\t\t\t\t\t\"PREFIX conll: <http://ufal.mff.cuni.cz/conll2009-st/task-description.html#>\\n\"+\n-\t\t\t\t\t\t\t\t\t\t\"PREFIX fn: <http://www.w3.org/2005/xpath-functions#>\\n\"+\n-\t\t\t\t\t\t\t\t\t\t\"SELECT ?edge ?length WHERE { <\"+word+\"> conll:EDGE ?edge. BIND(fn:string-length(?edge) AS ?length) } ORDER BY DESC(?length) LIMIT 1\",\n-\t\t\t\t\t\t\t\t\t\tm).execSelect().next().get(\"?edge\").toString());\n-\t\t\t\t\t} catch(NoSuchElementException e) {\n-\t\t\t\t\t\tedges.add(\"\");\n-\t\t\t\t\t}\n-\t\t\t\t\tmaxEdgeLength=Math.max(maxEdgeLength,edges.get(edges.size()-1).length());\n-\t\t\t\t\t\n-\t\t\t\t\tString term = \"\";\n-\t\t\t\t\tif(includeTermConcepts) {\n-\t\t\t\t\t\tResultSet terms_raw = QueryExecutionFactory.create(\n-\t\t\t\t\t\t\t\t\"PREFIX conll: <http://ufal.mff.cuni.cz/conll2009-st/task-description.html#>\\n\"+\n-\t\t\t\t\t\t\t\t\"SELECT ?term WHERE { <\"+word+\"> a ?term \\n\"\n+\t\tString word = null;\n+\t\ttry {\n+\t\t\tword = QueryExecutionFactory.create(\n+\t\t\t\t\t\"PREFIX nif: <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#>\\n\"\n+\t\t\t\t\t\t\t+ \"SELECT ?first WHERE { ?first a nif:Word. FILTER(NOT EXISTS{ [] nif:nextWord ?first })} LIMIT 1\",\n+\t\t\t\tm).execSelect().next().get(\"?first\").toString();\n+\t\t\twhile (true) {\n+\t\t\t\tids.add(word.replaceAll(\".*[\\\\\\\\/#:]\", \"\"));\n+\t\t\t\tmaxIdLength = Math.max(maxIdLength, ids.get(ids.size() - 1).length());\n+\t\t\t\ttry {\n+\t\t\t\t\twords.add(QueryExecutionFactory\n+\t\t\t\t\t\t\t.create(\"PREFIX conll: <http://ufal.mff.cuni.cz/conll2009-st/task-description.html#>\\n\"\n+\t\t\t\t\t\t\t\t\t+ \"SELECT ?word WHERE { <\" + word + \"> conll:WORD ?word } LIMIT 1\", m)\n+\t\t\t\t\t\t\t.execSelect().next().get(\"?word\").toString());\n+\t\t\t\t} catch (NoSuchElementException e) {\n+\t\t\t\t\tLOG.warn(\"Warning: no conll:WORD (WORD column) found\");\n+\t\t\t\t\twords.add(\"\");\n+\t\t\t\t}\n+\t\t\t\tmaxWordLength = Math.max(maxWordLength, words.get(words.size() - 1).length());\n+\t\t\t\tString anno = \"\";\n+\t\t\t\tResultSet annos_raw = QueryExecutionFactory.create(\n+\t\t\t\t\t\t\"PREFIX conll: <http://ufal.mff.cuni.cz/conll2009-st/task-description.html#>\\n\"\n+\t\t\t\t\t\t\t\t+ \"SELECT ?rel ?val WHERE { <\" + word + \"> ?rel ?val \\n\"\n+\t\t\t\t\t\t\t\t+ \"FILTER(contains(str(?rel),'http://ufal.mff.cuni.cz/conll2009-st/task-description.html#'))\\n\"\n+\t\t\t\t\t\t\t\t+ \"FILTER(?rel!=conll:HEAD && ?rel!=conll:EDGE && ?rel!=conll:WORD) } ORDER BY ASC(?rel)\",\n+\t\t\t\t\t\tm).execSelect();\n+\t\t\t\tString rel = \"\";\n+\t\t\t\twhile (annos_raw.hasNext()) {\n+\t\t\t\t\tQuerySolution next = annos_raw.next();\n+\t\t\t\t\tString nextRel = next.get(\"?rel\").toString().replaceFirst(\".*#\", \"\");\n+\t\t\t\t\tif (!rel.equals(nextRel))\n+\t\t\t\t\t\tanno = anno + ANSI_BLUE + ANSI_ULINE + nextRel + ANSI_RESET + \" \";\n+\t\t\t\t\trel = nextRel;\n+\t\t\t\t\tanno = anno + next.get(\"?val\").toString()\n+\t\t\t\t\t\t\t.replaceFirst(\"^http://purl.org/acoli/open-ie/(.*)$\", ANSI_YLW_BK + \"$1\" + ANSI_RESET)\n+\t\t\t\t\t\t\t.replaceFirst(\".*#\", \"\") + \" \";\n+\t\t\t\t}\n+\n+\t\t\t\t// we append OLiA annotations to CoNLL annotations\n+\t\t\t\tResultSet olia_types = QueryExecutionFactory.create(\n+\t\t\t\t\t\t\"PREFIX conll: <http://ufal.mff.cuni.cz/conll2009-st/task-description.html#>\\n\"\n+\t\t\t\t\t\t\t\t+ \"SELECT ?concept WHERE { <\" + word + \"> a ?concept \\n\"\n+\t\t\t\t\t\t\t\t+ \"FILTER(contains(str(?concept),'http://purl.org/olia'))\\n\" + \"} ORDER BY ASC(?val)\",\n+\t\t\t\t\t\tm).execSelect();\n+\t\t\t\twhile (olia_types.hasNext())\n+\t\t\t\t\tanno = anno + ANSI_RED + olia_types.next().get(\"?concept\").toString()\n+\t\t\t\t\t\t\t.replaceFirst(\"^.*/([^/]*)\\\\.(owl|rdf)[#/]\", \"$1:\") + ANSI_RESET + \" \";\n+\n+\t\t\t\t// append OLiA features\n+\t\t\t\tResultSet olia_feats = QueryExecutionFactory.create(\n+\t\t\t\t\t\t\"PREFIX conll: <http://ufal.mff.cuni.cz/conll2009-st/task-description.html#>\\n\"\n+\t\t\t\t\t\t\t\t+ \"SELECT ?rel ?concept WHERE { <\" + word + \"> ?rel ?val. ?val a ?concept.\\n\"\n+\t\t\t\t\t\t\t\t+ \"FILTER(contains(str(?rel),'http://purl.org/olia'))\\n\"\n+\t\t\t\t\t\t\t\t+ \"FILTER(contains(str(?concept),'http://purl.org/olia'))\\n\" + \"} ORDER BY ASC(?rel)\",\n+\t\t\t\t\t\tm).execSelect();\n+\t\t\t\twhile (olia_feats.hasNext()) {\n+\t\t\t\t\tQuerySolution next = olia_feats.next();\n+\t\t\t\t\tanno = anno + ANSI_RED + ANSI_ULINE\n+\t\t\t\t\t\t\t+ next.get(\"?rel\").toString().replaceFirst(\"^.*/([^/]*)\\\\.(owl|rdf)[#/]\", \"$1:\")\n+\t\t\t\t\t\t\t+ ANSI_RESET + \".\" + ANSI_RED\n+\t\t\t\t\t\t\t+ next.get(\"?concept\").toString().replaceFirst(\"^.*/([^/]*)\\\\.(owl|rdf)[#/]\", \"$1:\")\n+\t\t\t\t\t\t\t+ ANSI_RESET + \" \";\n+\t\t\t\t}\n+\n+\t\t\t\tannos.add(anno);\n+\n+\t\t\t\tString head = \"\";\n+\t\t\t\ttry {\n+\t\t\t\t\thead = QueryExecutionFactory\n+\t\t\t\t\t\t\t.create(\"PREFIX conll: <http://ufal.mff.cuni.cz/conll2009-st/task-description.html#>\\n\"\n+\t\t\t\t\t\t\t\t\t+ \"SELECT ?head WHERE { <\" + word + \"> conll:HEAD ?head} LIMIT 1\", m)\n+\t\t\t\t\t\t\t.execSelect().next().get(\"?head\").toString();\n+\t\t\t\t\tif (Integer.parseInt(head.replaceAll(\"[^0-9]\", \"\")) < Integer\n+\t\t\t\t\t\t\t.parseInt(word.replaceAll(\"[^0-9]\", \"\")))\n+\t\t\t\t\t\theadDir.add(\" \\\\ \");\n+\t\t\t\t\telse\n+\t\t\t\t\t\theadDir.add(\" / \");\n+\t\t\t\t} catch (NumberFormatException e) {\n+\t\t\t\t\te.printStackTrace();\n+\t\t\t\t\tif (head.compareTo(word) < 1)\n+\t\t\t\t\t\theadDir.add(\" \\\\ \");\n+\t\t\t\t\telse\n+\t\t\t\t\t\theadDir.add(\" / \");\n+\t\t\t\t} catch (NoSuchElementException e) {\n+\t\t\t\t\theadDir.add(\"   \");\n+\t\t\t\t}\n+\n+\t\t\t\ttry {\n+\t\t\t\t\tdepth.add(Integer.parseInt(QueryExecutionFactory\n+\t\t\t\t\t\t\t.create(\"PREFIX conll: <http://ufal.mff.cuni.cz/conll2009-st/task-description.html#>\\n\"\n+\t\t\t\t\t\t\t\t\t+ \"SELECT (COUNT(DISTINCT ?head) AS ?depth) WHERE { <\" + word\n+\t\t\t\t\t\t\t\t\t+ \"> conll:HEAD+ ?head }\", m)\n+\t\t\t\t\t\t\t.execSelect().next().get(\"?depth\").toString().replaceFirst(\"^\\\"?([0-9]+)[\\\\^\\\"].*\", \"$1\")));\n+\t\t\t\t} catch (NoSuchElementException e) {\n+\t\t\t\t\tif (depth.size() == 0)\n+\t\t\t\t\t\tdepth.add(1);\n+\t\t\t\t\telse\n+\t\t\t\t\t\tdepth.add(depth.get(depth.size() - 1));\n+\t\t\t\t}\n+\t\t\t\tmaxDepth = Math.max(maxDepth, depth.get(depth.size() - 1));\n+\n+\t\t\t\ttry { // return the longest edge\n+\t\t\t\t\tedges.add(QueryExecutionFactory.create(\n+\t\t\t\t\t\t\t\"PREFIX conll: <http://ufal.mff.cuni.cz/conll2009-st/task-description.html#>\\n\"\n+\t\t\t\t\t\t\t\t\t+ \"PREFIX fn: <http://www.w3.org/2005/xpath-functions#>\\n\"\n+\t\t\t\t\t\t\t\t\t+ \"SELECT ?edge ?length WHERE { <\" + word\n+\t\t\t\t\t\t\t\t\t+ \"> conll:EDGE ?edge. BIND(fn:string-length(?edge) AS ?length) } ORDER BY DESC(?length) LIMIT 1\",\n+\t\t\t\t\t\t\tm).execSelect().next().get(\"?edge\").toString());\n+\t\t\t\t} catch (NoSuchElementException e) {\n+\t\t\t\t\tedges.add(\"\");\n+\t\t\t\t}\n+\t\t\t\tmaxEdgeLength = Math.max(maxEdgeLength, edges.get(edges.size() - 1).length());\n+\n+\t\t\t\tString term = \"\";\n+\t\t\t\tif (includeTermConcepts) {\n+\t\t\t\t\tResultSet terms_raw = QueryExecutionFactory\n+\t\t\t\t\t\t\t.create(\"PREFIX conll: <http://ufal.mff.cuni.cz/conll2009-st/task-description.html#>\\n\"\n+\t\t\t\t\t\t\t\t\t+ \"SELECT ?term WHERE { <\" + word + \"> a ?term \\n\"\n \t\t\t\t\t\t\t\t\t+ \"FILTER(contains(str(?term),'http://purl.org/acoli/open-ie/'))\\n\"\n-\t\t\t\t\t\t\t\t\t+ \" } ORDER BY ASC(?term)\",\n-\t\t\t\t\t\t\t\tm).execSelect();\n-\t\t\t\t\t\twhile(terms_raw.hasNext())\n-\t\t\t\t\t\t\tterm=term+terms_raw.next().get(\"?term\").toString().\n-\t\t\t\t\t\t\t\t\treplaceFirst(\"http://purl.org/acoli/open-ie/\",\"\")+\" \";\n-\t\t\t\t\t\t\t\t\t//replaceFirst(\"http://purl.org/acoli/open-ie/\",\"terms:\")+\" \";\n-\t\t\t\t\t}\n-\t\t\t\t\tterms.add(term.trim());\n-\t\t\t\t\tmaxTermLength=Math.max(maxTermLength, term.trim().length());\n-\t\t\t\t\t\n-\t\t\t\t\tword = QueryExecutionFactory.create(\n-\t\t\t\t\t\t\t\"PREFIX nif: <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#>\\n\"+\n-\t\t\t\t\t\t\t\"SELECT ?next WHERE { <\"+word+\"> nif:nextWord ?next } LIMIT 1\",\n-\t\t\t\t\t\t\tm).execSelect().next().get(\"?next\").toString();\n+\t\t\t\t\t\t\t\t\t+ \" } ORDER BY ASC(?term)\", m)\n+\t\t\t\t\t\t\t.execSelect();\n+\t\t\t\t\twhile (terms_raw.hasNext())\n+\t\t\t\t\t\tterm = term + terms_raw.next().get(\"?term\").toString()\n+\t\t\t\t\t\t\t\t.replaceFirst(\"http://purl.org/acoli/open-ie/\", \"\") + \" \";\n+\t\t\t\t\t// replaceFirst(\"http://purl.org/acoli/open-ie/\",\"terms:\")+\" \";\n \t\t\t\t}\n-\t\t\t} catch (NoSuchElementException e) {\n-\t\t\t} catch(Exception e) {\n-\t\t\t\te.printStackTrace();\n+\t\t\t\tterms.add(term.trim());\n+\t\t\t\tmaxTermLength = Math.max(maxTermLength, term.trim().length());\n+\n+\t\t\t\tword = QueryExecutionFactory\n+\t\t\t\t\t\t.create(\"PREFIX nif: <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#>\\n\"\n+\t\t\t\t\t\t\t\t+ \"SELECT ?next WHERE { <\" + word + \"> nif:nextWord ?next } LIMIT 1\", m)\n+\t\t\t\t\t\t.execSelect().next().get(\"?next\").toString();\n \t\t\t}\n+\t\t} catch (NoSuchElementException e) {\n+\t\t} catch (Exception e) {\n+\t\t\te.printStackTrace();\n+\t\t}\n \n-\t\t\tString result = \"\";\n-\t\t\t\n-\t\t\t\n-\t\t\tfor(int i = 0; i<words.size(); i++) {\n-\t\t\t\tresult=result+ids.get(i);\n-\t\t\t\tfor(int j = ids.get(i).length(); j<maxIdLength; j++)\n-\t\t\t\t\tresult=result+\" \";\n-\t\t\t\tresult=result+ANSI_WHITE;\n-\t\t\t\tfor(int j=depth.get(i);j>0;j--)\n-\t\t\t\t\tresult=result+\" .\";\n-\t\t\t\tresult=result+ANSI_RESET;\n-\t\t\t\tresult=result+headDir.get(i);\n-\t\t\t\tresult=result+edges.get(i);\n-\t\t\t\tfor(int j = maxDepth-depth.get(i);j>0;j--)\n-\t\t\t\t\tif(depth.get(i)>1) result=result+\"--\"; else result=result+\"  \";\n-\t\t\t\tfor(int j = edges.get(i).length();j<maxEdgeLength;j++)\n-\t\t\t\t\tif(depth.get(i)>1) result=result+\"-\"; else result=result+\" \";\n-\t\t\t\tresult=result+\" \"+words.get(i);\n-\t\t\t\tfor(int j = words.get(i).length(); j<maxWordLength; j++)\n-\t\t\t\t\tresult=result+\" \";\n-\t\t\t\tresult=result+\" \"+ANSI_YLW_BK+terms.get(i)+ANSI_RESET;\n-\t\t\t\tfor(int j = terms.get(i).length(); j<maxTermLength; j++)\n-\t\t\t\t\tresult=result+\" \";\n-\t\t\t\tresult=result+\" \"+annos.get(i)+\"\\n\";\n-\t\t\t}\t\t\t\n-\t\t\treturn result;\n-\t\t}\n-\t\t\t\n-\t\t/** default: include type assignments */\n-\t\tprotected static String extractTermGraph(String buffer) {\n-\t\t\treturn extractTermGraph(buffer, true);\n-\t\t}\n-\t\t\n-\t\tprotected static String extractTermGraph(String buffer, boolean includeTermConcepts) {\n-\t\t\tModel m = ModelFactory.createDefaultModel().read(new StringReader(buffer),null, \"TTL\");\n-\t\t\tString word = null;\n-\t\t\tString result = \"\";\n-\t\t\tString s = \"\";\n-\t\t\tString r = \"\";\n-\t\t\tString o = \"\";\n-\t\t\ttry {\n-\t\t\t\t// write original sentence\n-\t\t\t\tResultSet sentence = QueryExecutionFactory.create(\n-\t\t\t\t\t\"PREFIX nif: <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#>\\n\"\n-\t\t\t\t\t+ \"PREFIX conll: <http://ufal.mff.cuni.cz/conll2009-st/task-description.html#>\\n\"\n-\t\t\t\t\t+ \"SELECT ?w ?word (COUNT(DISTINCT ?pre) AS ?pos)\\n\"\n-\t\t\t\t\t+ \"WHERE {\\n\"\n-\t\t\t\t\t+ \"?w conll:WORD ?word.\\n\"\n-\t\t\t\t\t+ \"?pre nif:nextWord* ?w.\\n\"\n-\t\t\t\t\t+ \"} GROUP BY ?w ?word ORDER BY ASC(?pos)\",m).execSelect();\n-\t\t\t\twhile(sentence.hasNext())\n-\t\t\t\t\tresult=result+sentence.next().get(\"?word\")+\" \";\t\t\t\t\t\t\t\n-\t\t\t\t\n-\t\t\t\t// write result set\n-\t\t\t\tResultSet semgraph = QueryExecutionFactory.create(\n-\t\t\t\t\t\"PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\\n\"\n-\t\t\t\t\t+\"PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\\n\"\n-\t\t\t\t\t+\"SELECT DISTINCT ?s ?sl ?r ?o ?ol ?in ?out\\n\"\n-\t\t\t\t\t+\"WHERE { \"\n-\t\t\t\t\t+ \"?s ?r [].\\n\"\n-\t\t\t\t\t+ \"OPTIONAL { ?s ?r ?o }. \\n\"\t\t\t//  ?o can be blank\n+\t\tString result = \"\";\n+\n+\t\tfor (int i = 0; i < words.size(); i++) {\n+\t\t\tresult = result + ids.get(i);\n+\t\t\tfor (int j = ids.get(i).length(); j < maxIdLength; j++)\n+\t\t\t\tresult = result + \" \";\n+\t\t\tresult = result + ANSI_WHITE;\n+\t\t\tfor (int j = depth.get(i); j > 0; j--)\n+\t\t\t\tresult = result + \" .\";\n+\t\t\tresult = result + ANSI_RESET;\n+\t\t\tresult = result + headDir.get(i);\n+\t\t\tresult = result + edges.get(i);\n+\t\t\tfor (int j = maxDepth - depth.get(i); j > 0; j--)\n+\t\t\t\tif (depth.get(i) > 1)\n+\t\t\t\t\tresult = result + \"--\";\n+\t\t\t\telse\n+\t\t\t\t\tresult = result + \"  \";\n+\t\t\tfor (int j = edges.get(i).length(); j < maxEdgeLength; j++)\n+\t\t\t\tif (depth.get(i) > 1)\n+\t\t\t\t\tresult = result + \"-\";\n+\t\t\t\telse\n+\t\t\t\t\tresult = result + \" \";\n+\t\t\tresult = result + \" \" + words.get(i);\n+\t\t\tfor (int j = words.get(i).length(); j < maxWordLength; j++)\n+\t\t\t\tresult = result + \" \";\n+\t\t\tresult = result + \" \" + ANSI_YLW_BK + terms.get(i) + ANSI_RESET;\n+\t\t\tfor (int j = terms.get(i).length(); j < maxTermLength; j++)\n+\t\t\t\tresult = result + \" \";\n+\t\t\tresult = result + \" \" + annos.get(i) + \"\\n\";\n+\t\t}\n+\t\treturn result;\n+\t}\n+\n+\t/** default: include type assignments */\n+\tprotected static String extractTermGraph(String buffer) {\n+\t\treturn extractTermGraph(buffer, true);\n+\t}\n+\n+\tprotected static String extractTermGraph(String buffer, boolean includeTermConcepts) {\n+\t\tModel m = ModelFactory.createDefaultModel().read(new StringReader(buffer), null, \"TTL\");\n+\t\tString word = null;\n+\t\tString result = \"\";\n+\t\tString s = \"\";\n+\t\tString r = \"\";\n+\t\tString o = \"\";\n+\t\ttry {\n+\t\t\t// write original sentence\n+\t\t\tResultSet sentence = QueryExecutionFactory\n+\t\t\t\t\t.create(\"PREFIX nif: <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#>\\n\"\n+\t\t\t\t\t\t\t+ \"PREFIX conll: <http://ufal.mff.cuni.cz/conll2009-st/task-description.html#>\\n\"\n+\t\t\t\t\t\t\t+ \"SELECT ?w ?word (COUNT(DISTINCT ?pre) AS ?pos)\\n\" + \"WHERE {\\n\"\n+\t\t\t\t\t\t\t+ \"?w conll:WORD ?word.\\n\" + \"?pre nif:nextWord* ?w.\\n\"\n+\t\t\t\t\t\t\t+ \"} GROUP BY ?w ?word ORDER BY ASC(?pos)\", m)\n+\t\t\t\t\t.execSelect();\n+\t\t\twhile (sentence.hasNext())\n+\t\t\t\tresult = result + sentence.next().get(\"?word\") + \" \";\n+\n+\t\t\t// write result set\n+\t\t\tResultSet semgraph = QueryExecutionFactory.create(\"PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\\n\"\n+\t\t\t\t\t+ \"PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\\n\"\n+\t\t\t\t\t+ \"SELECT DISTINCT ?s ?sl ?r ?o ?ol ?in ?out\\n\" + \"WHERE { \" + \"?s ?r [].\\n\"\n+\t\t\t\t\t+ \"OPTIONAL { ?s ?r ?o }. \\n\" // ?o can be blank\n \t\t\t\t\t+ \"FILTER(contains(concat(str(?r),str(?o)),'http://purl.org/acoli/open-ie/') &&\\n\"\n \t\t\t\t\t+ \"       !contains(str(?r),'http://ufal.mff.cuni.cz/conll2009-st/task-description.html#'))\\n\"\n-\t\t\t\t\t+ \"OPTIONAL {?s rdfs:label ?sl }\\n\"\n-\t\t\t\t\t+ \"OPTIONAL {?o rdfs:label ?ol }\\n\"\n+\t\t\t\t\t+ \"OPTIONAL {?s rdfs:label ?sl }\\n\" + \"OPTIONAL {?o rdfs:label ?ol }\\n\"\n \t\t\t\t\t+ \"BIND(xsd:integer(REPLACE(STR(?s),'[^0-9]','')) AS ?snr)\\n\"\n \t\t\t\t\t+ \"BIND(xsd:integer(REPLACE(STR(?o),'[^0-9]','')) AS ?onr)\\n\"\n \t\t\t\t\t+ \"{ FILTER(!BOUND(?snr)) BIND(?snr AS ?nr) } UNION\"\n \t\t\t\t\t+ \"{ FILTER(BOUND(?snr)) BIND(?onr AS ?nr) } \\n\"\n \t\t\t\t\t+ \"OPTIONAL { SELECT ?s (COUNT(DISTINCT *) AS ?in)\\n\"\n \t\t\t\t\t+ \"  WHERE { ?sin ?rin ?s FILTER(!ISBLANK(?sin)) FILTER(contains(str(?rin),'http://purl.org/acoli/open-ie/')) } GROUP BY ?s \\n\"\n-\t\t\t\t\t+ \"}\"\n-\t\t\t\t\t+ \"OPTIONAL { SELECT ?s (COUNT(DISTINCT *) AS ?out)\\n\"\n+\t\t\t\t\t+ \"}\" + \"OPTIONAL { SELECT ?s (COUNT(DISTINCT *) AS ?out)\\n\"\n \t\t\t\t\t+ \"  WHERE { ?s ?rout ?sout FILTER(!ISBLANK(?sout)) FILTER(contains(str(?rout),'http://purl.org/acoli/open-ie/'))} GROUP BY ?s \\n\"\n-\t\t\t\t\t+ \"}\"\n-\t\t\t\t\t+ \"}\"\n-\t\t\t\t\t+ \"ORDER BY ASC(?nr) ASC(?snr) ASC(?onr) ?r ?s ?o\",\n-\t\t\t\t\tm).execSelect();\n-\t\t\t\twhile(semgraph.hasNext()) {\n-\t\t\t\t\tQuerySolution next = semgraph.next();\n-\t\t\t\t\tRDFNode sNode = next.get(\"?s\");\n-\t\t\t\t\tString nextS = sNode.toString().replaceAll(\".*[#/]\",\"\");\n-\t\t\t\t\tif(!sNode.isURIResource()) nextS=\"[]\";\n-\t\t\t\t\tif(next.get(\"?sl\")!=null) nextS=nextS+\" \"+ANSI_CYAN+\"\\\"\"+next.get(\"?sl\")+\"\\\"\"+ANSI_RESET;\n-\t\t\t\t\tif(!nextS.equals(s)) {\n-\t\t\t\t\t\tresult=result+\"\\n\"+nextS+\" (\"+\n-\t\t\t\t\t\t\t\t(\"0\"+next.get(\"?in\")).replaceFirst(\"[^0-9].*\",\"\").replaceFirst(\"^0*([^0])\",\"$1\")+\" > node > \"+\n-\t\t\t\t\t\t\t\t(\"0\"+next.get(\"?out\")).toString().replaceFirst(\"[^0-9].*\",\"\").replaceFirst(\"^0*([^0])\",\"$1\")+\")\";\n-\t\t\t\t\t}\n-\t\t\t\t\tString nextR = next.get(\"?r\").toString()\n-\t\t\t\t\t\t\t.replaceAll(\"http://ufal.mff.cuni.cz/conll2009-st/task-description.html#(.*)$\",ANSI_BLUE+ANSI_ULINE+\"$1\"+ANSI_RESET) \n-\t\t\t\t\t\t\t.replaceAll(\"http://purl.org/acoli/open-ie/(.*)\",ANSI_YLW_BK+\"terms:$1\"+ANSI_RESET)\n-\t\t\t\t\t\t\t.replaceAll(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\",\"a\");\n-\t\t\t\t\t\n-\t\t\t\t\tString nextO = next.get(\"?o\").toString()\n-\t\t\t\t\t\t\t.replaceAll(\"http://purl.org/acoli/open-ie/(.*)\",ANSI_YLW_BK+\"terms:$1\"+ANSI_RESET)\n-\t\t\t\t\t\t\t.replaceAll(\"[^ \\t]*[#/]\",\"\");\n-\t\t\t\t\tif(next.get(\"?ol\")!=null)\n-\t\t\t\t\t\tnextO=nextO+\" \"+ANSI_CYAN+\"\\\"\"+next.get(\"?ol\")+\"\\\"\"+ANSI_RESET;\n-\t\t\t\t\t\n-\t\t\t\t\tif(!nextR.equals(\"a\") || includeTermConcepts==true) {\n-\t\t\t\t\t\tif(!nextS.equals(s) || !nextR.equals(r))\n-\t\t\t\t\t\t\tresult=result+\"\\n\\t\"+nextR;\n-\t\t\t\t\t\telse if(!nextO.equals(o)) result=result+\"; \";\n-\t\t\t\t\t\tif(!nextS.equals(s) || !nextR.equals(r) || !nextO.equals(o)) {\n-\t\t\t\t\t\t\tresult=result+\" \"+nextO;\n-\t\t\t\t\t\t}\n+\t\t\t\t\t+ \"}\" + \"}\" + \"ORDER BY ASC(?nr) ASC(?snr) ASC(?onr) ?r ?s ?o\", m).execSelect();\n+\t\t\twhile (semgraph.hasNext()) {\n+\t\t\t\tQuerySolution next = semgraph.next();\n+\t\t\t\tRDFNode sNode = next.get(\"?s\");\n+\t\t\t\tString nextS = sNode.toString().replaceAll(\".*[#/]\", \"\");\n+\t\t\t\tif (!sNode.isURIResource())\n+\t\t\t\t\tnextS = \"[]\";\n+\t\t\t\tif (next.get(\"?sl\") != null)\n+\t\t\t\t\tnextS = nextS + \" \" + ANSI_CYAN + \"\\\"\" + next.get(\"?sl\") + \"\\\"\" + ANSI_RESET;\n+\t\t\t\tif (!nextS.equals(s)) {\n+\t\t\t\t\tresult = result + \"\\n\" + nextS + \" (\"\n+\t\t\t\t\t\t\t+ (\"0\" + next.get(\"?in\")).replaceFirst(\"[^0-9].*\", \"\").replaceFirst(\"^0*([^0])\", \"$1\")\n+\t\t\t\t\t\t\t+ \" > node > \" + (\"0\" + next.get(\"?out\")).toString().replaceFirst(\"[^0-9].*\", \"\")\n+\t\t\t\t\t\t\t\t\t.replaceFirst(\"^0*([^0])\", \"$1\")\n+\t\t\t\t\t\t\t+ \")\";\n+\t\t\t\t}\n+\t\t\t\tString nextR = next.get(\"?r\").toString()\n+\t\t\t\t\t\t.replaceAll(\"http://ufal.mff.cuni.cz/conll2009-st/task-description.html#(.*)$\",\n+\t\t\t\t\t\t\t\tANSI_BLUE + ANSI_ULINE + \"$1\" + ANSI_RESET)\n+\t\t\t\t\t\t.replaceAll(\"http://purl.org/acoli/open-ie/(.*)\", ANSI_YLW_BK + \"terms:$1\" + ANSI_RESET)\n+\t\t\t\t\t\t.replaceAll(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\", \"a\");\n+\n+\t\t\t\tString nextO = next.get(\"?o\").toString()\n+\t\t\t\t\t\t.replaceAll(\"http://purl.org/acoli/open-ie/(.*)\", ANSI_YLW_BK + \"terms:$1\" + ANSI_RESET)\n+\t\t\t\t\t\t.replaceAll(\"[^ \\t]*[#/]\", \"\");\n+\t\t\t\tif (next.get(\"?ol\") != null)\n+\t\t\t\t\tnextO = nextO + \" \" + ANSI_CYAN + \"\\\"\" + next.get(\"?ol\") + \"\\\"\" + ANSI_RESET;\n+\n+\t\t\t\tif (!nextR.equals(\"a\") || includeTermConcepts == true) {\n+\t\t\t\t\tif (!nextS.equals(s) || !nextR.equals(r))\n+\t\t\t\t\t\tresult = result + \"\\n\\t\" + nextR;\n+\t\t\t\t\telse if (!nextO.equals(o))\n+\t\t\t\t\t\tresult = result + \"; \";\n+\t\t\t\t\tif (!nextS.equals(s) || !nextR.equals(r) || !nextO.equals(o)) {\n+\t\t\t\t\t\tresult = result + \" \" + nextO;\n \t\t\t\t\t}\n-\t\t\t\t\ts=nextS;\n-\t\t\t\t\tr=nextR;\n-\t\t\t\t\to=nextO;\n \t\t\t\t}\n-\t\t\t} catch (NoSuchElementException e) {\n-\t\t\t} catch (Exception e) {\n-\t\t\t\te.printStackTrace();\n+\t\t\t\ts = nextS;\n+\t\t\t\tr = nextR;\n+\t\t\t\to = nextO;\n \t\t\t}\n-\t\t\treturn result+\"\\n\";\n+\t\t} catch (NoSuchElementException e) {\n+\t\t} catch (Exception e) {\n+\t\t\te.printStackTrace();\n \t\t}\n-\t\t\n-\t\t/** require that every line starts with a subject, sort: @ (prefix) & # (comment) > lines, lines sorted lexiconumerically, i.e., normalize length of integers (regardless of position) before sorting */\n-\t\tprotected static String reorderTTLBuffer(String buffer, List<String> cols) {\n-\t\t\tString result =\"\";\n-\t\t\ttry {\n-\t\t\t\tBufferedReader in = new BufferedReader(new StringReader(buffer));\n-\t\t\t\tHashtable<String,String> key2line = new Hashtable<String,String>();\n-\t\t\t\tString line;\n-\t\t\t\twhile((line=in.readLine())!=null) {\n-\t\t\t\t\tline=line.trim();\n-\t\t\t\t\tif(line.startsWith(\"@\")) result=result+line+\"\\n\"; else\n-\t\t\t\t\tif(line.startsWith(\"#\")) result=result+line+\"\\n\"; else \n-\t\t\t\t\tif(!line.equals(\"\")) {\n-\t\t\t\t\t\t//reorder columns according to user list.\n-\t\t\t\t\t\tString orderedLine = \"\";\n-\t\t\t\t\t\tList<String> statements = new ArrayList<String>(Arrays.asList(line.substring(0, line.lastIndexOf(\".\")-1).split(\";\\\\s*\\t\"))); //TODO: only consider ; not \";\"\n-\t\t\t\t\t\tList<String> columns = new ArrayList<String>();\n-\t\t\t\t\t\t // Subject is always first. Change if complications occur.\n-\t\t\t\t\t\tif (statements.get(0).contains(\"nif:Word\")) {\n-\t\t\t\t\t\t\t//do rdf:type reorder\n-\t\t\t\t\t\t\tList<String> concepts = new ArrayList<String>(Arrays.asList(statements.get(0).split(\",\")));\n-\t\t\t\t\t\t\tString[] subject = concepts.get(0).split(\"\\\\sa\\\\s\");\n-\t\t\t\t\t\t\tif (subject.length == 2) {\n-\t\t\t\t\t\t\t\torderedLine += subject[0] + \" a nif:Word\";\n-\t\t\t\t\t\t\t\tif (!subject[1].contains(\"nif:Word\")) {\n-\t\t\t\t\t\t\t\t\tconcepts.set(0, subject[1]);\n-\t\t\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\t\t\tconcepts.remove(0);\n-\t\t\t\t\t\t\t\t}\n+\t\treturn result + \"\\n\";\n+\t}\n+\n+\t/**\n+\t * require that every line starts with a subject, sort: @ (prefix) & # (comment)\n+\t * > lines, lines sorted lexiconumerically, i.e., normalize length of integers\n+\t * (regardless of position) before sorting\n+\t */\n+\tprotected static String reorderTTLBuffer(String buffer, List<String> cols) {\n+\t\tString result = \"\";\n+\t\ttry {\n+\t\t\tBufferedReader in = new BufferedReader(new StringReader(buffer));\n+\t\t\tHashtable<String, String> key2line = new Hashtable<String, String>();\n+\t\t\tString line;\n+\t\t\twhile ((line = in.readLine()) != null) {\n+\t\t\t\tline = line.trim();\n+\t\t\t\tif (line.startsWith(\"@\"))\n+\t\t\t\t\tresult = result + line + \"\\n\";\n+\t\t\t\telse if (line.startsWith(\"#\"))\n+\t\t\t\t\tresult = result + line + \"\\n\";\n+\t\t\t\telse if (!line.equals(\"\")) {\n+\t\t\t\t\t// reorder columns according to user list.\n+\t\t\t\t\tString orderedLine = \"\";\n+\t\t\t\t\tList<String> statements = new ArrayList<String>(\n+\t\t\t\t\t\t\tArrays.asList(line.substring(0, line.lastIndexOf(\".\") - 1).split(\";\\\\s*\\t\"))); // TODO: only\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t// consider\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t// ; not \";\"\n+\t\t\t\t\tList<String> columns = new ArrayList<String>();\n+\t\t\t\t\t// Subject is always first. Change if complications occur.\n+\t\t\t\t\tif (statements.get(0).contains(\"nif:Word\")) {\n+\t\t\t\t\t\t// do rdf:type reorder\n+\t\t\t\t\t\tList<String> concepts = new ArrayList<String>(Arrays.asList(statements.get(0).split(\",\")));\n+\t\t\t\t\t\tString[] subject = concepts.get(0).split(\"\\\\sa\\\\s\");\n+\t\t\t\t\t\tif (subject.length == 2) {\n+\t\t\t\t\t\t\torderedLine += subject[0] + \" a nif:Word\";\n+\t\t\t\t\t\t\tif (!subject[1].contains(\"nif:Word\")) {\n+\t\t\t\t\t\t\t\tconcepts.set(0, subject[1]);\n \t\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\t\torderedLine += concepts.get(0);\n \t\t\t\t\t\t\t\tconcepts.remove(0);\n \t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\tfor (String concept:concepts) {\n-\t\t\t\t\t\t\t\tif (concept.contains(\"nif:Word\")) continue;\n-\t\t\t\t\t\t\t\torderedLine += \", \" + concept.trim();\n-\t\t\t\t\t\t\t}\n \t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\torderedLine = statements.get(0).trim();\n+\t\t\t\t\t\t\torderedLine += concepts.get(0);\n+\t\t\t\t\t\t\tconcepts.remove(0);\n \t\t\t\t\t\t}\n-\t\t\t\t\t\tstatements.remove(0);\n-\t\t\t\t\t\t//do column reorder\n-\t\t\t\t\t\tcolumns.add(\"nif:Word\");\n-\t\t\t\t\t\tcolumns.add(\"conll:WORD\");\n-\t\t\t\t\t\tcolumns.addAll(cols);\n-\t\t\t\t\t\tfor (String col:columns) {\n-\t\t\t\t\t\t\tfor (int i = 0; i < statements.size();i++) {\n-\t\t\t\t\t\t\t\tif (statements.get(i).contains(col)) {\n-\t\t\t\t\t\t\t\t\torderedLine += \"; \" + statements.get(i).trim();\n-\t\t\t\t\t\t\t\t\tstatements.remove(i);\n-\t\t\t\t\t\t\t\t\tbreak;\n-\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t}\n+\t\t\t\t\t\tfor (String concept : concepts) {\n+\t\t\t\t\t\t\tif (concept.contains(\"nif:Word\"))\n+\t\t\t\t\t\t\t\tcontinue;\n+\t\t\t\t\t\t\torderedLine += \", \" + concept.trim();\n \t\t\t\t\t\t}\n-\t\t\t\t\t\t//add rest of columns to the end\n-\t\t\t\t\t\tString nifnext = \"\";\n-\t\t\t\t\t\tfor (int i = 0; i < statements.size();i++) {\n-\t\t\t\t\t\t\tif (statements.get(i).contains(\"nif:nextWord\")) \n-\t\t\t\t\t\t\t\tnifnext = \"; \" + statements.get(i).trim();\n-\t\t\t\t\t\t\telse\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\torderedLine = statements.get(0).trim();\n+\t\t\t\t\t}\n+\t\t\t\t\tstatements.remove(0);\n+\t\t\t\t\t// do column reorder\n+\t\t\t\t\tcolumns.add(\"nif:Word\");\n+\t\t\t\t\tcolumns.add(\"conll:WORD\");\n+\t\t\t\t\tcolumns.addAll(cols);\n+\t\t\t\t\tfor (String col : columns) {\n+\t\t\t\t\t\tfor (int i = 0; i < statements.size(); i++) {\n+\t\t\t\t\t\t\tif (statements.get(i).contains(col)) {\n \t\t\t\t\t\t\t\torderedLine += \"; \" + statements.get(i).trim();\n+\t\t\t\t\t\t\t\tstatements.remove(i);\n+\t\t\t\t\t\t\t\tbreak;\n+\t\t\t\t\t\t\t}\n \t\t\t\t\t\t}\n-\t\t\t\t\t\tif (!orderedLine.equals(\"\")) {\n-\t\t\t\t\t\t\torderedLine += nifnext + \" .\";\n-\t\t\t\t\t\t\tline = orderedLine;\n-\t\t\t\t\t\t} \n-\t\t\t\t\t\t\n-\t\t\t\t\t\t\n-\t\t\t\t\t\t//reorder lines\n-\t\t\t\t\t\tString tmp=line.replaceAll(\"\\t\",\" \").replaceAll(\"([^0-9])([0-9])\",\"$1\\t$2\").replaceAll(\"([0-9])([^0-9])\",\"$1\\t$2\"); \t// key \\t-split\n-\t\t\t\t\t\tString key=\"\";\n-\t\t\t\t\t\tfor(String s : tmp.split(\"\\t\")) {\n-\t\t\t\t\t\t\tif(s.matches(\"^[0-9]+$\"))\n-\t\t\t\t\t\t\t\twhile(s.length()<64) s=\"0\"+s;\n-\t\t\t\t\t\t\tkey=key+s;\n-\t\t\t\t\t\t}\n-\t\t\t\t\t\tkey2line.put(key,line);\n \t\t\t\t\t}\n+\t\t\t\t\t// add rest of columns to the end\n+\t\t\t\t\tString nifnext = \"\";\n+\t\t\t\t\tfor (int i = 0; i < statements.size(); i++) {\n+\t\t\t\t\t\tif (statements.get(i).contains(\"nif:nextWord\"))\n+\t\t\t\t\t\t\tnifnext = \"; \" + statements.get(i).trim();\n+\t\t\t\t\t\telse\n+\t\t\t\t\t\t\torderedLine += \"; \" + statements.get(i).trim();\n+\t\t\t\t\t}\n+\t\t\t\t\tif (!orderedLine.equals(\"\")) {\n+\t\t\t\t\t\torderedLine += nifnext + \" .\";\n+\t\t\t\t\t\tline = orderedLine;\n+\t\t\t\t\t}\n+\n+\t\t\t\t\t// reorder lines\n+\t\t\t\t\tString tmp = line.replaceAll(\"\\t\", \" \").replaceAll(\"([^0-9])([0-9])\", \"$1\\t$2\")\n+\t\t\t\t\t\t\t.replaceAll(\"([0-9])([^0-9])\", \"$1\\t$2\"); // key \\t-split\n+\t\t\t\t\tString key = \"\";\n+\t\t\t\t\tfor (String s : tmp.split(\"\\t\")) {\n+\t\t\t\t\t\tif (s.matches(\"^[0-9]+$\"))\n+\t\t\t\t\t\t\twhile (s.length() < 64)\n+\t\t\t\t\t\t\t\ts = \"0\" + s;\n+\t\t\t\t\t\tkey = key + s;\n+\t\t\t\t\t}\n+\t\t\t\t\tkey2line.put(key, line);\n \t\t\t\t}\n-\t\t\t\tList<String> keys = new ArrayList<String>(key2line.keySet());\n-\t\t\t\tCollections.sort(keys);\n-\t\t\t\tfor(String key: keys)\n-\t\t\t\t\tresult=result+key2line.get(key)+\"\\n\";\n-\t\t\t} catch (IOException e) {\n-\t\t\t\te.printStackTrace();\n \t\t\t}\n-\t\t\treturn result;\n-\t\t}\n-\n-\t\t/** note: the last column must contain literal values, not HEAD */\n-\t\tpublic static String columnsAsSelect(List<String> cols) {\n-\t\t\tString select = \"\"\n-\t\t\t+ \"PREFIX nif: <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#>\\n\"\n-\t\t\t+ \"PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\\n\"\n-\t\t\t+ \"PREFIX conll: <http://ufal.mff.cuni.cz/conll2009-st/task-description.html#>\\n\"\n-\t\t\t+ \"PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\\n\"\n-\t\t\t\n-\t\t\t+ \"SELECT \";\n-\t\t\tfor (String col:cols) {\n-\t\t\t\tselect += \"?\"+col+\" \";\n-\t\t\t}\n-\t\t\t\n-\t\t\tselect += \"{\\n\";\n-\t\t\tselect += \"\tSELECT \\n\";\n-\t\t\tselect += \"\t?sid ?wid \\n\";\n-\t\t\t\n-\t\t\tfor (String col:cols) {\n-\t\t\t\tselect += \"\t(group_concat(?\"+col+\"s;separator='|') as ?\"+col+\")\\n\";\n-\t\t\t}\n-\t\t\t\n-\t\t\tString lastCol = cols.get(cols.size()-1);\n-\t\t\t\n-\t\t\tselect += \"\tWHERE {\\n\";\n-\t\t\tselect += \"\t\t?word a nif:Word .\\n\";\n-\t\t\tselect += \"\t\t{\\n\";\n-\t\t\tselect += \"\t\t\tSELECT ?word (count(distinct ?preS) as ?sid) (count(distinct ?pre) as ?wid)\\n\";\n-\t\t\tselect += \"\t\t\tWHERE {\\n\";\n-\t\t\tselect += \"\t\t\t\t?word a nif:Word .\\n\";\n-\t\t\tselect += \"\t\t\t\t?pre nif:nextWord* ?word .\\n\";\n-\t\t\tselect += \"             ?word conll:HEAD+ ?s. ?s a nif:Sentence. ?preS nif:nextSentence* ?s.\\n\";\n-\t\t\tselect += \"\t\t\t}\\n\";\n-\t\t\tselect += \"\t\t\tgroup by ?word\\n\";\n-\t\t\tselect += \"\t\t}\\n\";\n-\t\t\tfor (String col:cols) {\n-\t\t\t\tif(col.equals(lastCol)) {\t// cast to string\n-\t\t\t\t\tif (col.equals(\"HEAD\")) { //TODO: streamline! only difference to statement below is binding to HEADa instead of HEADs\n-\t\t\t\t\t\tselect += \"\t\tOPTIONAL {\\n\";\n-\t\t\t\t\t\tselect += \"\t\t\t?word conll:HEAD ?headurl .\\n\";\n-\t\t\t\t\t\tselect += \"\t\t\tbind(replace(str(?headurl), '^.*s[0-9]+_([0-9]+)$', '$1') as ?HEADa) .\\n\";\n-\t\t\t\t\t\tselect += \"\t\t} .\\n\";\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\tselect += \"\t\tOPTIONAL{?word conll:\"+col+\" ?\"+col+\"_raw .\";\n-\t\t\t\t\t\tselect += \"\t\t \t\t BIND(str(?\"+col+\"_raw) as ?\"+col+\"a)} .\\n\";\n-\t\t\t\t\t}\n-\t\t\t\t\tselect += \"     BIND(concat(if(bound(?\"+col+\"a),?\"+col+\"a,'_'),\\n\";\n-\t\t\t\t\tselect += \"                 IF(EXISTS { ?word nif:nextWord [] }, '', '\\\\n')) as ?\"+col+\"s)\\n\";\n-\t\t\t\t\t// we append a linebreak to the value of the last column to generate sentence breaks within a local graph\n-\t\t\t\t} else if (col.equals(\"HEAD\")) {\n+\t\t\tList<String> keys = new ArrayList<String>(key2line.keySet());\n+\t\t\tCollections.sort(keys);\n+\t\t\tfor (String key : keys)\n+\t\t\t\tresult = result + key2line.get(key) + \"\\n\";\n+\t\t} catch (IOException e) {\n+\t\t\te.printStackTrace();\n+\t\t}\n+\t\treturn result;\n+\t}\n+\n+\t/** note: the last column must contain literal values, not HEAD */\n+\tpublic static String columnsAsSelect(List<String> cols) {\n+\t\tString select = \"\" + \"PREFIX nif: <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#>\\n\"\n+\t\t\t\t+ \"PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\\n\"\n+\t\t\t\t+ \"PREFIX conll: <http://ufal.mff.cuni.cz/conll2009-st/task-description.html#>\\n\"\n+\t\t\t\t+ \"PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\\n\"\n+\n+\t\t\t\t+ \"SELECT \";\n+\t\tfor (String col : cols) {\n+\t\t\tselect += \"?\" + col + \" \";\n+\t\t}\n+\n+\t\tselect += \"{\\n\";\n+\t\tselect += \"\tSELECT \\n\";\n+\t\tselect += \"\t?sid ?wid \\n\";\n+\n+\t\tfor (String col : cols) {\n+\t\t\tselect += \"\t(group_concat(?\" + col + \"s;separator='|') as ?\" + col + \")\\n\";\n+\t\t}\n+\n+\t\tString lastCol = cols.get(cols.size() - 1);\n+\n+\t\tselect += \"\tWHERE {\\n\";\n+\t\tselect += \"\t\t?word a nif:Word .\\n\";\n+\t\tselect += \"\t\t{\\n\";\n+\t\tselect += \"\t\t\tSELECT ?word (count(distinct ?preS) as ?sid) (count(distinct ?pre) as ?wid)\\n\";\n+\t\tselect += \"\t\t\tWHERE {\\n\";\n+\t\tselect += \"\t\t\t\t?word a nif:Word .\\n\";\n+\t\tselect += \"\t\t\t\t?pre nif:nextWord* ?word .\\n\";\n+\t\tselect += \"             ?word conll:HEAD+ ?s. ?s a nif:Sentence. ?preS nif:nextSentence* ?s.\\n\";\n+\t\tselect += \"\t\t\t}\\n\";\n+\t\tselect += \"\t\t\tgroup by ?word\\n\";\n+\t\tselect += \"\t\t}\\n\";\n+\t\tfor (String col : cols) {\n+\t\t\tif (col.equals(lastCol)) { // cast to string\n+\t\t\t\tif (col.equals(\"HEAD\")) { // TODO: streamline! only difference to statement below is binding to HEADa\n+\t\t\t\t\t\t\t\t\t\t\t// instead of HEADs\n \t\t\t\t\tselect += \"\t\tOPTIONAL {\\n\";\n \t\t\t\t\tselect += \"\t\t\t?word conll:HEAD ?headurl .\\n\";\n-\t\t\t\t\tselect += \"\t\t\tbind(replace(str(?headurl), '^.*s[0-9]+_([0-9]+)$', '$1') as ?HEADs) .\\n\";\n+\t\t\t\t\tselect += \"\t\t\tbind(replace(str(?headurl), '^.*s[0-9]+_([0-9]+)$', '$1') as ?HEADa) .\\n\";\n \t\t\t\t\tselect += \"\t\t} .\\n\";\n \t\t\t\t} else {\n-\t\t\t\t\tselect += \"\t\tOPTIONAL{?word conll:\"+col+\" ?\"+col+\"_raw .\";\n-\t\t\t\t\tselect += \"\t\t \t\t BIND(str(?\"+col+\"_raw) as ?\"+col+\"s)} .\\n\";\t// cast to string\n+\t\t\t\t\tselect += \"\t\tOPTIONAL{?word conll:\" + col + \" ?\" + col + \"_raw .\";\n+\t\t\t\t\tselect += \"\t\t \t\t BIND(str(?\" + col + \"_raw) as ?\" + col + \"a)} .\\n\";\n \t\t\t\t}\n+\t\t\t\tselect += \"     BIND(concat(if(bound(?\" + col + \"a),?\" + col + \"a,'_'),\\n\";\n+\t\t\t\tselect += \"                 IF(EXISTS { ?word nif:nextWord [] }, '', '\\\\n')) as ?\" + col + \"s)\\n\";\n+\t\t\t\t// we append a linebreak to the value of the last column to generate sentence\n+\t\t\t\t// breaks within a local graph\n+\t\t\t} else if (col.equals(\"HEAD\")) {\n+\t\t\t\tselect += \"\t\tOPTIONAL {\\n\";\n+\t\t\t\tselect += \"\t\t\t?word conll:HEAD ?headurl .\\n\";\n+\t\t\t\tselect += \"\t\t\tbind(replace(str(?headurl), '^.*s[0-9]+_([0-9]+)$', '$1') as ?HEADs) .\\n\";\n+\t\t\t\tselect += \"\t\t} .\\n\";\n+\t\t\t} else {\n+\t\t\t\tselect += \"\t\tOPTIONAL{?word conll:\" + col + \" ?\" + col + \"_raw .\";\n+\t\t\t\tselect += \"\t\t \t\t BIND(str(?\" + col + \"_raw) as ?\" + col + \"s)} .\\n\"; // cast to string\n+\t\t\t}\n+\t\t}\n+\t\tselect += \"\t}\\n\";\n+\t\tselect += \"\tgroup by ?word ?sid ?wid\\n\";\n+\t\tselect += \"\torder by ?sid ?wid\\n\";\n+\t\tselect += \"}\\n\";\n+\n+\t\treturn select;\n+\t}\n+\n+\t/**\n+\t * run either SELECT statement (cf.\n+\t * https://jena.apache.org/documentation/query/app_api.html) and return\n+\t * CoNLL-like TSV or just TTL <br>\n+\t * Note: this CoNLL-like export has limitations, of course: it will export one\n+\t * property per column, hence, collapsed dependencies or SRL annotations cannot\n+\t * be reconverted\n+\t */\n+\tpublic static void printSparql(String buffer, String select, Writer out) throws IOException {\n+\t\tModel m = ModelFactory.createDefaultModel().read(new StringReader(buffer), null, \"TTL\");\n+\t\tString selectComments = \"PREFIX nif: <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#>\\n\"\n+\t\t\t\t+ \"PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\\n\"\n+\t\t\t\t+ \"SELECT ?c WHERE {?x a nif:Sentence . ?x rdfs:comment ?c}\";\n+\t\tQueryExecution qexec = QueryExecutionFactory.create(selectComments, m);\n+\t\tResultSet results = qexec.execSelect();\n+\t\tSet<String> comments = new HashSet<>();\n+\t\tboolean hasGlobalComments = false;\n+\t\twhile (results.hasNext()) {\n+\t\t\tfor (String result : results.next().getLiteral(\"c\").toString().split(\"\\\\\\\\n\")) {\n+\t\t\t\tif (result.trim().matches(\"^\\\\s?global\\\\.columns\\\\s?=.*\"))\n+\t\t\t\t\thasGlobalComments = true;\n+\t\t\t\telse\n+\t\t\t\t\tcomments.add(result);\n \t\t\t}\n-\t\t\tselect += \"\t}\\n\";\n-\t\t\tselect += \"\tgroup by ?word ?sid ?wid\\n\";\n-\t\t\tselect += \"\torder by ?sid ?wid\\n\";\n-\t\t\tselect += \"}\\n\";\n-\t\t\t\n-\t\t\treturn select;\n \t\t}\n-\t\t\n-\t\t/** run either SELECT statement (cf. https://jena.apache.org/documentation/query/app_api.html) and return CoNLL-like TSV or just TTL <br>\n-\t\t*  Note: this CoNLL-like export has limitations, of course: it will export one property per column, hence, collapsed dependencies or \n-\t\t*  SRL annotations cannot be reconverted */\t\t\n-\t\tpublic static void printSparql(String buffer, String select, Writer out) throws IOException {\n-\t\t\tModel m = ModelFactory.createDefaultModel().read(new StringReader(buffer),null, \"TTL\");\n-\t\t\tString selectComments = \"PREFIX nif: <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#>\\n\"\n-\t\t\t\t\t+ \"PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\\n\"\n-\t\t\t\t\t+ \"SELECT ?c WHERE {?x a nif:Sentence . ?x rdfs:comment ?c}\";\n-\t\t\tQueryExecution qexec = QueryExecutionFactory.create(selectComments, m);\n-\t\t\tResultSet results = qexec.execSelect();\n-\t\t\tSet<String> comments = new HashSet<>();\n-\t\t\tboolean hasGlobalComments = false;\n-\t\t\twhile (results.hasNext()) {\n-\t\t\t\tfor (String result : results.next().getLiteral(\"c\").toString().split(\"\\\\\\\\n\")) {\n-\t\t\t\t\tif (result.trim().matches(\"^\\\\s?global\\\\.columns\\\\s?=.*\") )\n+\t\tqexec = QueryExecutionFactory.create(select, m);\n+\t\tresults = qexec.execSelect();\n+\t\tList<String> cols = results.getResultVars();\n+\t\tBufferedReader in = new BufferedReader(new StringReader(buffer));\n+\t\tHashtable<String, String> key2line = new Hashtable<String, String>();\n+\t\tString line;\n+\t\twhile ((line = in.readLine()) != null) {\n+\t\t\tif (line.trim().startsWith(\"#\")) {\n+\t\t\t\tfor (String splitComment : line.split(\"\\t\")) {\n+\t\t\t\t\tif (splitComment.trim().matches(\"^#\\\\s?global\\\\.columns\\\\s?=.*\"))\n \t\t\t\t\t\thasGlobalComments = true;\n \t\t\t\t\telse\n-\t\t\t\t\t\tcomments.add(result);\n+\t\t\t\t\t\tcomments.add(splitComment.replace(\"#\", \"\"));\n \t\t\t\t}\n \t\t\t}\n-\t\t\tqexec = QueryExecutionFactory.create(select, m);\n-\t\t\tresults = qexec.execSelect();\n-\t\t\tList<String> cols = results.getResultVars();\n-\t\t\tBufferedReader in = new BufferedReader(new StringReader(buffer));\n-\t\t\tHashtable<String,String> key2line = new Hashtable<String,String>();\n-\t\t\tString line;\n-\t\t\twhile((line=in.readLine())!=null) {\n-\t\t\t\tif (line.trim().startsWith(\"#\")) {\n-\t\t\t\t\tfor (String splitComment : line.split(\"\\t\")) {\n-\t\t\t\t\t\tif (splitComment.trim().matches(\"^#\\\\s?global\\\\.columns\\\\s?=.*\"))\n-\t\t\t\t\t\t\thasGlobalComments = true;\n-\t\t\t\t\t\telse\n-\t\t\t\t\t\t\tcomments.add(splitComment.replace(\"#\",\"\"));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\n-\t\t\t}\n-\t\t\tif (hasGlobalComments)\n-\t\t\t\tout.write(\"# global.columns = \" + String.join(\" \", cols) + \"\\n\");\n-\t\t\telse {\n-\t\t\t\tout.write(\"# global.columns = \"+String.join(\" \", cols)+\"\\n\");\n-\t\t\t}\n-\t\t\tfor (String comment : comments) {\n-\t\t\t\tout.write(\"#\"+comment+\"\\n\");\n-\t\t\t}\n+\t\t}\n+\t\tif (hasGlobalComments)\n+\t\t\tout.write(\"# global.columns = \" + String.join(\" \", cols) + \"\\n\");\n+\t\telse {\n+\t\t\tout.write(\"# global.columns = \" + String.join(\" \", cols) + \"\\n\");\n+\t\t}\n+\t\tfor (String comment : comments) {\n+\t\t\tout.write(\"#\" + comment + \"\\n\");\n+\t\t}\n \n-\t\t\twhile(results.hasNext()) {\n-\t\t\t\tQuerySolution sol = results.next();\n-\t\t\t\tfor(String col : cols)\n-\t\t\t\t\tif(sol.get(col)==null) out.write(\"_\\t\");\t\t// CoNLL practice\n-\t\t\t\t\telse out.write(sol.get(col)+\"\\t\");\n-\t\t\t\tout.write(\"\\n\");\n-\t\t\t\tout.flush();\n-\t\t\t}\n+\t\twhile (results.hasNext()) {\n+\t\t\tQuerySolution sol = results.next();\n+\t\t\tfor (String col : cols)\n+\t\t\t\tif (sol.get(col) == null)\n+\t\t\t\t\tout.write(\"_\\t\"); // CoNLL practice\n+\t\t\t\telse\n+\t\t\t\t\tout.write(sol.get(col) + \"\\t\");\n \t\t\tout.write(\"\\n\");\n \t\t\tout.flush();\n \t\t}\n-\t\t\n-\t\tpublic static void main(String[] argv) throws IOException {\n-\t\t\tLOG.info(\"synopsis: CoNLLRDFFormatter [-rdf [COLS]] [-debug] [-grammar] [-semantics] [-conll COLS] [-sparqltsv SPARQL]\\n\"\n-\t\t\t\t\t+ \"\\t-rdf  write formatted CoNLL-RDF to stdout (sorted by list of CoNLL COLS, if provided)\\n\"\n-\t\t\t\t\t+ \"\\t-conll  write formatted CoNLL to stdout (only specified COLS)\\n\"\n-\t\t\t\t\t+ \"\\t-debug     write formatted, color-highlighted full turtle to stderr\\n\"\n-\t\t\t\t\t+ \"\\t-grammar   write CoNLL data structures to stdout\\n\"\n-\t\t\t\t\t+ \"\\t-semantics write semantic graph to stdout\\n\"\n-\t\t\t\t\t+ \"\\t-sparqltsv write TSV generated from SPARQL statement to stdout.\\n\"\n-\t\t\t\t\t+ \"\\t           if with -grammar, then skip type assignments\\n\"\n-\t\t\t\t\t+ \"read TTL from stdin => format CoNLL-RDF or extract and highlight CoNLL (namespace conll:) and semantic (namespace terms:) subgraphs\\n\"\n-\t\t\t\t\t+ \"if no parameters are supplied, -conllrdf is inferred\");\n-\t\t\tString args = Arrays.asList(argv).toString().replaceAll(\"[\\\\[\\\\], ]+\",\" \").trim().toLowerCase();\n-\t\t\t\n-\t\t\tCoNLLRDFFormatter f = new CoNLLRDFFormatter();\n-\t\t\t\n-\t\t\tf.setInputStream(new BufferedReader(new InputStreamReader(System.in)));\n-\t\t\tf.setOutputStream(System.out);\n-\t\t\t\n-\t\t\t\n-\t\t\tboolean CONLLRDF = args.contains(\"-rdf\");\n-\t\t\tboolean CONLL = args.contains(\"-conll\");\n-\t\t\tboolean DEBUG = args.contains(\"-debug\");\n-\t\t\tboolean SPARQLTSV = args.contains(\"-sparqltsv\");\n-\t\t\tboolean GRAMMAR = args.contains(\"-grammar\");\n-\t\t\tboolean SEMANTICS = args.contains(\"-semantics\");\n-\t\t\t// if(!GRAMMAR && !SEMANTICS) { // default\n-\t\t\t\t// GRAMMAR=true;\n-\t\t\t\t// SEMANTICS=true;\n-\t\t\t// }\n-\t\t\tif(!CONLLRDF && !CONLL && !SPARQLTSV && !GRAMMAR && !SEMANTICS && !DEBUG) { // default\n-\t\t\t\tCONLLRDF = true;\n-\t\t\t}\n-\t\t\t\n-\t\t\tif(GRAMMAR) {\n-\t\t\t\tModule m = new Module();\n-\t\t\t\tm.setMode(Mode.GRAMMAR);\n-\t\t\t\tm.setOutputStream(f.getOutputStream());\n-\t\t\t\tf.getModules().add(m);\n-\t\t\t}\n-\t\t\t\n-\t\t\tif(SEMANTICS) {\n-\t\t\t\tModule m = new Module();\n-\t\t\t\tm.setMode(Mode.SEMANTICS);\n-\t\t\t\tm.setOutputStream(f.getOutputStream());\n-\t\t\t\tf.getModules().add(m);\n-\t\t\t}\n-\t\t\t\n-\t\t\tif(DEBUG) {\n-\t\t\t\tModule m = new Module();\n-\t\t\t\tm.setMode(Mode.DEBUG);\n-\t\t\t\tm.setOutputStream(System.err);\n-\t\t\t\tf.getModules().add(m);\n-\t\t\t}\n-\t\t\t\n-\t\t\tif(CONLL) {\n-\t\t\t\tModule m = new Module();\n-\t\t\t\tm.setMode(Mode.CONLL);\n-\t\t\t\tm.setOutputStream(f.getOutputStream());\n-\t\t\t\tm.getCols().clear();\n-\t\t\t\tint i = 0;\n-\t\t\t\twhile(i<argv.length && argv[i].toLowerCase().matches(\"^-+conll$\")) i++;\n-\t\t\t\twhile(i<argv.length && !argv[i].toLowerCase().matches(\"^-+.*$\"))\n-\t\t\t\t\tm.getCols().add(argv[i++]);\n-\t\t\t\tf.getModules().add(m);\n-\t\t\t}\n-\t\t\t\n-\t\t\tif(CONLLRDF) {\n-\t\t\t\tModule m = new Module();\n-\t\t\t\tm.setMode(Mode.CONLLRDF);\n-\t\t\t\tm.setOutputStream(f.getOutputStream());\n-\t\t\t\tm.getCols().clear();\n-\t\t\t\tint i = 0;\n-\t\t\t\twhile(i<argv.length && argv[i].toLowerCase().matches(\"^-+rdf$\")) i++;\n-\t\t\t\twhile(i<argv.length && !argv[i].toLowerCase().matches(\"^-+.*$\"))\n-\t\t\t\t\tm.getCols().add(argv[i++]);\n-\t\t\t\tf.getModules().add(m);\n-\t\t\t}\n-\t\t\t\n-\t\t\tif(SPARQLTSV) {\n-\t\t\t\tModule m = new Module();\n-\t\t\t\tm.setMode(Mode.SPARQLTSV);\n-\t\t\t\tm.setOutputStream(f.getOutputStream());\n-\t\t\t\tString select = \"\";\n-\t\t\t\tint i = 1;\n-\t\t\t\twhile(i<argv.length && argv[i].toLowerCase().matches(\"^-+sparqltsv$\")) i++;\n-\t\t\t\tif(i<argv.length)\n-\t\t\t\t\tselect=argv[i++];\n-\t\t\t\twhile(i<argv.length)\n-\t\t\t\t\tselect=select+\" \"+argv[i++]; // because queries may be parsed by the shell (Cygwin)\n-\t\t\t\t\n-\t\t\t\tReader sparqlreader = new StringReader(select);\n-\t\t\t\tFile file = new File(select);\n-\t\t\t\tURL u = null;\n-\t\t\t\ttry {\n-\t\t\t\t\tu = new URL(select);\n-\t\t\t\t} catch (MalformedURLException e) {}\n-\t\t\t\t\n-\t\t\t\tif(file.exists()) {\t\t\t// can be read from a file\n-\t\t\t\t\tsparqlreader = new FileReader(file);\n-\t\t\t\t\tLOG.debug(\"f\");\n-\t\t\t\t} else if(u!=null) {\n-\t\t\t\t\ttry {\n-\t\t\t\t\t\tsparqlreader = new InputStreamReader(u.openStream());\n-\t\t\t\t\t\tLOG.debug(\"u\");\n-\t\t\t\t\t} catch (Exception e) {}\n-\t\t\t\t}\n-\n-\t\t\t\tBufferedReader in = new BufferedReader(sparqlreader);\n-\t\t\t\tselect=\"\";\n-\t\t\t\tfor(String line = in.readLine(); line!=null; line=in.readLine())\n-\t\t\t\t\tselect=select+line+\"\\n\";\n-\t\t\t\tm.setSelect(select);\n-\t\t\t\tf.getModules().add(m);\n-\t\t\t}\n-\t\t\t\n-\t\t\tf.processSentenceStream();\n-\t\t\t\n-\t\t}\n+\t\tout.write(\"\\n\");\n+\t\tout.flush();\n+\t}\n \n \t/**\n \t * Searches a string buffer that is expected to represent a sentence for any\n-\t * <code>rdfs:comment</code> properties and checks them for a CoNLL-U Plus like global.columns comments.\n-\t * Defaults to an empty columnNames Array if not present.\n+\t * <code>rdfs:comment</code> properties and checks them for a CoNLL-U Plus like\n+\t * global.columns comments. Defaults to an empty columnNames Array if not\n+\t * present.\n+\t * \n \t * @param buffer a string buffer representing a sentence in conll-rdf\n \t * @return ArrayList of column names, empty if not present.\n \t */\n \tprivate List<String> findColumnNamesInRDFBuffer(String buffer) {\n-\t\t\tList<String> columnNames = new ArrayList<>();\n-\t\t\tModel m = ModelFactory.createDefaultModel().read(new StringReader(buffer),null, \"TTL\");\n-\t\t\tString selectComments = \"PREFIX nif: <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#>\\n\"\n-\t\t\t\t\t+ \"PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\\n\"\n-\t\t\t\t\t+ \"SELECT ?c WHERE {?x a nif:Sentence . ?x rdfs:comment ?c}\";\n-\t\t\tQueryExecution qexec = QueryExecutionFactory.create(selectComments, m);\n-\t\t\tResultSet results = qexec.execSelect();\n-\t\t\twhile (results.hasNext()) {\n-\t\t\t\tString[] comments = results.next().getLiteral(\"c\").toString().split(\"\\\\\\\\n\");\n-\t\t\t\tfor (String comment : comments) {\n-\t\t\t\t\tif (comment.matches(\"^\\\\s?global\\\\.columns\\\\s?=.*\")) {\n-\t\t\t\t\t\tcolumnNames.addAll(Arrays.asList(comment.trim()\n-\t\t\t\t\t\t\t\t.replaceFirst(\"\\\\s?global\\\\.columns\\\\s?=\", \"\")\n-\t\t\t\t\t\t\t\t.trim().split(\" |\\t\")));\n-\t\t\t\t\t\tLOG.info(\"Found global columns comment in rdfs:comment\");\n-\t\t\t\t\t\treturn columnNames;\n-\t\t\t\t\t}\n+\t\tList<String> columnNames = new ArrayList<>();\n+\t\tModel m = ModelFactory.createDefaultModel().read(new StringReader(buffer), null, \"TTL\");\n+\t\tString selectComments = \"PREFIX nif: <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#>\\n\"\n+\t\t\t\t+ \"PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\\n\"\n+\t\t\t\t+ \"SELECT ?c WHERE {?x a nif:Sentence . ?x rdfs:comment ?c}\";\n+\t\tQueryExecution qexec = QueryExecutionFactory.create(selectComments, m);\n+\t\tResultSet results = qexec.execSelect();\n+\t\twhile (results.hasNext()) {\n+\t\t\tString[] comments = results.next().getLiteral(\"c\").toString().split(\"\\\\\\\\n\");\n+\t\t\tfor (String comment : comments) {\n+\t\t\t\tif (comment.matches(\"^\\\\s?global\\\\.columns\\\\s?=.*\")) {\n+\t\t\t\t\tcolumnNames.addAll(Arrays\n+\t\t\t\t\t\t\t.asList(comment.trim().replaceFirst(\"\\\\s?global\\\\.columns\\\\s?=\", \"\").trim().split(\" |\\t\")));\n+\t\t\t\t\tLOG.info(\"Found global columns comment in rdfs:comment\");\n+\t\t\t\t\treturn columnNames;\n \t\t\t\t}\n \t\t\t}\n-\t\t\treturn columnNames;\n \t\t}\n+\t\treturn columnNames;\n+\t}\n \n-\t\tpublic void processSentenceStream() throws IOException {\n-\t\t\tString line;\n-\t\t\tString lastLine =\"\";\n-\t\t\tString buffer=\"\";\n-\t\t\twhile((line = getInputStream().readLine())!=null) {\n-\t\t\t\tline=line.replaceAll(\"[\\t ]+\",\" \").trim();\n-\n-\t\t\t\tif(!buffer.trim().equals(\"\"))\n-\t\t\t\t\tif((line.startsWith(\"@\") || line.startsWith(\"#\")) && !lastLine.startsWith(\"@\") && !lastLine.startsWith(\"#\")) { //!buffer.matches(\"@[^\\n]*\\n?$\")) {\n-\t\t\t\t\t\tfor (Module m:modules) {\n-\t\t\t\t\t\t\tif(m.getMode()==Mode.CONLLRDF) m.getOutputStream().println(reorderTTLBuffer(buffer, m.getCols()));\n-\t\t\t\t\t\t\tif(m.getMode()==Mode.DEBUG) System.err.println(colorTTL(reorderTTLBuffer(buffer, m.getCols())));\n-\t\t\t\t\t\t\tif(m.getMode()==Mode.CONLL) {\n-\t\t\t\t\t\t\t\tif (m.getCols().size() < 1) {// no column args supplied\n-\t\t\t\t\t\t\t\t\tLOG.info(\"No column names in cmd args, searching rdf comments..\");\n-\t\t\t\t\t\t\t\t\tList<String> conllColumns = findColumnNamesInRDFBuffer(buffer);\n-\t\t\t\t\t\t\t\t\tif (conllColumns.size()>0) {\n-\t\t\t\t\t\t\t\t\t\tLOG.info(\"Using #global.comments from rdf\");\n+\t@Override\n+\tprotected void processSentenceStream() throws IOException {\n+\t\tString line;\n+\t\tString lastLine = \"\";\n+\t\tString buffer = \"\";\n+\t\twhile ((line = getInputStream().readLine()) != null) {\n+\t\t\tline = line.replaceAll(\"[\\t ]+\", \" \").trim();\n+\n+\t\t\tif (!buffer.trim().equals(\"\"))\n+\t\t\t\tif ((line.startsWith(\"@\") || line.startsWith(\"#\")) && !lastLine.startsWith(\"@\")\n+\t\t\t\t\t\t&& !lastLine.startsWith(\"#\")) { // !buffer.matches(\"@[^\\n]*\\n?$\")) {\n+\t\t\t\t\tfor (Module m : modules) {\n+\t\t\t\t\t\tif (m.getMode() == Mode.CONLLRDF)\n+\t\t\t\t\t\t\tm.getOutputStream().println(reorderTTLBuffer(buffer, m.getCols()));\n+\t\t\t\t\t\tif (m.getMode() == Mode.DEBUG)\n+\t\t\t\t\t\t\tSystem.err.println(colorTTL(reorderTTLBuffer(buffer, m.getCols())));\n+\t\t\t\t\t\tif (m.getMode() == Mode.CONLL) {\n+\t\t\t\t\t\t\tif (m.getCols().size() < 1) {// no column args supplied\n+\t\t\t\t\t\t\t\tLOG.info(\"No column names in cmd args, searching rdf comments..\");\n+\t\t\t\t\t\t\t\tList<String> conllColumns = findColumnNamesInRDFBuffer(buffer);\n+\t\t\t\t\t\t\t\tif (conllColumns.size() > 0) {\n+\t\t\t\t\t\t\t\t\tLOG.info(\"Using #global.comments from rdf\");\n+\t\t\t\t\t\t\t\t\tm.setCols(conllColumns);\n+\t\t\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\t\t\tLOG.info(\"Trying conll columns now..\");\n+\t\t\t\t\t\t\t\t\tconllColumns = CoNLLStreamExtractor.findFieldsFromComments(\n+\t\t\t\t\t\t\t\t\t\t\tnew BufferedReader(new StringReader(buffer.trim())), 1);\n+\t\t\t\t\t\t\t\t\tif (conllColumns.size() > 0) {\n \t\t\t\t\t\t\t\t\t\tm.setCols(conllColumns);\n-\t\t\t\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\t\t\t\tLOG.info(\"Trying conll columns now..\");\n-\t\t\t\t\t\t\t\t\t\tconllColumns = CoNLLStreamExtractor.findFieldsFromComments(new BufferedReader(new StringReader(buffer.trim())), 1);\n-\t\t\t\t\t\t\t\t\t\tif (conllColumns.size()>0) {\n-\t\t\t\t\t\t\t\t\t\t\tm.setCols(conllColumns);\n-\t\t\t\t\t\t\t\t\t\t}\n \t\t\t\t\t\t\t\t\t}\n \t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\tif (m.getCols().size() < 1) {\n-\t\t\t\t\t\t\t\t\tLOG.info(\"Supply column names some way! (-conll arg, global.columns or rdf comments\");\n-\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\telse\n-\t\t\t\t\t\t\t\t\tprintSparql(buffer, columnsAsSelect(m.getCols()), new OutputStreamWriter(m.getOutputStream()));\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\tif(m.getMode()==Mode.SPARQLTSV) printSparql(buffer, m.getSelect(), new OutputStreamWriter(m.getOutputStream()));\n-\t\t\t\t\t\t\tif(m.getMode()==Mode.GRAMMAR) m.getOutputStream().println(extractCoNLLGraph(buffer,true));\n-\t\t\t\t\t\t\tif(m.getMode()==Mode.SEMANTICS) m.getOutputStream().println(extractTermGraph(buffer,true));\n-\t\t\t\t\t\t\tif(m.getMode()==Mode.GRAMMAR_SEMANTICS) {\n-\t\t\t\t\t\t\t\tm.getOutputStream().println(extractCoNLLGraph(buffer,true));\n-\t\t\t\t\t\t\t\tm.getOutputStream().println(extractTermGraph(buffer,false));\n \t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\tif (m.getCols().size() < 1) {\n+\t\t\t\t\t\t\t\tLOG.info(\"Supply column names some way! (-conll arg, global.columns or rdf comments\");\n+\t\t\t\t\t\t\t} else\n+\t\t\t\t\t\t\t\tprintSparql(buffer, columnsAsSelect(m.getCols()),\n+\t\t\t\t\t\t\t\t\t\tnew OutputStreamWriter(m.getOutputStream()));\n+\t\t\t\t\t\t}\n+\t\t\t\t\t\tif (m.getMode() == Mode.SPARQLTSV)\n+\t\t\t\t\t\t\tprintSparql(buffer, m.getSelect(), new OutputStreamWriter(m.getOutputStream()));\n+\t\t\t\t\t\tif (m.getMode() == Mode.GRAMMAR)\n+\t\t\t\t\t\t\tm.getOutputStream().println(extractCoNLLGraph(buffer, true));\n+\t\t\t\t\t\tif (m.getMode() == Mode.SEMANTICS)\n+\t\t\t\t\t\t\tm.getOutputStream().println(extractTermGraph(buffer, true));\n+\t\t\t\t\t\tif (m.getMode() == Mode.GRAMMAR_SEMANTICS) {\n+\t\t\t\t\t\t\tm.getOutputStream().println(extractCoNLLGraph(buffer, true));\n+\t\t\t\t\t\t\tm.getOutputStream().println(extractTermGraph(buffer, false));\n \t\t\t\t\t\t}\n-\t\t\t\t\t\tbuffer=\"\";\n \t\t\t\t\t}\n-\t\t\t\t//System.err.println(ANSI_RED+\"> \"+line+ANSI_RESET);\n-\t\t\t\tif(line.trim().startsWith(\"@\") && !lastLine.trim().endsWith(\".\")) \n-\t\t\t\t\t//System.out.print(\"\\n\");\n-\t\t\t\t\tbuffer=buffer+\"\\n\";\n-\n-\t\t\t\tif(line.trim().startsWith(\"#\") && (!lastLine.trim().startsWith(\"#\"))) \n-\t\t\t\t\t// System.out.print(\"\\n\");\n-\t\t\t\t\tbuffer=buffer+\"\\n\";\n-\t\t\t\t\n-\t\t\t\t//System.out.print(\"  \"+color(line));\n-\t\t\t\t//System.out.print(color(line));\n-\t\t\t\tbuffer=buffer+line+\"\\t\";//+\"\\n\";\n-\n-\t\t\t\tif(line.trim().endsWith(\".\") || line.trim().matches(\"^(.*>)?[^<]*#\")) \n-\t\t\t\t\t//System.out.print(\"\\n\");\n-\t\t\t\t\tbuffer=buffer+\"\\n\";\n-\n-\t\t\t\t//System.out.println();\t\t\t\t\n-\t\t\t\tlastLine=line;\n-\t\t\t}\n-\t\t\t\n-\t\t\tfor (Module m:modules) {\n-\t\t\t\tif(m.getMode()==Mode.CONLLRDF) m.getOutputStream().println(reorderTTLBuffer(buffer, m.getCols()));\n-\t\t\t\tif(m.getMode()==Mode.DEBUG) System.err.println(colorTTL(reorderTTLBuffer(buffer, m.getCols())));\n-\t\t\t\tif(m.getMode()==Mode.CONLL) {\n-\t\t\t\t\tif (m.getCols().size() < 1) {\n-\t\t\t\t\t\tLOG.info(\"No column names in cmd args, searching rdf comments..\");\n-\t\t\t\t\t\tList<String> conllColumns = findColumnNamesInRDFBuffer(buffer);\n-\t\t\t\t\t\tif (conllColumns.size()>0) {\n-\t\t\t\t\t\t\tLOG.info(\"Using #global.comments from rdf\");\n+\t\t\t\t\tbuffer = \"\";\n+\t\t\t\t}\n+\t\t\t// System.err.println(ANSI_RED+\"> \"+line+ANSI_RESET);\n+\t\t\tif (line.trim().startsWith(\"@\") && !lastLine.trim().endsWith(\".\"))\n+\t\t\t\t// System.out.print(\"\\n\");\n+\t\t\t\tbuffer = buffer + \"\\n\";\n+\n+\t\t\tif (line.trim().startsWith(\"#\") && (!lastLine.trim().startsWith(\"#\")))\n+\t\t\t\t// System.out.print(\"\\n\");\n+\t\t\t\tbuffer = buffer + \"\\n\";\n+\n+\t\t\t// System.out.print(\" \"+color(line));\n+\t\t\t// System.out.print(color(line));\n+\t\t\tbuffer = buffer + line + \"\\t\";// +\"\\n\";\n+\n+\t\t\tif (line.trim().endsWith(\".\") || line.trim().matches(\"^(.*>)?[^<]*#\"))\n+\t\t\t\t// System.out.print(\"\\n\");\n+\t\t\t\tbuffer = buffer + \"\\n\";\n+\n+\t\t\t// System.out.println();\n+\t\t\tlastLine = line;\n+\t\t}\n+\n+\t\tfor (Module m : modules) {\n+\t\t\tif (m.getMode() == Mode.CONLLRDF)\n+\t\t\t\tm.getOutputStream().println(reorderTTLBuffer(buffer, m.getCols()));\n+\t\t\tif (m.getMode() == Mode.DEBUG)\n+\t\t\t\tSystem.err.println(colorTTL(reorderTTLBuffer(buffer, m.getCols())));\n+\t\t\tif (m.getMode() == Mode.CONLL) {\n+\t\t\t\tif (m.getCols().size() < 1) {\n+\t\t\t\t\tLOG.info(\"No column names in cmd args, searching rdf comments..\");\n+\t\t\t\t\tList<String> conllColumns = findColumnNamesInRDFBuffer(buffer);\n+\t\t\t\t\tif (conllColumns.size() > 0) {\n+\t\t\t\t\t\tLOG.info(\"Using #global.comments from rdf\");\n+\t\t\t\t\t\tm.setCols(conllColumns);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tLOG.info(\"Trying conll columns now..\");\n+\t\t\t\t\t\tconllColumns = CoNLLStreamExtractor\n+\t\t\t\t\t\t\t\t.findFieldsFromComments(new BufferedReader(new StringReader(buffer.trim())), 1);\n+\t\t\t\t\t\tif (conllColumns.size() > 0) {\n \t\t\t\t\t\t\tm.setCols(conllColumns);\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tLOG.info(\"Trying conll columns now..\");\n-\t\t\t\t\t\t\tconllColumns = CoNLLStreamExtractor.findFieldsFromComments(new BufferedReader(new StringReader(buffer.trim())), 1);\n-\t\t\t\t\t\t\tif (conllColumns.size()>0) {\n-\t\t\t\t\t\t\t\tm.setCols(conllColumns);\n-\t\t\t\t\t\t\t}\n \t\t\t\t\t\t}\n \t\t\t\t\t}\n-\t\t\t\t\tif (m.getCols().size() < 1)\n-\t\t\t\t\t\tthrow new IOException(\"-conll argument needs at least one COL to export!\");\n-\t\t\t\t\telse\n-\t\t\t\t\t\tprintSparql(buffer, columnsAsSelect(m.getCols()), new OutputStreamWriter(m.getOutputStream()));\n-\t\t\t\t}\n-\t\t\t\tif(m.getMode()==Mode.SPARQLTSV) printSparql(buffer, m.getSelect(), new OutputStreamWriter(m.getOutputStream()));\n-\t\t\t\tif(m.getMode()==Mode.GRAMMAR) m.getOutputStream().println(extractCoNLLGraph(buffer,true));\n-\t\t\t\tif(m.getMode()==Mode.SEMANTICS) m.getOutputStream().println(extractTermGraph(buffer,true));\n-\t\t\t\tif(m.getMode()==Mode.GRAMMAR_SEMANTICS) {\n-\t\t\t\t\tm.getOutputStream().println(extractCoNLLGraph(buffer,true));\n-\t\t\t\t\tm.getOutputStream().println(extractTermGraph(buffer,false));\n \t\t\t\t}\n+\t\t\t\tif (m.getCols().size() < 1)\n+\t\t\t\t\tthrow new IOException(\"-conll argument needs at least one COL to export!\");\n+\t\t\t\telse\n+\t\t\t\t\tprintSparql(buffer, columnsAsSelect(m.getCols()), new OutputStreamWriter(m.getOutputStream()));\n+\t\t\t}\n+\t\t\tif (m.getMode() == Mode.SPARQLTSV)\n+\t\t\t\tprintSparql(buffer, m.getSelect(), new OutputStreamWriter(m.getOutputStream()));\n+\t\t\tif (m.getMode() == Mode.GRAMMAR)\n+\t\t\t\tm.getOutputStream().println(extractCoNLLGraph(buffer, true));\n+\t\t\tif (m.getMode() == Mode.SEMANTICS)\n+\t\t\t\tm.getOutputStream().println(extractTermGraph(buffer, true));\n+\t\t\tif (m.getMode() == Mode.GRAMMAR_SEMANTICS) {\n+\t\t\t\tm.getOutputStream().println(extractCoNLLGraph(buffer, true));\n+\t\t\t\tm.getOutputStream().println(extractTermGraph(buffer, false));\n \t\t\t}\n \t\t}\n+\t}\n \n-\t\t@Override\n-\t\tpublic void run() {\n+\t//TODO move method @Override\n+\tpublic void configureFromCommandLine(String[] argv) throws IOException, ParseException {\n+\t\tLOG.info(\n+\t\t\t\"synopsis: CoNLLRDFFormatter [-rdf [COLS]] [-debug] [-grammar] [-semantics] [-conll COLS] [-sparqltsv SPARQL]\\n\"\n+\t\t\t\t\t+ \"\\t-rdf  write formatted CoNLL-RDF to stdout (sorted by list of CoNLL COLS, if provided)\\n\"\n+\t\t\t\t\t+ \"\\t-conll  write formatted CoNLL to stdout (only specified COLS)\\n\"\n+\t\t\t\t\t+ \"\\t-debug     write formatted, color-highlighted full turtle to stderr\\n\"\n+\t\t\t\t\t+ \"\\t-grammar   write CoNLL data structures to stdout\\n\"\n+\t\t\t\t\t+ \"\\t-semantics write semantic graph to stdout\\n\"\n+\t\t\t\t\t+ \"\\t-sparqltsv write TSV generated from SPARQL statement to stdout.\\n\"\n+\t\t\t\t\t+ \"\\t           if with -grammar, then skip type assignments\\n\"\n+\t\t\t\t\t+ \"read TTL from stdin => format CoNLL-RDF or extract and highlight CoNLL (namespace conll:) and semantic (namespace terms:) subgraphs\\n\"\n+\t\t\t\t\t+ \"if no parameters are supplied, -conllrdf is inferred\");\n+\t\tfinal String args = Arrays.asList(argv).toString().replaceAll(\"[\\\\[\\\\], ]+\", \" \").trim().toLowerCase();\n+\n+\t\tboolean CONLLRDF = args.contains(\"-rdf\");\n+\t\tfinal boolean CONLL = args.contains(\"-conll\");\n+\t\tfinal boolean DEBUG = args.contains(\"-debug\");\n+\t\tfinal boolean SPARQLTSV = args.contains(\"-sparqltsv\");\n+\t\tfinal boolean GRAMMAR = args.contains(\"-grammar\");\n+\t\tfinal boolean SEMANTICS = args.contains(\"-semantics\");\n+\n+\t\tModule module;\n+\n+\t\tif (!CONLLRDF && !CONLL && !SPARQLTSV && !GRAMMAR && !SEMANTICS && !DEBUG) { // default\n+\t\t\tCONLLRDF = true;\n+\t\t}\n+\n+\t\tif (GRAMMAR) {\n+\t\t\tmodule = new Module();\n+\t\t\tmodule.setMode(Mode.GRAMMAR);\n+\t\t\tmodule.setOutputStream(getOutputStream());\n+\t\t\tgetModules().add(module);\n+\t\t}\n+\t\tif (SEMANTICS) {\n+\t\t\tmodule = new Module();\n+\t\t\tmodule.setMode(Mode.SEMANTICS);\n+\t\t\tmodule.setOutputStream(getOutputStream());\n+\t\t\tgetModules().add(module);\n+\t\t}\n+\t\tif (DEBUG) {\n+\t\t\tmodule = new Module();\n+\t\t\tmodule.setMode(Mode.DEBUG);\n+\t\t\tmodule.setOutputStream(System.err);\n+\t\t\tgetModules().add(module);\n+\t\t}\n+\t\tif (CONLL) {\n+\t\t\tmodule = new Module();\n+\t\t\tmodule.setMode(Mode.CONLL);\n+\t\t\tmodule.setOutputStream(getOutputStream());\n+\t\t\tmodule.getCols().clear();\n+\t\t\tint i = 0;\n+\t\t\twhile (i < argv.length && argv[i].toLowerCase().matches(\"^-+conll$\"))\n+\t\t\t\ti++;\n+\t\t\twhile (i < argv.length && !argv[i].toLowerCase().matches(\"^-+.*$\"))\n+\t\t\t\tmodule.getCols().add(argv[i++]);\n+\t\t\tgetModules().add(module);\n+\t\t}\n+\t\tif (CONLLRDF) {\n+\t\t\tmodule = new Module();\n+\t\t\tmodule.setMode(Mode.CONLLRDF);\n+\t\t\tmodule.setOutputStream(getOutputStream());\n+\t\t\tmodule.getCols().clear();\n+\t\t\tint i = 0;\n+\t\t\twhile (i < argv.length && argv[i].toLowerCase().matches(\"^-+rdf$\"))\n+\t\t\t\ti++;\n+\t\t\twhile (i < argv.length && !argv[i].toLowerCase().matches(\"^-+.*$\"))\n+\t\t\t\tmodule.getCols().add(argv[i++]);\n+\t\t\tgetModules().add(module);\n+\t\t}\n+\t\tif (SPARQLTSV) {\n+\t\t\tmodule = new Module();\n+\t\t\tmodule.setMode(Mode.SPARQLTSV);\n+\t\t\tmodule.setOutputStream(getOutputStream());\n+\t\t\tString select = \"\";\n+\t\t\tint i = 1;\n+\t\t\twhile (i < argv.length && argv[i].toLowerCase().matches(\"^-+sparqltsv$\"))\n+\t\t\t\ti++;\n+\t\t\tif (i < argv.length)\n+\t\t\t\tselect = argv[i++];\n+\t\t\twhile (i < argv.length)\n+\t\t\t\tselect = select + \" \" + argv[i++]; // because queries may be parsed by the shell (Cygwin)\n+\n+\t\t\tReader sparqlreader = new StringReader(select);\n+\t\t\tFile file = new File(select);\n+\t\t\tURL u = null;\n \t\t\ttry {\n-\t\t\t\tprocessSentenceStream();\n-\t\t\t} catch (Exception e) {\n-\t\t\t\te.printStackTrace();\n-\t\t\t\tSystem.exit(0);\n+\t\t\t\tu = new URL(select);\n+\t\t\t} catch (MalformedURLException e) {\n+\t\t\t}\n+\n+\t\t\tif (file.exists()) { // can be read from a file\n+\t\t\t\tsparqlreader = new FileReader(file);\n+\t\t\t\tLOG.debug(\"f\");\n+\t\t\t} else if (u != null) {\n+\t\t\t\ttry {\n+\t\t\t\t\tsparqlreader = new InputStreamReader(u.openStream());\n+\t\t\t\t\tLOG.debug(\"u\");\n+\t\t\t\t} catch (Exception e) {\n+\t\t\t\t}\n \t\t\t}\n+\n+\t\t\tBufferedReader in = new BufferedReader(sparqlreader);\n+\t\t\tselect = \"\";\n+\t\t\tfor (String line = in.readLine(); line != null; line = in.readLine())\n+\t\t\t\tselect = select + line + \"\\n\";\n+\t\t\tmodule.setSelect(select);\n+\t\t\tgetModules().add(module);\n \t\t}\n+\t}\n \n-\t\t@Override\n-\t\tpublic void start() {\n-\t\t\trun();\n+\tpublic static void main(String[] args) throws IOException {\n+\t\tfinal CoNLLRDFFormatter formatter;\n+\t\ttry {\n+\t\t\tformatter = CoNLLRDFFormatterFactory.getFormatter(args);\n+\t\t} catch (ParseException e) {\n+\t\t\tLOG.error(e);\n+\t\t\tSystem.exit(1);\n+\t\t\treturn;\n \t\t}\n-}\n\\ No newline at end of file\n+\t\tformatter.processSentenceStream();\n+\t}\n+}"
  },
  {
    "sha": "e05d987836b37420fb63012a71574bdb17744817",
    "filename": "src/main/java/org/acoli/conll/rdf/CoNLLRDFFormatterFactory.java",
    "status": "added",
    "additions": 126,
    "deletions": 0,
    "changes": 126,
    "blob_url": "https://github.com/acoli-repo/conll-rdf/blob/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/CoNLLRDFFormatterFactory.java",
    "raw_url": "https://github.com/acoli-repo/conll-rdf/raw/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/CoNLLRDFFormatterFactory.java",
    "contents_url": "https://api.github.com/repos/acoli-repo/conll-rdf/contents/src/main/java/org/acoli/conll/rdf/CoNLLRDFFormatterFactory.java?ref=4f49b0dd21f0cfe1898b860851c18fcd891fdf13",
    "patch": "@@ -0,0 +1,126 @@\n+package org.acoli.conll.rdf;\n+\n+import java.io.*;\n+import java.net.MalformedURLException;\n+import java.net.URL;\n+import java.util.Arrays;\n+\n+import org.acoli.conll.rdf.CoNLLRDFFormatter.Mode;\n+import org.acoli.conll.rdf.CoNLLRDFFormatter.Module;\n+import org.apache.commons.cli.ParseException;\n+import org.apache.log4j.Logger;\n+\n+public class CoNLLRDFFormatterFactory {\n+    static Logger LOG = Logger.getLogger(CoNLLRDFFormatterFactory.class);\n+    public static CoNLLRDFFormatter getFormatter(String[] argv) throws IOException, ParseException {\n+        CoNLLRDFFormatter formatter = new CoNLLRDFFormatter();\n+\t\tLOG.info(\n+\t\t\t\"synopsis: CoNLLRDFFormatter [-rdf [COLS]] [-debug] [-grammar] [-semantics] [-conll COLS] [-sparqltsv SPARQL]\\n\"\n+\t\t\t\t\t+ \"\\t-rdf  write formatted CoNLL-RDF to stdout (sorted by list of CoNLL COLS, if provided)\\n\"\n+\t\t\t\t\t+ \"\\t-conll  write formatted CoNLL to stdout (only specified COLS)\\n\"\n+\t\t\t\t\t+ \"\\t-debug     write formatted, color-highlighted full turtle to stderr\\n\"\n+\t\t\t\t\t+ \"\\t-grammar   write CoNLL data structures to stdout\\n\"\n+\t\t\t\t\t+ \"\\t-semantics write semantic graph to stdout\\n\"\n+\t\t\t\t\t+ \"\\t-sparqltsv write TSV generated from SPARQL statement to stdout.\\n\"\n+\t\t\t\t\t+ \"\\t           if with -grammar, then skip type assignments\\n\"\n+\t\t\t\t\t+ \"read TTL from stdin => format CoNLL-RDF or extract and highlight CoNLL (namespace conll:) and semantic (namespace terms:) subgraphs\\n\"\n+\t\t\t\t\t+ \"if no parameters are supplied, -conllrdf is inferred\");\n+\t\tfinal String args = Arrays.asList(argv).toString().replaceAll(\"[\\\\[\\\\], ]+\", \" \").trim().toLowerCase();\n+\n+\t\tboolean CONLLRDF = args.contains(\"-rdf\");\n+\t\tfinal boolean CONLL = args.contains(\"-conll\");\n+\t\tfinal boolean DEBUG = args.contains(\"-debug\");\n+\t\tfinal boolean SPARQLTSV = args.contains(\"-sparqltsv\");\n+\t\tfinal boolean GRAMMAR = args.contains(\"-grammar\");\n+\t\tfinal boolean SEMANTICS = args.contains(\"-semantics\");\n+\n+\t\tModule module;\n+\n+\t\tif (!CONLLRDF && !CONLL && !SPARQLTSV && !GRAMMAR && !SEMANTICS && !DEBUG) { // default\n+\t\t\tCONLLRDF = true;\n+\t\t}\n+\n+\t\tif (GRAMMAR) {\n+\t\t\tmodule = new Module();\n+\t\t\tmodule.setMode(Mode.GRAMMAR);\n+\t\t\tmodule.setOutputStream(formatter.getOutputStream());\n+\t\t\tformatter.getModules().add(module);\n+\t\t}\n+\t\tif (SEMANTICS) {\n+\t\t\tmodule = new Module();\n+\t\t\tmodule.setMode(Mode.SEMANTICS);\n+\t\t\tmodule.setOutputStream(formatter.getOutputStream());\n+\t\t\tformatter.getModules().add(module);\n+\t\t}\n+\t\tif (DEBUG) {\n+\t\t\tmodule = new Module();\n+\t\t\tmodule.setMode(Mode.DEBUG);\n+\t\t\tmodule.setOutputStream(System.err);\n+\t\t\tformatter.getModules().add(module);\n+\t\t}\n+\t\tif (CONLL) {\n+\t\t\tmodule = new Module();\n+\t\t\tmodule.setMode(Mode.CONLL);\n+\t\t\tmodule.setOutputStream(formatter.getOutputStream());\n+\t\t\tmodule.getCols().clear();\n+\t\t\tint i = 0;\n+\t\t\twhile (i < argv.length && argv[i].toLowerCase().matches(\"^-+conll$\"))\n+\t\t\t\ti++;\n+\t\t\twhile (i < argv.length && !argv[i].toLowerCase().matches(\"^-+.*$\"))\n+\t\t\t\tmodule.getCols().add(argv[i++]);\n+\t\t\tformatter.getModules().add(module);\n+\t\t}\n+\t\tif (CONLLRDF) {\n+\t\t\tmodule = new Module();\n+\t\t\tmodule.setMode(Mode.CONLLRDF);\n+\t\t\tmodule.setOutputStream(formatter.getOutputStream());\n+\t\t\tmodule.getCols().clear();\n+\t\t\tint i = 0;\n+\t\t\twhile (i < argv.length && argv[i].toLowerCase().matches(\"^-+rdf$\"))\n+\t\t\t\ti++;\n+\t\t\twhile (i < argv.length && !argv[i].toLowerCase().matches(\"^-+.*$\"))\n+\t\t\t\tmodule.getCols().add(argv[i++]);\n+\t\t\tformatter.getModules().add(module);\n+\t\t}\n+\t\tif (SPARQLTSV) {\n+\t\t\tmodule = new Module();\n+\t\t\tmodule.setMode(Mode.SPARQLTSV);\n+\t\t\tmodule.setOutputStream(formatter.getOutputStream());\n+\t\t\tString select = \"\";\n+\t\t\tint i = 1;\n+\t\t\twhile (i < argv.length && argv[i].toLowerCase().matches(\"^-+sparqltsv$\"))\n+\t\t\t\ti++;\n+\t\t\tif (i < argv.length)\n+\t\t\t\tselect = argv[i++];\n+\t\t\twhile (i < argv.length)\n+\t\t\t\tselect = select + \" \" + argv[i++]; // because queries may be parsed by the shell (Cygwin)\n+\n+\t\t\tReader sparqlreader = new StringReader(select);\n+\t\t\tFile file = new File(select);\n+\t\t\tURL u = null;\n+\t\t\ttry {\n+\t\t\t\tu = new URL(select);\n+\t\t\t} catch (MalformedURLException e) {\n+\t\t\t}\n+\n+\t\t\tif (file.exists()) { // can be read from a file\n+\t\t\t\tsparqlreader = new FileReader(file);\n+\t\t\t\tLOG.debug(\"f\");\n+\t\t\t} else if (u != null) {\n+\t\t\t\ttry {\n+\t\t\t\t\tsparqlreader = new InputStreamReader(u.openStream());\n+\t\t\t\t\tLOG.debug(\"u\");\n+\t\t\t\t} catch (Exception e) {\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\tBufferedReader in = new BufferedReader(sparqlreader);\n+\t\t\tselect = \"\";\n+\t\t\tfor (String line = in.readLine(); line != null; line = in.readLine())\n+\t\t\t\tselect = select + line + \"\\n\";\n+\t\t\tmodule.setSelect(select);\n+\t\t\tformatter.getModules().add(module);\n+\t\t}\n+        return formatter;\n+\t}\n+}"
  },
  {
    "sha": "15b280b80bb775d7ae35beae53956d7a42fe9e5d",
    "filename": "src/main/java/org/acoli/conll/rdf/CoNLLRDFManager.java",
    "status": "modified",
    "additions": 6,
    "deletions": 5,
    "changes": 11,
    "blob_url": "https://github.com/acoli-repo/conll-rdf/blob/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/CoNLLRDFManager.java",
    "raw_url": "https://github.com/acoli-repo/conll-rdf/raw/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/CoNLLRDFManager.java",
    "contents_url": "https://api.github.com/repos/acoli-repo/conll-rdf/contents/src/main/java/org/acoli/conll/rdf/CoNLLRDFManager.java?ref=4f49b0dd21f0cfe1898b860851c18fcd891fdf13",
    "patch": "@@ -23,12 +23,13 @@\n \r\n import org.acoli.conll.rdf.CoNLLRDFFormatter.Mode;\r\n import org.acoli.conll.rdf.CoNLLRDFFormatter.Module;\r\n-import org.acoli.conll.rdf.CoNLLRDFUpdater.Triple;\r\n import org.apache.commons.cli.CommandLine;\r\n import org.apache.commons.cli.CommandLineParser;\r\n import org.apache.commons.cli.DefaultParser;\r\n import org.apache.commons.cli.Options;\r\n import org.apache.commons.cli.ParseException;\r\n+import org.apache.commons.lang3.tuple.ImmutableTriple;\r\n+import org.apache.commons.lang3.tuple.Triple;\r\n \r\n public class CoNLLRDFManager {\r\n \tprivate ObjectNode config;\r\n@@ -109,7 +110,7 @@ private PrintStream parseConfAsOutputStream(String confEntry) throws IOException\n \t\treturn output;\r\n \t}\r\n \r\n-\tpublic void buildComponentStack() throws IOException {\r\n+\tpublic void buildComponentStack() throws IOException, ParseException {\r\n \t\t//READ INPUT PARAMETER\r\n \t\tinput = parseConfAsInputStream(config.get(\"input\").asText());\r\n \r\n@@ -178,7 +179,7 @@ private CoNLLRDFComponent buildStreamExtractor(ObjectNode conf) throws IOExcepti\n \t\treturn ex;\r\n \t}\r\n \r\n-\tprivate CoNLLRDFComponent buildUpdater(ObjectNode conf) throws IOException {\r\n+\tprivate CoNLLRDFComponent buildUpdater(ObjectNode conf) throws IOException, ParseException {\r\n \r\n \t\t// READ THREAD PARAMETERS\r\n \t\tint threads = 0;\r\n@@ -228,7 +229,7 @@ private CoNLLRDFComponent buildUpdater(ObjectNode conf) throws IOException {\n \t\tif (conf.get(\"prefixDeduplication\") != null) {\r\n \t\t\tBoolean prefixDeduplication = conf.get(\"prefixDeduplication\").asBoolean();\r\n \t\t\tif (prefixDeduplication)\r\n-\t\t\t\tupdater.activateRemovePrefixDuplicates();\r\n+\t\t\t\tupdater.activatePrefixDeduplication();\r\n \t\t}\r\n \r\n \t\t// READ ALL UPDATES\r\n@@ -245,7 +246,7 @@ private CoNLLRDFComponent buildUpdater(ObjectNode conf) throws IOException {\n \t\t\t\t\tthrow e;\r\n \t\t\t}\r\n \t\t\tString path = update.get(\"path\").asText();\r\n-\t\t\tupdates.add(new Triple<String, String, String>(path, path, freq));\r\n+\t\t\tupdates.add(new ImmutableTriple<String, String, String>(path, path, freq));\r\n \t\t}\r\n \t\tupdater.parseUpdates(updates);\r\n \r"
  },
  {
    "sha": "d23c41a649355c61f325a15d0cc36cae367fa51d",
    "filename": "src/main/java/org/acoli/conll/rdf/CoNLLRDFUpdater.java",
    "status": "modified",
    "additions": 258,
    "deletions": 456,
    "changes": 714,
    "blob_url": "https://github.com/acoli-repo/conll-rdf/blob/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/CoNLLRDFUpdater.java",
    "raw_url": "https://github.com/acoli-repo/conll-rdf/raw/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/CoNLLRDFUpdater.java",
    "contents_url": "https://api.github.com/repos/acoli-repo/conll-rdf/contents/src/main/java/org/acoli/conll/rdf/CoNLLRDFUpdater.java?ref=4f49b0dd21f0cfe1898b860851c18fcd891fdf13",
    "patch": "@@ -15,17 +15,16 @@\n  */\r\n package org.acoli.conll.rdf;\r\n \r\n+import static org.acoli.conll.rdf.CoNLLRDFCommandLine.*;\r\n+\r\n import java.io.BufferedReader;\r\n import java.io.File;\r\n import java.io.FileNotFoundException;\r\n import java.io.FileOutputStream;\r\n-import java.io.FileReader;\r\n import java.io.IOException;\r\n import java.io.InputStreamReader;\r\n import java.io.OutputStreamWriter;\r\n import java.io.PrintStream;\r\n-import java.io.PrintWriter;\r\n-import java.io.Reader;\r\n import java.io.StringReader;\r\n import java.io.StringWriter;\r\n import java.io.UnsupportedEncodingException;\r\n@@ -44,11 +43,13 @@\n import java.util.zip.GZIPInputStream;\r\n \r\n import org.apache.commons.cli.CommandLine;\r\n-import org.apache.commons.cli.DefaultParser;\r\n-import org.apache.commons.cli.HelpFormatter;\r\n import org.apache.commons.cli.Option;\r\n-import org.apache.commons.cli.Options;\r\n import org.apache.commons.cli.ParseException;\r\n+import org.apache.commons.lang3.tuple.ImmutablePair;\r\n+import org.apache.commons.lang3.tuple.ImmutableTriple;\r\n+import org.apache.commons.lang3.tuple.MutableTriple;\r\n+import org.apache.commons.lang3.tuple.Pair;\r\n+import org.apache.commons.lang3.tuple.Triple;\r\n import org.apache.jena.query.Dataset;\r\n import org.apache.jena.query.DatasetFactory;\r\n import org.apache.jena.query.QueryParseException;\r\n@@ -59,68 +60,61 @@\n import org.apache.jena.update.UpdateAction;\r\n import org.apache.jena.update.UpdateFactory;\r\n import org.apache.jena.update.UpdateRequest;\r\n-import org.apache.log4j.Level;\r\n import org.apache.log4j.Logger;\r\n \r\n-\r\n /**\r\n  *  @author Christian Chiarcos {@literal chiarcos@informatik.uni-frankfurt.de}\r\n  *  @author Christian Faeth {@literal faeth@em.uni-frankfurt.de}\r\n  */\r\n public class CoNLLRDFUpdater extends CoNLLRDFComponent {\r\n+\tstatic final Logger LOG = Logger.getLogger(CoNLLRDFUpdater.class);\r\n \t\r\n-\t@SuppressWarnings(\"serial\")\r\n-\tprivate static final List<Integer> CHECKINTERVAL = new ArrayList<Integer>() {{add(3); add(10); add(25); add(50); add(100); add(200); add(500);}};\r\n-\tstatic final int MAXITERATE = 999; // maximum update iterations allowed until the update loop is cancelled and an error message is thrown - to prevent faulty update scripts running in an endless loop\r\n-\tprivate static final Logger LOG = Logger.getLogger(CoNLLRDFUpdater.class.getName());\r\n-\tprivate static final String DEFAULTUPDATENAME = \"DIRECTUPDATE\";\r\n-\t\r\n-\tpublic static class Pair<F, S> {\r\n-\t\tpublic F key;\r\n-\t\tpublic S value;\r\n-\t\tpublic Pair (F key, S value) {\r\n-\t\t\tthis.key = key;\r\n-\t\t\tthis.value = value;\r\n-\t\t}\r\n-\t\tpublic F getKey() {\r\n-\t\t\treturn key;\r\n-\t\t}\r\n-\t\tpublic void setKey(F key) {\r\n-\t\t\tthis.key = key;\r\n-\t\t}\r\n-\t\tpublic S getValue() {\r\n-\t\t\treturn value;\r\n-\t\t}\r\n-\t\tpublic void setValue(S value) {\r\n-\t\t\tthis.value = value;\r\n-\t\t}\r\n-\t}\r\n-\t\r\n-\tpublic static class Triple<F, S, M> {\r\n-\t\tpublic F first;\r\n-\t\tpublic S second;\r\n-\t\tpublic M third;\r\n-\t\tpublic Triple (F first, S second, M third) {\r\n-\t\t\tthis.first = first;\r\n-\t\t\tthis.second = second;\r\n-\t\t\tthis.third = third;\r\n-\t\t}\r\n-\t}\r\n-\t\r\n-\t\r\n+\tprivate final Dataset dataset;\r\n+\r\n+\t// Configuration Variables with defaults set\r\n+\tprivate boolean prefixDeduplication = false;\r\n+\tprivate int threads = 0;\r\n+\tprivate int lookahead_snts = 0;\r\n+\tprivate int lookback_snts = 0;\r\n+\tprivate File graphOutputDir = null;\r\n+\tprivate File triplesOutputDir = null;\r\n+\r\n+\t//for updates\r\n+\tprivate final List<Triple<String, String, String>> updates = Collections.synchronizedList(new ArrayList<Triple<String, String, String>>());\r\n+\t//For graphsout and triplesout\r\n+\tprivate final List<String> graphOutputSentences = Collections.synchronizedList(new ArrayList<String>());\r\n+\tprivate final List<String> triplesOutputSentences = Collections.synchronizedList(new ArrayList<String>());\r\n+\r\n+\t// for thread handling\r\n+\tprivate boolean running = false;\r\n+\tprivate final List<UpdateThread> updateThreads = Collections.synchronizedList(new ArrayList<UpdateThread>());\r\n+\t// Buffer providing each thread with its respective sentence(s) to process\r\n+\t// <List:lookbackBuffer>, <String:currentSentence>, <List:lookaheadBuffer>\r\n+\tprivate final List<Triple<List<String>, String, List<String>>> sentBufferThreads = Collections.synchronizedList(new ArrayList<Triple<List<String>, String, List<String>>>());\r\n+\r\n+\tprivate final List<String> sentBufferLookahead = Collections.synchronizedList(new ArrayList<String>());\r\n+\tprivate final List<String> sentBufferLookback = Collections.synchronizedList(new ArrayList<String>());\r\n+\t// Buffer for outputting sentences in original order\r\n+\tprivate final List<String> sentBufferOut = Collections.synchronizedList(new ArrayList<String>()); \r\n+\r\n+\t//for statistics\r\n+\tprivate final List<List<Pair<Integer,Long>>> dRTs = Collections.synchronizedList(new ArrayList<List<Pair<Integer,Long>>>());\r\n+\t// iterations and execution time of each update in seconds\r\n+\r\n+\r\n \tprivate class UpdateThread extends Thread {\r\n-\t\t\r\n+\r\n \t\tprivate CoNLLRDFUpdater updater;\r\n \t\tprivate int threadID;\r\n \t\tprivate Dataset memDataset;\r\n-\t\t\r\n+\r\n \t\t/**\r\n \t\t * Each UpdateThread receives its own ID and a back-reference to the calling Updater.\r\n-\t\t * \r\n+\t\t *\r\n \t\t * In the current implementation, each thread manages its own in-memory Dataset.\r\n \t\t * This is the fastest approach since no concurring access on a single Datasets occurs.\r\n \t\t * However: lots of RAM may be needed.\r\n-\t\t * \r\n+\t\t *\r\n \t\t * @param updater\r\n \t\t * \t\t\t\tThe calling Updater (= ThreadHandler)\r\n \t\t * @param id\r\n@@ -138,7 +132,7 @@ public UpdateThread(CoNLLRDFUpdater updater, int id) {\n \t\t\tmemDataset.addNamedModel(\"https://github.com/acoli-repo/conll-rdf/lookback\", ModelFactory.createDefaultModel());\r\n \t\t\tmemDataset.addNamedModel(\"https://github.com/acoli-repo/conll-rdf/lookahead\", ModelFactory.createDefaultModel());\r\n \t\t}\r\n-\t\t\r\n+\r\n \t\t/**\r\n \t\t * Run the update thread.\r\n \t\t * Load the buffer, execute the updates with all iterations and graphsout, unload the buffer.\r\n@@ -152,16 +146,16 @@ public void run() {\n \t\t\t\tStringWriter out = new StringWriter();\r\n \t\t\t\ttry {\r\n \t\t\t\t\tloadBuffer(sentBufferThread);\r\n-\t\t\t\t\t\r\n+\r\n \t\t\t\t\tList<Pair<Integer,Long> > ret = executeUpdates(updates);\r\n \t\t\t\t\tif (dRTs.get(threadID).isEmpty())\r\n \t\t\t\t\t\tdRTs.get(threadID).addAll(ret);\r\n \t\t\t\t\telse\r\n \t\t\t\t\t\tfor (int x = 0; x < ret.size(); ++x)\r\n-\t\t\t\t\t\t\tdRTs.get(threadID).set(x, new Pair<Integer, Long>(\r\n-\t\t\t\t\t\t\t\t\tdRTs.get(threadID).get(x).getKey() + ret.get(x).getKey(), \r\n+\t\t\t\t\t\t\tdRTs.get(threadID).set(x, new ImmutablePair<Integer, Long>(\r\n+\t\t\t\t\t\t\t\t\tdRTs.get(threadID).get(x).getKey() + ret.get(x).getKey(),\r\n \t\t\t\t\t\t\t\t\tdRTs.get(threadID).get(x).getValue() + ret.get(x).getValue()));\r\n-\t\t\t\t\t\r\n+\r\n \t\t\t\t\tunloadBuffer(sentBufferThread, out);\r\n \t\t\t\t} catch (Exception e) {\r\n //\t\t\t\t\tmemDataset.begin(ReadWrite.WRITE);\r\n@@ -182,12 +176,11 @@ public void run() {\n \t\t\t\t\t\tsentBufferOut.set(i, out.toString());\r\n \t\t\t\t\t\tbreak;\r\n \t\t\t\t\t}\r\n-\t\t\t\t}\t\t\t\t\r\n-\t\t\t\t\r\n+\t\t\t\t}\r\n+\r\n \t\t\t\t//go to sleep and let Updater take control\r\n \t\t\t\t\tLOG.trace(\"Updater notified by \"+threadID);\r\n \t\t\t\t\tupdater.notify();\r\n-\r\n \t\t\t\t}\r\n \t\t\t\ttry {\r\n \t\t\t\t\tsynchronized (this) {\r\n@@ -200,44 +193,43 @@ public void run() {\n \t\t\t\t}\r\n \t\t\t}\r\n \t\t}\r\n-\t\t\r\n+\r\n \t\t/**\r\n \t\t * Loads Data to this thread's working model.\r\n-\t\t * @param buffer \r\n+\t\t * @param buffer\r\n \t\t * \t\t\tthe model to be read.\r\n \t\t * @throws Exception\r\n \t\t */\r\n \t\tprivate void loadBuffer(Triple<List<String>, String, List<String>> sentBufferThread) throws Exception { //TODO: adjust for TXN-Models\r\n \t\t\t//check validity of current sentence\r\n-\t\t\tisValidUTF8(sentBufferThread.second, \"Input data encoding issue for \\\"\" + sentBufferThread.second + \"\\\"\");\r\n+\t\t\tisValidUTF8(sentBufferThread.getMiddle(), \"Input data encoding issue for \\\"\" + sentBufferThread.getMiddle() + \"\\\"\");\r\n \t\t\t//load ALL\r\n \t\t\ttry {\r\n //\t\t\t\tmemDataset.begin(ReadWrite.WRITE);\r\n-\t\t\t\t\r\n+\r\n \t\t\t\t// for lookback\r\n-\t\t\t\tfor (String sent:sentBufferThread.first) {\r\n+\t\t\t\tfor (String sent:sentBufferThread.getLeft()) {\r\n \t\t\t\t\tmemDataset.getNamedModel(\"https://github.com/acoli-repo/conll-rdf/lookback\").read(new StringReader(sent),null, \"TTL\");\r\n \t\t\t\t}\r\n-\t\t\t\t\r\n+\r\n \t\t\t\t// for current sentence\r\n-\t\t\t\tmemDataset.getDefaultModel().read(new StringReader(sentBufferThread.second),null, \"TTL\");\r\n-\t\t\t\t\r\n+\t\t\t\tmemDataset.getDefaultModel().read(new StringReader(sentBufferThread.getMiddle()),null, \"TTL\");\r\n+\r\n \t\t\t\t// for lookahead\r\n-\t\t\t\tfor (String sent:sentBufferThread.third) {\r\n+\t\t\t\tfor (String sent:sentBufferThread.getRight()) {\r\n \t\t\t\t\tmemDataset.getNamedModel(\"https://github.com/acoli-repo/conll-rdf/lookahead\").read(new StringReader(sent),null, \"TTL\");\r\n \t\t\t\t}\r\n-\t\t\t\t\r\n+\r\n //\t\t\t\tmemDataset.commit();\r\n //\t\t\t\tModel m = ModelFactory.createDefaultModel().read(new StringReader(buffer),null, \"TTL\");\r\n //\t\t\t\tmemAccessor.add(m);\r\n //\t\t\t\tmemDataset.getDefaultModel().setNsPrefixes(m.getNsPrefixMap());\r\n \t\t\t} catch (Exception ex) {\r\n-\t\t\t\tLOG.error(\"Exception while reading: \" + sentBufferThread.second);\r\n+\t\t\t\tLOG.error(\"Exception while reading: \" + sentBufferThread.getMiddle());\r\n \t\t\t\tthrow ex;\r\n \t\t\t} finally {\r\n //\t\t\t\tmemDataset.end();\r\n \t\t\t}\r\n-\t\t\t\r\n \t\t}\r\n \r\n \t\t/**\r\n@@ -250,7 +242,7 @@ private void loadBuffer(Triple<List<String>, String, List<String>> sentBufferThr\n \t\t * @throws Exception\r\n \t\t */\r\n \t\tprivate void unloadBuffer(Triple<List<String>, String, List<String>> sentBufferThread, Writer out) throws Exception { //TODO: adjust for TXN-Models\r\n-\t\t\tString buffer = sentBufferThread.second;\r\n+\t\t\tString buffer = sentBufferThread.getMiddle();\r\n \t\t\ttry {\r\n \t\t\t\tBufferedReader in = new BufferedReader(new StringReader(buffer));\r\n \t\t\t\tString line;\r\n@@ -272,12 +264,11 @@ private void unloadBuffer(Triple<List<String>, String, List<String>> sentBufferT\n //\t\t\t\tmemDataset.commit();\r\n //\t\t\t\tmemDataset.end();\r\n \t\t\t}\r\n-\t\t\t\r\n \t\t}\r\n-\t\t\r\n+\r\n \t\t/**\r\n \t\t * Executes updates on this thread. Data must be preloaded first.\r\n-\t\t * \r\n+\t\t *\r\n \t\t * @param updates\r\n \t\t * \t\t\tThe updates as a List of Triples containing\r\n \t\t * \t\t\t- update filename\r\n@@ -288,15 +279,15 @@ private void unloadBuffer(Triple<List<String>, String, List<String>> sentBufferT\n \t\t * \t\t\t- total no. of iterations\r\n \t\t * \t\t\t- total time\r\n \t\t */\r\n-\t\tprivate List<Pair<Integer, Long>> executeUpdates(List<Triple<String, String, String>> updates) { \r\n+\t\tprivate List<Pair<Integer, Long>> executeUpdates(List<Triple<String, String, String>> updates) {\r\n \r\n \t\t\tString sent = new String();\r\n \t\t\tboolean graphsout = false;\r\n \t\t\tboolean triplesout = false;\r\n \t\t\tif (graphOutputDir != null || triplesOutputDir != null) {\r\n \t\t\t\ttry {\r\n \t\t\t\tsent = memDataset.getDefaultModel().listSubjectsWithProperty(\r\n-\t\t\t\t\t\t\t\tmemDataset.getDefaultModel().getProperty(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\"), \r\n+\t\t\t\t\t\t\t\tmemDataset.getDefaultModel().getProperty(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\"),\r\n \t\t\t\t\t\t\t\tmemDataset.getDefaultModel().getProperty(\"http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#Sentence\")\r\n \t\t\t\t\t\t\t).next().getLocalName();\r\n \t\t\t\t} catch (Exception e) {\r\n@@ -335,15 +326,15 @@ private void unloadBuffer(Triple<List<String>, String, List<String>> sentBufferT\n \t\t\t\tint frq = MAXITERATE, v = 0;\r\n \t\t\t\tboolean change = true;\r\n \t\t\t\ttry {\r\n-\t\t\t\t\tfrq = Integer.parseInt(update.third);\r\n+\t\t\t\t\tfrq = Integer.parseInt(update.getRight());\r\n \t\t\t\t} catch (NumberFormatException e) {\r\n-\t\t\t\t\tif (!\"*\".equals(update.third))\r\n+\t\t\t\t\tif (!\"*\".equals(update.getRight()))\r\n \t\t\t\t\t\tthrow e;\r\n \t\t\t\t}\r\n \t\t\t\twhile(v < frq && change) {\r\n \t\t\t\t\ttry {\r\n \t\t\t\t\t\tUpdateRequest updateRequest;\r\n-\t\t\t\t\t\tupdateRequest = UpdateFactory.create(update.second);\r\n+\t\t\t\t\t\tupdateRequest = UpdateFactory.create(update.getMiddle());\r\n \t\t\t\t\t\tif (graphsout || triplesout) { //execute Update-block step by step and output intermediate results\r\n \t\t\t\t\t\t\tint step = 1;\r\n \t\t\t\t\t\t\tModel dM = memDataset.getDefaultModel();\r\n@@ -357,15 +348,15 @@ private void unloadBuffer(Triple<List<String>, String, List<String>> sentBufferT\n \t\t\t\t\t\t\t\t//\t\t\t\t\t\t\tmemDataset.end();\r\n \t\t\t\t\t\t\t\tif (cLdM.hasChanged() && (!dMS.equals(memDataset.getDefaultModel().toString()))) {\r\n \t\t\t\t\t\t\t\t\tif (graphsout) try {\r\n-\t\t\t\t\t\t\t\t\t\tproduceDot(defaultModel, update.first, operation.toString(), sent, upd_id, iter_id, step);\r\n+\t\t\t\t\t\t\t\t\t\tproduceDot(defaultModel, update.getLeft(), operation.toString(), sent, upd_id, iter_id, step);\r\n \t\t\t\t\t\t\t\t\t} catch (IOException e) {\r\n-\t\t\t\t\t\t\t\t\t\tLOG.error(\"Error while producing DOT for update No. \"+upd_id+\": \"+update.first);\r\n+\t\t\t\t\t\t\t\t\t\tLOG.error(\"Error while producing DOT for update No. \"+upd_id+\": \"+update.getLeft());\r\n \t\t\t\t\t\t\t\t\t\te.printStackTrace();\r\n \t\t\t\t\t\t\t\t\t}\r\n \t\t\t\t\t\t\t\t\tif (triplesout) try {\r\n-\t\t\t\t\t\t\t\t\t\tproduceNTRIPLES(defaultModel, update.first, operation.toString(), sent, upd_id, iter_id, step);\r\n+\t\t\t\t\t\t\t\t\t\tproduceNTRIPLES(defaultModel, update.getLeft(), operation.toString(), sent, upd_id, iter_id, step);\r\n \t\t\t\t\t\t\t\t\t} catch (IOException e) {\r\n-\t\t\t\t\t\t\t\t\t\tLOG.error(\"Error while producing NTRIPLES for update No. \"+upd_id+\": \"+update.first);\r\n+\t\t\t\t\t\t\t\t\t\tLOG.error(\"Error while producing NTRIPLES for update No. \"+upd_id+\": \"+update.getLeft());\r\n \t\t\t\t\t\t\t\t\t\te.printStackTrace();\r\n \t\t\t\t\t\t\t\t\t}\r\n \t\t\t\t\t\t\t\t}\r\n@@ -378,11 +369,10 @@ private void unloadBuffer(Triple<List<String>, String, List<String>> sentBufferT\n \t\t\t\t\t\t\t//\t\t\t\t\t\tmemDataset.end();\r\n \t\t\t\t\t\t}\r\n \t\t\t\t\t} catch (Exception e) {\r\n-\t\t\t\t\t\tLOG.error(\"Error while processing update No. \"+upd_id+\": \"+update.first);\r\n+\t\t\t\t\t\tLOG.error(\"Error while processing update No. \"+upd_id+\": \"+update.getLeft());\r\n \t\t\t\t\t\te.printStackTrace();\r\n \t\t\t\t\t}\r\n-\t\t\t\t\t\r\n-\t\t\t\t\t\r\n+\r\n \t\t\t\t\tif (oldModel.isEmpty()) {\r\n \t\t\t\t\t\tchange = cL.hasChanged();\r\n \t\t\t\t\t\tLOG.trace(\"cl.hasChanged(): \"+change);\r\n@@ -396,17 +386,17 @@ private void unloadBuffer(Triple<List<String>, String, List<String>> sentBufferT\n \t\t\t\t\titer_id++;\r\n \t\t\t\t}\r\n \t\t\t\tif (v == MAXITERATE)\r\n-\t\t\t\t\tLOG.warn(\"Warning: MAXITERATE reached for \" + update.first + \".\");\r\n-\t\t\t\tresult.add(new Pair<Integer, Long>(v, System.currentTimeMillis() - startTime));\r\n+\t\t\t\t\tLOG.warn(\"Warning: MAXITERATE reached for \" + update.getLeft() + \".\");\r\n+\t\t\t\tresult.add(new ImmutablePair<Integer, Long>(v, System.currentTimeMillis() - startTime));\r\n \t\t\t\tdefaultModel.unregister(cL);\r\n \t\t\t\tupd_id++;\r\n-\t\t\t}\t\t\t\r\n+\t\t\t}\r\n \t\t\treturn result;\r\n \t\t}\r\n-\t\t\r\n+\r\n \t\t/**\r\n \t\t * Produce dotFile for a specific update iteration.\r\n-\t\t * \r\n+\t\t *\r\n \t\t * @param m\r\n \t\t * \t\t\tThe current model.\r\n \t\t * @param updateSrc\r\n@@ -427,20 +417,20 @@ private void produceDot(Model m, String updateSrc, String updateQuery, String se\n \t\t\tif (graphOutputDir != null) {\r\n \t\t\t\tString updateName = (new File(updateSrc)).getName();\r\n \t\t\t\tupdateName = (updateName != null && !updateName.isEmpty()) ? updateName : UUID.randomUUID().toString();\r\n-\t\t\t\t\r\n+\r\n \t\t\t\tFile outputFile = new File(graphOutputDir, sent\r\n \t\t\t\t\t\t\t\t+\"__U\"+String.format(\"%03d\", upd_id)\r\n \t\t\t\t\t\t\t\t+\"_I\" +String.format(\"%04d\", iter_id)\r\n \t\t\t\t\t\t\t\t+\"_S\" +String.format(\"%03d\", step)\r\n \t\t\t\t\t\t\t\t+\"__\" +updateName.replace(\".sparql\", \"\")+\".dot\");\r\n \t\t\t\tWriter w = new OutputStreamWriter(new FileOutputStream(outputFile), StandardCharsets.UTF_8);\r\n \t\t\t\tCoNLLRDFViz.produceDot(m, w, updateQuery);\r\n-\t\t\t}\t\t\r\n+\t\t\t}\r\n \t\t}\r\n-\t\t\r\n+\r\n \t\t/**\r\n \t\t * Produce lexicographically sorted ntriples-file for a specific update iteration.\r\n-\t\t * \r\n+\t\t *\r\n \t\t * @param m\r\n \t\t * \t\t\tThe current model.\r\n \t\t * @param updateSrc\r\n@@ -461,7 +451,7 @@ private void produceNTRIPLES(Model m, String updateSrc, String updateQuery, Stri\n \t\t\tif (triplesOutputDir != null) {\r\n \t\t\t\tString updateName = (new File(updateSrc)).getName();\r\n \t\t\t\tupdateName = (updateName != null && !updateName.isEmpty()) ? updateName : UUID.randomUUID().toString();\r\n-\t\t\t\t\r\n+\r\n \t\t\t\tFile outputFile = new File(triplesOutputDir, sent\r\n \t\t\t\t\t\t\t\t+\"__U\"+String.format(\"%03d\", upd_id)\r\n \t\t\t\t\t\t\t\t+\"_I\" +String.format(\"%04d\", iter_id)\r\n@@ -480,69 +470,30 @@ private void produceNTRIPLES(Model m, String updateSrc, String updateQuery, Stri\n \t\t\t\t}\r\n \t\t\t\tout.flush();\r\n \t\t\t\tout.close();\r\n-\t\t\t}\t\t\r\n+\t\t\t}\r\n \t\t}\r\n \t}\r\n-\t\r\n-\t\r\n-\tprivate final Dataset dataset;\r\n-\t\r\n-\t//for segmented data with single prefix header\r\n-\tprivate String prefixCache;\r\n-\tprivate String prefixCacheOut;\r\n-\tprivate boolean removePrefixDuplicates;\r\n-\t\r\n-\t//for updates\r\n-\tprivate final List<Triple<String, String, String>> updates;\r\n-\t\r\n-\t//for thread handling\r\n-\tprivate final List<UpdateThread> updateThreads;\r\n-\tprivate final List<String> sentBufferOut; //Buffer for outputting sentences in original order\r\n-\t// Buffer providing each thread with its respective sentence(s) to process\r\n-\t// <List:lookbackBuffer>, <String:currentSentence>, <List:lookaheadBuffer>\r\n-\tprivate final List<Triple<List<String>, String, List<String>>> sentBufferThreads; \r\n \r\n \r\n-\t//For lookahead\r\n-\tprivate final List<String> sentBufferLookahead; \r\n-\tprivate int lookahead_snts = 0;\r\n-\t\r\n-\t//For lookback\r\n-\tprivate final List<String> sentBufferLookback; \r\n-\tprivate int lookback_snts = 0;\r\n-\t\r\n-\t//For graphsout\r\n-\tprivate final List<String> graphOutputSentences; \r\n-\tprivate File graphOutputDir;\r\n-\t\r\n-\t//For triplesout\r\n-\tprivate final List<String> triplesOutputSentences; \r\n-\tprivate File triplesOutputDir;\r\n-\t\r\n-\t//for statistics\r\n-\tprivate final List<List<Pair<Integer,Long>>> dRTs; // iterations and execution time of each update in seconds\r\n-\t//private int parsedSentences = 0; // no longer used since graphsout default is set at sentence readin\r\n-\tprivate boolean running = false;\r\n-\r\n \t/**\r\n \t * Default Constructor providing empty data to the standard constructor.\r\n \t */\r\n \tpublic CoNLLRDFUpdater() {\r\n \t\tthis(\"\", \"\", 0);\r\n \t}\r\n-\t\r\n+\r\n \t/**\r\n \t * Standard Constructor for Updater. Creates Threads and Buffers for Thread handling.\r\n \t * Also creates the database modules for the respective execution modes.\r\n \t * @param type: The type of database to be used:\r\n-\t * \t\t\t\tMEM: fully independent in-memory datasets per thread \r\n+\t * \t\t\t\tMEM: fully independent in-memory datasets per thread\r\n \t * \t\t\t\t\t\t(fastest, no transactions, high RAM usage, no HDD)\r\n \t * \t\t\t\tTXN: single transactional in-memory dataset for all threads\r\n \t * \t\t\t\t\t\t(in development, medium speed and RAM, no HDD)\r\n \t * \t\t\t\tTDB2: single transactional TDB2-database for all threads\r\n \t * \t\t\t\t\t\t(in development, slow-medium speed, low RAM usage, high HDD usage)\r\n \t * \t\t\t\tdefault: MEM\r\n-\t * @param path: \r\n+\t * @param path:\r\n \t * \t\t\t\tpath to database (only for TDB2 or other DB-backed modes)\r\n \t * @param threads\r\n \t * \t\t\t\tMaximum amount of threads for execution.\r\n@@ -559,50 +510,28 @@ public CoNLLRDFUpdater(String type, String path, int threads) {\n \t\t}\r\n //\t\tmemAccessor = DatasetAccessorFactory.create(memDataset);\r\n \r\n-\t\t//for segmented data with single prefix header\r\n-\t\tprefixCache = new String();\r\n-\t\tprefixCacheOut = new String();\r\n-\t\tremovePrefixDuplicates = false;\r\n+\t\tsetThreads(threads);\r\n \r\n-\t\t//updates\r\n-\t\tupdates = Collections.synchronizedList(new ArrayList<Triple<String, String, String>>());\r\n-\r\n-\t\t//threads\r\n-\t\t// Use the processor cores available to runtime (but at least 1) as thread count, if an invalid thread count is provided.\r\n-\t\tif (threads <= 0) {\r\n-\t\t\tthreads = (Runtime.getRuntime().availableProcessors()>0)?(Runtime.getRuntime().availableProcessors()):(1);\r\n-\t\t\tLOG.info(\"Falling back to default thread maximum.\");\r\n-\t\t}\r\n-\t\tLOG.info(\"Executing on \"+threads+\" processor cores, max.\");\r\n-\t\tupdateThreads = Collections.synchronizedList(new ArrayList<UpdateThread>());\r\n-\t\tsentBufferThreads = Collections.synchronizedList(new ArrayList<Triple<List<String>, String, List<String>>>());\r\n-\t\tdRTs = Collections.synchronizedList(new ArrayList<List<Pair<Integer,Long>>>());\r\n-\t\tfor (int i = 0; i < threads; i++) {\r\n-\t\t\tupdateThreads.add(null);\r\n-\t\t\tdataset.addNamedModel(\"http://thread\"+i, ModelFactory.createDefaultModel());\r\n-\t\t\tsentBufferThreads.add(new Triple<List<String>, String, List<String>>(\r\n-\t\t\t\t\tnew ArrayList<String>(), new String(), new ArrayList<String>()));\r\n-\t\t\tdRTs.add(Collections.synchronizedList(new ArrayList<Pair<Integer,Long> >()));\r\n-\t\t}\r\n-\t\tsentBufferOut = Collections.synchronizedList(new ArrayList<String>());\r\n-\t\t\r\n-\t\t//lookahead+lookback\r\n-\t\tsentBufferLookahead = Collections.synchronizedList(new ArrayList<String>());\r\n-\t\tsentBufferLookback = Collections.synchronizedList(new ArrayList<String>());\r\n-\t\t\r\n-\t\t//graphsout\r\n-\t\tgraphOutputSentences = Collections.synchronizedList(new ArrayList<String>());\r\n-\t\tgraphOutputDir = null;\r\n-\t\t\r\n-\t\t//triplesout\r\n-\t\ttriplesOutputSentences = Collections.synchronizedList(new ArrayList<String>());\r\n-\t\ttriplesOutputDir = null;\r\n-\t\t\r\n-\t\t//runtime\r\n-\t\t//parsedSentences = 0;\r\n \t\trunning = false;\r\n \t}\r\n \r\n+\tpublic void setThreads(int threads) {\r\n+\t\tthis.threads = threads;\r\n+\t}\r\n+\tpublic int getThreads() {\r\n+\t\treturn threads;\r\n+\t}\r\n+\r\n+\tpublic String[] getUpdateNames() {\r\n+\t\treturn updates.stream().map(t -> t.getLeft()).toArray(String[]::new);\r\n+\t}\r\n+\tpublic String[] getUpdateStrings() {\r\n+\t\treturn updates.stream().map(t -> t.getMiddle()).toArray(String[]::new);\r\n+\t}\r\n+\tpublic String[] getUpdateMaxIterations() {\r\n+\t\treturn updates.stream().map(t -> t.getRight()).toArray(String[]::new);\r\n+\t}\r\n+\r\n \t/**\r\n \t * Activates the lookahead mode for caching a fixed number of additional sentences per thread.\r\n \t * @param lookahead_snts\r\n@@ -612,7 +541,10 @@ public void activateLookahead(int lookahead_snts) {\n \t\tif (lookahead_snts < 0) lookahead_snts = 0;\r\n \t\tthis.lookahead_snts = lookahead_snts;\r\n \t}\r\n-\t\r\n+\tpublic int getLookahead() {\r\n+\t\treturn lookahead_snts;\r\n+\t}\r\n+\r\n \t/**\r\n \t * Activates the lookback mode for caching a fixed number of preceding sentences per thread.\r\n \t * @param lookback_snts\r\n@@ -622,7 +554,10 @@ public void activateLookback(int lookback_snts) {\n \t\tif (lookback_snts < 0) lookback_snts = 0;\r\n \t\tthis.lookback_snts = lookback_snts;\r\n \t}\r\n-\t\r\n+\tpublic int getLookback() {\r\n+\t\treturn lookback_snts;\r\n+\t}\r\n+\r\n \t/**\r\n \t * Activates the graphsout mode for single graphviz .dot files per execution step.\r\n \t * @param dir\r\n@@ -645,6 +580,12 @@ public void activateGraphsOut(String dir, List<String> sentences) throws IOExcep\n \t\t}\r\n \t\tgraphOutputSentences.addAll(sentences);\r\n \t}\r\n+\tpublic File getGraphOutputDir() {\r\n+\t\treturn graphOutputDir;\r\n+\t}\r\n+\tpublic String[] getGraphOutputSentences() {\r\n+\t\treturn graphOutputSentences.toArray(new String[graphOutputSentences.size()]);\r\n+\t}\r\n \r\n \t/**\r\n \t * Activates the triplesout mode for single ntriples-files per execution step.\r\n@@ -668,18 +609,27 @@ public void activateTriplesOut(String dir, List<String> sentences) throws IOExce\n \t\t}\r\n \t\ttriplesOutputSentences.addAll(sentences);\r\n \t}\r\n+\tpublic File getTriplesOutputDir() {\r\n+\t\treturn triplesOutputDir;\r\n+\t}\r\n+\tpublic String[] getTriplesOutputSentences() {\r\n+\t\treturn triplesOutputSentences.toArray(new String[triplesOutputSentences.size()]);\r\n+\t}\r\n \r\n \t/**\r\n \t * Instruct the Updater to remove duplicates of RDF prefixes, to avoid issues with segmented data using a single prefix header.\r\n \t */\r\n-\tpublic void activateRemovePrefixDuplicates() {\r\n-\t\tthis.removePrefixDuplicates = true;\r\n+\tpublic void activatePrefixDeduplication() {\r\n+\t\tthis.prefixDeduplication = true;\r\n+\t}\r\n+\tpublic boolean getPrefixDeduplication() {\r\n+\t\treturn prefixDeduplication;\r\n \t}\r\n \r\n \t/**\r\n-\t * Load external RDF file into a named graph of the local dataset. \r\n+\t * Load external RDF file into a named graph of the local dataset.\r\n \t * This graph is permanent for the runtime and is accessed read-only by all threads.\r\n-\t * The default graph of the local dataset is reserved for updating nif:Sentences and \r\n+\t * The default graph of the local dataset is reserved for updating nif:Sentences and\r\n \t * can not be defined here.\r\n \t * @param url\r\n \t * \t\t\tlocation of the RDF file to be loaded\r\n@@ -707,6 +657,13 @@ public void loadGraph(URI url, URI graph) throws IOException {\n \t\t}\r\n \t\tLOG.info(\"done...\");\r\n \t}\r\n+\tpublic boolean hasGraph(String name) {\r\n+\t\treturn dataset.containsNamedModel(name);\r\n+\t}\r\n+\tpublic Model getGraph(String name) {\r\n+\t\t//TODO return a copy instead of a reference\r\n+\t\treturn dataset.getNamedModel(name);\r\n+\t}\r\n \r\n \t/**\r\n \t * Define a set of updates to be executed for each sentence processed by this CoNLLRDFUpdater.\r\n@@ -716,17 +673,16 @@ public void loadGraph(URI url, URI graph) throws IOException {\n \t * \t\t\t<Name of Update>, <update script>OR<path to script>, <iterations>\r\n \t * @throws IOException\r\n \t */\r\n-\tpublic void parseUpdates(List<Triple<String, String, String>> updatesRaw) throws IOException {\r\n-\t\tthis.updates.clear();\r\n+\tpublic void parseUpdates(List<Triple<String, String, String>> updatesRaw) throws IOException, ParseException {\r\n+\t\tupdates.clear();\r\n \t\tfinal List<Triple<String, String, String>> updatesOut = new ArrayList<Triple<String, String, String>>(updatesRaw.size());\r\n-\t\tfinal StringBuilder sb = new StringBuilder(); // debug output\r\n \r\n \t\tint updateNo = 0;\r\n \t\tfor(Triple<String, String, String> update: updatesRaw) {\r\n-\t\t\tString updateName = update.first;\r\n-\t\t\tfinal String updateScriptRaw = update.second; // either an URL/ a path to, or the verbatim sparql\r\n-\t\t\tfinal String updateScript; // will eventually contain the sparql query\r\n-\t\t\tfinal String updateIterations = update.third;\r\n+\t\t\tString updateName = update.getLeft();\r\n+\t\t\tfinal String updateScriptRaw = update.getMiddle(); // either an URL/ a path to, or the verbatim sparql\r\n+\t\t\tString updateScript = null; // will eventually contain the sparql query\r\n+\t\t\tfinal String updateIterations = update.getRight();\r\n \t\t\tupdateNo++; // Used for logging\r\n \r\n \t\t\tLOG.debug(\"Update No.\"+updateNo+\" named \"+updateName+\" with \"+updateIterations+\" iterations is\\n\"+updateScriptRaw);\r\n@@ -738,76 +694,55 @@ public void parseUpdates(List<Triple<String, String, String>> updatesRaw) throws\n \t\t\t * - verbatim query was not quoted\r\n \t\t\t */\r\n \r\n-\t\t\tReader sparqlreader = null;\r\n+\t\t\t// Try if update is an URL\r\n \t\t\tURL url = null;\r\n+\t\t\ttry {\r\n+\t\t\t\turl = new URL(updateScriptRaw);\r\n+\t\t\t\tLOG.debug(\"URL Stream ok\");\r\n+\t\t\t\tupdateScript = readUrl(url);\r\n+\t\t\t} catch (MalformedURLException e) {\r\n+\t\t\t\tLOG.trace(e);\r\n+\t\t\t\tLOG.debug(\"Update is not a valid URL \" + updateScriptRaw); // this occurs if the update is verbatim\r\n+\t\t\t} catch (IOException e) {\r\n+\t\t\t\tthrow new IOException(\"Failed to open input stream from URL \" + updateScriptRaw, e);\r\n+\t\t\t}\r\n \r\n \t\t\t// Try if Update is a FilePath\r\n \t\t\tfinal File file = new File(updateScriptRaw);\r\n-\t\t\tif(file.exists()) { // can be read from a file\r\n+\t\t\tif(updateScript == null && file.exists()) { // can be read from a file\r\n \t\t\t\ttry {\r\n-\t\t\t\t\tsparqlreader = new FileReader(file);\r\n+\t\t\t\t\tupdateScript = readString(file.toPath());\r\n \t\t\t\t\tLOG.debug(\"FileReader ok\");\r\n-\t\t\t\t\tsb.append(\"f\");\r\n \t\t\t\t} catch (FileNotFoundException e) {\r\n \t\t\t\t\t// the update is a path to a file which exists, but it could not be opened\r\n-\t\t\t\t\tLOG.error(\"Failed to read file \" + updateScriptRaw, e);\r\n-\t\t\t\t\tSystem.exit(1);\r\n-\t\t\t\t}\r\n-\t\t\t}\r\n-\r\n-\t\t\t// Try if update is an URL\r\n-\t\t\tif (sparqlreader == null) {\r\n-\t\t\t\ttry {\r\n-\t\t\t\t\turl = new URL(updateScriptRaw);\r\n-\t\t\t\t\tsparqlreader = new InputStreamReader(url.openStream());\r\n-\t\t\t\t\tLOG.debug(\"URL Stream ok\");\r\n-\t\t\t\t\tsb.append(\"u\");\r\n-\t\t\t\t} catch (MalformedURLException e) {\r\n-\t\t\t\t\tLOG.debug(\"Update is not a valid URL \" + updateScriptRaw); // this occurs if the update is verbatim\r\n-\t\t\t\t\tLOG.trace(\"Trace:\", e);\r\n-\t\t\t\t} catch (IOException e) {\r\n-\t\t\t\t\tLOG.error(\"Failed to open input stream from URL \" + updateScriptRaw, e); // this is probably bad\r\n-\t\t\t\t\tSystem.exit(1);\r\n+\t\t\t\t\tthrow new IOException(\"Failed to read file \" + updateScriptRaw, e);\r\n \t\t\t\t}\r\n \t\t\t}\r\n \r\n \t\t\t// check for String as Update and set update name to default\r\n-\t\t\tif (sparqlreader != null) {\r\n+\t\t\tif (updateScript == null) {\r\n \t\t\t\tupdateName = DEFAULTUPDATENAME;\r\n-\t\t\t} else {\r\n-\t\t\t\tsparqlreader = new StringReader(updateScriptRaw);\r\n+\t\t\t\tupdateScript = updateScriptRaw;\r\n \t\t\t\tLOG.debug(\"StringReader ok\");\r\n \t\t\t}\r\n \r\n-\t\t\tfinal BufferedReader in = new BufferedReader(sparqlreader);\r\n-\t\t\tfinal StringBuilder updateBuff = new StringBuilder();\r\n-\t\t\tfor(String line = in.readLine(); line!=null; line=in.readLine()) {\r\n-\t\t\t\tupdateBuff.append(line + \"\\n\");\r\n-\t\t\t}\r\n-\r\n-\t\t\tupdateScript = updateBuff.toString();\r\n-\t\t\tisValidUTF8(updateScript, \"SPARQL update String is not UTF-8 encoded for \" + updateName);\r\n-\r\n \t\t\ttry {\r\n \t\t\t\t@SuppressWarnings(\"unused\")\r\n \t\t\t\tUpdateRequest qexec = UpdateFactory.create(updateScript);\r\n \t\t\t} catch (QueryParseException e) {\r\n \t\t\t\tLOG.error(\"Failed to parse argument as sparql\");\r\n \t\t\t\t// if update looks like a file, but can't be found\r\n \t\t\t\tif(updateScriptRaw.toLowerCase().endsWith(\".sparql\") && !(file.exists()) && (url == null)) {\r\n-\t\t\t\t\tLOG.error(\"The passed update No. \"+updateNo+\" looks like a file-path, however the file \" + updateScriptRaw + \" could not be found.\");\r\n \t\t\t\t\tLOG.debug(\"SPARQL parse exception for Update No. \"+updateNo+\": \"+updateName+\"\\n\" + e + \"\\n\" + updateScript);\r\n+\t\t\t\t\tthrow new ParseException(\"The passed update No. \"+updateNo+\" looks like a file-path, however the file \" + updateScriptRaw + \" could not be found.\");\r\n \t\t\t\t} else {\r\n-\t\t\t\t\tLOG.error(\"SPARQL parse exception for Update No. \"+updateNo+\": \"+updateName+\"\\n\" + e + \"\\n\" + updateScript); // this is SPARQL code with broken SPARQL syntax\r\n+\t\t\t\t\tthrow new ParseException(\"SPARQL parse exception for Update No. \"+updateNo+\": \"+updateName+\"\\n\" + e + \"\\n\" + updateScript); // this is SPARQL code with broken SPARQL syntax\r\n \t\t\t\t}\r\n-\t\t\t\tSystem.exit(1);\r\n \t\t\t}\r\n-\t\t\tupdatesOut.add(new Triple<String, String, String> (updateName, updateScript, updateIterations));\r\n+\t\t\tupdatesOut.add(new ImmutableTriple<String, String, String> (updateName, updateScript, updateIterations));\r\n \t\t\tLOG.debug(\"Update parsed ok\");\r\n-\t\t\tsb.append(\".\");\r\n \t\t}\r\n-\t\tthis.updates.addAll(Collections.synchronizedList(updatesOut));\r\n-\t\tLOG.debug(sb.toString());\r\n+\t\tupdates.addAll(Collections.synchronizedList(updatesOut));\r\n \t}\r\n \r\n \t/**\r\n@@ -838,28 +773,33 @@ private static String readInURI(URI uri) throws MalformedURLException, IOExcepti\n \t\t}\r\n \t\treturn result;\r\n \t}\r\n-\t\r\n+\r\n \tprivate static void isValidUTF8(String s, String message) {\r\n-\t\ttry \r\n+\t\ttry\r\n \t\t{\r\n \t\t\ts.getBytes(\"UTF-8\");\r\n-\t\t} \r\n+\t\t}\r\n \t\tcatch (UnsupportedEncodingException e)\r\n \t\t{\r\n-\t\t    LOG.error(message + \" - Encoding error: \" + e.getMessage());\r\n-\t\t    System.exit(-1);\r\n-\t\t}\t\t\r\n+\t\t\tLOG.error(message + \" - Encoding error: \" + e.getMessage());\r\n+\t\t\tSystem.exit(-1);\r\n+\t\t}\r\n \t}\r\n \r\n \t/**\r\n \t * Processes CoNLL-RDF on the local dataset using the predfined updates and threads.\r\n-\t * Streams data from a buffered reader to a buffered writer. Distributes the processing \r\n+\t * Streams data from a buffered reader to a buffered writer. Distributes the processing\r\n \t * across available threads. Each thread handles one sentence at a time.\r\n \t * Caches and outputs the resulting sentences in-order.\r\n \t * @throws IOException\r\n \t */\r\n-\tpublic void processSentenceStream() throws IOException {\r\n+\t@Override\r\n+\tprotected void processSentenceStream() throws IOException {\r\n+\t\tinitThreads();\r\n \t\trunning = true;\r\n+\r\n+\t\t\r\n+\t\tString prefixCache = new String();\r\n \t\tString line;\r\n \t\tString lastLine =\"\";\r\n \t\tString buffer=\"\";\r\n@@ -895,22 +835,16 @@ public void processSentenceStream() throws IOException {\n \t\t\t\t\tLOG.debug(\"Triples Output defaults to first sentence: \" + sentID);\r\n \t\t\t\t}\r\n \r\n-\t\t\t\t// --> deprecated\r\n-\t\t\t\t//parsedSentences++;\r\n-\t\t\t\t//execute updates using thread handler  --> now in lookahead handling\r\n-\t\t\t\t//executeThread(buffer);\r\n-\t\t\t\t// <-- deprecated \r\n-\r\n \t\t\t\t//lookahead\r\n \t\t\t\t//add ALL sentences to sentBufferLookahead\r\n \t\t\t\tsentBufferLookahead.add(buffer);\r\n \t\t\t\tif (sentBufferLookahead.size() > lookahead_snts) {\r\n-\t\t\t\t\t//READY TO PROCESS \r\n+\t\t\t\t\t//READY TO PROCESS\r\n \t\t\t\t\t// remove first sentence from buffer and process it.\r\n \t\t\t\t\t// !!if lookahead = 0 then only current buffer is in sentBufferLookahead!!\r\n \t\t\t\t\texecuteThread(sentBufferLookahead.remove(0));\r\n-\t\t\t\t}\t\t\r\n-\t\t\t\t\r\n+\t\t\t\t}\r\n+\r\n \t\t\t\t//lookback\r\n \t\t\t\t//needs to consider lookahead buffer. The full buffer size needs to be lookahead + lookback.\r\n \t\t\t\tif (lookback_snts > 0) {\r\n@@ -924,14 +858,10 @@ public void processSentenceStream() throws IOException {\n \t\t\tbuffer=buffer+line+\"\\n\";\r\n \t\t\tlastLine=line;\r\n \t\t}\r\n-\t\t// --> deprecated\r\n-\t\t//parsedSentences++;\r\n-\t\t//executeThread(buffer);\r\n-\t\t// --> deprecated\r\n \r\n \t\t// FINAL SENTENCE (with prefixes if necessary)\r\n \t\tif (!buffer.contains(\"@prefix\"))  {\r\n-\t\t    buffer = prefixCache+buffer;\r\n+\t\t\tbuffer = prefixCache+buffer;\r\n \t\t}\r\n \r\n \t\t// To address the edge case of no comments or prefixes occuring after the first sentence of a stream\r\n@@ -956,8 +886,7 @@ public void processSentenceStream() throws IOException {\n \t\t\t\twhile (sentBufferLookback.size() >= lookback_snts + sentBufferLookahead.size()) sentBufferLookback.remove(0);\r\n \t\t\t}\r\n \t\t}\r\n-\t\t\t\r\n-\t\t\r\n+\r\n \t\t//wait for threads to finish work\r\n \t\tboolean threadsRunning = true;\r\n \t\twhile(threadsRunning) {\r\n@@ -981,26 +910,24 @@ public void processSentenceStream() throws IOException {\n \t\t\t\t}\r\n \t\t\t}\r\n \t\t}\r\n-\t\t\r\n+\r\n \t\t//sum up statistics\r\n \t\tList<Pair<Integer,Long>> dRTs_sum = new ArrayList<Pair<Integer,Long> >();\r\n \t\tfor (List<Pair<Integer,Long>> dRT_thread:dRTs) {\r\n \t\t\tif (dRTs_sum.isEmpty())\r\n \t\t\t\tdRTs_sum.addAll(dRT_thread);\r\n \t\t\telse\r\n \t\t\t\tfor (int x = 0; x < dRT_thread.size(); ++x)\r\n-\t\t\t\t\tdRTs_sum.set(x, new Pair<Integer, Long>(\r\n-\t\t\t\t\t\t\tdRTs_sum.get(x).getKey() + dRT_thread.get(x).getKey(), \r\n+\t\t\t\t\tdRTs_sum.set(x, new ImmutablePair<Integer, Long>(\r\n+\t\t\t\t\t\t\tdRTs_sum.get(x).getKey() + dRT_thread.get(x).getKey(),\r\n \t\t\t\t\t\t\tdRTs_sum.get(x).getValue() + dRT_thread.get(x).getValue()));\r\n-\t\t\t\r\n \t\t}\r\n \t\tif (!dRTs_sum.isEmpty())\r\n \t\t\tLOG.debug(\"Done - List of iterations and execution times for the updates done (in given order):\\n\\t\\t\" + dRTs_sum.toString());\r\n \r\n \t\t//final flush\r\n \t\tflushOutputBuffer(getOutputStream());\r\n \t\tgetOutputStream().close();\r\n-\t\t\r\n \t}\r\n \r\n \t/**\r\n@@ -1015,13 +942,32 @@ private String readFirstSentenceID(String buffer) {\n \t\treturn sentID;\r\n \t}\r\n \r\n+\tprivate void initThreads() {\r\n+\t\t// Use the processor cores available to runtime (but at least 1) as thread count, if an invalid thread count is provided.\r\n+\t\tif (threads <= 0) {\r\n+\t\t\tthreads = (Runtime.getRuntime().availableProcessors()>0)?(Runtime.getRuntime().availableProcessors()):(1);\r\n+\t\t\tLOG.info(\"Falling back to default thread maximum.\");\r\n+\t\t}\r\n+\t\tLOG.info(\"Executing on \"+threads+\" processor cores, max.\");\r\n+\t\tfor (int i = 0; i < threads; i++) {\r\n+\t\t\tupdateThreads.add(null);\r\n+\t\t\tdataset.addNamedModel(\"http://thread\"+i, ModelFactory.createDefaultModel());\r\n+\t\t\tsentBufferThreads.add(new ImmutableTriple<List<String>, String, List<String>>(\r\n+\t\t\t\t\tnew ArrayList<String>(), new String(), new ArrayList<String>()));\r\n+\t\t\tdRTs.add(Collections.synchronizedList(new ArrayList<Pair<Integer,Long> >()));\r\n+\t\t}\r\n+\t}\r\n+\r\n \tprivate synchronized void flushOutputBuffer(PrintStream out) {\r\n \t\tLOG.trace(\"OutBufferSize: \"+sentBufferOut.size());\r\n+\r\n+\t\tString prefixCacheOut = new String();\r\n+\r\n \t\twhile (!sentBufferOut.isEmpty()) {\r\n \t\t\tif (sentBufferOut.get(0).matches(\"\\\\d+\")) break;\r\n-\t\t\t\r\n+\r\n \t\t\tString outString = new String();\r\n-\t\t\tif (removePrefixDuplicates) {\r\n+\t\t\tif (prefixDeduplication) {\r\n \t\t\t\tString prefixCacheTMP = new String();\r\n \t\t\t\tfor (String buffLine:sentBufferOut.remove(0).split(\"\\n\")) {\r\n \t\t\t\t\tif (buffLine.trim().startsWith(\"@prefix\")) {\r\n@@ -1043,17 +989,17 @@ private synchronized void flushOutputBuffer(PrintStream out) {\n \t}\r\n \r\n \tprivate void executeThread(String buffer) {\r\n-\t\tTriple<List<String>, String, List<String>>sentBufferThread = \r\n-\t\t\t\tnew Triple<List<String>, String, List<String>>(\r\n+\t\tMutableTriple<List<String>, String, List<String>>sentBufferThread =\r\n+\t\t\t\tnew MutableTriple<List<String>, String, List<String>>(\r\n \t\t\t\tnew ArrayList<String>(), new String(), new ArrayList<String>());\r\n-\t\t//sentBufferLookback only needs to be filled up to the current sentence. \r\n-\t\t//All other sentences are for further lookahead iterations \r\n-//\t\tsentBufferThread.first.addAll(sentBufferLookback);\r\n+\t\t//sentBufferLookback only needs to be filled up to the current sentence.\r\n+\t\t//All other sentences are for further lookahead iterations\r\n+//\t\tsentBufferThread.getLeft().addAll(sentBufferLookback);\r\n \t\tfor (int i = 0; i < sentBufferLookback.size() - sentBufferLookahead.size(); i++) {\r\n-\t\t\tsentBufferThread.first.add(sentBufferLookback.get(i));\r\n+\t\t\tsentBufferThread.getLeft().add(sentBufferLookback.get(i));\r\n \t\t}\r\n-\t\tsentBufferThread.second = buffer;\r\n-\t\tsentBufferThread.third.addAll(sentBufferLookahead);\r\n+\t\tsentBufferThread.setMiddle(buffer);\r\n+\t\tsentBufferThread.getRight().addAll(sentBufferLookahead);\r\n \t\tint i = 0;\r\n \r\n \t\twhile(i < updateThreads.size()) {\r\n@@ -1066,7 +1012,7 @@ private void executeThread(String buffer) {\n \t\t\t\tLOG.trace(\"restart \"+i);\r\n \t\t\t\tLOG.trace(\"OutBufferSize: \"+sentBufferOut.size());\r\n \t\t\t\tbreak;\r\n-\t\t\t} else \r\n+\t\t\t} else\r\n \t\t\t\tif (updateThreads.get(i).getState() == Thread.State.WAITING) {\r\n \t\t\t\tsynchronized(updateThreads.get(i)) {\r\n \t\t\t\tsentBufferThreads.set(i, sentBufferThread);\r\n@@ -1075,15 +1021,15 @@ private void executeThread(String buffer) {\n \t\t\t\t}\r\n \t\t\t\tLOG.trace(\"wake up \"+i);\r\n \t\t\t\tbreak;\r\n-\t\t\t} else \r\n+\t\t\t} else\r\n \t\t\t\tif (updateThreads.get(i).getState() == Thread.State.NEW) {\r\n \t\t\t\tsentBufferThreads.set(i, sentBufferThread);\r\n \t\t\t\tsentBufferOut.add(String.valueOf(i)); //add last sentences to the end of the output queue.\r\n \t\t\t\tupdateThreads.get(i).start();\r\n \t\t\t\tLOG.trace(\"start \"+i);\r\n \t\t\t\tLOG.trace(\"OutBufferSize: \"+sentBufferOut.size());\r\n \t\t\t\tbreak;\r\n-\t\t\t} else \r\n+\t\t\t} else\r\n \t\t\t\tif (updateThreads.get(i).getState() == Thread.State.TERMINATED) {\r\n \t\t\t\tsentBufferThreads.set(i, sentBufferThread);\r\n \t\t\t\tsentBufferOut.add(String.valueOf(i)); //add last sentences to the end of the output queue.\r\n@@ -1093,7 +1039,7 @@ private void executeThread(String buffer) {\n \t\t\t\tLOG.trace(\"OutBufferSize: \"+sentBufferOut.size());\r\n \t\t\t\tbreak;\r\n \t\t\t}\r\n-\t\t\t\r\n+\r\n \t\t\ti++;\r\n \t\t\tif (i >= updateThreads.size()) {\r\n \t\t\t\ttry {\r\n@@ -1110,193 +1056,49 @@ private void executeThread(String buffer) {\n \t\t}\r\n \t}\r\n \r\n-\tprivate static Options getOptions() {\r\n-\t\tfinal Options options = new Options();\r\n-\t\t// Define cli options in the correct order for the help-message\r\n-\t\toptions\r\n-\t\t\t.addOption(Option.builder(\"loglevel\").hasArg(true).desc(\"set log level to LEVEL\").argName(\"level\").build())\r\n-\t\t\t.addOption(\"threads\", true, \"use T threads max\\ndefault: half of available logical processor cores\")\r\n-\t\t\t.addOption(\"lookahead\", true, \"cache N further sentences in lookahead graph\")\r\n-\t\t\t.addOption(\"lookback\", true, \"cache N preceeding sentences in lookback graph\")\r\n-\t\t\t.addOption(\"prefixDeduplication\", false, \"Remove duplicates of TTL-Prefixes\")\r\n-\t\t\t.addOption(Option.builder(\"custom\").hasArg(false).desc(\"use custom update scripts\")/*.required()*/.build())\r\n-\t\t\t.addOption(\"model\", true, \"to load additional Models into local graph\")\r\n-\t\t\t.addOption(\"graphsout\", true, \"output directory for the .dot graph files\\nfollowed by the IDs of the sentences to be visualized\\ndefault: first sentence only\")\r\n-\t\t\t.addOption(\"triplesout\", true, \"same as graphsout but write N-TRIPLES for text debug instead.\")\r\n-\t\t\t.addOption(\"updates\", true, \"followed by SPARQL scripts paired with {iterations/u}\");\r\n-\t\treturn options;\r\n+\tprivate void addUpdate(String updateRaw, String updateIterationLimit) throws IOException, ParseException {\r\n+\t\tString updateName;\r\n+\t\tString updateString;\r\n+\t\t// TODO implement method\r\n+\t\tFile file = new File(updateRaw);\r\n+\t\tif (file.exists() && file.isFile()) {\r\n+\t\t\tupdateString = readString(file.toPath());\r\n+\t\t\tupdates.add(new ImmutableTriple<String, String, String>(updateRaw, updateString, updateIterationLimit));\r\n+\t\t}\r\n+\t\t// URL\r\n+\t\ttry {\r\n+\t\t\tURL url = new URL(updateRaw);\r\n+\t\t\tLOG.debug(\"URL Stream ok\");\r\n+\t\t\tupdateString = readUrl(url);\r\n+\t\t\tupdates.add(new ImmutableTriple<String, String, String>(updateRaw, updateString, updateIterationLimit));\r\n+\t\t} catch (MalformedURLException e) {\r\n+\t\t\tLOG.trace(e);\r\n+\t\t\tLOG.debug(\"Update is not a valid URL \" + updateRaw); // this occurs if the update is verbatim\r\n+\t\t} catch (IOException e) {\r\n+\t\t\tthrow new IOException(\"Failed to open input stream from URL \" + updateRaw, e);\r\n+\t\t}\r\n+\t\tif (updateRaw.endsWith(\".sparql\")) {\r\n+\t\t\tthrow new ParseException(\"The passed update looks like a file-path, however the file \" + updateRaw + \" could not be found.\");\r\n+\t\t} else {\r\n+\t\t\tupdateName = CoNLLRDFComponent.DEFAULTUPDATENAME;\r\n+\t\t\tupdateString = updateRaw;\r\n+\t\t}\r\n+\t\tupdates.add(new ImmutableTriple<String, String, String>(updateName, updateString, updateIterationLimit));\r\n+\t\t// updates.add(new ImmutableTriple<String, UpdateRequest, String> (updateName, UpdateFactory.create(updateQuery), updateIterationLimit));\r\n \t}\r\n \r\n-\tpublic static void main(String[] args) throws URISyntaxException, IOException {\r\n+\tpublic static void main(String[] args) throws IOException {\r\n \t\tfinal CoNLLRDFUpdater updater;\r\n-\t\tfinal Options options = getOptions();\r\n-\t\tfinal HelpFormatter formatter = new HelpFormatter();\r\n-\t\tfinal CommandLine cmd;\r\n-\t\t// PRINT USAGE HELP MESSAGE\r\n-\t\tfinal StringWriter info = new StringWriter();\r\n-\t\tfinal PrintWriter pw = new PrintWriter(info);\r\n-\t\tformatter.setOptionComparator(null); // don't sort cli-options in help message\r\n-\t\tformatter.setSyntaxPrefix(\"synopsis: \");\r\n-\t\tformatter.printHelp(pw, 80, \"CoNLLRDFUpdater\", \"read TTL from stdin => update CoNLL-RDF\", options,\r\n-\t\t\tformatter.getLeftPadding(), formatter.getDescPadding(), null);\r\n-\t\t//formatter.printHelp(\"CoNLLRDFUpdater\", \"read TTL from stdin => update CoNLL-RDF\", options, null, true);\r\n-\t\tpw.flush();\r\n-\t\tLOG.info(info);\r\n-\t\t/** LOG.info(\"synopsis: CoNLLRDFUpdater [-loglevel LEVEL] [-threads T] [-lookahead N] [-lookback N]\\n\"\r\n-\t\t* + \"\\t[-custom [-model URI [GRAPH]]* [-graphsout DIR [SENT_ID]] [-triplesout DIR [SENT_ID]] -updates [UPDATE]]\\n\");\r\n-\t\t*/\r\n-\t\t// PARSE the CommandLine Options\r\n \t\ttry {\r\n-\t\t\tcmd = new DefaultParser().parse(options, args);\r\n+\t\t\tupdater = CoNLLRDFUpdaterFactory.getUpdater(args);\r\n \t\t} catch (ParseException e) {\r\n \t\t\tLOG.error(e);\r\n \t\t\tSystem.exit(1);\r\n \t\t\treturn;\r\n \t\t}\r\n-\t\tint i;\r\n-\t\t// READ LOGLEVEL\r\n-\t\tif (cmd.hasOption(\"loglevel\")) {\r\n-\t\t\tfinal Level level = Level.toLevel(cmd.getOptionValue(\"loglevel\"));\r\n-\t\t\tLOG.setLevel(level);\r\n-\t\t\tLOG.info(\"loglevel set to \" + level.toString());\r\n-\t\t}\r\n-\t\t// debug cli parsing (after setting the loglevel)\r\n-\t\tLOG.debug(Arrays.asList(args).toString());\r\n-\t\tLOG.debug(Arrays.asList(cmd.getOptions()).toString());\r\n-\t\tLOG.debug(cmd.getArgList().toString());\r\n-\t\t// READ THREAD PARAMETERS\r\n-\t\tint threads = 0;\r\n-\t\tif (cmd.hasOption(\"threads\")) {\r\n-\t\t\ttry {\r\n-\t\t\t\tthreads = Integer.parseInt(cmd.getOptionValue(\"threads\"));\r\n-\t\t\t} catch (Exception e) {\r\n-\t\t\t\tLOG.error(\"Wrong usage of threads parameter. NaN.\");\r\n-\t\t\t\tSystem.exit(1);\r\n-\t\t\t\treturn;\r\n-\t\t\t}\r\n-\t\t}\r\n-\t\tupdater = new CoNLLRDFUpdater(\"\",\"\",threads);\r\n-\t\t// READ LOOKAHEAD PARAMETERS\r\n-\t\tif (cmd.hasOption(\"lookahead\")) {\r\n-\t\t\ttry {\r\n-\t\t\t\tupdater.activateLookahead(Integer.parseInt(cmd.getOptionValue(\"lookahead\")));\r\n-\t\t\t} catch (Exception e) {\r\n-\t\t\t\tLOG.error(\"Wrong usage of lookahead parameter. NaN.\");\r\n-\t\t\t\tSystem.exit(1);\r\n-\t\t\t\treturn;\r\n-\t\t\t}\r\n-\t\t}\r\n-\t\t// READ LOOKBACK PARAMETERS\r\n-\t\tif (cmd.hasOption(\"lookback\")) {\r\n-\t\t\ttry {\r\n-\t\t\t\tupdater.activateLookback(Integer.parseInt(cmd.getOptionValue(\"lookback\")));\r\n-\t\t\t} catch (Exception e) {\r\n-\t\t\t\tLOG.error(\"Wrong usage of lookback parameter. NaN.\");\r\n-\t\t\t\tSystem.exit(1);\r\n-\t\t\t\treturn;\r\n-\t\t\t}\r\n-\t\t}\r\n-\t\t// PREFIX DUPLICATES\r\n-\t\tif (cmd.hasOption(\"prefixDeduplication\")) {\r\n-\t\t\tLOG.debug(\"Activated Prefix Deduplication\");\r\n-\t\t\tupdater.activateRemovePrefixDuplicates();\r\n-\t\t}\r\n-\t\t// READ MODE (currently only CUSTOM)\r\n-\t\tboolean CUSTOM = cmd.hasOption(\"custom\");\r\n-\t\tif(!CUSTOM ) { // no default possible here\r\n-\t\t\tLOG.error(\"Please specify update script.\");\r\n-\t\t}\r\n-\t\t// READ GRAPHSOUT PARAMETERS\r\n-\t\tif (cmd.hasOption(\"graphsout\")) {\r\n-\t\t\tList<String> graphOutputSentences = new ArrayList<String>();\r\n-\t\t\ti = 0;\r\n-\t\t\twhile(i<args.length && !args[i].toLowerCase().matches(\"^-+graphsout$\")) i++;\r\n-\t\t\ti++;\r\n-\t\t\tString graphOutputDir = args[i];\r\n-\t\t\ti++;\r\n-\t\t\twhile(i<args.length && !args[i].toLowerCase().matches(\"^-+.*$\")) {\r\n-\t\t\t\tgraphOutputSentences.add(args[i++]);\r\n-\t\t\t}\r\n-\t\t\tupdater.activateGraphsOut(graphOutputDir, graphOutputSentences);\r\n-\t\t}\r\n-\t\t// READ TRIPLESOUT PARAMETERS\r\n-\t\tif (cmd.hasOption(\"triplesout\")) {\r\n-\t\t\tList<String> triplesOutputSentences = new ArrayList<String>();\r\n-\t\t\ti = 0;\r\n-\t\t\twhile(i<args.length && !args[i].toLowerCase().matches(\"^-+triplesout$\")) i++;\r\n-\t\t\ti++;\r\n-\t\t\tString triplesOutputDir = args[i];\r\n-\t\t\ti++;\r\n-\t\t\twhile(i<args.length && !args[i].toLowerCase().matches(\"^-+.*$\")) {\r\n-\t\t\t\ttriplesOutputSentences.add(args[i++]);\r\n-\t\t\t}\r\n-\t\t\tupdater.activateTriplesOut(triplesOutputDir, triplesOutputSentences);\r\n-\t\t}\r\n-\t\t//CUSTOM UPDATE SCRIPT MODE\r\n-\t\tif(CUSTOM) {\r\n-\t\t\t// READ ALL MODELS from System.in\r\n-\t\t\ti = 0;\r\n-\t\t\t// should be <#UPDATEFILENAMEORSTRING, #UPDATESTRING, #UPDATEITER>\r\n-\t\t\tList<Triple<String, String, String>> updates = new ArrayList<Triple<String, String, String>>();\r\n-\t\t\tList<String> models = new ArrayList<String>();\r\n-\r\n-\t\t\twhile(i<args.length && !args[i].toLowerCase().matches(\"^-+custom$\")) i++;\r\n-\t\t\ti++;\r\n-\t\t\twhile(i<args.length) {\r\n-\t\t\t\twhile(i<args.length && !args[i].toLowerCase().matches(\"^-+model$\")) i++;\r\n-\t\t\t\ti++;\r\n-\t\t\t\twhile(i<args.length && !args[i].toLowerCase().matches(\"^-+.*$\"))\r\n-\t\t\t\t\tmodels.add(args[i++]);\r\n-\t\t\t\tif (models.size()==1) {\r\n-\t\t\t\t\tupdater.loadGraph(new URI(models.get(0)), new URI(models.get(0)));\r\n-\t\t\t\t} else if (models.size()==2){\r\n-\t\t\t\t\tupdater.loadGraph(new URI(models.get(0)), new URI(models.get(1)));\r\n-\t\t\t\t} else if (models.size()>2){\r\n-\t\t\t\t\tthrow new IOException(\"Error while loading model: Please use -custom [-model URI [GRAPH]]* -updates [UPDATE]+\");\r\n-\t\t\t\t}\r\n-\t\t\t\tmodels.removeAll(models);\r\n-\t\t\t}\r\n-\r\n-\t\t\t// READ ALL UPDATES from System.in\r\n-\t\t\ti=0;\r\n-\t\t\twhile(i<args.length && !args[i].toLowerCase().matches(\"^-+custom$\")) i++;\r\n-\t\t\ti++;\r\n-\t\t\twhile(i<args.length && !args[i].toLowerCase().matches(\"^-+updates$\")) i++;\r\n-\t\t\ti++;\r\n-\t\t\twhile(i<args.length && !args[i].toLowerCase().matches(\"^-+.*$\")) {\r\n-\t\t\t\tString freq;\r\n-\t\t\t\tfreq = args[i].replaceFirst(\".*\\\\{([0-9u*]+)\\\\}$\", \"$1\");\r\n-\t\t\t\tif (args[i].equals(freq)) // update script without iterations in curly brackets defaults to 1\r\n-\t\t\t\t\tfreq = \"1\";\r\n-\t\t\t\telse if (freq.equals(\"u\"))\r\n-\t\t\t\t\tfreq = \"*\";\r\n-\t\t\t\tString update = args[i++].replaceFirst(\"\\\\{[0-9u*]+\\\\}$\", \"\");\r\n-\t\t\t\tupdates.add(new Triple<String, String, String>(update, update, freq));\r\n-\t\t\t}\r\n-\t\t\tupdater.parseUpdates(updates);\r\n-\r\n-\t\t\tlong start = System.currentTimeMillis();\r\n-\r\n-\t\t\tupdater.setInputStream(new BufferedReader(new InputStreamReader(System.in)));\r\n-\t\t\tupdater.setOutputStream(System.out);\r\n-\t\t\t//READ SENTENCES from System.in  \r\n-\t\t\tupdater.processSentenceStream();\r\n-\t\t\tLOG.debug((System.currentTimeMillis()-start)/1000 + \" seconds\");\r\n-\t\t}\r\n-\t}\r\n-\r\n-\t@Override\r\n-\tpublic void start() {\r\n-\t\trun();\r\n-\t}\r\n-\r\n-\t@Override\r\n-\tpublic void run() {\r\n-\t\ttry {\r\n-\t\t\tprocessSentenceStream();\r\n-\t\t} catch (Exception e) {\r\n-\t\t\te.printStackTrace();\r\n-\t\t\tSystem.exit(0);\r\n-\t\t}\r\n+\t\tlong start = System.currentTimeMillis();\r\n+\t\t// READ SENTENCES from System.in\r\n+\t\tupdater.processSentenceStream();\r\n+\t\tLOG.debug((System.currentTimeMillis()-start)/1000 + \" seconds\");\r\n \t}\r\n }\r"
  },
  {
    "sha": "a56f848a7d8d0c755a863bced1e3c598f646c952",
    "filename": "src/main/java/org/acoli/conll/rdf/CoNLLRDFUpdaterFactory.java",
    "status": "added",
    "additions": 105,
    "deletions": 0,
    "changes": 105,
    "blob_url": "https://github.com/acoli-repo/conll-rdf/blob/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/CoNLLRDFUpdaterFactory.java",
    "raw_url": "https://github.com/acoli-repo/conll-rdf/raw/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/CoNLLRDFUpdaterFactory.java",
    "contents_url": "https://api.github.com/repos/acoli-repo/conll-rdf/contents/src/main/java/org/acoli/conll/rdf/CoNLLRDFUpdaterFactory.java?ref=4f49b0dd21f0cfe1898b860851c18fcd891fdf13",
    "patch": "@@ -0,0 +1,105 @@\n+package org.acoli.conll.rdf;\n+\n+import static org.acoli.conll.rdf.CoNLLRDFCommandLine.parseUpdate;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import org.apache.commons.cli.*;\n+import org.apache.commons.lang3.tuple.ImmutableTriple;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.apache.commons.lang3.tuple.Triple;\n+import org.apache.log4j.Logger;\n+\n+public class CoNLLRDFUpdaterFactory {\n+\tstatic Logger LOG = Logger.getLogger(CoNLLRDFUpdaterFactory.class);\n+\n+\tpublic static CoNLLRDFUpdater getUpdater(String[] args) throws IOException, ParseException {\n+\t\tCoNLLRDFUpdater updater = new CoNLLRDFUpdater();\n+\t\tfinal CommandLine cmd = new CoNLLRDFCommandLine(\n+\t\t\t\t\"CoNLLRDFUpdater [-loglevel LEVEL] [-threads T] [-lookahead N] [-lookback N] [-custom [-model URI [GRAPH]]* [-graphsout DIR [SENT_ID ...]] [-triplesout DIR [SENT_ID ...]] -updates [UPDATE ...]]\",\n+\t\t\t\t\"read TTL from stdin => update CoNLL-RDF\", new Option[] {\n+\t\t\t\t\t\t// Define cli options in the correct order for the help-message\n+\t\t\t\t\t\tOption.builder(\"loglevel\").hasArg().desc(\"set log level to LEVEL\").argName(\"level\").build(),\n+\t\t\t\t\t\tOption.builder(\"threads\").hasArg()\n+\t\t\t\t\t\t\t\t.desc(\"use T threads max\\ndefault: half of available logical processor cores\")\n+\t\t\t\t\t\t\t\t.type(Number.class).build(),\n+\t\t\t\t\t\tOption.builder(\"lookahead\").hasArg().desc(\"cache N further sentences in lookahead graph\")\n+\t\t\t\t\t\t\t\t.type(Number.class).build(),\n+\t\t\t\t\t\tOption.builder(\"lookback\").hasArg().desc(\"cache N preceeding sentences in lookback graph\")\n+\t\t\t\t\t\t\t\t.type(Number.class).build(),\n+\t\t\t\t\t\tnew Option(\"prefixDeduplication\", false, \"Remove duplicates of TTL-Prefixes\"),\n+\t\t\t\t\t\tOption.builder(\"custom\").hasArg(false).desc(\"use custom update scripts\")\n+\t\t\t\t\t\t\t\t./* required(). */build(),\n+\t\t\t\t\t\tOption.builder(\"model\").hasArgs().desc(\"to load additional Models into local graph\").build(),\n+\t\t\t\t\t\tOption.builder(\"graphsout\").hasArgs().desc(\n+\t\t\t\t\t\t\t\t\"output directory for the .dot graph files\\nfollowed by the IDs of the sentences to be visualized\\ndefault: first sentence only\")\n+\t\t\t\t\t\t\t\t.build(),\n+\t\t\t\t\t\tOption.builder(\"triplesout\").hasArgs()\n+\t\t\t\t\t\t\t\t.desc(\"same as graphsout but write N-TRIPLES for text debug instead.\").build(),\n+\t\t\t\t\t\tOption.builder(\"updates\").hasArgs()\n+\t\t\t\t\t\t\t\t.desc(\"followed by SPARQL scripts paired with {iterations/u}\").build() },\n+\t\t\t\tCoNLLRDFUpdater.LOG).parseArgs(args);\n+\n+\t\tif (cmd.hasOption(\"threads\")) {\n+\t\t\tupdater.setThreads(((Number) cmd.getParsedOptionValue(\"threads\")).intValue());\n+\t\t}\n+\t\tif (cmd.hasOption(\"lookahead\")) {\n+\t\t\tupdater.activateLookahead(((Number) cmd.getParsedOptionValue(\"lookahead\")).intValue());\n+\t\t}\n+\t\tif (cmd.hasOption(\"lookback\")) {\n+\t\t\tupdater.activateLookback(((Number) cmd.getParsedOptionValue(\"lookback\")).intValue());\n+\t\t}\n+\t\tif (cmd.hasOption(\"prefixDeduplication\")) {\n+\t\t\tupdater.activatePrefixDeduplication();\n+\t\t}\n+\t\t// READ GRAPHSOUT PARAMETERS\n+\t\tif (cmd.hasOption(\"graphsout\")) {\n+\t\t\tString[] graphsoutArgs = cmd.getOptionValues(\"graphsout\");\n+\t\t\tString outputDir = graphsoutArgs[0];\n+\t\t\tList<String> outputSentences = Arrays.asList(Arrays.copyOfRange(graphsoutArgs, 1, graphsoutArgs.length));\n+\t\t\tupdater.activateGraphsOut(outputDir, outputSentences);\n+\t\t}\n+\t\t// READ TRIPLESOUT PARAMETERS\n+\t\tif (cmd.hasOption(\"triplesout\")) {\n+\t\t\tString[] triplesoutArgs = cmd.getOptionValues(\"triplesout\");\n+\t\t\tString outputDir = triplesoutArgs[0];\n+\t\t\tList<String> outputSentences = Arrays.asList(Arrays.copyOfRange(triplesoutArgs, 1, triplesoutArgs.length));\n+\t\t\tupdater.activateTriplesOut(outputDir, outputSentences);\n+\t\t}\n+\n+\t\tif (cmd.hasOption(\"model\")) {\n+\t\t\tfor (Option opt : cmd.getOptions()) {\n+\t\t\t\tif (opt.getOpt() == \"model\") { // opt.equals(model)\n+\t\t\t\t\tString[] model = opt.getValues();\n+\t\t\t\t\ttry {\n+\t\t\t\t\t\tif (model.length == 1) {\n+\t\t\t\t\t\t\tupdater.loadGraph(new URI(model[0]), new URI(model[0]));\n+\t\t\t\t\t\t} else if (model.length == 2) {\n+\t\t\t\t\t\t\tupdater.loadGraph(new URI(model[0]), new URI(model[1]));\n+\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\tthrow new ParseException(\"Error while loading model: Please provide one or two URIs\");\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} catch (URISyntaxException e) {\n+\t\t\t\t\t\tthrow new ParseException(\"Error while loading model: Could not parse given arguments as URI\");\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\tif (cmd.hasOption(\"updates\")) {\n+\t\t\tList<Triple<String, String, String>> updates = new ArrayList<>();\n+\t\t\tfor (String arg : Arrays.asList(cmd.getOptionValues(\"updates\"))) {\n+\t\t\t\tPair<String, String> parsed = parseUpdate(arg);\n+\t\t\t\t// should be <#UPDATEFILENAMEORSTRING, #UPDATESTRING, #UPDATEITER>\n+\t\t\t\tupdates.add(new ImmutableTriple<String, String, String>(parsed.getKey(), parsed.getKey(),\n+\t\t\t\t\t\tparsed.getValue()));\n+\t\t\t}\n+\t\t\tupdater.parseUpdates(updates);\n+\t\t}\n+\t\treturn updater;\n+\t}\n+}"
  },
  {
    "sha": "fc403276ed2b1e315840b8e4f64cda1ca77a1586",
    "filename": "src/main/java/org/acoli/conll/rdf/CoNLLRDFViz.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/acoli-repo/conll-rdf/blob/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/CoNLLRDFViz.java",
    "raw_url": "https://github.com/acoli-repo/conll-rdf/raw/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/CoNLLRDFViz.java",
    "contents_url": "https://api.github.com/repos/acoli-repo/conll-rdf/contents/src/main/java/org/acoli/conll/rdf/CoNLLRDFViz.java?ref=4f49b0dd21f0cfe1898b860851c18fcd891fdf13",
    "patch": "@@ -41,7 +41,7 @@\n  * @author Christian Chiarcos {@literal chiarcos@informatik.uni-frankfurt.de}\r\n  */\r\n public class CoNLLRDFViz {\r\n-\tprivate static final Logger LOG = Logger.getLogger(CoNLLRDFViz.class.getName());\r\n+\tprivate static final Logger LOG = Logger.getLogger(CoNLLRDFViz.class);\r\n \r\n \tprotected static String dotId(Resource r) {\r\n \t\tif (r.isAnon())\r"
  },
  {
    "sha": "ad631ca375550a44285a86e7d338a3f246504e7e",
    "filename": "src/main/java/org/acoli/conll/rdf/CoNLLStreamExtractor.java",
    "status": "modified",
    "additions": 137,
    "deletions": 217,
    "changes": 354,
    "blob_url": "https://github.com/acoli-repo/conll-rdf/blob/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/CoNLLStreamExtractor.java",
    "raw_url": "https://github.com/acoli-repo/conll-rdf/raw/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/CoNLLStreamExtractor.java",
    "contents_url": "https://api.github.com/repos/acoli-repo/conll-rdf/contents/src/main/java/org/acoli/conll/rdf/CoNLLStreamExtractor.java?ref=4f49b0dd21f0cfe1898b860851c18fcd891fdf13",
    "patch": "@@ -1,12 +1,12 @@\n /*\r\n  * Copyright [2017] [ACoLi Lab, Prof. Dr. Chiarcos, Goethe University Frankfurt]\r\n- * \r\n+ *\r\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n  * you may not use this file except in compliance with the License.\r\n  * You may obtain a copy of the License at\r\n- * \r\n+ *\r\n  *     http://www.apache.org/licenses/LICENSE-2.0\r\n- * \r\n+ *\r\n  * Unless required by applicable law or agreed to in writing, software\r\n  * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n@@ -23,46 +23,25 @@\n import org.apache.jena.rdf.model.*;\r\n import org.apache.jena.update.*;\r\n import org.apache.log4j.Logger;\r\n+import org.apache.commons.cli.ParseException;\r\n+import org.apache.commons.lang3.tuple.ImmutablePair;\r\n+import org.apache.commons.lang3.tuple.Pair;\r\n import org.apache.jena.query.*;\r\n \r\n-/** extracts RDF data from CoNLL files, transforms the result using SPARQL UPDATE queries,\r\n- * \toptionally followed by SPARQL SELECT to produce TSV output<br>\r\n- *  NOTE: queries can be provided as literal queries, file or as URLs, the latter two are preferred as\r\n- *  a literal query string may be parsed by the JVM/shell\r\n- *  @author Christian Chiarcos {@literal chiarcos@informatik.uni-frankfurt.de}\r\n- *  @author Christian Faeth {@literal faeth@em.uni-frankfurt.de}\r\n+/**\r\n+ * extracts RDF data from CoNLL files, transforms the result using SPARQL UPDATE\r\n+ * queries, optionally followed by SPARQL SELECT to produce TSV output<br>\r\n+ * NOTE: queries can be provided as literal queries, file or as URLs, the latter\r\n+ * two are preferred as a literal query string may be parsed by the JVM/shell\r\n+ * \r\n+ * @author Christian Chiarcos {@literal chiarcos@informatik.uni-frankfurt.de}\r\n+ * @author Christian Faeth {@literal faeth@em.uni-frankfurt.de}\r\n  */\r\n public class CoNLLStreamExtractor extends CoNLLRDFComponent {\r\n-\tpublic static class Pair<F, S> {\r\n-\t\tpublic F key;\r\n-\t\tpublic S value;\r\n-\t\tpublic Pair (F key, S value) {\r\n-\t\t\tthis.key = key;\r\n-\t\t\tthis.value = value;\r\n-\t\t}\r\n-\t\tpublic F getKey() {\r\n-\t\t\treturn key;\r\n-\t\t}\r\n-\t\tpublic void setKey(F key) {\r\n-\t\t\tthis.key = key;\r\n-\t\t}\r\n-\t\tpublic S getValue() {\r\n-\t\t\treturn value;\r\n-\t\t}\r\n-\t\tpublic void setValue(S value) {\r\n-\t\t\tthis.value = value;\r\n-\t\t}\r\n-\t}\r\n-\t\r\n-\tprivate static Logger LOG = Logger.getLogger(CoNLLStreamExtractor.class.getName());\r\n-\t\r\n-\t@SuppressWarnings(\"serial\")\r\n-\tprivate static List<Integer> CHECKINTERVAL = new ArrayList<Integer>() {{add(3); add(10); add(25); add(50); add(100); add(200); add(500);}};\r\n+\tprivate static Logger LOG = Logger.getLogger(CoNLLStreamExtractor.class);\r\n \r\n-\tstatic final int MAXITERATE = 999; // maximal update iterations allowed until the update loop is canceled and an error msg is thrown - to prevent faulty update scripts running in an endless loop\r\n-\t\r\n-\t\r\n \tprivate String baseURI;\r\n+\r\n \tpublic String getBaseURI() {\r\n \t\treturn baseURI;\r\n \t}\r\n@@ -95,70 +74,76 @@ public void setUpdates(List<Pair<String, String>> updates) {\n \t\tthis.updates = updates;\r\n \t}\r\n \r\n-\r\n-\tprivate List<String> columns = new ArrayList<String>(); \r\n+\tprivate List<String> columns = new ArrayList<String>();\r\n \tprivate String select = null;\r\n \tList<Pair<String, String>> updates = new ArrayList<Pair<String, String>>();\r\n-\t\r\n-\tprotected void processSentenceStream() throws Exception {\r\n-\t\tint current_sentence = 1; // keeps track of sentence id from CoNLL2RDF\r\n+\r\n+\t@Override\r\n+\tprotected void processSentenceStream() throws IOException {\r\n \t\tCoNLL2RDF conll2rdf = new CoNLL2RDF(baseURI, columns.toArray(new String[columns.size()]));\r\n-\t\tList<Pair<Integer,Long> > dRTs = new ArrayList<Pair<Integer,Long> >(); // iterations and execution time of each update in seconds\r\n+\t\tList<Pair<Integer, Long>> dRTs = new ArrayList<Pair<Integer, Long>>(); // iterations and execution time of each\r\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t// update in seconds\r\n \t\tLOG.info(\"process input ..\");\r\n \t\tBufferedReader in = getInputStream();\r\n \t\tOutputStreamWriter out = new OutputStreamWriter(getOutputStream());\r\n \t\tString buffer = \"\";\r\n \t\tArrayList<String> comments = new ArrayList<>();\r\n-\t\tfor(String line = \"\"; line !=null; line=in.readLine()) {\r\n-\t\t\tif(line.contains(\"#\")) {\r\n+\t\tfor (String line = \"\"; line != null; line = in.readLine()) {\r\n+\t\t\tif (line.contains(\"#\")) {\r\n \t\t\t\tout.write(line.replaceAll(\"^[^#]*#\", \"#\") + \"\\n\");\r\n \t\t\t\tcomments.add(line.replaceAll(\"^[^#]*#\", \"\"));\r\n \t\t\t}\r\n-\t\t\tline=line.replaceAll(\"<[\\\\/]?[psPS]( [^>]*>|>)\",\"\").trim(); // in this way, we can also read sketch engine data and split at s and p elements\r\n-\t\t\tif(!(line.matches(\"^<[^>]*>$\")))\t\t\t\t\t\t\t// but we skip all other XML elements, as used by Sketch Engine or TreeTagger chunker\r\n-\t\t\t\tif(line.equals(\"\") && !buffer.trim().equals(\"\")) {\r\n-\t\t\t\t\tModel m = conll2rdf.conll2model(new StringReader(buffer+\"\\n\"));\r\n-\t\t\t\t\tif(m!=null) { // null if an error occurred\r\n-\t\t\t\t\t\tList<Pair<Integer,Long> > ret = update(m, updates);\r\n+\t\t\tline = line.replaceAll(\"<[\\\\/]?[psPS]( [^>]*>|>)\", \"\").trim(); // in this way, we can also read sketch\r\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t// engine data and split at s and p elements\r\n+\t\t\tif (!(line.matches(\"^<[^>]*>$\"))) // but we skip all other XML elements, as used by Sketch Engine or\r\n+\t\t\t\t\t\t\t\t\t\t\t\t// TreeTagger chunker\r\n+\t\t\t\tif (line.equals(\"\") && !buffer.trim().equals(\"\")) {\r\n+\t\t\t\t\tModel m = conll2rdf.conll2model(new StringReader(buffer + \"\\n\"));\r\n+\t\t\t\t\tif (m != null) { // null if an error occurred\r\n+\t\t\t\t\t\tList<Pair<Integer, Long>> ret = update(m, updates);\r\n \t\t\t\t\t\tif (dRTs.isEmpty())\r\n \t\t\t\t\t\t\tdRTs = ret;\r\n \t\t\t\t\t\telse\r\n \t\t\t\t\t\t\tfor (int x = 0; x < ret.size(); ++x)\r\n-\t\t\t\t\t\t\t\tdRTs.set(x, new Pair<Integer, Long>(dRTs.get(x).getKey() + ret.get(x).getKey(), dRTs.get(x).getValue() + ret.get(x).getValue()));\r\n+\t\t\t\t\t\t\t\tdRTs.set(x, new ImmutablePair<Integer, Long>(dRTs.get(x).getKey() + ret.get(x).getKey(),\r\n+\t\t\t\t\t\t\t\t\t\tdRTs.get(x).getValue() + ret.get(x).getValue()));\r\n \t\t\t\t\t\tif (comments.size() > 0) {\r\n \t\t\t\t\t\t\tm = injectSentenceComments(m, comments);\r\n \t\t\t\t\t\t\tcomments.clear();\r\n \t\t\t\t\t\t}\r\n-\t\t\t\t\t\tprint(m,select, out);\r\n+\t\t\t\t\t\tprint(m, select, out);\r\n \t\t\t\t\t}\r\n-\t\t\t\t\tbuffer=\"\";\r\n+\t\t\t\t\tbuffer = \"\";\r\n \t\t\t\t} else\r\n-\t\t\t\t\tbuffer=buffer+line+\"\\n\";\r\n+\t\t\t\t\tbuffer = buffer + line + \"\\n\";\r\n \t\t}\r\n-\t\tif(!buffer.trim().equals(\"\")) {\r\n-\t\t\tModel m = conll2rdf.conll2model(new StringReader(buffer+\"\\n\"));\r\n-\t\t\tList<Pair<Integer,Long> > ret = update(m, updates);\r\n+\t\tif (!buffer.trim().equals(\"\")) {\r\n+\t\t\tModel m = conll2rdf.conll2model(new StringReader(buffer + \"\\n\"));\r\n+\t\t\tList<Pair<Integer, Long>> ret = update(m, updates);\r\n \t\t\tif (dRTs.isEmpty())\r\n \t\t\t\tdRTs = ret;\r\n \t\t\telse\r\n \t\t\t\tfor (int x = 0; x < ret.size(); ++x)\r\n-\t\t\t\t\tdRTs.set(x, new Pair<Integer, Long>(dRTs.get(x).getKey() + ret.get(x).getKey(), dRTs.get(x).getValue() + ret.get(x).getValue()));\r\n+\t\t\t\t\tdRTs.set(x, new ImmutablePair<Integer, Long>(dRTs.get(x).getKey() + ret.get(x).getKey(),\r\n+\t\t\t\t\t\t\tdRTs.get(x).getValue() + ret.get(x).getValue()));\r\n \t\t\tif (comments.size() > 0) {\r\n \t\t\t\tm = injectSentenceComments(m, comments);\r\n \t\t\t\tcomments.clear();\r\n \t\t\t}\r\n-\t\t\tprint(m,select,out);\r\n+\t\t\tprint(m, select, out);\r\n \t\t}\r\n \t\tif (!dRTs.isEmpty())\r\n-\t\t\tLOG.debug(\"Done - List of interations and execution times for the updates done (in given order):\\n\\t\\t\" + dRTs.toString());\r\n+\t\t\tLOG.debug(\"Done - List of interations and execution times for the updates done (in given order):\\n\\t\\t\"\r\n+\t\t\t\t\t+ dRTs.toString());\r\n \r\n \t\tgetOutputStream().close();\r\n-\t\r\n \t}\r\n \r\n \t/**\r\n-\t * Adds a list of conll comments to a sentence model as a rdfs:comment property separated by escaped newlines.\r\n-\t * @param model a RDF Model representing a sentence\r\n+\t * Adds a list of conll comments to a sentence model as a rdfs:comment property\r\n+\t * separated by escaped newlines.\r\n+\t * \r\n+\t * @param model    a RDF Model representing a sentence\r\n \t * @param comments a list of single line comments\r\n \t * @return the updated model\r\n \t */\r\n@@ -167,21 +152,26 @@ private Model injectSentenceComments(Model model, ArrayList<String> comments) {\n \t\t// alternative to ParameterizedSparqlString: UpdateQuery\r\n \t\tParameterizedSparqlString s = new ParameterizedSparqlString();\r\n \t\ts.setCommandText(\"PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\\n\"\r\n-\t\t\t+\"PREFIX nif: <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#>\\n\"\r\n-\t\t\t+\"INSERT { ?node rdfs:comment ?comment . }\"\r\n-\t\t\t+\"WHERE { ?node a nif:Sentence . }\");\r\n+\t\t\t\t+ \"PREFIX nif: <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#>\\n\"\r\n+\t\t\t\t+ \"INSERT { ?node rdfs:comment ?comment . }\" + \"WHERE { ?node a nif:Sentence . }\");\r\n \t\ts.setLiteral(\"comment\", String.join(\"\\\\n\", comments));\r\n \r\n \t\tUpdateAction.execute(s.asUpdate(), model);\r\n \t\treturn model;\r\n-\t} \r\n+\t}\r\n+\r\n \t/**\r\n-\t * Searches a BufferedReader for a global.columns = field to extract the column names from (CoNLL-U Plus feature).\r\n-\t * We allow for arbitrary lines to search, however as of September 2019, CoNLL-U Plus only allows first line.\r\n-\t * Does NOT validate if custom columns are in a separate name space, as required by the format.\r\n-\t * @see <a href=\"https://universaldependencies.org/ext-format.html\">CoNLL-U Plus Format</a>\r\n-\t * @param inputStream an untouched BufferedReader\r\n-\t * @param maxLinesToSearch how many lines to search. CoNLL-U Plus requires this to be 1.\r\n+\t * Searches a BufferedReader for a global.columns = field to extract the column\r\n+\t * names from (CoNLL-U Plus feature). We allow for arbitrary lines to search,\r\n+\t * however as of September 2019, CoNLL-U Plus only allows first line. Does NOT\r\n+\t * validate if custom columns are in a separate name space, as required by the\r\n+\t * format.\r\n+\t * \r\n+\t * @see <a href=\"https://universaldependencies.org/ext-format.html\">CoNLL-U Plus\r\n+\t *      Format</a>\r\n+\t * @param inputStream      an untouched BufferedReader\r\n+\t * @param maxLinesToSearch how many lines to search. CoNLL-U Plus requires this\r\n+\t *                         to be 1.\r\n \t */\r\n \tstatic List<String> findFieldsFromComments(BufferedReader inputStream, int maxLinesToSearch) {\r\n \t\tList<String> fields = new ArrayList<>();\r\n@@ -202,23 +192,21 @@ private Model injectSentenceComments(Model model, ArrayList<String> comments) {\n \t\t\t\tmeanByteSizeOfLine = ((meanByteSizeOfLine * (m - 1)) + peekedLine.length()) / m;\r\n \t\t\t\tLOG.debug(\"Mean Byte size: \" + meanByteSizeOfLine);\r\n \t\t\t\tpeekedChars += peekedLine.length();\r\n-\t\t\t\tSystem.err.println(\"Peeking line: \"+peekedLine);\r\n+\t\t\t\tSystem.err.println(\"Peeking line: \" + peekedLine);\r\n \t\t\t\tif ((peekedChars + meanByteSizeOfLine) > readAheadLimit) {\r\n \t\t\t\t\tLOG.info(\"Couldn't find CoNLL-U Plus columns.\");\r\n \t\t\t\t\tinputStream.reset();\r\n \t\t\t\t\treturn fields;\r\n \t\t\t\t}\r\n \t\t\t\tLOG.debug(\"Testing line: \" + peekedLine);\r\n \t\t\t\tif (peekedLine.matches(\"^#\\\\s?global\\\\.columns\\\\s?=.*\")) {\r\n-\t\t\t\t\tfields.addAll(Arrays.asList(peekedLine.trim()\r\n-\t\t\t\t\t\t\t.replaceFirst(\"#\\\\s?global\\\\.columns\\\\s?=\", \"\")\r\n-\t\t\t\t\t\t\t.split(\"#\")[0]\r\n-\t\t\t\t\t\t\t.trim().split(\" |\\t\")));\r\n+\t\t\t\t\tfields.addAll(\r\n+\t\t\t\t\t\t\tArrays.asList(peekedLine.trim().replaceFirst(\"#\\\\s?global\\\\.columns\\\\s?=\", \"\").split(\"#\")[0]\r\n+\t\t\t\t\t\t\t\t\t.trim().split(\" |\\t\")));\r\n \t\t\t\t\tinputStream.reset();\r\n \t\t\t\t\tLOG.info(\"Success\");\r\n \t\t\t\t\treturn fields;\r\n \t\t\t\t}\r\n-\r\n \t\t\t}\r\n \t\t\tinputStream.reset();\r\n \t\t} catch (IOException e) {\r\n@@ -227,9 +215,10 @@ private Model injectSentenceComments(Model model, ArrayList<String> comments) {\n \t\t}\r\n \t\treturn fields;\r\n \t}\r\n-\tpublic List<Pair<Integer,Long> > update(Model m, List<Pair<String,String>> updates) {\r\n-\t\tList<Pair<Integer,Long> > result = new ArrayList<Pair<Integer,Long> >();\r\n-\t\tfor(Pair<String,String> update : updates) {\r\n+\r\n+\tpublic List<Pair<Integer, Long>> update(Model m, List<Pair<String, String>> updates) {\r\n+\t\tList<Pair<Integer, Long>> result = new ArrayList<Pair<Integer, Long>>();\r\n+\t\tfor (Pair<String, String> update : updates) {\r\n \t\t\tLong startTime = System.currentTimeMillis();\r\n \t\t\tChangedListener cL = new ChangedListener();\r\n \t\t\tm.register(cL);\r\n@@ -242,7 +231,7 @@ private Model injectSentenceComments(Model model, ArrayList<String> comments) {\n \t\t\t\tif (!\"*\".equals(update.getValue()))\r\n \t\t\t\t\tthrow e;\r\n \t\t\t}\r\n-\t\t\twhile(v < frq && change) {\r\n+\t\t\twhile (v < frq && change) {\r\n \t\t\t\tUpdateAction.execute(UpdateFactory.create(update.getKey()), m);\r\n \t\t\t\tif (oldModel.isEmpty())\r\n \t\t\t\t\tchange = cL.hasChanged();\r\n@@ -256,30 +245,37 @@ private Model injectSentenceComments(Model model, ArrayList<String> comments) {\n \t\t\t}\r\n \t\t\tif (v == MAXITERATE)\r\n \t\t\t\tLOG.warn(\"Warning: MAXITERATE reached.\");\r\n-\t\t\tresult.add(new Pair<Integer, Long>(v, System.currentTimeMillis() - startTime));\r\n+\t\t\tresult.add(new ImmutablePair<Integer, Long>(v, System.currentTimeMillis() - startTime));\r\n \t\t\tm.unregister(cL);\r\n \t\t}\r\n \t\treturn result;\r\n \t}\r\n-\t\t\r\n-\t/** run either SELECT statement (cf. https://jena.apache.org/documentation/query/app_api.html) and return CoNLL-like TSV or just TTL <br>\r\n-\t *  Note: this CoNLL-like export has limitations, of course: it will export one property per column, hence, collapsed dependencies or \r\n-\t *  SRL annotations cannot be reconverted */\r\n+\r\n+\t/**\r\n+\t * run either SELECT statement (cf.\r\n+\t * https://jena.apache.org/documentation/query/app_api.html) and return\r\n+\t * CoNLL-like TSV or just TTL <br>\r\n+\t * Note: this CoNLL-like export has limitations, of course: it will export one\r\n+\t * property per column, hence, collapsed dependencies or SRL annotations cannot\r\n+\t * be reconverted\r\n+\t */\r\n \tpublic void print(Model m, String select, Writer out) throws IOException {\r\n-\t\tif(select!=null) {\r\n+\t\tif (select != null) {\r\n \t\t\tQueryExecution qexec = QueryExecutionFactory.create(select, m);\r\n \t\t\tResultSet results = qexec.execSelect();\r\n \t\t\tList<String> cols = results.getResultVars();\r\n-\t\t\tout.write(\"# \"); \t\t\t\t\t\t\t\t\t// well, this may be redundant, but permitted in CoNLL\r\n-\t\t\tfor(String col : cols)\r\n-\t\t\t\tout.write(col+\"\\t\");\r\n+\t\t\tout.write(\"# \"); // well, this may be redundant, but permitted in CoNLL\r\n+\t\t\tfor (String col : cols)\r\n+\t\t\t\tout.write(col + \"\\t\");\r\n \t\t\tout.write(\"\\n\");\r\n \t\t\tout.flush();\r\n-\t\t\twhile(results.hasNext()) {\r\n+\t\t\twhile (results.hasNext()) {\r\n \t\t\t\tQuerySolution sol = results.next();\r\n-\t\t\t\tfor(String col : cols)\r\n-\t\t\t\t\tif(sol.get(col)==null) out.write(\"_\\t\");\t\t// CoNLL practice\r\n-\t\t\t\t\telse out.write(sol.get(col)+\"\\t\");\r\n+\t\t\t\tfor (String col : cols)\r\n+\t\t\t\t\tif (sol.get(col) == null)\r\n+\t\t\t\t\t\tout.write(\"_\\t\"); // CoNLL practice\r\n+\t\t\t\t\telse\r\n+\t\t\t\t\t\tout.write(sol.get(col) + \"\\t\");\r\n \t\t\t\tout.write(\"\\n\");\r\n \t\t\t\tout.flush();\r\n \t\t\t}\r\n@@ -291,134 +287,58 @@ public void print(Model m, String select, Writer out) throws IOException {\n \t\t}\r\n \t}\r\n \r\n-\r\n-\tpublic static void main(String[] argv) throws Exception {\r\n-\t\tLOG.info(\"synopsis: CoNLLStreamExtractor baseURI FIELD1[.. FIELDn] [-u SPARQL_UPDATE1..m] [-s SPARQL_SELECT]\\n\"+\r\n-\t\t\t\"\\tbaseURI       CoNLL base URI, cf. CoNLL2RDF\\n\"+\r\n-\t\t\t\"\\tFIELDi        CoNLL field label, cf. CoNLL2RDF\\n\"+\r\n-\t\t\t\"\\tSPARQL_UPDATE SPARQL UPDATE (DELETE/INSERT) query, either literally or its location (file/uri)\\n\"+\r\n-\t\t\t\"\\t              can be followed by an optional integer in {}-parentheses = number of repetitions\\n\"+\r\n-\t\t\t\"\\t              The SPARQL_UPDATE parameter is DEPRECATED - please use CoNLLRDFUpdater instead!\\n\"+\r\n-\t\t\t\"\\tSPARQL_SELECT SPARQL SELECT statement to produce TSV output\\n\"+\r\n-\t\t\t\"\\treads CoNLL from stdin, splits sentences, creates CoNLL RDF, applies SPARQL queries\");\r\n-\t\t\r\n-\t\tString baseURI = argv[0];\r\n-\t\tList<String> fields = new ArrayList<String>();\r\n-\t\tList<Pair<String, String>> updates = new ArrayList<Pair<String, String>>();\r\n-\t\tString select = null;\r\n-\t\tBufferedReader inputStream = new BufferedReader(new InputStreamReader(System.in));\r\n-\t\t\r\n-\t\tint i = 1;\r\n-\t\twhile(i<argv.length && !argv[i].toLowerCase().matches(\"^-+u$\"))\r\n-\t\t\tfields.add(argv[i++]);\r\n-\t\twhile(i<argv.length && argv[i].toLowerCase().matches(\"^-+u$\")) i++;\r\n-\t\twhile(i<argv.length && !argv[i].toLowerCase().matches(\"^-+s$\")) {\r\n-\t\t\tString freq;\r\n-\t\t\tfreq = argv[i].replaceFirst(\".*\\\\{([0-9u*]+)\\\\}$\", \"$1\");\r\n-\t\t\tif (argv[i].equals(freq))\r\n-\t\t\t\tfreq = \"1\";\r\n-\t\t\telse if (freq.equals(\"u\"))\r\n-\t\t\t\tfreq = \"*\";\r\n-\t\t\tString update =argv[i++].replaceFirst(\"\\\\{[0-9*]+\\\\}$\", \"\");\r\n-\t\t\tupdates.add(new Pair<String, String>(update, freq));\r\n+\tpublic Pair<String, String> parseUpdate(String updateArg) throws IOException {\r\n+\t\tString freq;\r\n+\t\t// FIXME\r\n+\t\tfreq = updateArg.replaceFirst(\".*\\\\{([0-9u*]+)\\\\}$\", \"$1\");\r\n+\t\tif (updateArg.equals(freq)) {\r\n+\t\t\tfreq = \"1\";\r\n+\t\t} else if (freq.equals(\"u\")) {\r\n+\t\t\tfreq = \"*\";\r\n \t\t}\r\n-\t\twhile(i<argv.length && argv[i].toLowerCase().matches(\"^-+s$\")) i++;\r\n-\t\tif(i<argv.length)\r\n-\t\t\tselect=argv[i++];\r\n-\t\twhile(i<argv.length)\r\n-\t\t\tselect=select+\" \"+argv[i++]; // because queries may be parsed by the shell (Cygwin)\r\n-\t\t\r\n-\t\tif (fields.size() == 0) { // might be conllu plus, we check the first line for col names.\r\n-\t\t\tfields = findFieldsFromComments(inputStream, 1);\r\n-\t\t}\r\n-\r\n-\t\tLOG.info(\"running CoNLLStreamExtractor\");\r\n-\t\tLOG.info(\"\\tbaseURI:       \"+baseURI);\r\n-\t\tLOG.info(\"\\tCoNLL columns: \"+fields);\r\n-\t\tLOG.info(\"\\tSPARQL update: \"+updates);\r\n-\t\tLOG.info(\"\\tSPARQL select: \"+select);\r\n-\t\t\r\n-\t\tLOG.info(\"read SPARQL ..\");\r\n-\t\t//UpdateRequest request = UpdateFactory.create();\r\n-\t\tStringBuilder sb = new StringBuilder();\r\n-\t\tfor(i = 0; i<updates.size(); i++) {\r\n-\t\t\tReader sparqlreader = new StringReader(updates.get(i).getKey());\r\n-\t\t\tFile f = new File(updates.get(i).getKey());\r\n-\t\t\tURL u = null;\r\n-\t\t\ttry {\r\n-\t\t\t\tu = new URL(updates.get(i).getKey());\r\n-\t\t\t} catch (MalformedURLException e) {}\r\n+\t\tfinal String updateRaw = updateArg.replaceFirst(\"\\\\{[0-9*]+\\\\}$\", \"\");\r\n+\t\treturn new ImmutablePair<String, String>(updateRaw, freq);\r\n+\t\t// UpdateRequest request = UpdateFactory.create();\r\n+\t}\r\n \r\n-\t\t\tif(f.exists()) {\t\t\t// can be read from a file\r\n-\t\t\t\tsparqlreader = new FileReader(f);\r\n-\t\t\t\tsb.append(\"f\");\r\n-\t\t\t} else if(u!=null) {\r\n-\t\t\t\ttry {\r\n-\t\t\t\t\tsparqlreader = new InputStreamReader(u.openStream());\r\n-\t\t\t\t\tsb.append(\"u\");\r\n-\t\t\t\t} catch (Exception e) {}\r\n-\t\t\t}\r\n+\tpublic String parseSparqlArg(String sparqlArg) throws IOException {\r\n+\t\t// FIXME\r\n+\t\t// SELECT\r\n+\t\tString sparql = \"\";\r\n \r\n-\t\t\tupdates.set(i,new Pair<String, String>(\"\", updates.get(i).getValue()));\r\n-\t\t\tBufferedReader in = new BufferedReader(sparqlreader);\r\n-\t\t\tfor(String line = in.readLine(); line!=null; line=in.readLine())\r\n-\t\t\t\tupdates.set(i,new Pair<String, String>(updates.get(i).getKey()+line+\"\\n\",updates.get(i).getValue()));\r\n-\t\t\tsb.append(\".\");\r\n+\t\tReader sparqlreader = new StringReader(sparqlArg);\r\n+\t\tFile file = new File(sparqlArg);\r\n+\t\tURL url = null;\r\n+\t\ttry {\r\n+\t\t\turl = new URL(sparqlArg);\r\n+\t\t} catch (MalformedURLException e) {\r\n \t\t}\r\n-\t\tsb.append(\".\");\r\n-\t\t\r\n-\t\tif(select!=null) {\r\n-\t\t\tReader sparqlreader = new StringReader(select);\r\n-\t\t\tFile f = new File(select);\r\n-\t\t\tURL u = null;\r\n+\r\n+\t\tif (file.exists()) { // can be read from a file\r\n+\t\t\tsparqlreader = new FileReader(file);\r\n+\t\t} else if (url != null) {\r\n \t\t\ttry {\r\n-\t\t\t\tu = new URL(select);\r\n-\t\t\t} catch (MalformedURLException e) {}\r\n-\t\t\t\r\n-\t\t\tif(f.exists()) {\t\t\t// can be read from a file\r\n-\t\t\t\tsparqlreader = new FileReader(f);\r\n-\t\t\t\tsb.append(\"f\");\r\n-\t\t\t} else if(u!=null) {\r\n-\t\t\t\ttry {\r\n-\t\t\t\t\tsparqlreader = new InputStreamReader(u.openStream());\r\n-\t\t\t\t\tsb.append(\"u\");\r\n-\t\t\t\t} catch (Exception e) {}\r\n+\t\t\t\tsparqlreader = new InputStreamReader(url.openStream());\r\n+\t\t\t} catch (Exception e) {\r\n \t\t\t}\r\n+\t\t}\r\n \r\n-\t\t\tBufferedReader in = new BufferedReader(sparqlreader);\r\n-\t\t\tselect=\"\";\r\n-\t\t\tfor(String line = in.readLine(); line!=null; line=in.readLine())\r\n-\t\t\t\tselect=select+line+\"\\n\";\r\n-\t\t}\t\t\r\n-\t\tsb.append(\". ok\");\r\n-\t\tLOG.info(sb.toString());\r\n-\t\t\r\n-\r\n-\t\tCoNLLStreamExtractor ex = new CoNLLStreamExtractor();\r\n-\t\tex.setBaseURI(baseURI);\r\n-\t\tex.setColumns(fields);\r\n-\t\tex.setUpdates(updates);\r\n-\t\tex.setSelect(select);\r\n-\t\tex.setInputStream(inputStream);\r\n-\t\tex.setOutputStream(System.out);\r\n-\t\t\r\n-\t\tex.processSentenceStream();\r\n-\t\t\r\n-\t}\r\n-\t\r\n-\t\r\n-\t@Override\r\n-\tpublic void run() {\r\n-\t\ttry {\r\n-\t\t\tprocessSentenceStream();\r\n-\t\t} catch (Exception e) {\r\n-\t\t\te.printStackTrace();\r\n-\t\t\tSystem.exit(0);\r\n+\t\tBufferedReader in = new BufferedReader(sparqlreader);\r\n+\t\tfor (String line = in.readLine(); line != null; line = in.readLine()) {\r\n+\t\t\tsparql = sparql + line + \"\\n\";\r\n \t\t}\r\n+\t\treturn sparql;\r\n \t}\r\n \r\n-\t@Override\r\n-\tpublic void start() {\r\n-\t\trun();\r\n+\tpublic static void main(String[] args) throws IOException {\r\n+\t\tfinal CoNLLStreamExtractor extractor;\r\n+\t\ttry {\r\n+\t\t\textractor = CoNLLStreamExtractorFactory.getStreamExtractor(args);\r\n+\t\t} catch (ParseException e) {\r\n+\t\t\tLOG.error(e);\r\n+\t\t\tSystem.exit(1);\r\n+\t\t\treturn;\r\n+\t\t}\r\n+\t\textractor.processSentenceStream();\r\n \t}\r\n-}\n\\ No newline at end of file\n+}\r"
  },
  {
    "sha": "a030a0c3c852880f9d532d91872b71910adec9e0",
    "filename": "src/main/java/org/acoli/conll/rdf/CoNLLStreamExtractorFactory.java",
    "status": "added",
    "additions": 67,
    "deletions": 0,
    "changes": 67,
    "blob_url": "https://github.com/acoli-repo/conll-rdf/blob/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/CoNLLStreamExtractorFactory.java",
    "raw_url": "https://github.com/acoli-repo/conll-rdf/raw/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/CoNLLStreamExtractorFactory.java",
    "contents_url": "https://api.github.com/repos/acoli-repo/conll-rdf/contents/src/main/java/org/acoli/conll/rdf/CoNLLStreamExtractorFactory.java?ref=4f49b0dd21f0cfe1898b860851c18fcd891fdf13",
    "patch": "@@ -0,0 +1,67 @@\n+package org.acoli.conll.rdf;\n+\n+import static org.acoli.conll.rdf.CoNLLStreamExtractor.findFieldsFromComments;\n+\n+import java.io.*;\n+import java.util.*;\n+\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.commons.cli.Option;\n+import org.apache.commons.cli.ParseException;\n+import org.apache.commons.lang3.tuple.*;\n+import org.apache.log4j.Logger;\n+\n+public class CoNLLStreamExtractorFactory {\n+    static Logger LOG = Logger.getLogger(CoNLLStreamExtractorFactory.class);\n+    public static CoNLLStreamExtractor getStreamExtractor(String[] args) throws IOException, ParseException {\n+        CoNLLStreamExtractor extractor = new CoNLLStreamExtractor();\n+\t\t//FIXME\n+\t\tList<Pair<String, String>> updates = new ArrayList<Pair<String, String>>();\n+\n+\t\tfinal CommandLine cmd = new CoNLLRDFCommandLine(\"synopsis: CoNLLStreamExtractor baseURI FIELD1[.. FIELDn] [-u SPARQL_UPDATE1..m] [-s SPARQL_SELECT]\\n\"\n+\t\t+ \"\\tbaseURI       CoNLL base URI, cf. CoNLL2RDF\\n\"\n+\t\t+ \"\\tFIELDi        CoNLL field label, cf. CoNLL2RDF\",\n+\t\t\"reads CoNLL from stdin, splits sentences, creates CoNLL RDF, applies SPARQL queries\",\n+\t\tnew Option[] {\n+\t\t\tOption.builder(\"s\").hasArg().hasArgs().desc(\"SPARQL SELECT statement to produce TSV output\").build(),\n+\t\t\tOption.builder(\"u\").hasArgs().argName(\"sparql_update\").desc(\"DEPRECATED - please use CoNLLRDFUpdater instead!\").build()\n+\t\t\t/* \"SPARQL_UPDATE SPARQL UPDATE (DELETE/INSERT) query, either literally or its location (file/uri).\n+\t\t\tCan be followed by an optional integer in {}-parentheses = number of repetitions\" */\n+\t\t}, LOG).parseArgs(args);\n+\n+\t\tList<String> argList = cmd.getArgList();\n+\t\tif (argList.isEmpty()) {\n+\t\t\tthrow new ParseException(\"Missing required Argument baseURI\");\n+\t\t}\n+\t\textractor.setBaseURI(argList.remove(0));\n+\n+\t\tif (argList.isEmpty()) { // might be conllu plus, we check the first line for col names.\n+\t\t\textractor.setColumns(findFieldsFromComments(extractor.getInputStream(), 1));\n+\t\t\tif (extractor.getColumns().isEmpty()) { // FIXME this should probably be a catch block\n+\t\t\t\tthrow new ParseException(\"Missing required Argument Fields/Columns not found as global.columns either\");\n+\t\t\t}\n+\t\t} else {\n+\t\t\textractor.setColumns(argList);\n+\t\t}\n+\n+\t\tif (cmd.hasOption(\"s\")) {\n+\t\t\t// setSelect(parseSparqlArg(String.join(\" \", Arrays.asList(cmd.getOptionValues(\"s\")))));\n+\t\t\textractor.setSelect(String.join(\" \", Arrays.asList(cmd.getOptionValues(\"s\")))); //FIXME\n+\t\t}\n+\n+\t\tif (cmd.hasOption(\"u\")) {\n+\t\t\tLOG.warn(\"using -u to provide updates is deprecated\");\n+\t\t\tfor (String arg : cmd.getOptionValues(\"u\")) {\n+\t\t\t\tPair<String, String> update = extractor.parseUpdate(arg);\n+\t\t\t\tupdates.add(new ImmutablePair<String, String>(extractor.parseSparqlArg(update.getKey()), update.getValue()));\n+\t\t\t\t// FIXME\n+\t\t\t}\n+\t\t}\n+\n+\t\tLOG.info(\"running CoNLLStreamExtractor\");\n+\t\tLOG.info(\"\\tbaseURI:       \" + extractor.getBaseURI());\n+\t\tLOG.info(\"\\tCoNLL columns: \" + extractor.getColumns());\n+\n+        return extractor;\n+\t}\n+}"
  },
  {
    "sha": "6f0d3ab8b98075bf9debd31727f0fac16066c688",
    "filename": "src/main/java/org/acoli/conll/rdf/Format2RDF.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/acoli-repo/conll-rdf/blob/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/Format2RDF.java",
    "raw_url": "https://github.com/acoli-repo/conll-rdf/raw/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/Format2RDF.java",
    "contents_url": "https://api.github.com/repos/acoli-repo/conll-rdf/contents/src/main/java/org/acoli/conll/rdf/Format2RDF.java?ref=4f49b0dd21f0cfe1898b860851c18fcd891fdf13",
    "patch": "@@ -38,7 +38,7 @@\n  **/\r\n abstract class Format2RDF {\r\n \t\r\n-\tprivate static Logger LOG = Logger.getLogger(Format2RDF.class.getName());\r\n+\tprivate static Logger LOG = Logger.getLogger(Format2RDF.class);\r\n \t\r\n \t/** can be null */\r\n \tprotected final BufferedWriter out;\r"
  },
  {
    "sha": "3dcecc12518da9402c764724cd8760db991c9146",
    "filename": "src/main/java/org/acoli/conll/rdf/SimpleLineBreakSplitter.java",
    "status": "modified",
    "additions": 22,
    "deletions": 27,
    "changes": 49,
    "blob_url": "https://github.com/acoli-repo/conll-rdf/blob/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/SimpleLineBreakSplitter.java",
    "raw_url": "https://github.com/acoli-repo/conll-rdf/raw/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/SimpleLineBreakSplitter.java",
    "contents_url": "https://api.github.com/repos/acoli-repo/conll-rdf/contents/src/main/java/org/acoli/conll/rdf/SimpleLineBreakSplitter.java?ref=4f49b0dd21f0cfe1898b860851c18fcd891fdf13",
    "patch": "@@ -1,56 +1,51 @@\n package org.acoli.conll.rdf;\n \n-import java.io.BufferedReader;\n import java.io.IOException;\n-import java.io.InputStreamReader;\n \n-public class SimpleLineBreakSplitter extends CoNLLRDFComponent {\n+import org.apache.commons.cli.Option;\n+import org.apache.commons.cli.ParseException;\n+import org.apache.log4j.Logger;\n \n+public class SimpleLineBreakSplitter extends CoNLLRDFComponent {\n+\tprivate static final Logger LOG = Logger.getLogger(SimpleLineBreakSplitter.class);\n \n-\tprivate void processSentenceStream() throws IOException {\n+\t@Override\n+\tprotected void processSentenceStream() throws IOException {\n \t\tString line;\n \t\tint empty = 0;\n-\t\twhile((line = getInputStream().readLine())!=null) {\n+\t\twhile ((line = getInputStream().readLine()) != null) {\n \t\t\tif (line.trim().isEmpty()) {\n \t\t\t\tempty++;\n \t\t\t} else {\n \t\t\t\tif (empty > 0) {\n \t\t\t\t\tgetOutputStream().print(\"\\n#newsegment\\n\");\n \t\t\t\t\tempty = 0;\n \t\t\t\t}\n-\t\t\t\tgetOutputStream().print(line+\"\\n\");\n+\t\t\t\tgetOutputStream().print(line + \"\\n\");\n \t\t\t}\n \t\t}\n \t\tgetOutputStream().close();\n \t}\n \n-\t@Override\n-\tpublic void run() {\n-\t\ttry {\n-\t\t\tprocessSentenceStream();\n-\t\t} catch (Exception e) {\n-\t\t\te.printStackTrace();\n-\t\t\tSystem.exit(0);\n-\t\t}\n-\t}\n-\n-\t@Override\n-\tpublic void start() {\n-\t\trun();\n+\t//TODO move method @Override\n+\tpublic void configureFromCommandLine(String[] args) throws IOException, ParseException {\n+\t\tnew CoNLLRDFCommandLine(\"SimpleLineBreakSplitter\", \"\", new Option[] {}, LOG).parseArgs(args);\n+\t\t// Nothing to do\n \t}\n \n \tpublic static void main(String[] args) throws IOException {\n-\t\tSystem.err.println(\"synopsis: SimpleLineBreakSplitter\");\n \t\tSimpleLineBreakSplitter splitter = new SimpleLineBreakSplitter();\n \n-\t\tlong start = System.currentTimeMillis();\n-\n-\t\tsplitter.setInputStream(new BufferedReader(new InputStreamReader(System.in)));\n-\t\tsplitter.setOutputStream(System.out);\n+\t\ttry {\n+\t\t\tsplitter.configureFromCommandLine(args);\n+\t\t} catch (ParseException e) {\n+\t\t\tLOG.error(e);\n+\t\t\tSystem.exit(1);\n+\t\t}\n \n-\t\t//READ SENTENCES from System.in\n+\t\tlong start = System.currentTimeMillis();\n+\t\t// READ SENTENCES from System.in\n \t\tsplitter.processSentenceStream();\n-\t\tSystem.err.println(((System.currentTimeMillis()-start)/1000 + \" seconds\"));\n+\t\tLOG.info((System.currentTimeMillis() - start) / 1000 + \" seconds\");\n \t}\n-\n }"
  },
  {
    "sha": "adeecb759d8546830ee9a6736d2466567c9e34e9",
    "filename": "src/main/java/org/acoli/conll/rdf/TenTen2XMLTSV.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/acoli-repo/conll-rdf/blob/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/TenTen2XMLTSV.java",
    "raw_url": "https://github.com/acoli-repo/conll-rdf/raw/4f49b0dd21f0cfe1898b860851c18fcd891fdf13/src/main/java/org/acoli/conll/rdf/TenTen2XMLTSV.java",
    "contents_url": "https://api.github.com/repos/acoli-repo/conll-rdf/contents/src/main/java/org/acoli/conll/rdf/TenTen2XMLTSV.java?ref=4f49b0dd21f0cfe1898b860851c18fcd891fdf13",
    "patch": "@@ -14,7 +14,7 @@\n \n \n \tList<String> tagsContainingData = Arrays.asList(\"kwik\", \"left\", \"right\");\n-\tprivate static Logger LOG = Logger.getLogger(TenTen2XMLTSV.class.getName());\n+\tprivate static Logger LOG = Logger.getLogger(TenTen2XMLTSV.class);\n \tpublic boolean isKEEP() {\n \t\treturn KEEP;\n \t}"
  }
]
