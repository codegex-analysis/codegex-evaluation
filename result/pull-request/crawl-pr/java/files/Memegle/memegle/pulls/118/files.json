[
  {
    "sha": "5cbe817a59d0a942ffc26b8bb23275f3356ba115",
    "filename": "server/scripts/baidu-scrapper.py",
    "status": "modified",
    "additions": 85,
    "deletions": 38,
    "changes": 123,
    "blob_url": "https://github.com/Memegle/memegle/blob/47aef16824179a214b8b6dde7d19431fb8307ffc/server/scripts/baidu-scrapper.py",
    "raw_url": "https://github.com/Memegle/memegle/raw/47aef16824179a214b8b6dde7d19431fb8307ffc/server/scripts/baidu-scrapper.py",
    "contents_url": "https://api.github.com/repos/Memegle/memegle/contents/server/scripts/baidu-scrapper.py?ref=47aef16824179a214b8b6dde7d19431fb8307ffc",
    "patch": "@@ -1,23 +1,54 @@\n import requests\n import urllib.request\n from os import mkdir\n-from os.path import exists, isdir\n+from os.path import exists, isdir,join,isfile,abspath\n import re\n+import argparse\n import unicodedata\n import json as js\n+import csv\n \n # Please manually filter out bad images after running the scripts.\n \n # Always change this two variable before executing\n # The + sign force the result to include pattern 表情包\n-query = '问号 +表情包'\n-tags = ['???', '问号', '？？？']\n+\n+\n+\n+# This function checks whether the input is string.\n+#\n+def string_check(val):\n+    fl= False\n+    if isinstance(val,str) is fl:\n+        raise argparse.ArgumentTypeError(\"Cannot accept non-string input\")\n+    return val\n+    \n+# This function checks whether the input is an integer.\n+#\n+def positive_int(num):\n+    val = int(num)\n+    if val < 0:\n+        raise argparse.ArgumentTypeError(\"Cannot accept negative start id\")\n+    return val\n+\n+parser = argparse.ArgumentParser()\n+parser.add_argument('searching_word', type=string_check)\n+parser.add_argument('count', type=positive_int)\n+parser.add_argument('-o', '--output_dir', default='./data/raw/')\n+\n+args = parser.parse_args()\n+query = args.searching_word\n+tags = [query]\n \n # Constants\n DOWNLOAD_FOLDER = 'data/raw/' + ';'.join(tags) + '/'\n+CSV_PATH = 'data/raw/meta.csv'\n+HEADERS = ['source_url','title','path', 'source']\n+\n # Each page returns 30 images, [start, end)\n-START_PAGE = 0\n-END_PAGE = 3\n+PHOTO_COUNT = args.count\n+#START_PAGE = 0\n+#END_PAGE = 3\n \n success = 0\n fail = 0\n@@ -26,6 +57,8 @@\n session.headers['User-Agent'] = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'\n \n \n+\n+\n def slugify(value, allow_unicode=False):\n     \"\"\"\n     Convert to ASCII if 'allow_unicode' is False. Convert spaces to hyphens.\n@@ -45,39 +78,53 @@ def slugify(value, allow_unicode=False):\n     print('Creating new folder {}'.format(DOWNLOAD_FOLDER))\n     mkdir(DOWNLOAD_FOLDER)\n \n-for page in range(START_PAGE, END_PAGE, 1):\n-    print('Processing page {}'.format(page))\n-    start_ind = page * 30\n-    url = 'https://tupian.baidu.com/search/acjson?tn=resultjson_com&logid=9967771140088488215&ipn=rj&ct=201326592&is=&fp=result&queryWord=' \\\n-          + query + '&cl=2&lm=-1&ie=utf-8&oe=utf-8&adpicid=&st=-1&z=&ic=0&hd=&latest=&copyright=&word=' \\\n-          + query + '&s=&se=&tab=&width=&height=&face=0&istype=2&qc=&nc=1&fr=&expermode=&force=&pn=' \\\n-          + str(start_ind) + '&rn=30&gsm=1e&1603393461671='\n-\n-    response = session.get(url)\n-    # handle invalid escapes\n-    text = response.text\n-    text = re.sub(r'\"fromPageTitle\":\".*?[^\\\\]\",', '', text)\n-    text = re.sub(r'\\\\([^\\/u\"])', '\\\\1', text, re.MULTILINE)\n-    result = re.sub(r'\\\\\\\\\"', '', text, re.MULTILINE)\n-\n-    json = js.loads(result, strict=False)\n-    data = json['data']\n-\n-    # download each image\n-    for i in range(len(data) - 1):\n-        try:\n-            d = data[i]\n-            img_url = d['middleURL']\n-            filename = slugify(d['fromPageTitleEnc'], allow_unicode=True)\n-            ext = '.' + d['type']\n-            path = DOWNLOAD_FOLDER + filename + ext\n-            urllib.request.urlretrieve(img_url, path)\n-            print('{}.{}: Saving {}'.format(page, i, path))\n-            success += 1\n-\n-        except Exception as e:\n-            print('{}.{}: {}'.format(page, i, str(e)))\n-            fail += 1\n \n+if isfile(CSV_PATH):\n+    f = open(CSV_PATH, 'a')\n+    writer = csv.DictWriter(f, fieldnames=HEADERS)\n+else:\n+    f = open(CSV_PATH, 'w')\n+    writer = csv.DictWriter(f, fieldnames=HEADERS)\n+    writer.writeheader()\n+\n+\n+#for page in range(START_PAGE, END_PAGE, 1):\n+page = 0\n+print('Processing page {}'.format(page))\n+start_ind = page\n+url = 'https://tupian.baidu.com/search/acjson?tn=resultjson_com&logid=9967771140088488215&ipn=rj&ct=201326592&is=&fp=result&queryWord=' \\\n+    + query + '&cl=2&lm=-1&ie=utf-8&oe=utf-8&adpicid=&st=-1&z=&ic=0&hd=&latest=&copyright=&word=' \\\n+    + query + '&s=&se=&tab=&width=&height=&face=0&istype=2&qc=&nc=1&fr=&expermode=&force=&pn=' \\\n+    + str(start_ind) + '&rn=30&gsm=1e&1603393461671='\n+\n+response = session.get(url)\n+# handle invalid escapes\n+text = response.text\n+text = re.sub(r'\"fromPageTitle\":\".*?[^\\\\]\",', '', text)\n+text = re.sub(r'\\\\([^\\/u\"])', '\\\\1', text, re.MULTILINE)\n+result = re.sub(r'\\\\\\\\\"', '', text, re.MULTILINE)\n+\n+json = js.loads(result, strict=False)\n+data = json['data']\n+\n+# download each image\n+for i in range(PHOTO_COUNT):\n+    try:\n+        d = data[i]\n+        img_url = d['middleURL']\n+        filename = slugify(d['fromPageTitleEnc'], allow_unicode=True)\n+        ext = '.' + d['type']\n+        path = DOWNLOAD_FOLDER + filename + ext\n+        if isfile(path):\n+            print('file already exists, skipping: {}'.format(path))\n+            continue\n+        urllib.request.urlretrieve(img_url, path)\n+        writer.writerow({'source_url': img_url,'title':filename, 'path': path, 'source':query})\n+        print('{}.{}: Saving {}'.format(page, i, path))\n+        success += 1\n+\n+    except Exception as e:\n+        print('{}.{}: {}'.format(page, i, str(e)))\n+        fail += 1\n \n print('successfully download {} images, failing {}'.format(success, fail))\n\\ No newline at end of file"
  }
]
