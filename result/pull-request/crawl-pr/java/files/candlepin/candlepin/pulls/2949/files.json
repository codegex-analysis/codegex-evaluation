[
  {
    "sha": "0dc253c75abbecfe57538b62acbfac624dabe282",
    "filename": "server/bin/deploy",
    "status": "modified",
    "additions": 2,
    "deletions": 2,
    "changes": 4,
    "blob_url": "https://github.com/candlepin/candlepin/blob/5808013a6c129a7fc63204c47d33e6fdedd667df/server/bin/deploy",
    "raw_url": "https://github.com/candlepin/candlepin/raw/5808013a6c129a7fc63204c47d33e6fdedd667df/server/bin/deploy",
    "contents_url": "https://api.github.com/repos/candlepin/candlepin/contents/server/bin/deploy?ref=5808013a6c129a7fc63204c47d33e6fdedd667df",
    "patch": "@@ -102,9 +102,9 @@ update_keystore() {\n \n upload_products() {\n   if [ \"$TESTDATA\" = \"1\" ]; then\n-      $SELF_DIR/import_test_data.rb $SELF_DIR/test_data.json\n+      $SELF_DIR/test_data_importer.py $SELF_DIR/test_data.json\n   elif [ \"$TESTDATA\" = \"MIN\" ]; then\n-      $SELF_DIR/import_test_data.rb $SELF_DIR/min_test_data.json\n+      $SELF_DIR/test_data_importer.py $SELF_DIR/min_test_data.json\n   fi\n }\n "
  },
  {
    "sha": "524145dd5566f12c12cd617a04e1e942d2a000bf",
    "filename": "server/bin/import_test_data.rb",
    "status": "removed",
    "additions": 0,
    "deletions": 447,
    "changes": 447,
    "blob_url": "https://github.com/candlepin/candlepin/blob/4fb7bd3b4fbba7e87046ed13c1811a92691c5260/server/bin/import_test_data.rb",
    "raw_url": "https://github.com/candlepin/candlepin/raw/4fb7bd3b4fbba7e87046ed13c1811a92691c5260/server/bin/import_test_data.rb",
    "contents_url": "https://api.github.com/repos/candlepin/candlepin/contents/server/bin/import_test_data.rb?ref=4fb7bd3b4fbba7e87046ed13c1811a92691c5260",
    "patch": "@@ -1,447 +0,0 @@\n-#!/usr/bin/env ruby\n-\n-require_relative \"../client/ruby/candlepin_api\"\n-require_relative \"../client/ruby/hostedtest_api\"\n-require_relative \"./thread_pool\"\n-\n-require 'rubygems'\n-require 'date'\n-require 'json'\n-require 'pp'\n-\n-# Impl note:\n-# We use print instead of puts throughout this script, as a majority of the operations are\n-# performed in parallel threads. Internally, puts prints the message and an automatic line break as\n-# two separate, non-atomic operations. This allows time for the threads to print their messages to\n-# the same line before the line break is written. We work around this here by using print with a\n-# line break in the message itself, reducing/eliminating the possibility for two+ threads to write\n-# a message to the same line.\n-\n-include HostedTest\n-SMALL_SUB_QUANTITY = 5\n-LARGE_SUB_QUANTITY = 10\n-\n-script_home = File.dirname(File.expand_path($0))\n-filenames=[\"#{script_home}/test_data.json\"]\n-if not ARGV.empty?\n-  filenames.clear\n-  ARGV.each do |filename|\n-    filenames.push(filename)\n-  end\n-end\n-\n-$data = {'products'=> [], 'content'=> [], 'owners'=> [], 'users'=> [], 'roles'=> []}\n-@sourceSubId = 0\n-\n-filenames.each do |filename|\n-  # puts filename\n-  product_data_buf = File.read(filename)\n-  product_data = JSON(product_data_buf, {})\n-  $data['products'] = $data.fetch('products') + product_data['products'] unless product_data['products'].nil?\n-  $data['content'] = $data.fetch('content') + product_data['content'] unless product_data['content'].nil?\n-  $data['owners'] = $data.fetch('owners') + product_data['owners'] unless product_data['owners'].nil?\n-  $data['users'] = $data.fetch('users') + product_data['users'] unless product_data['users'].nil?\n-  $data['roles'] = $data.fetch('roles') + product_data['roles'] unless product_data['roles'].nil?\n-end\n-\n-cp = Candlepin.new('admin', 'admin', nil, nil, 'localhost', 8443)\n-@cp = cp\n-\n-print \"\\nCreating owners\\n\"\n-def create_owner(cp, new_owner)\n-  owner_name =  new_owner['name']\n-  displayName = new_owner['displayName']\n-\n-  print \"owner: #{owner_name}\\n\"\n-  print \"    displayName: #{displayName}\\n\"\n-\n-  owner = cp.create_owner(owner_name, new_owner)\n-\n-  # Create one dummy activation key for the owner\n-  cp.create_activation_key(owner['key'], \"default_key\")\n-  cp.create_activation_key(owner['key'], \"awesome_os_pool\")\n-end\n-\n-thread_pool = ThreadPool.new(5)\n-$data['owners'].each do |new_owner|\n-    thread_pool.schedule(new_owner) do |new_owner|\n-        create_owner(cp, new_owner)\n-    end\n-end\n-thread_pool.shutdown\n-\n-\n-def create_user(cp, new_user)\n-  user_name = new_user['username']\n-  user_pass = new_user['password']\n-  user_super = new_user['superadmin'] || false\n-\n-  print \"user: #{user_name}\\n\"\n-  print \"    password: #{user_pass}\\n\"\n-  print \"    super_user: #{user_super}\\n\"\n-\n-  cp.create_user(user_name, user_pass, user_super)\n-end\n-\n-print \"\\nCreate some users\\n\"\n-thread_pool = ThreadPool.new(5)\n-$data['users'].each do |new_user|\n-    thread_pool.schedule(new_user) {|new_user| create_user(cp, new_user) }\n-end\n-thread_pool.shutdown\n-\n-# Create roles:\n-print \"\\nCreate some roles\\n\"\n-\n-def create_role(cp, new_role)\n-  role_name = new_role['name']\n-  perms = new_role['permissions']\n-  users = new_role['users']\n-\n-  print \"role_name: #{role_name}\\n\"\n-  perms.each do |perm|\n-    # convert owner key to owner objects\n-    if (perm['owner'].is_a? String)\n-      perm['owner'] = { :key => perm['owner'] }\n-    end\n-\n-    print \"    owner: #{perm['owner']['key']}\\n\"\n-    print \"    access: #{perm['access']}\\n\"\n-  end\n-\n-  role = cp.create_role(role_name, perms)\n-\n-  users.each do |user|\n-    print \"    user: #{user['username']}\\n\"\n-    cp.add_role_user(role['name'], user['username'])\n-  end\n-end\n-\n-thread_pool = ThreadPool.new(5)\n-$data['roles'].each do |new_role|\n-    thread_pool.schedule(new_role) {|new_role| create_role(cp, new_role) }\n-end\n-thread_pool.shutdown\n-\n-\n-def create_content(cp, owner, product, content)\n-  print \"    content: #{owner['name']}/#{content['name']}/#{product['id']}\\n\"\n-\n-  params = {}\n-  modified_products = content['modified_products'] || []\n-  if content.has_key?('metadata_expire')\n-    params[:metadata_expire] = content['metadata_expire']\n-  end\n-\n-  if content.has_key?('required_tags')\n-    params[:required_tags] = content['required_tags']\n-  end\n-\n-  params[:content_url] = content['content_url'] + '/' + product['id'].to_s\n-  params[:arches] = content['arches']\n-  params[:gpg_url] = content['gpg_url']\n-  params[:modified_products] = modified_products\n-\n-  content_name = product['id'].to_s + '-' + content['name']\n-  content_id = product['id'].to_s + content['id'].to_s\n-  content_label = product['id'].to_s + '-' + content['label']\n-\n-  cp.create_content(\n-    owner['name'],\n-    content_name,\n-    content_id,\n-    content_label,\n-    content['type'],\n-    content['vendor'],\n-    params\n-  )\n-end\n-\n-\n-owners = cp.list_owners({:fetch => true})\n-owner_keys = owners.map{|owner| owner['key']}.compact\n-admin_owner_key = 'admin'\n-\n-CERT_DIR='generated_certs'\n-if not File.directory? CERT_DIR\n-  Dir.mkdir(CERT_DIR)\n-end\n-\n-print \"\\nImport product data...\\n\"\n-\n-def create_product(cp, owner, product)\n-  name = product['name']\n-  id = product['id']\n-  multiplier = product['multiplier'] || 1\n-  version = product['version'] || \"1.0\"\n-  variant = product['variant'] || \"ALL\"\n-  arch = product['arch'] || \"ALL\"\n-  type = product['type'] || \"SVC\"\n-  provided_products = product['provided_products'] || []\n-  attrs = product['attributes'] || {}\n-  product_content = product['content'] || []\n-  dependent_products = product['dependencies'] || []\n-  relies_on = product['relies_on'] || []\n-  derived_product = { :id => product['derived_product_id'] } if product['derived_product_id']\n-\n-  # To create branding information for marketing product\n-  if !provided_products.empty? && product['name'].include?('OS') && type == 'MKT'\n-    branding = [\n-        {\n-            :productId => product['provided_products'][0],\n-            :type => 'OS',\n-            :name => 'Branded ' + product['name']\n-        }\n-    ]\n-  else\n-    branding = []\n-  end\n-\n-  attrs['version'] = version\n-  attrs['variant'] = variant\n-  attrs['arch'] = arch\n-  attrs['type'] = type\n-\n-  product_ret = cp.create_product(owner['name'], id, name, {\n-    :multiplier => multiplier,\n-    :attributes => attrs,\n-    :dependentProductIds => dependent_products,\n-    :relies_on => relies_on,\n-    :branding => branding,\n-    :providedProducts => provided_products,\n-    :derivedProduct => derived_product\n-  })\n-\n-  print \"product name: #{name} version: #{version} arch: #{arch} type: #{type}; owner: #{owner['name']}\\n\"\n-  return product_ret\n-end\n-\n-\n-def find_content(content_id)\n-  $data['owners'].each do |owner|\n-    if owner.has_key?('content')\n-      owner['content'].each do |content|\n-        if content['id'] == content_id\n-          return content\n-        end\n-      end\n-    end\n-\n-    $data['content'].each do |content|\n-      if content['id'] == content_id\n-        return content\n-      end\n-    end\n-  end\n-  return nil\n-end\n-\n-\n-def create_eng_product(cp, owner, product)\n-  product_ret = create_product(cp, owner, product)\n-  product_content = product['content'] || []\n-\n-  # Generate a product id cert in generated_certs for each engineering product\n-  attempts = 3\n-  product_cert = nil\n-\n-  (0..attempts).each do |i|\n-    begin\n-      # This can fail with an ISE, which is bad but we should just retry a couple times\n-      product_cert = cp.get_product_cert(owner['name'], product_ret['id'])\n-      break\n-    rescue RestClient::InternalServerError => e\n-      sleep(0.5)\n-    end\n-  end\n-\n-  if product_cert\n-    cert_file = File.new(CERT_DIR + '/' + product_ret['id'] + '.pem', 'w+')\n-    cert_file.puts(product_cert['cert'])\n-  else\n-    raise \"Unable to fetch product certificate for product: #{product['id']}\"\n-  end\n-\n-  if not product_content.empty?\n-    product_content.each do |content_id, enabled|\n-      content = find_content(content_id)\n-      if content != nil\n-        create_content(cp, owner, product, content)\n-      end\n-    end\n-\n-    # Modify IDs of content in product_content to match ids used in create_content\n-    prod_id = product['id'].to_s\n-    mod_prod_content = product_content.map {|content_id, enabled| [prod_id + content_id.to_s, enabled]}\n-    cp.add_all_content_to_product(owner['name'], product_ret['id'], mod_prod_content)\n-  end\n-end\n-\n-\n-def create_mkt_product_and_pools(cp, owner, product)\n-  product_ret = create_product(cp, owner, product)\n-  if product.has_key?('skip_pools')\n-    return\n-  end\n-\n-  small_quantity = SMALL_SUB_QUANTITY\n-  large_quantity = LARGE_SUB_QUANTITY\n-  if product.has_key?('quantity')\n-    small_quantity = large_quantity = product['quantity']\n-  end\n-\n-  # Create a SMALL and a LARGE with the slightly similar begin/end dates.\n-  contract_number = 0\n-  account_number = '12331131231'\n-  order_number = 'order-8675309'\n-  start_date =  Date.today\n-  end_date =  start_date + 365\n-\n-  @sourceSubId += 1\n-\n-  pool = cp.create_pool(owner['name'], product_ret['id'], {\n-    :quantity => small_quantity,\n-    :contract_number => contract_number,\n-    :account_number => account_number,\n-    :order_number => order_number,\n-    :start_date => start_date,\n-    :end_date => end_date,\n-    :subscription_id => \"#{@sourceSubId}\"\n-  })\n-\n-  contract_number += 1\n-  @sourceSubId += 1\n-\n-  pool = cp.create_pool(owner['name'], product_ret['id'], {\n-    :quantity => large_quantity,\n-    :contract_number => contract_number,\n-    :account_number => account_number,\n-    :order_number => order_number,\n-    :start_date => start_date,\n-    :end_date => end_date,\n-    :subscription_id => \"#{@sourceSubId}\"\n-  })\n-\n-  # Create a pool for the future:\n-  @sourceSubId += 1\n-\n-  pool = cp.create_pool(owner['name'], product_ret['id'], {\n-    :quantity => 15,\n-    :contract_number => contract_number,\n-    :account_number => account_number,\n-    :order_number => order_number,\n-    :start_date => start_date + 365,\n-    :end_date => end_date - 10,\n-    :subscription_id => \"#{@sourceSubId}\"\n-  })\n-\n-end\n-\n-\n-\n-owner_products = {}\n-owner_product_refs = {}\n-\n-$data['owners'].each do |owner|\n-    product_map = {}\n-    product_refs = {}\n-\n-    owner_products[owner] = product_map\n-    owner_product_refs[owner] = product_refs\n-\n-    def catalog_product(product_map, product_refs, product)\n-        product_map[product['id']] = product\n-\n-        refs = []\n-        product_refs[product['id']] = refs\n-\n-        if product.has_key?('provided_products')\n-            refs.concat(product['provided_products'])\n-        end\n-\n-        if product.has_key?('derived_product_id')\n-            refs << product['derived_product_id']\n-        end\n-    end\n-\n-    $data['products'].each do |product|\n-        catalog_product(product_map, product_refs, product)\n-    end\n-\n-    if owner.has_key?('products')\n-        owner['products'].each do |product|\n-            catalog_product(product_map, product_refs, product)\n-        end\n-    end\n-end\n-\n-print \"\\nCreating products...\\n\"\n-thread_pool = ThreadPool.new(5)\n-\n-owner_products.each do |owner, product_map|\n-    product_refs = owner_product_refs[owner]\n-\n-    def build_product(cp, owner, product_refs, product_map, created_pids, product)\n-        pid = product['id']\n-\n-        # Check if we've already created this product\n-        if created_pids.include?(pid)\n-            return\n-        end\n-\n-        # Create any references first\n-        if product_refs.has_key?(pid) && !product_refs[pid].empty?\n-            product_refs[pid].each do |child_pid|\n-                build_product(cp, owner, product_refs, product_map, created_pids, product_map[child_pid])\n-            end\n-        end\n-\n-        # Create the product\n-        if product['type'] == 'MKT'\n-            create_mkt_product_and_pools(cp, owner, product)\n-        else\n-            create_eng_product(cp, owner, product)\n-        end\n-\n-        # Add its ID to the list of created PIDs\n-        created_pids << pid\n-    end\n-\n-    # Actual per-thread block\n-    thread_pool.schedule(owner, product_map, product_refs) do |owner, product_map, product_refs|\n-        created_pids = []\n-\n-        product_map.each do |pid, product|\n-            build_product(cp, owner, product_refs, product_map, created_pids, product)\n-        end\n-    end\n-end\n-thread_pool.shutdown\n-\n-print \"\\nrefreshing owners...\\n\"\n-thread_pool = ThreadPool.new(6)\n-owner_keys.each do |owner_key|\n-    thread_pool.schedule(owner_key) do |owner_key|\n-        cp.refresh_pools(owner_key)\n-    end\n-end\n-thread_pool.shutdown\n-\n-print \"\\nCreating activation keys...\\n\"\n-def create_activation_key_for_pool(cp, pool, owner_key)\n-    key_name = owner_key + '-' + pool['productId'] + '-key-' + pool['id']\n-    print \"creating activation_key \" + key_name + \"\\n\"\n-    key = cp.create_activation_key(owner_key, key_name)\n-    cp.add_pool_to_key(key['id'], pool['id'])\n-end\n-\n-thread_pool = ThreadPool.new(6)\n-owner_keys.each do |owner_key|\n-    thread_pool.schedule(owner_key) do |owner_key|\n-        pools = cp.list_owner_pools(owner_key)\n-        pools.each do |pool|\n-            create_activation_key_for_pool(cp, pool, owner_key)\n-        end\n-    end\n-end\n-thread_pool.shutdown"
  },
  {
    "sha": "0376c0ff5fc9aa5d40ee5776797228c24bc71eed",
    "filename": "server/bin/test_data_importer.py",
    "status": "added",
    "additions": 846,
    "deletions": 0,
    "changes": 846,
    "blob_url": "https://github.com/candlepin/candlepin/blob/5808013a6c129a7fc63204c47d33e6fdedd667df/server/bin/test_data_importer.py",
    "raw_url": "https://github.com/candlepin/candlepin/raw/5808013a6c129a7fc63204c47d33e6fdedd667df/server/bin/test_data_importer.py",
    "contents_url": "https://api.github.com/repos/candlepin/candlepin/contents/server/bin/test_data_importer.py?ref=5808013a6c129a7fc63204c47d33e6fdedd667df",
    "patch": "@@ -0,0 +1,846 @@\n+#!/usr/bin/env python\n+from __future__ import print_function, division, absolute_import\n+\n+\"\"\"\n+Injects synthetic data from one or more JSON files into Candlepin for testing\n+\"\"\"\n+\n+import argparse\n+import datetime\n+import logging\n+import json\n+import os\n+import random\n+import re\n+import requests\n+import string\n+import sys\n+import time\n+\n+\n+def build_logger(name, msg_format):\n+    \"\"\"Builds and configures our logger\"\"\"\n+    class EmptyLineFilter(logging.Filter):\n+        \"\"\"Logging filter implementation that filters on empty or whitespace-only lines\"\"\"\n+        def __init__(self, invert=False):\n+            self.invert = invert\n+\n+        def filter(self, record):\n+            result = bool(record.msg) and not record.msg.isspace()\n+            if self.invert:\n+                result = not result\n+\n+            return result\n+\n+    # Create the base/standard handler\n+    std_handler = logging.StreamHandler(sys.stdout)\n+    std_handler.setFormatter(logging.Formatter(fmt=msg_format))\n+    std_handler.addFilter(EmptyLineFilter())\n+\n+    # Create an empty-line handler\n+    empty_handler = logging.StreamHandler(sys.stdout)\n+    empty_handler.setFormatter(logging.Formatter(fmt=''))\n+    empty_handler.addFilter(EmptyLineFilter(True))\n+\n+    # Create a logger using the above handlers\n+    logger = logging.getLogger(name)\n+    logger.setLevel(logging.INFO)\n+    logger.addHandler(std_handler)\n+    logger.addHandler(empty_handler)\n+\n+    return logger\n+\n+# Disable HTTP warnings that'll be kicked out by urllib (used internally by requests)\n+requests.packages.urllib3.disable_warnings()\n+\n+# We don't care about urllib/requests logging\n+logging.getLogger(\"requests\").setLevel(logging.WARNING)\n+logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n+\n+log = build_logger('test_data_importer', '%(asctime)-15s %(levelname)-7s %(name)s -- %(message)s')\n+\n+\n+def ci_in(src_dict, search):\n+    \"\"\"Performs a case-insensitive search in a dictionary\"\"\"\n+\n+    search = search.lower()\n+\n+    for key in src_dict:\n+        if key.lower() == search:\n+            return True\n+\n+    return False\n+\n+def safe_merge(source, dest, include=None, exclude=None):\n+    \"\"\"\n+    Merges two dictionaries, converting the keys in the source dictionary from snake case to camel case\n+    where appropriate.\n+    \"\"\"\n+\n+    def handle_match(match):\n+        return '{chr1}{chr2}'.format(chr1=match.group(1), chr2=match.group(2).upper())\n+\n+    for key, value in source.items():\n+        key = re.sub('(\\\\w)_(\\\\w)', handle_match, key)\n+\n+        if (include is None or key in include) and (exclude is None or key not in exclude):\n+            dest[key] = value\n+\n+    return dest\n+\n+def random_id(prefix, suffix_length=8):\n+    \"\"\"Generates a string with a random suffix, suitable to be used as an ID\"\"\"\n+    suffix = ''.join(random.choice(string.digits) for i in range(suffix_length))\n+    return \"{prefix}-{suffix}\".format(prefix=prefix, suffix=suffix)\n+\n+\n+class Candlepin:\n+    \"\"\"Class providing methods for performing actions against a specific Candlepin server\"\"\"\n+    status_cache = None\n+    hosted_test = None\n+\n+    def __init__(self, host='localhost', port=8443, username='admin', password='admin', prefix='candlepin'):\n+        \"\"\"\n+        Creates a new Candlepin object which will connect to the server at the given host/port using the\n+        specified credentials\n+        \"\"\"\n+\n+        self.host = host\n+        self.port = port\n+        self.username = username\n+        self.password = password\n+        self.prefix = prefix\n+\n+        self.determine_mode()\n+\n+    def build_url(self, endpoint):\n+        \"\"\"Builds a complete URL for the specified endpoint on the target Candlepin instance\"\"\"\n+\n+        # Remove leading slash if it exists\n+        if endpoint and endpoint[0] == '/':\n+            endpoint = endpoint[1:]\n+\n+        return 'https://{host}:{port}/{prefix}/{endpoint}'.format(\n+            host=self.host, port=self.port, prefix=self.prefix, endpoint=endpoint)\n+\n+    def get(self, endpoint, params=None, headers=None, content_type='text/plain',\n+        accept_type='application/json'):\n+\n+        return self.request('GET', endpoint, None, params, headers, content_type, accept_type)\n+\n+    def post(self, endpoint, data=None, params=None, headers=None, content_type='application/json',\n+        accept_type='application/json'):\n+\n+        return self.request('POST', endpoint, data, params, headers, content_type, accept_type)\n+\n+    def put(self, endpoint, data=None, params=None, headers=None, content_type='application/json',\n+        accept_type='application/json'):\n+\n+        return self.request('PUT', endpoint, data, params, headers, content_type, accept_type)\n+\n+    def delete(self, endpoint, data=None, params=None, headers=None, content_type='application/json',\n+        accept_type='application/json'):\n+\n+        return self.request('DELETE', endpoint, data, params, headers, content_type, accept_type)\n+\n+    def request(self, req_type, endpoint, data=None, params=None, headers=None,\n+        content_type='application/json', accept_type='application/json'):\n+\n+        if params is None:\n+            params = {}\n+\n+        if headers is None:\n+            headers = {}\n+\n+        # Add our content-type and accept type if they weren't explicitly provided in the headers already\n+        if not ci_in(headers, 'Content-Type'):\n+            headers['Content-Type'] = content_type\n+\n+        if not ci_in(headers, 'Accept'):\n+            headers['Accept'] = accept_type\n+\n+        log.debug('Sending request: {req} {endpoint} (params: {params}, headers: {headers}, data: {data})'\n+            .format(req=req_type, endpoint=endpoint, params=params, headers=headers, data=data))\n+\n+        response = requests.request(req_type, self.build_url(endpoint),\n+            json=data,\n+            auth=(self.username, self.password),\n+            params=params,\n+            headers=headers,\n+            verify=False)\n+\n+        response.raise_for_status()\n+        return response\n+\n+    def status(self):\n+        if self.status_cache is None:\n+            self.status_cache = self.get('status').json()\n+\n+        return self.status_cache\n+\n+    def determine_mode(self):\n+        if self.hosted_test is None:\n+            self.hosted_test = not self.status()['standalone']\n+\n+            if self.hosted_test:\n+                try:\n+                    self.get('hostedtest/alive', accept_type='text/plain')\n+                except requests.exceptions.HTTPError:\n+                    self.hosted_test = False\n+\n+        return self.hosted_test\n+\n+    # Candlepin API functions\n+    def create_owner(self, owner_data):\n+        owner = {\n+            'key': owner_data['name'],\n+            'displayName': owner_data['displayName']\n+        }\n+\n+        if 'contentAccessModeList' in owner_data:\n+            owner['contentAccessModeList'] = owner_data['contentAccessModeList']\n+\n+        if 'contentAccessMode' in owner_data:\n+            owner['contentAccessMode'] = owner_data['contentAccessMode']\n+\n+        owner = self.post('owners', owner).json()\n+\n+        # If we're operating in hosted-test mode, also create the org upstream for consistency\n+        if self.hosted_test:\n+            self.post('hostedtest/owners', { 'key': owner['key'] })\n+\n+        return owner\n+\n+    def create_user(self, user_data):\n+        user = safe_merge(user_data, {}, include=['username', 'password'])\n+        user['superAdmin'] = user_data['superadmin'] if 'superadmin' in user_data else False\n+\n+        return self.post('users', user).json()\n+\n+    def create_role(self, role_name, permissions):\n+        role = {\n+            'name': role_name,\n+            'permissions': permissions\n+        }\n+\n+        return self.post('roles', role).json()\n+\n+    def add_role_to_user(self, username, role_name):\n+        return self.post('roles/{rolename}/users/{username}'.format(rolename=role_name, username=username)).json()\n+\n+    def create_content(self, owner_key, content_data):\n+        content = safe_merge(content_data, {})\n+\n+        if self.hosted_test:\n+            url = 'hostedtest/content'\n+        else:\n+            url = 'owners/{owner_key}/content'.format(owner_key=owner_key)\n+\n+        return self.post(url, content).json()\n+\n+    def create_product(self, owner_key, product_data):\n+        attributes = product_data['attributes'] if 'attributes' in product_data else {}\n+        attributes['version'] = product_data['version'] if 'version' in product_data else \"1.0\"\n+        attributes['variant'] = product_data['variant'] if 'variant' in product_data else \"ALL\"\n+        attributes['arch'] = product_data['arch'] if 'arch' in product_data else \"ALL\"\n+        attributes['type'] = product_data['type'] if 'type' in product_data else \"SVC\"\n+\n+        branding = []\n+        content = product_data['content'] if 'content' in product_data else []\n+        dependent_products = product_data['dependencies'] if 'dependencies' in product_data else []\n+        relies_on = product_data['relies_on'] if 'relies_on' in product_data else []\n+        provided_products = []\n+        derived_product = None\n+\n+        # correct provided products and derived product\n+        if 'provided_products' in product_data:\n+            provided_products = [ { 'id': value } for value in product_data['provided_products'] ]\n+\n+        if 'derived_product_id' in product_data:\n+            derived_product = { 'id': product_data['derived_product_id'] }\n+\n+        # Add branding to some of the products\n+        # TODO: FIXME: couldn't we just do this in the test data instead?\n+        if provided_products and 'OS' in product_data['name'] and 'type' in product_data and product_data['type'] == 'MKT':\n+            branding.append({\n+                'productId': provided_products[0]['id'],\n+                'type': 'OS',\n+                'name': 'Branded {name}'.format(name=product_data['name'])\n+            })\n+\n+        product = {\n+            'id': product_data['id'],\n+            'name': product_data['name'],\n+            'multiplier': product_data['multiplier'] if 'multiplier' in product_data else 1,\n+            'attributes': attributes,\n+            'dependentProductIds': dependent_products,\n+            'relies_on': relies_on,\n+            'branding': branding,\n+            'providedProducts': provided_products,\n+            'derivedProduct': derived_product\n+        }\n+\n+        # Determine URL and submit\n+        if self.hosted_test:\n+            endpoint = 'hostedtest/products'\n+        else:\n+            endpoint = 'owners/{owner_key}/products'.format(owner_key=owner_key)\n+\n+        return self.post(endpoint, product).json()\n+\n+    def get_product_cert(self, owner_key, product_id):\n+        endpoint = 'owners/{owner_key}/products/{pid}/certificate'.format(owner_key=owner_key, pid=product_id)\n+        return self.get(endpoint).json()\n+\n+    def add_content_to_product(self, owner_key, product_id, content_map):\n+        if self.hosted_test:\n+            endpoint = 'hostedtest/products/{pid}/content'.format(pid=product_id)\n+        else:\n+            endpoint = 'owners/{owner_key}/products/{pid}/batch_content'.format(owner_key=owner_key, pid=product_id)\n+\n+        return self.post(endpoint, content_map).json()\n+\n+    def create_pool(self, owner_key, pool_data):\n+        start_date = pool_data['start_date'] if 'start_date' in pool_data else datetime.datetime.now()\n+        end_date = pool_data['end_date'] if 'end_date' in pool_data else start_date + datetime.timedelta(days=365)\n+\n+        pool = {\n+            'startDate': start_date,\n+            'endDate': end_date,\n+            'quantity': pool_data['quantity'] if 'quantity' in pool_data else 1\n+        }\n+\n+        # Merge in the other keys we care about for pools\n+        keys = ['branding', 'contractNumber', 'accountNumber', 'orderNumber', 'subscriptionId', 'upstreamPoolId']\n+        safe_merge(pool_data, pool, include=keys)\n+\n+        if self.hosted_test:\n+            # Need to set a subscription ID if we're generating this upstream\n+            pool['id'] = random_id('upstream')\n+\n+            # Product is an object in hosted test mode\n+            pool['product'] = { 'id': pool_data['product_id'] }\n+\n+            # The owner must be set in the subscription data for upstream subscriptions\n+            pool['owner'] = { 'key': owner_key }\n+\n+            endpoint = 'hostedtest/subscriptions'\n+        else:\n+            # Copy over the pool (special case because hosted test data is different)\n+            pool['productId'] = pool_data['product_id']\n+\n+            # Generate fields that make this look like an upstream subscription\n+            if not 'subscriptionId' in pool:\n+                pool['subscriptionId'] = random_id('srcsub')\n+\n+            if 'subscription_subkey' in pool_data:\n+                pool['subscriptionSubKey'] = pool_data['subscription_subkey']\n+            elif 'subscriptionSubKey' in pool_data:\n+                pool['subscriptionSubKey'] = pool_data['subscriptionSubKey']\n+            else:\n+                pool['subscriptionSubKey'] = 'master'\n+\n+            if not 'upstreamPoolId' in pool:\n+                pool['upstreamPoolId'] = random_id('upstream')\n+\n+            endpoint = 'owners/{owner_key}/pools'.format(owner_key=owner_key)\n+\n+        return self.post(endpoint, pool).json()\n+\n+    def list_pools(self, owner_key):\n+        endpoint = 'owners/{owner_key}/pools'.format(owner_key=owner_key)\n+        return self.get(endpoint).json()\n+\n+    def create_activation_key(self, owner_key, key_data):\n+        key = safe_merge(key_data, {})\n+        return self.post('owners/{owner_key}/activation_keys'.format(owner_key=owner_key), key).json()\n+\n+    def add_pool_to_activation_key(self, key_id, pool_id, quantity=None):\n+        endpoint = 'activation_keys/{key_id}/pools/{pool_id}'.format(key_id=key_id, pool_id=pool_id)\n+        params = {}\n+\n+        if quantity is not None:\n+            params['quantity'] = quantity\n+\n+        return self.post(endpoint, None, params).json()\n+\n+    def refresh_pools(self, owner_key, wait_for_completion=True):\n+        endpoint = 'owners/{owner_key}/subscriptions'.format(owner_key=owner_key)\n+        job_status = self.put(endpoint, None, params={ 'lazy_regen': True }).json()\n+\n+        if job_status and wait_for_completion:\n+            finished_states = ['ABORTED', 'FINISHED', 'CANCELED', 'FAILED']\n+\n+            while job_status['state'] not in finished_states:\n+                time.sleep(1)\n+                job_status = self.get(job_status['statusPath']).json()\n+\n+            # Clean up job status\n+            # self.delete('jobs', {'id': job_status['id']})\n+\n+        return job_status\n+\n+def create_owners(cp, data):\n+    \"\"\"\n+    Creates any owners present in the test data and returns a dictionary consisting of the owner keys\n+    mapped to the owner objects returned by Candlepin.\n+    \"\"\"\n+\n+    owners = {}\n+\n+    if 'owners' in data:\n+        log.info('')\n+        log.info('Creating {count} owner(s)...'.format(count=len(data['owners'])))\n+\n+        for owner_data in data['owners']:\n+            log.info('  Owner: {name}  [display name: {display_name}]'\n+                .format(name=owner_data['name'], display_name=owner_data['displayName']))\n+\n+            owner = cp.create_owner(owner_data)\n+            owners[owner['key']] = owner_data\n+\n+    return owners\n+\n+\n+def create_users(cp, data):\n+    \"\"\"\n+    Creates any users present in the test data\n+    \"\"\"\n+\n+    if 'users' in data:\n+        log.info('')\n+        log.info('Creating {count} user(s)...'.format(count=len(data['users'])))\n+\n+        for user_data in data['users']:\n+            log.info('  User: {username}  [password: {password}, superuser: {superuser}'.format(\n+                username=user_data['username'], password=user_data['password'],\n+                superuser=user_data['superadmin'] if 'superadmin' in user_data else False))\n+\n+            user = cp.create_user(user_data)\n+\n+def create_roles(cp, data):\n+    \"\"\"Creates any roles present in the test data, mapping them to users as appropriate\"\"\"\n+\n+    if 'roles' in data:\n+        log.info('')\n+        log.info('Creating {count} role(s)...'.format(count=len(data['roles'])))\n+\n+        for role_data in data['roles']:\n+            permissions = role_data['permissions']\n+            users = role_data['users']\n+\n+            log.info('  Role: {name}  (permissions: {perm_count}, users: {user_count})'\n+                .format(name=role_data['name'], perm_count=len(permissions), user_count=len(users)))\n+\n+            # Correct & list permissions\n+            for perm_data in permissions:\n+                # convert owner keys to owner objects:\n+                if type(perm_data['owner']) != dict:\n+                    perm_data['owner'] = { 'key': perm_data['owner'] }\n+\n+                log.info('    Permission: [type: {type}, owner: {owner}, access: {access}]'\n+                    .format(type=perm_data['type'], owner=perm_data['owner'], access=perm_data['access']))\n+\n+            role = cp.create_role(role_data['name'], permissions)\n+\n+            # Add role to users\n+            for user_data in users:\n+                log.info('    Applying to user: {user}'.format(user=user_data['username']))\n+                cp.add_role_to_user(user_data['username'], role_data['name'])\n+\n+def gather_content(data, hosted_test):\n+    \"\"\"\n+    Gathers content from the test data and orders them in a way to ensure that linear creation of the\n+    content data is safe\n+    \"\"\"\n+\n+    content = []\n+\n+    if hosted_test:\n+        # In hosted mode, we don't need per-org product or content definitions, so only\n+        # add the global content once\n+        if 'content' in data:\n+            content.extend([(None, value) for value in data['content']])\n+\n+        if 'owners' in data:\n+            for owner in data['owners']:\n+                if 'content' in owner:\n+                    content.extend([(None, value) for value in owner['content']])\n+\n+    else:\n+        # In standalone mode, we can only add stuff if we have an org to add it to\n+        if 'owners' in data:\n+            for owner in data['owners']:\n+                owner_key = owner['name']\n+\n+                if 'content' in data:\n+                    content.extend([(owner_key, value) for value in data['content']])\n+\n+                if 'content' in owner:\n+                    content.extend([(owner_key, value) for value in owner['content']])\n+\n+    return content\n+\n+def create_content(cp, data):\n+    \"\"\"Creates any content present in the test data\"\"\"\n+\n+    log.info('')\n+    content_data = gather_content(data, cp.hosted_test)\n+\n+    log.info('Creating {count} content...'.format(count=len(content_data)))\n+    for owner_key, content in content_data:\n+        cp.create_content(owner_key, content)\n+\n+def gather_products(data, hosted_test):\n+    \"\"\"\n+    Gathers products from the test data and orders them in a way to ensure that linear creation of the\n+    product data is safe\n+    \"\"\"\n+    def order_products(owner_key, product_map):\n+        ordered = []\n+        processed_pids = []\n+\n+        def traverse_product(product):\n+            if product['id'] in processed_pids:\n+                return\n+\n+            if 'derived_product_id' in product:\n+                if product['derived_product_id'] not in product_map:\n+                    errmsg = 'product references a non-existent product: {name} ({pid}) => {ref_pid}'.format(\n+                        name=product['name'], pid=product['id'], ref_pid=product['derived_product_id'])\n+\n+                    raise ReferenceError(errmsg)\n+\n+                traverse_product(product_map[product['derived_product_id']])\n+\n+            if 'provided_products' in product:\n+                for pid in product['provided_products']:\n+                    if pid not in product_map:\n+                        errmsg = 'product references a non-existent product: {name} ({pid}) => {ref_pid}'.format(\n+                            name=product['name'], pid=product['id'], ref_pid=pid)\n+\n+                        raise ReferenceError(ermsg)\n+\n+                    traverse_product(product_map[pid])\n+\n+            processed_pids.append(product['id'])\n+            ordered.append((owner_key, product))\n+\n+        for product in product_map.values():\n+            traverse_product(product)\n+\n+        return ordered\n+\n+    products = []\n+\n+    if hosted_test:\n+        product_map = {}\n+\n+        # In hosted mode, we don't need per-org products, so we only add the global products once\n+        if 'products' in data:\n+            for product in data['products']:\n+                product_map[product['id']] = product\n+\n+        if 'owners' in data:\n+            for owner in data['owners']:\n+                if 'products' in owner:\n+                    for product in owner['products']:\n+                        product_map[product['id']] = product\n+\n+        products.extend(order_products(None, product_map))\n+    else:\n+        # In standalone mode, we can only add stuff if we have an org to add it to\n+        if 'owners' in data:\n+            for owner in data['owners']:\n+                owner_key = owner['name']\n+                product_map = {}\n+\n+                if 'products' in data:\n+                    for product in data['products']:\n+                        product_map[product['id']] = product\n+\n+                if 'products' in owner:\n+                    for product in owner['products']:\n+                        product_map[product['id']] = product\n+\n+                products.extend(order_products(owner_key, product_map))\n+\n+    return products\n+\n+def map_content_to_product(owner_key, product_data):\n+    \"\"\"Performs content mapping for a given product\"\"\"\n+    content_map = {}\n+\n+    if 'content' in product_data:\n+        # test data stores this as an array of arrays for reasons. Convert it to a dictionary and pass it\n+        # to Candlepin\n+        for content_ref in product_data['content']:\n+            content_map[content_ref[0]] = content_ref[1]\n+\n+    if content_map:\n+        cp.add_content_to_product(owner_key, product_data['id'], content_map)\n+\n+def create_products(cp, data):\n+    \"\"\"Creates any products present in the test data and maps any content associated with them\"\"\"\n+\n+    log.info('')\n+    product_data = gather_products(data, cp.hosted_test)\n+\n+    log.info('Creating {count} product(s)...'.format(count=len(product_data)))\n+    for owner_key, product in product_data:\n+        log.info('  Product: {pid} [name: {name}, owner: {owner}]'\n+            .format(pid=product['id'], name=product['name'], owner=owner_key if owner_key else '-GLOBAL-'))\n+\n+        cp.create_product(owner_key, product)\n+\n+        # Link products to any content they reference\n+        map_content_to_product(owner_key, product)\n+\n+def fetch_product_certs(cp, cert_dir, engineering_products):\n+    \"\"\"\n+    Fetches product certs for the specified engineering products. Engineering products must be provided as\n+    a dictionary mapping a product ID to an owner key of one of the orgs owning the product.\n+    \"\"\"\n+\n+    if engineering_products:\n+        log.info('')\n+        log.info('Fetching certs for {count} engineering products...'.format(count=len(engineering_products)))\n+\n+        # make sure the cert directory exists\n+        if not os.path.exists(cert_dir):\n+            os.makedirs(cert_dir)\n+\n+        for pid, owner_key in engineering_products.items():\n+            try:\n+                # This could 404 if the product did not get pulled down during refresh (hosted mode)\n+                # because it's not linked/used by anything. We'll issue a warning in this case.\n+                product_cert = cp.get_product_cert(owner_key, pid)\n+                cert_filename = '{dir}/{prod_id}.pem'.format(dir=cert_dir, prod_id=pid)\n+\n+                with open(cert_filename, 'w+') as file:\n+                    log.info('  Writing certificate: {filename}'.format(filename=cert_filename))\n+                    file.write(product_cert['cert'])\n+            except requests.exceptions.HTTPError as e:\n+                if e.response.status_code == 404:\n+                    log.warn('  Skipping certificate for unused product: {product}'.format(product=pid))\n+                else:\n+                    raise e\n+\n+def create_subscriptions(cp, options, owners, data):\n+    \"\"\"\n+    Creates subscriptions from the test data and fetches engineering product certificates,\n+    refreshing the affected orgs as necessary.\n+    \"\"\"\n+\n+    log.info('')\n+    log.info(\"Creating subscriptions...\")\n+\n+    now = datetime.datetime.now()\n+    engineering_products = {}\n+\n+    for owner_key, owner_data in owners.items():\n+        account_number = ''.join(random.choice(string.digits) for i in range(10))\n+        order_number = random_id('order')\n+\n+        products = []\n+        pools = []\n+\n+        if 'products' in data:\n+            products.extend(data['products'])\n+\n+        if 'products' in owner_data:\n+            products.extend(owner_data['products'])\n+\n+        for product in products:\n+            if 'type' in product and product['type'] == 'MKT':\n+                if 'quantity' in product:\n+                    small_quantity = int(product['quantity'])\n+                    large_quantity = small_quantity * 10 if small_quantity > 0 else 10\n+                else:\n+                    small_quantity = 5\n+                    large_quantity = 50\n+\n+                start_date = (now + datetime.timedelta(days=-5)).isoformat()\n+                end_date = (now + datetime.timedelta(days=360)).isoformat()\n+\n+                # Small quantity pool\n+                pools.append({\n+                    'product_id': product['id'],\n+                    'quantity': small_quantity,\n+                    'contract_number': 0,\n+                    'account_number': account_number,\n+                    'order_number': order_number,\n+                    'start_date': start_date,\n+                    'end_date': end_date\n+                })\n+\n+                # Large quantity pool\n+                pools.append({\n+                    'product_id': product['id'],\n+                    'quantity': large_quantity,\n+                    'contract_number': 0,\n+                    'account_number': account_number,\n+                    'order_number': order_number,\n+                    'start_date': start_date,\n+                    'end_date': end_date\n+                })\n+\n+                # Future pool\n+                pools.append({\n+                    'product_id': product['id'],\n+                    'quantity': large_quantity,\n+                    'contract_number': 0,\n+                    'account_number': account_number,\n+                    'order_number': order_number,\n+                    'start_date': (now + datetime.timedelta(days=30)).isoformat(),\n+                    'end_date': end_date\n+                })\n+\n+            else:\n+                # Product is an engineering product. Flag the pid/owner combo so we can fetch the cert later\n+\n+                # Impl note:\n+                # We're abusing some CP knowledge here in that product certs are global -- it doesn't matter\n+                # *which* org we use to fetch them, but as the API requires *an* org, we just store the last\n+                # org that specifies the product which should still cover the entire collection of products\n+                # generated during this execution\n+                engineering_products[product['id']] = owner_key\n+\n+        log.info('  Creating {count} subscription(s) for owner {owner}...'\n+            .format(count=len(pools), owner=owner_key))\n+\n+        for pool in pools:\n+            cp.create_pool(owner_key, pool)\n+\n+        # Refresh (if necessary)\n+        if cp.hosted_test:\n+            log.info('  Refreshing owner: {owner}'.format(owner=owner_key))\n+            cp.refresh_pools(owner_key)\n+\n+    # Fetch engineering product certs (if necessary)\n+    fetch_product_certs(cp, options.cert_dir, engineering_products)\n+\n+def create_activation_keys(cp, owners):\n+    \"\"\"\n+    Creates activation keys for the specified owners. The owners should be provided as a dictionary\n+    consisting of owner keys mapped to owner data\n+    \"\"\"\n+    activation_keys = []\n+\n+    # Build a set of activation key data to use to generate our keys\n+    for owner_key in owners:\n+        activation_keys.append((owner_key, \"default_key\", None))\n+        activation_keys.append((owner_key, \"awesome_os_pool\", None))\n+\n+        pools = cp.list_pools(owner_key)\n+\n+        for pool in pools:\n+            key_name = '{owner_key}-{prod_id}-key-{pool_id}'.format(\n+                owner_key=owner_key, prod_id=pool['productId'], pool_id=pool['id'])\n+\n+            activation_keys.append((owner_key, key_name, pool))\n+\n+    # Actually generate the keys (if necessary)\n+    if activation_keys:\n+        log.info('')\n+        log.info('Creating {count} activation keys...'.format(count=len(activation_keys)))\n+\n+        for owner_key, key_name, pool in activation_keys:\n+            log.info('  Activation key: {key_name} [owner: {owner}, pool: {pool}]'\n+                .format(key_name=key_name, owner=owner_key, pool=pool['id'] if pool is not None else 'n/a'))\n+\n+            key = cp.create_activation_key(owner_key, {'name': key_name})\n+            if pool is not None:\n+                cp.add_pool_to_activation_key(key['id'], pool['id'])\n+\n+\n+# Process a given JSON file, creating the test data it defines\n+def process_file(cp, options, filename):\n+    with open(filename) as file:\n+        data = json.load(file)\n+\n+    log.info('Importing data from file: {filename}'.format(filename=filename))\n+\n+    # Create owners\n+    owners = create_owners(cp, data)\n+\n+    # Create users\n+    create_users(cp, data)\n+\n+    # Create roles\n+    create_roles(cp, data)\n+\n+    # Create content\n+    create_content(cp, data)\n+\n+    # Create products\n+    create_products(cp, data)\n+\n+    # Create pools/subscriptions\n+    create_subscriptions(cp, options, owners, data)\n+\n+    # Create activation keys\n+    create_activation_keys(cp, owners)\n+\n+    # Done!\n+    log.info('')\n+    log.info('Finished importing data from file: {filename}'.format(filename=filename))\n+\n+# Parse CLI options\n+def parse_options():\n+    parser = argparse.ArgumentParser(description='Imports JSON-formatted test data to a Candlepin server', add_help=True)\n+\n+    parser.add_argument('--debug', action='store_true', default=False,\n+        help='Enables debug output')\n+\n+    parser.add_argument('--username', action='store', default='admin',\n+        help='The username to use when making requests to Candlepin; defaults to \\'admin\\'')\n+    parser.add_argument('--password', action='store', default='admin',\n+        help='The password to use when making requests to Candlepin; defaults to \\'admin\\'')\n+    parser.add_argument('--host', action='store', default='localhost',\n+        help='The hostname/address of the Candlepin server; defaults to \\'localhost\\'')\n+    parser.add_argument('--port', action='store', default=8443, type=int,\n+        help='The port to use when connecting to Candlepin; defaults to 8443')\n+    parser.add_argument('--prefix', action='store', default='candlepin',\n+        help='The endpoint prefix to use on the destination Candlepin server; defaults to \\'candlepin\\'')\n+\n+    parser.add_argument('--cert_dir', action='store', default='generated_certs',\n+        help='The directory in which to store generated product certificates; defaults to \\'generated_certs\\'')\n+\n+    parser.add_argument(\"json_files\", nargs='*', action='store')\n+\n+    options = parser.parse_args()\n+\n+    # Set logging level as appropriate\n+    if options.debug:\n+        log.setLevel(logging.DEBUG)\n+\n+    if len(options.json_files) < 1:\n+        log.debug(\"No test data files provided, defaulting to 'test_data.json'\")\n+\n+        filename = 'test_data.json'\n+        if sys.path[0]:\n+            filename = '{script_home}/{filename}'.format(script_home=sys.path[0], filename=filename)\n+\n+        options.json_files = [filename]\n+\n+    return options\n+\n+\n+# Execute script\n+try:\n+    log.info('Importing test data...')\n+    options = parse_options()\n+\n+    log.info('Connecting to Candlepin @ {host}:{port}'.format(host=options.host, port=options.port))\n+    cp = Candlepin(options.host, options.port, options.username, options.password, options.prefix)\n+\n+    log.info('Importing test data from {count} file(s)...'.format(count=len(options.json_files)))\n+    for file in options.json_files:\n+        process_file(cp, options, file)\n+\n+    log.info('Complete. Files imported: {count}'.format(count=len(options.json_files)))\n+\n+except requests.exceptions.ConnectionError as e:\n+    log.error('Connection error: {err}'.format(err=e))\n+    exit(1)"
  },
  {
    "sha": "07d48d4bb668a6a8f346310ece9138aae4037944",
    "filename": "server/bin/upstream_subscription_injector.rb",
    "status": "modified",
    "additions": 2,
    "deletions": 2,
    "changes": 4,
    "blob_url": "https://github.com/candlepin/candlepin/blob/5808013a6c129a7fc63204c47d33e6fdedd667df/server/bin/upstream_subscription_injector.rb",
    "raw_url": "https://github.com/candlepin/candlepin/raw/5808013a6c129a7fc63204c47d33e6fdedd667df/server/bin/upstream_subscription_injector.rb",
    "contents_url": "https://api.github.com/repos/candlepin/candlepin/contents/server/bin/upstream_subscription_injector.rb?ref=5808013a6c129a7fc63204c47d33e6fdedd667df",
    "patch": "@@ -46,8 +46,8 @@ def check_candlepin_configuration(candlepin)\n     hosted_adapter = false\n \n     begin\n-         response = candlepin.get('/hostedtest/alive', {}, 'text/plain', true)\n-         hosted_adapter = (response.to_s.downcase == 'true')\n+        response = candlepin.get('/hostedtest/alive', {}, 'text/plain', true)\n+        hosted_adapter = (response.to_s.downcase == 'true')\n     rescue RestClient::ResourceNotFound\n         # CP running without the Hosted adapter\n     end"
  },
  {
    "sha": "0b89b0d3feb8d5ccf6ffe3ac857ac79e14979de5",
    "filename": "server/src/main/java/org/candlepin/hostedtest/HostedTestResource.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/candlepin/candlepin/blob/5808013a6c129a7fc63204c47d33e6fdedd667df/server/src/main/java/org/candlepin/hostedtest/HostedTestResource.java",
    "raw_url": "https://github.com/candlepin/candlepin/raw/5808013a6c129a7fc63204c47d33e6fdedd667df/server/src/main/java/org/candlepin/hostedtest/HostedTestResource.java",
    "contents_url": "https://api.github.com/repos/candlepin/candlepin/contents/server/src/main/java/org/candlepin/hostedtest/HostedTestResource.java?ref=5808013a6c129a7fc63204c47d33e6fdedd667df",
    "patch": "@@ -335,7 +335,7 @@ public void clearData() {\n     @POST\n     @Consumes(MediaType.APPLICATION_JSON)\n     @Produces(MediaType.APPLICATION_JSON)\n-    @Path(\"/products/{product_id}/batch_content\")\n+    @Path(\"/products/{product_id}/content\")\n     @Transactional\n     public ProductInfo addContentToProduct(\n         @PathParam(\"product_id\") String productId,"
  }
]
