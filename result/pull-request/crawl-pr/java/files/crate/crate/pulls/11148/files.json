[
  {
    "sha": "dc9e376ae5e47b64082e8a7d7856d0dc3ab1292d",
    "filename": "server/src/main/java/io/crate/execution/dml/TransportShardAction.java",
    "status": "modified",
    "additions": 8,
    "deletions": 5,
    "changes": 13,
    "blob_url": "https://github.com/crate/crate/blob/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/main/java/io/crate/execution/dml/TransportShardAction.java",
    "raw_url": "https://github.com/crate/crate/raw/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/main/java/io/crate/execution/dml/TransportShardAction.java",
    "contents_url": "https://api.github.com/repos/crate/crate/contents/server/src/main/java/io/crate/execution/dml/TransportShardAction.java?ref=711161de73ff20b9d7af3ac1df8ee5d310aef77d",
    "patch": "@@ -40,6 +40,7 @@\n import org.elasticsearch.common.io.stream.Writeable;\n import org.elasticsearch.index.engine.Engine;\n import org.elasticsearch.index.mapper.Mapper;\n+import org.elasticsearch.index.mapper.Mapping;\n import org.elasticsearch.index.shard.IndexShard;\n import org.elasticsearch.index.shard.ShardId;\n import org.elasticsearch.indices.IndicesService;\n@@ -54,6 +55,7 @@\n import java.util.UUID;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.BiConsumer;\n import java.util.function.Function;\n \n /**\n@@ -65,7 +67,7 @@\n         implements KillAllListener {\n \n     private final ConcurrentHashMap<TaskId, KillableCallable<?>> activeOperations = new ConcurrentHashMap<>();\n-    private final MappingUpdatePerformer mappingUpdate;\n+    protected final BiConsumer<Mapping, ShardId> mappingUpdate;\n \n     protected TransportShardAction(String actionName,\n                                    TransportService transportService,\n@@ -192,13 +194,14 @@ public void kill(@Nullable Throwable t) {\n         }\n     }\n \n-    protected <T extends Engine.Result> T executeOnPrimaryHandlingMappingUpdate(ShardId shardId,\n-                                                                                CheckedSupplier<T, IOException> execute,\n-                                                                                Function<Exception, T> onMappingUpdateError) throws IOException {\n+    public static <T extends Engine.Result> T executeOnPrimaryHandlingMappingUpdate(BiConsumer<Mapping, ShardId> mappingUpdate,\n+                                                                             ShardId shardId,\n+                                                                             CheckedSupplier<T, IOException> execute,\n+                                                                             Function<Exception, T> onMappingUpdateError) throws IOException {\n         T result = execute.get();\n         if (result.getResultType() == Engine.Result.Type.MAPPING_UPDATE_REQUIRED) {\n             try {\n-                mappingUpdate.updateMappings(result.getRequiredMappingUpdate(), shardId);\n+                mappingUpdate.accept(result.getRequiredMappingUpdate(), shardId);\n             } catch (Exception e) {\n                 return onMappingUpdateError.apply(e);\n             }"
  },
  {
    "sha": "a4ca55b9f12e9818d2a8d3dfc859d390343cf7ad",
    "filename": "server/src/main/java/io/crate/execution/dml/upsert/ShardUpsertRequest.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/crate/crate/blob/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/main/java/io/crate/execution/dml/upsert/ShardUpsertRequest.java",
    "raw_url": "https://github.com/crate/crate/raw/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/main/java/io/crate/execution/dml/upsert/ShardUpsertRequest.java",
    "contents_url": "https://api.github.com/repos/crate/crate/contents/server/src/main/java/io/crate/execution/dml/upsert/ShardUpsertRequest.java?ref=711161de73ff20b9d7af3ac1df8ee5d310aef77d",
    "patch": "@@ -87,7 +87,7 @@ public ShardUpsertRequest(\n         @Nullable Reference[] insertColumns,\n         @Nullable Symbol[] returnValues) {\n         super(shardId, jobId);\n-        assert updateColumns != null || insertColumns != null : \"Missing updateAssignments, whether for update nor for insert\";\n+//        assert updateColumns != null || insertColumns != null : \"Missing updateAssignments, whether for update nor for insert\";\n         this.continueOnError = continueOnError;\n         this.validateConstraints = validateConstraints;\n         this.duplicateKeyAction = duplicateKeyAction;"
  },
  {
    "sha": "2a6a58ea4e9173e9582adc2cf6ff38607d60a36c",
    "filename": "server/src/main/java/io/crate/execution/dml/upsert/TransportShardUpsertAction.java",
    "status": "modified",
    "additions": 88,
    "deletions": 23,
    "changes": 111,
    "blob_url": "https://github.com/crate/crate/blob/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/main/java/io/crate/execution/dml/upsert/TransportShardUpsertAction.java",
    "raw_url": "https://github.com/crate/crate/raw/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/main/java/io/crate/execution/dml/upsert/TransportShardUpsertAction.java",
    "contents_url": "https://api.github.com/repos/crate/crate/contents/server/src/main/java/io/crate/execution/dml/upsert/TransportShardUpsertAction.java?ref=711161de73ff20b9d7af3ac1df8ee5d310aef77d",
    "patch": "@@ -42,6 +42,7 @@\n import io.crate.metadata.TransactionContext;\n import io.crate.metadata.doc.DocTableInfo;\n import io.crate.metadata.table.Operation;\n+import org.apache.logging.log4j.Logger;\n import org.elasticsearch.ExceptionsHelper;\n import org.elasticsearch.action.support.replication.TransportReplicationAction;\n import org.elasticsearch.cluster.action.shard.ShardStateAction;\n@@ -60,9 +61,11 @@\n import org.elasticsearch.index.engine.DocumentSourceMissingException;\n import org.elasticsearch.index.engine.Engine;\n import org.elasticsearch.index.engine.VersionConflictEngineException;\n+import org.elasticsearch.index.mapper.Mapping;\n import org.elasticsearch.index.mapper.SourceToParse;\n import org.elasticsearch.index.seqno.SequenceNumbers;\n import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n import org.elasticsearch.index.translog.Translog;\n import org.elasticsearch.indices.IndicesService;\n import org.elasticsearch.threadpool.ThreadPool;\n@@ -77,6 +80,7 @@\n import java.util.Map;\n import java.util.Set;\n import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.function.BiConsumer;\n \n import static io.crate.exceptions.SQLExceptions.userFriendlyCrateExceptionTopOnly;\n \n@@ -121,7 +125,18 @@ public TransportShardUpsertAction(ThreadPool threadPool,\n     protected WritePrimaryResult<ShardUpsertRequest, ShardResponse> processRequestItems(IndexShard indexShard,\n                                                                                         ShardUpsertRequest request,\n                                                                                         AtomicBoolean killed) {\n-        ShardResponse shardResponse = new ShardResponse(request.returnValues());\n+    return processRequestItems(indexShard, request, killed, schemas, nodeCtx, logger, mappingUpdate);\n+    }\n+\n+    public static WritePrimaryResult<ShardUpsertRequest, ShardResponse> processRequestItems(\n+        IndexShard indexShard,\n+        ShardUpsertRequest request,\n+        AtomicBoolean killed,\n+        Schemas schemas,\n+        NodeContext nodeCtx,\n+        Logger logger,\n+        BiConsumer<Mapping, ShardId> mappingUpdate\n+    ) {\n         String indexName = request.index();\n         DocTableInfo tableInfo = schemas.getTableInfo(RelationName.fromIndexName(indexName), Operation.INSERT);\n         Reference[] insertColumns = request.insertColumns();\n@@ -145,6 +160,22 @@ public TransportShardUpsertAction(ThreadPool threadPool,\n             ? null\n             : new ReturnValueGen(txnCtx, nodeCtx, tableInfo, request.returnValues());\n \n+        return processRequestItems(indexShard, request, killed, logger, mappingUpdate, updateSourceGen, insertSourceGen, returnValueGen);\n+    }\n+\n+    public static WritePrimaryResult<ShardUpsertRequest, ShardResponse> processRequestItems(\n+        IndexShard indexShard,\n+        ShardUpsertRequest request,\n+        AtomicBoolean killed,\n+        Logger logger,\n+        BiConsumer<Mapping, ShardId> mappingUpdate,\n+        @Nullable UpdateSourceGen updateSourceGen,\n+        @Nullable InsertSourceGen insertSourceGen,\n+        @Nullable ReturnValueGen returnValueGen\n+    ) {\n+\n+        ShardResponse shardResponse = new ShardResponse(request.returnValues());\n+\n         Translog.Location translogLocation = null;\n         for (ShardUpsertRequest.Item item : request.items()) {\n             int location = item.location();\n@@ -162,7 +193,9 @@ public TransportShardUpsertAction(ThreadPool threadPool,\n                     indexShard,\n                     updateSourceGen,\n                     insertSourceGen,\n-                    returnValueGen\n+                    returnValueGen,\n+                    logger,\n+                    mappingUpdate\n                 );\n                 if (indexItemResponse != null) {\n                     if (indexItemResponse.translog != null) {\n@@ -199,8 +232,15 @@ public TransportShardUpsertAction(ThreadPool threadPool,\n         return new WritePrimaryResult<>(request, shardResponse, translogLocation, null, indexShard);\n     }\n \n+\n+\n+\n     @Override\n     protected WriteReplicaResult<ShardUpsertRequest> processRequestItemsOnReplica(IndexShard indexShard, ShardUpsertRequest request) throws IOException {\n+        return processOnReplica(indexShard, request, logger);\n+    }\n+\n+    public static WriteReplicaResult<ShardUpsertRequest> processOnReplica(IndexShard indexShard, ShardUpsertRequest request, Logger logger) throws IOException {\n         Translog.Location location = null;\n         for (ShardUpsertRequest.Item item : request.items()) {\n             if (item.source() == null) {\n@@ -242,23 +282,24 @@ public TransportShardUpsertAction(ThreadPool threadPool,\n         return new WriteReplicaResult<>(request, location, null, indexShard, logger);\n     }\n \n-    @Nullable\n-    private IndexItemResponse indexItem(ShardUpsertRequest request,\n-                                        ShardUpsertRequest.Item item,\n-                                        IndexShard indexShard,\n-                                        @Nullable UpdateSourceGen updateSourceGen,\n-                                        @Nullable InsertSourceGen insertSourceGen,\n-                                        @Nullable ReturnValueGen returnValueGen) throws Exception {\n+    public static IndexItemResponse indexItem(ShardUpsertRequest request,\n+                                       ShardUpsertRequest.Item item,\n+                                       IndexShard indexShard,\n+                                       @Nullable UpdateSourceGen updateSourceGen,\n+                                       @Nullable InsertSourceGen insertSourceGen,\n+                                       @Nullable ReturnValueGen returnValueGen,\n+                                       Logger logger,\n+                                       BiConsumer<Mapping, ShardId> mappingUpdate) throws Exception {\n         VersionConflictEngineException lastException = null;\n         boolean tryInsertFirst = item.insertValues() != null;\n         boolean isRetry;\n         for (int retryCount = 0; retryCount < MAX_RETRY_LIMIT; retryCount++) {\n             try {\n                 isRetry = retryCount > 0;\n                 if (tryInsertFirst) {\n-                    return insert(request, item, indexShard, isRetry, returnValueGen, insertSourceGen);\n+                    return insert(request, item, indexShard, isRetry, returnValueGen, insertSourceGen, logger, mappingUpdate);\n                 } else {\n-                    return update(item, indexShard, isRetry, returnValueGen, updateSourceGen);\n+                    return update(item, indexShard, isRetry, returnValueGen, updateSourceGen, logger, mappingUpdate);\n                 }\n             } catch (VersionConflictEngineException e) {\n                 lastException = e;\n@@ -308,6 +349,17 @@ protected IndexItemResponse insert(ShardUpsertRequest request,\n                                        boolean isRetry,\n                                        @Nullable ReturnValueGen returnGen,\n                                        InsertSourceGen insertSourceGen) throws Exception {\n+        return insert(request, item, indexShard, isRetry, returnGen, insertSourceGen, logger, mappingUpdate);\n+    }\n+\n+    public static IndexItemResponse insert(ShardUpsertRequest request,\n+                                    ShardUpsertRequest.Item item,\n+                                    IndexShard indexShard,\n+                                    boolean isRetry,\n+                                    @Nullable ReturnValueGen returnGen,\n+                                    InsertSourceGen insertSourceGen,\n+                                    Logger logger,\n+                                    BiConsumer<Mapping, ShardId> mappingUpdate) throws Exception {\n         assert insertSourceGen != null : \"InsertSourceGen must not be null\";\n         BytesReference rawSource;\n         Map<String, Object> source = null;\n@@ -329,7 +381,7 @@ protected IndexItemResponse insert(ShardUpsertRequest request,\n         long seqNo = SequenceNumbers.UNASSIGNED_SEQ_NO;\n         long primaryTerm = SequenceNumbers.UNASSIGNED_PRIMARY_TERM;\n \n-        Engine.IndexResult indexResult = index(item, indexShard, isRetry, seqNo, primaryTerm, version);\n+        Engine.IndexResult indexResult = index(item, indexShard, isRetry, seqNo, primaryTerm, version, logger, mappingUpdate);\n         Object[] returnvalues = null;\n         if (returnGen != null) {\n             // This optimizes for the case where the insert value is already string-based, so only parse the source\n@@ -359,10 +411,20 @@ protected IndexItemResponse insert(ShardUpsertRequest request,\n     }\n \n     protected IndexItemResponse update(ShardUpsertRequest.Item item,\n-                                       IndexShard indexShard,\n-                                       boolean isRetry,\n-                                       @Nullable ReturnValueGen returnGen,\n-                                       UpdateSourceGen updateSourceGen) throws Exception {\n+                                    IndexShard indexShard,\n+                                    boolean isRetry,\n+                                    @Nullable ReturnValueGen returnGen,\n+                                    UpdateSourceGen updateSourceGen) throws Exception {\n+        return update(item, indexShard, isRetry, returnGen, updateSourceGen, logger, mappingUpdate);\n+    }\n+\n+    static IndexItemResponse update(ShardUpsertRequest.Item item,\n+                                    IndexShard indexShard,\n+                                    boolean isRetry,\n+                                    @Nullable ReturnValueGen returnGen,\n+                                    UpdateSourceGen updateSourceGen,\n+                                    Logger logger,\n+                                    BiConsumer<Mapping, ShardId> mappingUpdate) throws Exception {\n         assert updateSourceGen != null : \"UpdateSourceGen must not be null\";\n         Doc fetchedDoc = getDocument(indexShard, item.id(), item.version(), item.seqNo(), item.primaryTerm());\n         Map<String, Object> source = updateSourceGen.generateSource(\n@@ -376,7 +438,7 @@ protected IndexItemResponse update(ShardUpsertRequest.Item item,\n         long primaryTerm = item.primaryTerm();\n         long version = Versions.MATCH_ANY;\n \n-        Engine.IndexResult indexResult = index(item, indexShard, isRetry, seqNo, primaryTerm, version);\n+        Engine.IndexResult indexResult = index(item, indexShard, isRetry, seqNo, primaryTerm, version, logger, mappingUpdate);\n         Object[] returnvalues = null;\n         if (returnGen != null) {\n             returnvalues = returnGen.generateReturnValues(\n@@ -395,12 +457,14 @@ protected IndexItemResponse update(ShardUpsertRequest.Item item,\n         return new IndexItemResponse(indexResult.getTranslogLocation(), returnvalues);\n     }\n \n-    private Engine.IndexResult index(ShardUpsertRequest.Item item,\n-                                     IndexShard indexShard,\n-                                     boolean isRetry,\n-                                     long seqNo,\n-                                     long primaryTerm,\n-                                     long version) throws Exception {\n+    static Engine.IndexResult index(ShardUpsertRequest.Item item,\n+                                    IndexShard indexShard,\n+                                    boolean isRetry,\n+                                    long seqNo,\n+                                    long primaryTerm,\n+                                    long version,\n+                                    Logger logger,\n+                                    BiConsumer<Mapping, ShardId> mappingUpdate) throws Exception {\n         SourceToParse sourceToParse = new SourceToParse(\n             indexShard.shardId().getIndexName(),\n             item.id(),\n@@ -409,6 +473,7 @@ protected IndexItemResponse update(ShardUpsertRequest.Item item,\n         );\n \n         Engine.IndexResult indexResult = executeOnPrimaryHandlingMappingUpdate(\n+            mappingUpdate,\n             indexShard.shardId(),\n             () -> indexShard.applyIndexOperationOnPrimary(\n                 version,"
  },
  {
    "sha": "4a3982e04a2d2afe3b9a8536e60cc170fc31a20d",
    "filename": "server/src/main/java/io/crate/metadata/upgrade/MetadataIndexUpgrader.java",
    "status": "modified",
    "additions": 2,
    "deletions": 7,
    "changes": 9,
    "blob_url": "https://github.com/crate/crate/blob/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/main/java/io/crate/metadata/upgrade/MetadataIndexUpgrader.java",
    "raw_url": "https://github.com/crate/crate/raw/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/main/java/io/crate/metadata/upgrade/MetadataIndexUpgrader.java",
    "contents_url": "https://api.github.com/repos/crate/crate/contents/server/src/main/java/io/crate/metadata/upgrade/MetadataIndexUpgrader.java?ref=711161de73ff20b9d7af3ac1df8ee5d310aef77d",
    "patch": "@@ -83,13 +83,8 @@ MappingMetadata createUpdatedIndexMetadata(MappingMetadata mappingMetadata, Stri\n                     newMapping.put(fieldName, fieldNode);\n             }\n         }\n-        try {\n-            return new MappingMetadata(\n-                Constants.DEFAULT_MAPPING_TYPE, Map.of(Constants.DEFAULT_MAPPING_TYPE, newMapping));\n-        } catch (IOException e) {\n-            logger.error(\"Failed to upgrade mapping for index '\" + indexName + \"'\", e);\n-            return mappingMetadata;\n-        }\n+        return new MappingMetadata(\n+            Constants.DEFAULT_MAPPING_TYPE, Map.of(Constants.DEFAULT_MAPPING_TYPE, newMapping));\n     }\n \n     private static void handleDynamicTemplates(LinkedHashMap<String, Object> newMapping, String fieldName, List<?> fieldNode) {"
  },
  {
    "sha": "be1a4937f14b56d57d6743a5d732f383b2e7ee24",
    "filename": "server/src/main/java/org/elasticsearch/action/bulk/MappingUpdatePerformer.java",
    "status": "modified",
    "additions": 2,
    "deletions": 1,
    "changes": 3,
    "blob_url": "https://github.com/crate/crate/blob/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/main/java/org/elasticsearch/action/bulk/MappingUpdatePerformer.java",
    "raw_url": "https://github.com/crate/crate/raw/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/main/java/org/elasticsearch/action/bulk/MappingUpdatePerformer.java",
    "contents_url": "https://api.github.com/repos/crate/crate/contents/server/src/main/java/org/elasticsearch/action/bulk/MappingUpdatePerformer.java?ref=711161de73ff20b9d7af3ac1df8ee5d310aef77d",
    "patch": "@@ -22,11 +22,12 @@\n import org.elasticsearch.index.mapper.Mapping;\n import org.elasticsearch.index.shard.ShardId;\n \n+@FunctionalInterface\n public interface MappingUpdatePerformer {\n \n     /**\n      * Update the mappings on the master.\n      */\n-    void updateMappings(Mapping update, ShardId shardId);\n+    void accept(Mapping update, ShardId shardId);\n \n }"
  },
  {
    "sha": "ee1e9743f0d6b6570c472ea4b2ffda70a338871e",
    "filename": "server/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java",
    "status": "modified",
    "additions": 2,
    "deletions": 2,
    "changes": 4,
    "blob_url": "https://github.com/crate/crate/blob/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java",
    "raw_url": "https://github.com/crate/crate/raw/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java",
    "contents_url": "https://api.github.com/repos/crate/crate/contents/server/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java?ref=711161de73ff20b9d7af3ac1df8ee5d310aef77d",
    "patch": "@@ -252,13 +252,13 @@ private ClusterBlockException blockExceptions(final ClusterState state, final St\n         return null;\n     }\n \n-    protected boolean retryPrimaryException(final Throwable e) {\n+    protected static boolean retryPrimaryException(final Throwable e) {\n         return e.getClass() == ReplicationOperation.RetryOnPrimaryException.class\n                 || TransportActions.isShardNotAvailableException(e)\n                 || isRetryableClusterBlockException(e);\n     }\n \n-    boolean isRetryableClusterBlockException(final Throwable e) {\n+    static boolean isRetryableClusterBlockException(final Throwable e) {\n         if (e instanceof ClusterBlockException) {\n             return ((ClusterBlockException) e).retryable();\n         }"
  },
  {
    "sha": "e1a48a8f15397c810189c4f2476a67b2c95c97d8",
    "filename": "server/src/main/java/org/elasticsearch/action/support/replication/TransportWriteAction.java",
    "status": "modified",
    "additions": 4,
    "deletions": 4,
    "changes": 8,
    "blob_url": "https://github.com/crate/crate/blob/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/main/java/org/elasticsearch/action/support/replication/TransportWriteAction.java",
    "raw_url": "https://github.com/crate/crate/raw/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/main/java/org/elasticsearch/action/support/replication/TransportWriteAction.java",
    "contents_url": "https://api.github.com/repos/crate/crate/contents/server/src/main/java/org/elasticsearch/action/support/replication/TransportWriteAction.java?ref=711161de73ff20b9d7af3ac1df8ee5d310aef77d",
    "patch": "@@ -240,7 +240,7 @@ public ClusterBlockLevel indexBlockLevel() {\n      * callback used by {@link AsyncAfterWriteAction} to notify that all post\n      * process actions have been executed\n      */\n-    interface RespondingWriteResult {\n+    public interface RespondingWriteResult {\n         /**\n          * Called on successful processing of all post write actions\n          * @param forcedRefresh <code>true</code> iff this write has caused a refresh\n@@ -258,7 +258,7 @@ public ClusterBlockLevel indexBlockLevel() {\n      * translog syncs or waiting for a refresh to happen making the write operation\n      * visible.\n      */\n-    static final class AsyncAfterWriteAction {\n+    public static final class AsyncAfterWriteAction {\n         private final Location location;\n         private final boolean sync;\n         private final AtomicInteger pendingOps = new AtomicInteger(1);\n@@ -267,7 +267,7 @@ public ClusterBlockLevel indexBlockLevel() {\n         private final RespondingWriteResult respond;\n         private final IndexShard indexShard;\n \n-        AsyncAfterWriteAction(final IndexShard indexShard,\n+        public AsyncAfterWriteAction(final IndexShard indexShard,\n                              @Nullable final Translog.Location location,\n                              final RespondingWriteResult respond) {\n             this.indexShard = indexShard;\n@@ -292,7 +292,7 @@ private void maybeFinish() {\n             assert numPending >= 0 && numPending <= 2 : \"numPending must either 2, 1 or 0 but was \" + numPending;\n         }\n \n-        void run() {\n+        public void run() {\n             /*\n              * We either respond immediately (i.e., if we do not fsync per request or wait for\n              * refresh), or we there are past async operations and we wait for them to return to"
  },
  {
    "sha": "4fe0a40c8376af312e7f9d63ee5211409797ab4a",
    "filename": "server/src/main/java/org/elasticsearch/cluster/metadata/IndexMetadata.java",
    "status": "modified",
    "additions": 12,
    "deletions": 1,
    "changes": 13,
    "blob_url": "https://github.com/crate/crate/blob/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/main/java/org/elasticsearch/cluster/metadata/IndexMetadata.java",
    "raw_url": "https://github.com/crate/crate/raw/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/main/java/org/elasticsearch/cluster/metadata/IndexMetadata.java",
    "contents_url": "https://api.github.com/repos/crate/crate/contents/server/src/main/java/org/elasticsearch/cluster/metadata/IndexMetadata.java?ref=711161de73ff20b9d7af3ac1df8ee5d310aef77d",
    "patch": "@@ -34,6 +34,8 @@\n import org.elasticsearch.cluster.node.DiscoveryNodeFilters;\n import org.elasticsearch.cluster.routing.allocation.IndexMetadataUpdater;\n import javax.annotation.Nullable;\n+\n+import org.elasticsearch.common.Strings;\n import org.elasticsearch.common.bytes.BytesArray;\n import org.elasticsearch.common.collect.ImmutableOpenIntMap;\n import org.elasticsearch.common.collect.ImmutableOpenMap;\n@@ -52,6 +54,7 @@\n import org.elasticsearch.common.xcontent.XContentParser;\n import org.elasticsearch.gateway.MetadataStateFormat;\n import org.elasticsearch.index.Index;\n+import org.elasticsearch.index.mapper.MapperService;\n import org.elasticsearch.index.shard.ShardId;\n import org.elasticsearch.rest.RestStatus;\n \n@@ -66,6 +69,7 @@\n import java.util.Iterator;\n import java.util.Locale;\n import java.util.Map;\n+import java.util.Objects;\n import java.util.Set;\n import java.util.function.Function;\n \n@@ -876,11 +880,18 @@ public MappingMetadata mapping(String type) {\n         }\n \n         // TODO remove type here\n-        public Builder putMapping(String type, String source) throws IOException {\n+        public Builder putMapping(String type, Map<String, Object> source) throws IOException {\n+            putMapping(new MappingMetadata(type, source));\n+            return this;\n+        }\n+\n+        // TODO remove type here\n+        public Builder putMapping(String type, String source) {\n             putMapping(new MappingMetadata(type, XContentHelper.convertToMap(XContentFactory.xContent(source), source, true)));\n             return this;\n         }\n \n+\n         public Builder putMapping(MappingMetadata mappingMd) {\n             mappings.clear();\n             if (mappingMd != null) {"
  },
  {
    "sha": "df3a3291b7801d4fd6c3c456878c06a9cc471a14",
    "filename": "server/src/main/java/org/elasticsearch/cluster/metadata/MappingMetadata.java",
    "status": "modified",
    "additions": 9,
    "deletions": 4,
    "changes": 13,
    "blob_url": "https://github.com/crate/crate/blob/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/main/java/org/elasticsearch/cluster/metadata/MappingMetadata.java",
    "raw_url": "https://github.com/crate/crate/raw/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/main/java/org/elasticsearch/cluster/metadata/MappingMetadata.java",
    "contents_url": "https://api.github.com/repos/crate/crate/contents/server/src/main/java/org/elasticsearch/cluster/metadata/MappingMetadata.java?ref=711161de73ff20b9d7af3ac1df8ee5d310aef77d",
    "patch": "@@ -22,6 +22,7 @@\n import static org.elasticsearch.common.xcontent.support.XContentMapValues.nodeBooleanValue;\n \n import java.io.IOException;\n+import java.io.UncheckedIOException;\n import java.util.Map;\n \n import org.elasticsearch.ElasticsearchParseException;\n@@ -82,7 +83,7 @@ public MappingMetadata(DocumentMapper docMapper) {\n         this.routing = new Routing(docMapper.routingFieldMapper().required());\n     }\n \n-    public MappingMetadata(CompressedXContent mapping) throws IOException {\n+    public MappingMetadata(CompressedXContent mapping) {\n         this.source = mapping;\n         Map<String, Object> mappingMap = XContentHelper.convertToMap(mapping.compressedReference(), true).v2();\n         if (mappingMap.size() != 1) {\n@@ -92,10 +93,14 @@ public MappingMetadata(CompressedXContent mapping) throws IOException {\n         initMappers((Map<String, Object>) mappingMap.get(this.type));\n     }\n \n-    public MappingMetadata(String type, Map<String, Object> mapping) throws IOException {\n+    public MappingMetadata(String type, Map<String, Object> mapping) {\n         this.type = type;\n-        this.source = new CompressedXContent(\n-            (builder, params) -> builder.mapContents(mapping), XContentType.JSON, ToXContent.EMPTY_PARAMS);\n+        try {\n+            this.source = new CompressedXContent(\n+                (builder, params) -> builder.mapContents(mapping), XContentType.JSON, ToXContent.EMPTY_PARAMS);\n+        } catch (IOException e) {\n+            throw new UncheckedIOException(e);  // XContent exception, should never happen\n+        }\n         Map<String, Object> withoutType = mapping;\n         if (mapping.size() == 1 && mapping.containsKey(type)) {\n             withoutType = (Map<String, Object>) mapping.get(type);"
  },
  {
    "sha": "94234c9b32866916d523f0e708b538aa3f43b211",
    "filename": "server/src/main/java/org/elasticsearch/index/IndexSettings.java",
    "status": "modified",
    "additions": 4,
    "deletions": 2,
    "changes": 6,
    "blob_url": "https://github.com/crate/crate/blob/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/main/java/org/elasticsearch/index/IndexSettings.java",
    "raw_url": "https://github.com/crate/crate/raw/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/main/java/org/elasticsearch/index/IndexSettings.java",
    "contents_url": "https://api.github.com/repos/crate/crate/contents/server/src/main/java/org/elasticsearch/index/IndexSettings.java?ref=711161de73ff20b9d7af3ac1df8ee5d310aef77d",
    "patch": "@@ -653,8 +653,10 @@ public int getTranslogRetentionTotalFiles() {\n     }\n \n     private static boolean shouldDisableTranslogRetention(Settings settings) {\n-        return INDEX_SOFT_DELETES_SETTING.get(settings)\n-            && IndexMetadata.SETTING_INDEX_VERSION_CREATED.get(settings).onOrAfter(Version.V_4_3_0);\n+        Version version = IndexMetadata.SETTING_INDEX_VERSION_CREATED.get(settings);\n+        boolean b = INDEX_SOFT_DELETES_SETTING.get(settings)\n+                    && version.onOrAfter(Version.V_4_3_0);\n+        return b;\n     }\n \n     /**"
  },
  {
    "sha": "bd5a250b984cdf7caeaeed036de0511641d8fd79",
    "filename": "server/src/main/java/org/elasticsearch/index/seqno/ReplicationTracker.java",
    "status": "modified",
    "additions": 6,
    "deletions": 9,
    "changes": 15,
    "blob_url": "https://github.com/crate/crate/blob/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/main/java/org/elasticsearch/index/seqno/ReplicationTracker.java",
    "raw_url": "https://github.com/crate/crate/raw/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/main/java/org/elasticsearch/index/seqno/ReplicationTracker.java",
    "contents_url": "https://api.github.com/repos/crate/crate/contents/server/src/main/java/org/elasticsearch/index/seqno/ReplicationTracker.java?ref=711161de73ff20b9d7af3ac1df8ee5d310aef77d",
    "patch": "@@ -895,11 +895,10 @@ public ReplicationTracker(\n         this.pendingInSync = new HashSet<>();\n         this.routingTable = null;\n         this.replicationGroup = null;\n-        this.hasAllPeerRecoveryRetentionLeases = indexSettings.getIndexVersionCreated().onOrAfter(Version.V_4_5_0)\n-            || (indexSettings.isSoftDeleteEnabled() &&\n-               (indexSettings.getIndexVersionCreated().onOrAfter(Version.V_4_4_0) ||\n-               (indexSettings.getIndexVersionCreated().onOrAfter(Version.V_4_3_0) &&\n-                indexSettings.getIndexMetadata().getState() == IndexMetadata.State.OPEN)));\n+        this.hasAllPeerRecoveryRetentionLeases = indexSettings.isSoftDeleteEnabled() &&\n+                                                 (indexSettings.getIndexVersionCreated().onOrAfter(Version.V_4_5_0) ||\n+                                                  (indexSettings.getIndexVersionCreated().onOrAfter(Version.V_4_3_0) &&\n+                                                   indexSettings.getIndexMetadata().getState() == IndexMetadata.State.OPEN));\n \n         this.fileBasedRecoveryThreshold = IndexSettings.FILE_BASED_RECOVERY_THRESHOLD_SETTING.get(indexSettings.getSettings());\n         this.safeCommitInfoSupplier = safeCommitInfoSupplier;\n@@ -1342,9 +1341,7 @@ public synchronized void activateWithPrimaryContext(PrimaryContext primaryContex\n         // initializeWithPrimaryContext, we might still have missed a cluster state update. This is best effort.\n         runAfter.run();\n \n-        if (indexSettings.isSoftDeleteEnabled()) {\n-            addPeerRecoveryRetentionLeaseForSolePrimary();\n-        }\n+        addPeerRecoveryRetentionLeaseForSolePrimary();\n \n         assert invariant();\n     }\n@@ -1363,7 +1360,7 @@ public synchronized boolean hasAllPeerRecoveryRetentionLeases() {\n      * prior to {@link Version#V_4_3_0} that does not create peer-recovery retention leases.\n      */\n     public synchronized void createMissingPeerRecoveryRetentionLeases(ActionListener<Void> listener) {\n-        if (hasAllPeerRecoveryRetentionLeases == false) {\n+        if (indexSettings().isSoftDeleteEnabled() && hasAllPeerRecoveryRetentionLeases == false) {\n             final List<ShardRouting> shardRoutings = routingTable.assignedShards();\n             final GroupedActionListener<ReplicationResponse> groupedActionListener = new GroupedActionListener<>(ActionListener.wrap(vs -> {\n                 setHasAllPeerRecoveryRetentionLeases();"
  },
  {
    "sha": "999c0bc8bc30552552963d89ba0a529943ab3910",
    "filename": "server/src/main/java/org/elasticsearch/index/shard/IndexShard.java",
    "status": "modified",
    "additions": 1,
    "deletions": 0,
    "changes": 1,
    "blob_url": "https://github.com/crate/crate/blob/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/main/java/org/elasticsearch/index/shard/IndexShard.java",
    "raw_url": "https://github.com/crate/crate/raw/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/main/java/org/elasticsearch/index/shard/IndexShard.java",
    "contents_url": "https://api.github.com/repos/crate/crate/contents/server/src/main/java/org/elasticsearch/index/shard/IndexShard.java?ref=711161de73ff20b9d7af3ac1df8ee5d310aef77d",
    "patch": "@@ -407,6 +407,7 @@ public void updateShardState(final ShardRouting newRouting,\n                         if (currentRouting.isRelocationTarget() == false) {\n                             // the master started a recovering primary, activate primary mode.\n                             replicationTracker.activatePrimaryMode(getLocalCheckpoint());\n+                            ensurePeerRecoveryRetentionLeasesExist();\n                         }\n                     }\n                 } else {"
  },
  {
    "sha": "8dd896c7c27e602f918349da51ece26935f913b4",
    "filename": "server/src/test/java/io/crate/integrationtests/AnyIntegrationTest.java",
    "status": "modified",
    "additions": 1,
    "deletions": 0,
    "changes": 1,
    "blob_url": "https://github.com/crate/crate/blob/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/test/java/io/crate/integrationtests/AnyIntegrationTest.java",
    "raw_url": "https://github.com/crate/crate/raw/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/test/java/io/crate/integrationtests/AnyIntegrationTest.java",
    "contents_url": "https://api.github.com/repos/crate/crate/contents/server/src/test/java/io/crate/integrationtests/AnyIntegrationTest.java?ref=711161de73ff20b9d7af3ac1df8ee5d310aef77d",
    "patch": "@@ -22,6 +22,7 @@\n package io.crate.integrationtests;\n \n import io.crate.testing.TestingHelpers;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n import org.hamcrest.Matchers;\n import org.hamcrest.core.Is;\n import org.junit.Test;"
  },
  {
    "sha": "036f459fa342d3fc84b21be39b41e9ae2c45bbcd",
    "filename": "server/src/test/java/io/crate/metadata/doc/DocIndexMetadataTest.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/crate/crate/blob/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/test/java/io/crate/metadata/doc/DocIndexMetadataTest.java",
    "raw_url": "https://github.com/crate/crate/raw/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/test/java/io/crate/metadata/doc/DocIndexMetadataTest.java",
    "contents_url": "https://api.github.com/repos/crate/crate/contents/server/src/test/java/io/crate/metadata/doc/DocIndexMetadataTest.java?ref=711161de73ff20b9d7af3ac1df8ee5d310aef77d",
    "patch": "@@ -1019,7 +1019,7 @@ public void testAnalyzedColumnWithAnalyzer() throws Exception {\n \n     @Test\n     public void testGeoPointType() throws Exception {\n-        DocIndexMetadata md = getDocIndexMetadataFromStatement(\"create table foo (p geo_point)\");\n+        DocIndexMetadata md = getDocIndexMetadataFromStatement(\"create table foo (id integer)\");\n         assertThat(md.columns().size(), is(1));\n         Reference reference = md.columns().iterator().next();\n         assertThat(reference.valueType(), equalTo(DataTypes.GEO_POINT));"
  },
  {
    "sha": "1c949ea79b804a80d29b0b1ec47e1d74db94cd8b",
    "filename": "server/src/test/java/io/crate/test/integration/CrateDummyClusterServiceUnitTest.java",
    "status": "modified",
    "additions": 2,
    "deletions": 2,
    "changes": 4,
    "blob_url": "https://github.com/crate/crate/blob/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/test/java/io/crate/test/integration/CrateDummyClusterServiceUnitTest.java",
    "raw_url": "https://github.com/crate/crate/raw/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/test/java/io/crate/test/integration/CrateDummyClusterServiceUnitTest.java",
    "contents_url": "https://api.github.com/repos/crate/crate/contents/server/src/test/java/io/crate/test/integration/CrateDummyClusterServiceUnitTest.java?ref=711161de73ff20b9d7af3ac1df8ee5d310aef77d",
    "patch": "@@ -98,7 +98,7 @@ public void resetClusterService() {\n         return EMPTY_CLUSTER_SETTINGS;\n     }\n \n-    protected ClusterService createClusterService(Collection<Setting<?>> additionalClusterSettings, Version version) {\n+    public static ClusterService createClusterService(Collection<Setting<?>> additionalClusterSettings, Version version) {\n         Set<Setting<?>> clusterSettingsSet = Sets.newHashSet(ClusterSettings.BUILT_IN_CLUSTER_SETTINGS);\n         clusterSettingsSet.addAll(additionalClusterSettings);\n         ClusterSettings clusterSettings = new ClusterSettings(Settings.EMPTY, clusterSettingsSet);\n@@ -124,7 +124,7 @@ protected ClusterService createClusterService(Collection<Setting<?>> additionalC\n             .localNodeId(NODE_ID)\n             .masterNodeId(NODE_ID)\n             .build();\n-        ClusterState clusterState = ClusterState.builder(new ClusterName(this.getClass().getSimpleName()))\n+        ClusterState clusterState = ClusterState.builder(new ClusterName(CrateDummyClusterServiceUnitTest.class.getSimpleName()))\n             .nodes(nodes).blocks(ClusterBlocks.EMPTY_CLUSTER_BLOCK).build();\n \n         ClusterApplierService clusterApplierService = clusterService.getClusterApplierService();"
  },
  {
    "sha": "559a341bf2eebfe51e7ceff9a731760f9c9c470a",
    "filename": "server/src/test/java/org/elasticsearch/index/replication/ESIndexLevelReplicationTestCase.java",
    "status": "added",
    "additions": 974,
    "deletions": 0,
    "changes": 974,
    "blob_url": "https://github.com/crate/crate/blob/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/test/java/org/elasticsearch/index/replication/ESIndexLevelReplicationTestCase.java",
    "raw_url": "https://github.com/crate/crate/raw/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/test/java/org/elasticsearch/index/replication/ESIndexLevelReplicationTestCase.java",
    "contents_url": "https://api.github.com/repos/crate/crate/contents/server/src/test/java/org/elasticsearch/index/replication/ESIndexLevelReplicationTestCase.java?ref=711161de73ff20b9d7af3ac1df8ee5d310aef77d",
    "patch": "@@ -0,0 +1,974 @@\n+/*\n+ * Licensed to Crate under one or more contributor license agreements.\n+ * See the NOTICE file distributed with this work for additional\n+ * information regarding copyright ownership.  Crate licenses this file\n+ * to you under the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.  You may\n+ * obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n+ * implied.  See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ *\n+ * However, if you have executed another commercial license agreement\n+ * with Crate these terms will supersede the license and you may use the\n+ * software solely pursuant to the terms of the relevant commercial\n+ * agreement.\n+ */\n+\n+package org.elasticsearch.index.replication;\n+\n+import io.crate.Constants;\n+import io.crate.common.unit.TimeValue;\n+import io.crate.execution.dml.ShardResponse;\n+import io.crate.execution.dml.delete.ShardDeleteRequest;\n+import io.crate.execution.dml.upsert.FromRawInsertSource;\n+import io.crate.execution.dml.upsert.ShardUpsertRequest;\n+import io.crate.execution.dml.upsert.TransportShardUpsertAction;\n+import io.crate.metadata.Functions;\n+import io.crate.metadata.NodeContext;\n+import io.crate.metadata.RelationName;\n+import io.crate.metadata.SearchPath;\n+import io.crate.metadata.doc.DocIndexMetadata;\n+import io.crate.metadata.settings.SessionSettings;\n+import org.apache.lucene.store.AlreadyClosedException;\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.flush.FlushRequest;\n+import org.elasticsearch.action.resync.ResyncReplicationRequest;\n+import org.elasticsearch.action.resync.TransportResyncReplicationAction;\n+import org.elasticsearch.action.support.ActionTestUtils;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.replication.ReplicationOperation;\n+import org.elasticsearch.action.support.replication.ReplicationRequest;\n+import org.elasticsearch.action.support.replication.ReplicationResponse;\n+import org.elasticsearch.action.support.replication.TransportReplicationAction;\n+import org.elasticsearch.action.support.replication.TransportReplicationAction.ReplicaResponse;\n+import org.elasticsearch.action.support.replication.TransportWriteAction;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodeRole;\n+import org.elasticsearch.cluster.routing.AllocationId;\n+import org.elasticsearch.cluster.routing.IndexShardRoutingTable;\n+import org.elasticsearch.cluster.routing.RecoverySource;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.ShardRoutingHelper;\n+import org.elasticsearch.cluster.routing.ShardRoutingState;\n+import org.elasticsearch.cluster.routing.TestShardRouting;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.bytes.BytesArray;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.collect.Iterators;\n+import org.elasticsearch.common.lease.Releasable;\n+import org.elasticsearch.common.lease.Releasables;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentFactory;\n+import org.elasticsearch.common.xcontent.XContentHelper;\n+import org.elasticsearch.common.xcontent.XContentType;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.engine.DocIdSeqNoAndSource;\n+import org.elasticsearch.index.engine.EngineFactory;\n+import org.elasticsearch.index.engine.InternalEngineFactory;\n+import org.elasticsearch.index.seqno.GlobalCheckpointSyncAction;\n+import org.elasticsearch.index.seqno.RetentionLease;\n+import org.elasticsearch.index.seqno.RetentionLeaseSyncAction;\n+import org.elasticsearch.index.seqno.RetentionLeaseSyncer;\n+import org.elasticsearch.index.seqno.RetentionLeases;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.IndexShardTestCase;\n+import org.elasticsearch.index.shard.PrimaryReplicaSyncer;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.index.shard.ShardPath;\n+import org.elasticsearch.index.translog.Translog;\n+import org.elasticsearch.indices.recovery.RecoveryState;\n+import org.elasticsearch.indices.recovery.RecoveryTarget;\n+import org.elasticsearch.tasks.TaskManager;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import javax.annotation.Nullable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.UUID;\n+import java.util.concurrent.CopyOnWriteArrayList;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.FutureTask;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.function.BiConsumer;\n+import java.util.function.BiFunction;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThanOrEqualTo;\n+\n+public abstract class ESIndexLevelReplicationTestCase extends IndexShardTestCase {\n+\n+    protected final Index index = new Index(\"test\", \"uuid\");\n+    private final ShardId shardId = new ShardId(index, 0);\n+\n+    protected ReplicationGroup createGroup(int replicas) throws IOException {\n+        return createGroup(replicas, Settings.EMPTY);\n+    }\n+\n+    protected ReplicationGroup createGroup(int replicas, Settings settings) throws IOException {\n+        IndexMetadata metaData = buildIndexMetadata(replicas, settings);\n+        return new ReplicationGroup(metaData);\n+    }\n+\n+    protected IndexMetadata buildIndexMetadata(int replicas) throws IOException {\n+        return buildIndexMetadata(replicas, Settings.EMPTY);\n+    }\n+\n+    protected IndexMetadata buildIndexMetadata(int replicas, Settings indexSettings) {\n+        Settings settings = Settings.builder().put(IndexMetadata.SETTING_VERSION_CREATED, Version.CURRENT)\n+            .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, replicas)\n+            .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 1)\n+            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n+            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(),\n+                 randomBoolean() ? IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.get(Settings.EMPTY) : between(0, 1000))\n+            .build();\n+\n+        IndexMetadata.Builder metaData = IndexMetadata.builder(index.getName())\n+            .settings(settings)\n+            .primaryTerm(0, randomIntBetween(1, 100))\n+            .putMapping(\"default\",  \"{\\\"_meta\\\":{},\\\"dynamic\\\":\\\"strict\\\",\\\"properties\\\":{\\\"id\\\":{\\\"position\\\":1,\\\"type\\\":\\\"integer\\\"}}}\");\n+\n+\n+        IndexMetadata build = metaData.build();\n+\n+\n+        return build;\n+    }\n+\n+    protected DiscoveryNode getDiscoveryNode(String id) {\n+        return new DiscoveryNode(id, id, buildNewFakeTransportAddress(), Collections.emptyMap(),\n+            Collections.singleton(DiscoveryNodeRole.DATA_ROLE), Version.CURRENT);\n+    }\n+\n+    protected class ReplicationGroup implements AutoCloseable, Iterable<IndexShard> {\n+        private IndexShard primary;\n+        private IndexMetadata indexMetadata;\n+        private final List<IndexShard> replicas;\n+        private final AtomicInteger replicaId = new AtomicInteger();\n+        private final AtomicInteger docId = new AtomicInteger();\n+        boolean closed = false;\n+        private volatile ReplicationTargets replicationTargets;\n+\n+        private final PrimaryReplicaSyncer primaryReplicaSyncer = new PrimaryReplicaSyncer(\n+            new TaskManager(Settings.EMPTY, threadPool),\n+            (request, parentTask, primaryAllocationId, primaryTerm, listener) -> {\n+                try {\n+                    new ResyncAction(request, listener, ReplicationGroup.this).execute();\n+                } catch (Exception e) {\n+                    throw new AssertionError(e);\n+                }\n+            });\n+\n+        private final RetentionLeaseSyncer retentionLeaseSyncer = new RetentionLeaseSyncer(\n+            (shardId, primaryAllocationId, primaryTerm, retentionLeases, listener) ->\n+                syncRetentionLeases(shardId, retentionLeases, listener),\n+            (shardId, primaryAllocationId, primaryTerm, retentionLeases) -> syncRetentionLeases(shardId, retentionLeases,\n+                ActionListener.wrap(\n+                    r -> { },\n+                    e -> {\n+                        throw new AssertionError(\"failed to background sync retention lease\", e);\n+                    })));\n+\n+        protected ReplicationGroup(final IndexMetadata indexMetadata) throws IOException {\n+            final ShardRouting primaryRouting = this.createShardRouting(\"s0\", true);\n+            primary = newShard(primaryRouting, indexMetadata, getEngineFactory(primaryRouting), () -> {}, retentionLeaseSyncer);\n+            replicas = new CopyOnWriteArrayList<>();\n+            this.indexMetadata = indexMetadata;\n+            updateAllocationIDsOnPrimary();\n+            for (int i = 0; i < indexMetadata.getNumberOfReplicas(); i++) {\n+                addReplica();\n+            }\n+        }\n+\n+        private ShardRouting createShardRouting(String nodeId, boolean primary) {\n+            return TestShardRouting.newShardRouting(shardId, nodeId, primary, ShardRoutingState.INITIALIZING,\n+                primary ? RecoverySource.EmptyStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE);\n+        }\n+\n+        protected EngineFactory getEngineFactory(ShardRouting routing) {\n+            return new InternalEngineFactory();\n+        }\n+\n+\n+        public int indexDocs(final int numOfDoc) throws Exception {\n+            ShardUpsertRequest shardRequest = shardRequest().newRequest(shardId);\n+            for (int doc = 0; doc < numOfDoc; doc++) {\n+                String id = Integer.toString(docId.incrementAndGet());\n+                String source = \"{}\";\n+                final ShardUpsertRequest.Item item = new ShardUpsertRequest.Item(id, null, new Object[]{source}, null, null, null);\n+                shardRequest.add(0, item);\n+            }\n+            final ShardResponse response = index(shardRequest);\n+            return response.successRowCount();\n+        }\n+\n+//        public int appendDocs(final int numOfDoc) throws Exception {\n+//            for (int doc = 0; doc < numOfDoc; doc++) {\n+//                final ShardUpsertRequest.Item item = new ShardUpsertRequest.Item(id, null, null, null, null, null);\n+//                item.source(new BytesArray(\"{}\"));\n+//                final BulkItemResponse response = index(item);\n+//                if (response.isFailed()) {\n+//                    throw response.getFailure().getCause();\n+//                } else if (response.isFailed() == false) {\n+//                    assertEquals(DocWriteResponse.Result.CREATED, response.getResponse().getResult());\n+//                }\n+//            }\n+//            return numOfDoc;\n+//        }\n+\n+        ShardUpsertRequest.Builder shardRequest() {\n+            return new ShardUpsertRequest.Builder(new SessionSettings(\"dummy\", SearchPath.createSearchPathFrom(\"dummy\")),\n+                                                  TimeValue.timeValueSeconds(60),\n+                                                  ShardUpsertRequest.DuplicateKeyAction.IGNORE,\n+                                                  true,\n+                                                  null,\n+                                                  null,\n+                                                  null,\n+                                                  UUID.randomUUID(),\n+                                                  false);\n+        }\n+\n+        public ShardResponse index(ShardUpsertRequest indexRequest) throws Exception {\n+            return executeWriteRequest(indexRequest);\n+        }\n+\n+//        public ShardResponse delete(DeleteRequest deleteRequest) throws Exception {\n+//            return executeWriteRequest(deleteRequest, deleteRequest.getRefreshPolicy());\n+//        }\n+\n+        private ShardResponse executeWriteRequest(ShardUpsertRequest shardRequest) throws Exception {\n+            PlainActionFuture<ShardResponse> listener = new PlainActionFuture<>();\n+            final ActionListener<ShardResponse> wrapBulkListener =\n+                ActionListener.map(listener, shardResponse -> shardResponse);\n+            new WriteReplicationAction(shardRequest, wrapBulkListener, this).execute();\n+            return listener.get();\n+        }\n+\n+        public synchronized void startAll() throws IOException {\n+            startReplicas(replicas.size());\n+        }\n+\n+        public synchronized int startReplicas(int numOfReplicasToStart) throws IOException {\n+            if (primary.routingEntry().initializing()) {\n+                startPrimary();\n+            }\n+            int started = 0;\n+            for (IndexShard replicaShard : replicas) {\n+                if (replicaShard.routingEntry().initializing()) {\n+                    recoverReplica(replicaShard);\n+                    started++;\n+                    if (started > numOfReplicasToStart) {\n+                        break;\n+                    }\n+                }\n+            }\n+            return started;\n+        }\n+\n+        public void startPrimary() throws IOException {\n+            recoverPrimary(primary);\n+            computeReplicationTargets();\n+            HashSet<String> activeIds = new HashSet<>();\n+            activeIds.addAll(activeIds());\n+            activeIds.add(primary.routingEntry().allocationId().getId());\n+            ShardRouting startedRoutingEntry = ShardRoutingHelper.moveToStarted(primary.routingEntry());\n+            IndexShardRoutingTable routingTable = routingTable(shr -> shr == primary.routingEntry() ? startedRoutingEntry : shr);\n+            primary.updateShardState(startedRoutingEntry, primary.getPendingPrimaryTerm(), null,\n+                currentClusterStateVersion.incrementAndGet(), activeIds, routingTable);\n+            for (final IndexShard replica : replicas) {\n+                recoverReplica(replica);\n+            }\n+            computeReplicationTargets();\n+        }\n+\n+        public IndexShard addReplica() throws IOException {\n+            final ShardRouting replicaRouting = createShardRouting(\"s\" + replicaId.incrementAndGet(), false);\n+            final IndexShard replica =\n+                newShard(replicaRouting, indexMetadata, getEngineFactory(replicaRouting), () -> {}, retentionLeaseSyncer);\n+            addReplica(replica);\n+            return replica;\n+        }\n+\n+        public synchronized void addReplica(IndexShard replica) throws IOException {\n+            assert shardRoutings().stream().anyMatch(shardRouting -> shardRouting.isSameAllocation(replica.routingEntry())) == false :\n+                \"replica with aId [\" + replica.routingEntry().allocationId() + \"] already exists\";\n+            replicas.add(replica);\n+            if (replicationTargets != null) {\n+                replicationTargets.addReplica(replica);\n+            }\n+            updateAllocationIDsOnPrimary();\n+        }\n+\n+        protected synchronized void recoverPrimary(IndexShard primary) {\n+            final DiscoveryNode pNode = getDiscoveryNode(primary.routingEntry().currentNodeId());\n+            primary.markAsRecovering(\"store\", new RecoveryState(primary.routingEntry(), pNode, null));\n+            recoverFromStore(primary);\n+        }\n+\n+        public synchronized IndexShard addReplicaWithExistingPath(final ShardPath shardPath, final String nodeId) throws IOException {\n+            final ShardRouting shardRouting = TestShardRouting.newShardRouting(\n+                    shardId,\n+                    nodeId,\n+                    false, ShardRoutingState.INITIALIZING,\n+                    RecoverySource.PeerRecoverySource.INSTANCE);\n+\n+            final IndexShard newReplica =\n+                newShard(shardRouting, shardPath, indexMetadata, null, getEngineFactory(shardRouting), () -> {\n+                }, retentionLeaseSyncer, EMPTY_EVENT_LISTENER);\n+            replicas.add(newReplica);\n+            if (replicationTargets != null) {\n+                replicationTargets.addReplica(newReplica);\n+            }\n+            updateAllocationIDsOnPrimary();\n+            return newReplica;\n+        }\n+\n+        public synchronized List<IndexShard> getReplicas() {\n+            return Collections.unmodifiableList(replicas);\n+        }\n+\n+        /**\n+         * promotes the specific replica as the new primary\n+         */\n+        public Future<PrimaryReplicaSyncer.ResyncTask> promoteReplicaToPrimary(IndexShard replica) throws IOException {\n+            PlainActionFuture<PrimaryReplicaSyncer.ResyncTask> fut = new PlainActionFuture<>();\n+            promoteReplicaToPrimary(replica, (shard, listener) -> {\n+                computeReplicationTargets();\n+                primaryReplicaSyncer.resync(shard,\n+                    new ActionListener<PrimaryReplicaSyncer.ResyncTask>() {\n+                        @Override\n+                        public void onResponse(PrimaryReplicaSyncer.ResyncTask resyncTask) {\n+                            listener.onResponse(resyncTask);\n+                            fut.onResponse(resyncTask);\n+                        }\n+\n+                        @Override\n+                        public void onFailure(Exception e) {\n+                            listener.onFailure(e);\n+                            fut.onFailure(e);\n+                        }\n+                    });\n+            });\n+            return fut;\n+        }\n+\n+        public synchronized void promoteReplicaToPrimary(IndexShard replica,\n+                 BiConsumer<IndexShard, ActionListener<PrimaryReplicaSyncer.ResyncTask>> primaryReplicaSyncer)\n+            throws IOException {\n+            final long newTerm = indexMetadata.primaryTerm(shardId.id()) + 1;\n+            IndexMetadata.Builder newMetadata = IndexMetadata.builder(indexMetadata).primaryTerm(shardId.id(), newTerm);\n+            indexMetadata = newMetadata.build();\n+            assertTrue(replicas.remove(replica));\n+            closeShards(primary);\n+            primary = replica;\n+            assert primary.routingEntry().active() : \"only active replicas can be promoted to primary: \" + primary.routingEntry();\n+            ShardRouting primaryRouting = replica.routingEntry().moveActiveReplicaToPrimary();\n+            IndexShardRoutingTable routingTable = routingTable(shr -> shr == replica.routingEntry() ? primaryRouting : shr);\n+\n+            primary.updateShardState(primaryRouting, newTerm, primaryReplicaSyncer, currentClusterStateVersion.incrementAndGet(),\n+                activeIds(), routingTable);\n+        }\n+\n+        private synchronized Set<String> activeIds() {\n+            return shardRoutings().stream()\n+                .filter(ShardRouting::active).map(ShardRouting::allocationId).map(AllocationId::getId).collect(Collectors.toSet());\n+        }\n+\n+        private synchronized IndexShardRoutingTable routingTable(Function<ShardRouting, ShardRouting> transformer) {\n+            IndexShardRoutingTable.Builder routingTable = new IndexShardRoutingTable.Builder(primary.shardId());\n+            shardRoutings().stream().map(transformer).forEach(routingTable::addShard);\n+            return routingTable.build();\n+        }\n+\n+        public synchronized boolean removeReplica(IndexShard replica) throws IOException {\n+            final boolean removed = replicas.remove(replica);\n+            if (removed) {\n+                updateAllocationIDsOnPrimary();\n+                computeReplicationTargets();\n+            }\n+            return removed;\n+        }\n+\n+        public void recoverReplica(IndexShard replica) throws IOException {\n+            recoverReplica(replica, (r, sourceNode) -> new RecoveryTarget(r, sourceNode, recoveryListener));\n+        }\n+\n+        public void recoverReplica(IndexShard replica, BiFunction<IndexShard, DiscoveryNode, RecoveryTarget> targetSupplier)\n+            throws IOException {\n+            recoverReplica(replica, targetSupplier, true);\n+        }\n+\n+        public void recoverReplica(\n+            IndexShard replica,\n+            BiFunction<IndexShard, DiscoveryNode, RecoveryTarget> targetSupplier,\n+            boolean markAsRecovering) throws IOException {\n+            final IndexShardRoutingTable routingTable = routingTable(Function.identity());\n+            final Set<String> inSyncIds = activeIds();\n+            ESIndexLevelReplicationTestCase.this.recoverUnstartedReplica(replica, primary, targetSupplier, markAsRecovering, inSyncIds,\n+                routingTable);\n+            ESIndexLevelReplicationTestCase.this.startReplicaAfterRecovery(replica, primary, inSyncIds, routingTable);\n+            computeReplicationTargets();\n+        }\n+\n+        public synchronized DiscoveryNode getPrimaryNode() {\n+            return getDiscoveryNode(primary.routingEntry().currentNodeId());\n+        }\n+\n+        public Future<Void> asyncRecoverReplica(\n+                final IndexShard replica, final BiFunction<IndexShard, DiscoveryNode, RecoveryTarget> targetSupplier) {\n+            final FutureTask<Void> task = new FutureTask<>(() -> {\n+                recoverReplica(replica, targetSupplier);\n+                return null;\n+            });\n+            threadPool.generic().execute(task);\n+            return task;\n+        }\n+\n+        public synchronized void assertAllEqual(int expectedCount) throws IOException {\n+            Set<String> primaryIds = getShardDocUIDs(primary);\n+            assertThat(primaryIds.size(), equalTo(expectedCount));\n+            for (IndexShard replica : replicas) {\n+                Set<String> replicaIds = getShardDocUIDs(replica);\n+                Set<String> temp = new HashSet<>(primaryIds);\n+                temp.removeAll(replicaIds);\n+                assertThat(replica.routingEntry() + \" is missing docs\", temp, empty());\n+                temp = new HashSet<>(replicaIds);\n+                temp.removeAll(primaryIds);\n+                assertThat(replica.routingEntry() + \" has extra docs\", temp, empty());\n+            }\n+        }\n+\n+        public synchronized void refresh(String source) {\n+            for (IndexShard shard : this) {\n+                shard.refresh(source);\n+            }\n+        }\n+\n+        public synchronized void flush() {\n+            final FlushRequest request = new FlushRequest();\n+            for (IndexShard shard : this) {\n+                shard.flush(request);\n+            }\n+        }\n+\n+        public synchronized List<ShardRouting> shardRoutings() {\n+            return StreamSupport.stream(this.spliterator(), false).map(IndexShard::routingEntry).collect(Collectors.toList());\n+        }\n+\n+        @Override\n+        public synchronized void close() throws Exception {\n+            if (closed == false) {\n+                closed = true;\n+                try {\n+                    final List<DocIdSeqNoAndSource> docsOnPrimary = getDocIdAndSeqNos(primary);\n+                    for (IndexShard replica : replicas) {\n+                        assertThat(replica.getMaxSeenAutoIdTimestamp(), equalTo(primary.getMaxSeenAutoIdTimestamp()));\n+                        assertThat(replica.getMaxSeqNoOfUpdatesOrDeletes(), greaterThanOrEqualTo(primary.getMaxSeqNoOfUpdatesOrDeletes()));\n+                        assertThat(getDocIdAndSeqNos(replica), equalTo(docsOnPrimary));\n+                    }\n+                } catch (AlreadyClosedException ignored) { }\n+                closeShards(this);\n+            } else {\n+                throw new AlreadyClosedException(\"too bad\");\n+            }\n+        }\n+\n+        @Override\n+        public Iterator<IndexShard> iterator() {\n+            return Iterators.concat(replicas.iterator(), Collections.singleton(primary).iterator());\n+        }\n+\n+        public synchronized IndexShard getPrimary() {\n+            return primary;\n+        }\n+\n+        public synchronized void reinitPrimaryShard() throws IOException {\n+            primary = reinitShard(primary);\n+            computeReplicationTargets();\n+        }\n+\n+        public void syncGlobalCheckpoint() {\n+            PlainActionFuture<ReplicationResponse> listener = new PlainActionFuture<>();\n+            try {\n+                new GlobalCheckpointSync(listener, this).execute();\n+                listener.get();\n+            } catch (Exception e) {\n+                throw new AssertionError(e);\n+            }\n+        }\n+\n+        private void updateAllocationIDsOnPrimary() throws IOException {\n+\n+            primary.updateShardState(primary.routingEntry(), primary.getPendingPrimaryTerm(), null,\n+                currentClusterStateVersion.incrementAndGet(),\n+                activeIds(), routingTable(Function.identity()));\n+        }\n+\n+        private synchronized void computeReplicationTargets() {\n+            this.replicationTargets = new ReplicationTargets(this.primary, new ArrayList<>(this.replicas));\n+        }\n+\n+        private ReplicationTargets getReplicationTargets() {\n+            return replicationTargets;\n+        }\n+\n+        protected void syncRetentionLeases(ShardId shardId, RetentionLeases leases, ActionListener<ReplicationResponse> listener) {\n+            new SyncRetentionLeases(new RetentionLeaseSyncAction.Request(shardId, leases), this,\n+                ActionListener.map(listener, r -> new ReplicationResponse())).execute();\n+        }\n+\n+        public synchronized RetentionLease addRetentionLease(String id, long retainingSequenceNumber, String source,\n+                                                ActionListener<ReplicationResponse> listener) {\n+            return getPrimary().addRetentionLease(id, retainingSequenceNumber, source, listener);\n+        }\n+\n+        public synchronized RetentionLease renewRetentionLease(String id, long retainingSequenceNumber, String source) {\n+            return getPrimary().renewRetentionLease(id, retainingSequenceNumber, source);\n+        }\n+\n+        public synchronized void removeRetentionLease(String id, ActionListener<ReplicationResponse> listener) {\n+            getPrimary().removeRetentionLease(id, listener);\n+        }\n+\n+        public void executeRetentionLeasesSyncRequestOnReplica(RetentionLeaseSyncAction.Request request, IndexShard replica) {\n+            final PlainActionFuture<Releasable> acquirePermitFuture = new PlainActionFuture<>();\n+            replica.acquireReplicaOperationPermit(getPrimary().getOperationPrimaryTerm(), getPrimary().getLastKnownGlobalCheckpoint(),\n+                getPrimary().getMaxSeqNoOfUpdatesOrDeletes(), acquirePermitFuture, ThreadPool.Names.SAME, request);\n+            try (Releasable ignored = acquirePermitFuture.actionGet()) {\n+                replica.updateRetentionLeasesOnReplica(request.getRetentionLeases());\n+                replica.persistRetentionLeases();\n+            } catch (Exception e) {\n+                throw new AssertionError(\"failed to execute retention lease request on replica [\" + replica.routingEntry() + \"]\", e);\n+            }\n+        }\n+    }\n+\n+    static final class ReplicationTargets {\n+        final IndexShard primary;\n+        final List<IndexShard> replicas;\n+\n+        ReplicationTargets(IndexShard primary, List<IndexShard> replicas) {\n+            this.primary = primary;\n+            this.replicas = replicas;\n+        }\n+\n+        /**\n+         * This does not modify the replication targets, but only adds a replica to the list.\n+         * If the targets is updated to include the given replica, a replication action would\n+         * be able to find this replica to execute write requests on it.\n+         */\n+        synchronized void addReplica(IndexShard replica) {\n+            replicas.add(replica);\n+        }\n+\n+        synchronized IndexShard findReplicaShard(ShardRouting replicaRouting) {\n+            for (IndexShard replica : replicas) {\n+                if (replica.routingEntry().isSameAllocation(replicaRouting)) {\n+                    return replica;\n+                }\n+            }\n+            throw new AssertionError(\"replica [\" + replicaRouting + \"] is not found; replicas[\" + replicas + \"] primary[\" + primary + \"]\");\n+        }\n+    }\n+\n+    protected abstract class ReplicationAction<Request extends ReplicationRequest<Request>,\n+        ReplicaRequest extends ReplicationRequest<ReplicaRequest>,\n+        Response extends ReplicationResponse> {\n+        private final Request request;\n+        private ActionListener<Response> listener;\n+        private final ReplicationTargets replicationTargets;\n+        private final String opType;\n+\n+        protected ReplicationAction(Request request, ActionListener<Response> listener, ReplicationGroup group, String opType) {\n+            this.request = request;\n+            this.listener = listener;\n+            this.replicationTargets = group.getReplicationTargets();\n+            this.opType = opType;\n+        }\n+\n+        public void execute() {\n+            try {\n+                new ReplicationOperation<>(request, new PrimaryRef(),\n+                    ActionListener.map(listener, result -> {\n+                        adaptResponse(result.finalResponse, getPrimaryShard());\n+                        return result.finalResponse;\n+                    }),\n+                    new ReplicasRef(), logger, opType, primaryTerm)\n+                    .execute();\n+            } catch (Exception e) {\n+                listener.onFailure(e);\n+            }\n+        }\n+\n+        // to be overridden by subclasses\n+        protected void adaptResponse(Response response, IndexShard indexShard) {\n+\n+        }\n+\n+        protected IndexShard getPrimaryShard() {\n+            return replicationTargets.primary;\n+        }\n+\n+        protected abstract void performOnPrimary(IndexShard primary, Request request, ActionListener<PrimaryResult> listener);\n+\n+        protected abstract void performOnReplica(ReplicaRequest request, IndexShard replica) throws Exception;\n+\n+        class PrimaryRef implements ReplicationOperation.Primary<Request, ReplicaRequest, PrimaryResult> {\n+\n+            @Override\n+            public ShardRouting routingEntry() {\n+                return getPrimaryShard().routingEntry();\n+            }\n+\n+            @Override\n+            public void failShard(String message, Exception exception) {\n+                throw new UnsupportedOperationException(\"failing a primary isn't supported. failure: \" + message, exception);\n+            }\n+\n+            @Override\n+            public void perform(Request request, ActionListener<PrimaryResult> listener) {\n+                performOnPrimary(getPrimaryShard(), request, listener);\n+            }\n+\n+            @Override\n+            public void updateLocalCheckpointForShard(String allocationId, long checkpoint) {\n+                getPrimaryShard().updateLocalCheckpointForShard(allocationId, checkpoint);\n+            }\n+\n+            @Override\n+            public void updateGlobalCheckpointForShard(String allocationId, long globalCheckpoint) {\n+                getPrimaryShard().updateGlobalCheckpointForShard(allocationId, globalCheckpoint);\n+            }\n+\n+            @Override\n+            public long localCheckpoint() {\n+                return getPrimaryShard().getLocalCheckpoint();\n+            }\n+\n+            @Override\n+            public long globalCheckpoint() {\n+                return getPrimaryShard().getLastSyncedGlobalCheckpoint();\n+            }\n+\n+            @Override\n+            public long computedGlobalCheckpoint() {\n+                return getPrimaryShard().getLastKnownGlobalCheckpoint();\n+            }\n+\n+            @Override\n+            public long maxSeqNoOfUpdatesOrDeletes() {\n+                return getPrimaryShard().getMaxSeqNoOfUpdatesOrDeletes();\n+            }\n+\n+            @Override\n+            public org.elasticsearch.index.shard.ReplicationGroup getReplicationGroup() {\n+                return getPrimaryShard().getReplicationGroup();\n+            }\n+\n+        }\n+\n+        class ReplicasRef implements ReplicationOperation.Replicas<ReplicaRequest> {\n+\n+            @Override\n+            public void performOn(\n+                final ShardRouting replicaRouting,\n+                final ReplicaRequest request,\n+                final long primaryTerm,\n+                final long globalCheckpoint,\n+                final long maxSeqNoOfUpdatesOrDeletes,\n+                final ActionListener<ReplicationOperation.ReplicaResponse> listener) {\n+                IndexShard replica = replicationTargets.findReplicaShard(replicaRouting);\n+                replica.acquireReplicaOperationPermit(\n+                    getPrimaryShard().getPendingPrimaryTerm(),\n+                    globalCheckpoint,\n+                    maxSeqNoOfUpdatesOrDeletes,\n+                    ActionListener.delegateFailure(listener, (delegatedListener, releasable) -> {\n+                        try {\n+                            performOnReplica(request, replica);\n+                            releasable.close();\n+                            delegatedListener.onResponse(new ReplicaResponse(replica.getLocalCheckpoint(),\n+                                replica.getLastKnownGlobalCheckpoint()));\n+                        } catch (final Exception e) {\n+                            Releasables.closeWhileHandlingException(releasable);\n+                            delegatedListener.onFailure(e);\n+                        }\n+                    }),\n+                    ThreadPool.Names.WRITE, request);\n+            }\n+\n+            @Override\n+            public void failShardIfNeeded(ShardRouting replica, long primaryTerm, String message, Exception exception,\n+                                          ActionListener<Void> listener) {\n+                throw new UnsupportedOperationException(\"failing shard \" + replica + \" isn't supported. failure: \" + message, exception);\n+            }\n+\n+            @Override\n+            public void markShardCopyAsStaleIfNeeded(ShardId shardId, String allocationId, long primaryTerm,\n+                                                     ActionListener<Void> listener) {\n+                throw new UnsupportedOperationException(\"can't mark \" + shardId  + \", aid [\" + allocationId + \"] as stale\");\n+            }\n+        }\n+\n+        protected class PrimaryResult implements ReplicationOperation.PrimaryResult<ReplicaRequest> {\n+            final ReplicaRequest replicaRequest;\n+            final Response finalResponse;\n+\n+            public PrimaryResult(ReplicaRequest replicaRequest, Response finalResponse) {\n+                this.replicaRequest = replicaRequest;\n+                this.finalResponse = finalResponse;\n+            }\n+\n+            @Override\n+            public ReplicaRequest replicaRequest() {\n+                return replicaRequest;\n+            }\n+\n+            @Override\n+            public void setShardInfo(ReplicationResponse.ShardInfo shardInfo) {\n+                finalResponse.setShardInfo(shardInfo);\n+            }\n+\n+            @Override\n+            public void runPostReplicationActions(ActionListener<Void> listener) {\n+                listener.onResponse(null);\n+            }\n+        }\n+\n+    }\n+\n+    class WriteReplicationAction extends ReplicationAction<ShardUpsertRequest, ShardUpsertRequest, ShardResponse> {\n+\n+        WriteReplicationAction(ShardUpsertRequest request, ActionListener<ShardResponse> listener, ReplicationGroup replicationGroup) {\n+            super(request, listener, replicationGroup, \"indexing\");\n+        }\n+\n+        @Override\n+        protected void performOnPrimary(IndexShard primary, ShardUpsertRequest request, ActionListener<PrimaryResult> listener) {\n+            executeShardBulkOnPrimary(primary, request,\n+                ActionListener.map(listener, result -> new PrimaryResult(result.replicaRequest(), result.finalResponseIfSuccessful)));\n+        }\n+\n+        @Override\n+        protected void performOnReplica(ShardUpsertRequest request, IndexShard replica) throws Exception {\n+            executeShardBulkOnReplica(request, replica, getPrimaryShard().getPendingPrimaryTerm(),\n+                getPrimaryShard().getLastKnownGlobalCheckpoint(), getPrimaryShard().getMaxSeqNoOfUpdatesOrDeletes());\n+        }\n+    }\n+\n+    private void executeShardBulkOnPrimary(\n+        IndexShard primary,\n+        ShardUpsertRequest request,\n+        ActionListener<TransportWriteAction.WritePrimaryResult<ShardUpsertRequest, ShardResponse>> listener) {\n+        final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>();\n+        primary.acquirePrimaryOperationPermit(permitAcquiredFuture, ThreadPool.Names.SAME, request);\n+        try (Releasable ignored = permitAcquiredFuture.actionGet()) {\n+            TransportWriteAction.WritePrimaryResult<ShardUpsertRequest, ShardResponse> result = TransportShardUpsertAction.processRequestItems(\n+                primary,\n+                request,\n+                new AtomicBoolean(false),\n+                logger,\n+                (update, shardId) -> {\n+                },\n+                null,\n+                new FromRawInsertSource(),\n+                null);\n+                listener.onResponse(result);\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+        }\n+    }\n+\n+    private ShardUpsertRequest executeReplicationRequestOnPrimary(IndexShard primary, ShardUpsertRequest request) throws Exception {\n+        final PlainActionFuture<ShardUpsertRequest> res = new PlainActionFuture<>();\n+        executeShardBulkOnPrimary(primary, request, ActionListener.map(res, TransportReplicationAction.PrimaryResult::replicaRequest));\n+        return res.get();\n+    }\n+\n+    private void executeShardBulkOnReplica(ShardUpsertRequest request, IndexShard replica, long operationPrimaryTerm,\n+                                           long globalCheckpointOnPrimary, long maxSeqNoOfUpdatesOrDeletes) throws Exception {\n+        final PlainActionFuture<Releasable> permitAcquiredFuture = new PlainActionFuture<>();\n+        replica.acquireReplicaOperationPermit(operationPrimaryTerm, globalCheckpointOnPrimary,\n+            maxSeqNoOfUpdatesOrDeletes, permitAcquiredFuture, ThreadPool.Names.SAME, request);\n+        final Translog.Location location;\n+        try (Releasable ignored = permitAcquiredFuture.actionGet()) {\n+            TransportWriteAction.WriteReplicaResult<ShardUpsertRequest> result = TransportShardUpsertAction.processOnReplica(\n+                replica,\n+                request,\n+                logger);\n+            location = result.location;\n+        }\n+        performPostWriteActions(replica, location);\n+    }\n+\n+    /**\n+     * indexes the given requests on the supplied primary, modifying it for replicas\n+     */\n+    public ShardUpsertRequest indexOnPrimary(ShardUpsertRequest request, IndexShard primary) throws Exception {\n+        return executeReplicationRequestOnPrimary(primary, request);\n+    }\n+\n+//    /**\n+//     * Executes the delete request on the primary, and modifies it for replicas.\n+//     */\n+//    ShardUpsertRequest deleteOnPrimary(ShardDeleteRequest request, IndexShard primary) throws Exception {\n+//        return executeReplicationRequestOnPrimary(primary, request);\n+//    }\n+\n+    /**\n+     * indexes the given requests on the supplied replica shard\n+     */\n+    public void indexOnReplica(ShardUpsertRequest request, ReplicationGroup group, IndexShard replica) throws Exception {\n+        indexOnReplica(request, group, replica, group.primary.getPendingPrimaryTerm());\n+    }\n+\n+    void indexOnReplica(ShardUpsertRequest request, ReplicationGroup group, IndexShard replica, long term) throws Exception {\n+        executeShardBulkOnReplica(request, replica, term,\n+            group.primary.getLastKnownGlobalCheckpoint(), group.primary.getMaxSeqNoOfUpdatesOrDeletes());\n+    }\n+\n+//    /**\n+//     * Executes the delete request on the given replica shard.\n+//     */\n+//    void deleteOnReplica(ShardDeleteRequest request, ReplicationGroup group, IndexShard replica) throws Exception {\n+//        executeShardBulkOnReplica(request, replica, group.primary.getPendingPrimaryTerm(),\n+//            group.primary.getLastKnownGlobalCheckpoint(), group.primary.getMaxSeqNoOfUpdatesOrDeletes());\n+//    }\n+\n+    class GlobalCheckpointSync extends ReplicationAction<\n+            GlobalCheckpointSyncAction.Request,\n+            GlobalCheckpointSyncAction.Request,\n+            ReplicationResponse> {\n+\n+        GlobalCheckpointSync(final ActionListener<ReplicationResponse> listener, final ReplicationGroup replicationGroup) {\n+            super(\n+                    new GlobalCheckpointSyncAction.Request(replicationGroup.getPrimary().shardId()),\n+                    listener,\n+                    replicationGroup,\n+                    \"global_checkpoint_sync\");\n+        }\n+\n+        @Override\n+        protected void performOnPrimary(IndexShard primary, GlobalCheckpointSyncAction.Request request,\n+                ActionListener<PrimaryResult> listener) {\n+            ActionListener.completeWith(listener, () -> {\n+                primary.sync();\n+                return new PrimaryResult(request, new ReplicationResponse());\n+            });\n+        }\n+\n+        @Override\n+        protected void performOnReplica(final GlobalCheckpointSyncAction.Request request, final IndexShard replica) throws IOException {\n+            replica.sync();\n+        }\n+    }\n+\n+    class ResyncAction extends ReplicationAction<ResyncReplicationRequest, ResyncReplicationRequest, ReplicationResponse> {\n+\n+        ResyncAction(ResyncReplicationRequest request, ActionListener<ReplicationResponse> listener, ReplicationGroup group) {\n+            super(request, listener, group, \"resync\");\n+        }\n+\n+        @Override\n+        protected void performOnPrimary(IndexShard primary, ResyncReplicationRequest request, ActionListener<PrimaryResult> listener) {\n+            ActionListener.completeWith(listener, () -> {\n+                final TransportWriteAction.WritePrimaryResult<ResyncReplicationRequest, ReplicationResponse> result =\n+                    executeResyncOnPrimary(primary, request);\n+                return new PrimaryResult(result.replicaRequest(), result.finalResponseIfSuccessful);\n+            });\n+        }\n+\n+        @Override\n+        protected void performOnReplica(ResyncReplicationRequest request, IndexShard replica) throws Exception {\n+            executeResyncOnReplica(replica, request, getPrimaryShard().getPendingPrimaryTerm(),\n+                getPrimaryShard().getLastKnownGlobalCheckpoint(), getPrimaryShard().getMaxSeqNoOfUpdatesOrDeletes());\n+        }\n+    }\n+\n+    private TransportWriteAction.WritePrimaryResult<ResyncReplicationRequest, ReplicationResponse> executeResyncOnPrimary(\n+        IndexShard primary, ResyncReplicationRequest request) {\n+        final TransportWriteAction.WritePrimaryResult<ResyncReplicationRequest, ReplicationResponse> result =\n+            new TransportWriteAction.WritePrimaryResult<>(TransportResyncReplicationAction.performOnPrimary(request, primary),\n+                new ReplicationResponse(), null, null, primary);\n+        performPostWriteActions(primary, result.location);\n+        return result;\n+    }\n+\n+    private void executeResyncOnReplica(IndexShard replica, ResyncReplicationRequest request, long operationPrimaryTerm,\n+                                        long globalCheckpointOnPrimary, long maxSeqNoOfUpdatesOrDeletes) throws Exception {\n+        final Translog.Location location;\n+        final PlainActionFuture<Releasable> acquirePermitFuture = new PlainActionFuture<>();\n+        replica.acquireReplicaOperationPermit(operationPrimaryTerm, globalCheckpointOnPrimary,\n+            maxSeqNoOfUpdatesOrDeletes, acquirePermitFuture, ThreadPool.Names.SAME, request);\n+        try (Releasable ignored = acquirePermitFuture.actionGet()) {\n+            location = TransportResyncReplicationAction.performOnReplica(request, replica);\n+        }\n+        performPostWriteActions(replica, location);\n+    }\n+\n+    class SyncRetentionLeases extends ReplicationAction<\n+        RetentionLeaseSyncAction.Request, RetentionLeaseSyncAction.Request, ReplicationResponse> {\n+\n+        SyncRetentionLeases(RetentionLeaseSyncAction.Request request, ReplicationGroup group,\n+                            ActionListener<ReplicationResponse> listener)  {\n+            super(request, listener, group, \"sync-retention-leases\");\n+        }\n+\n+        @Override\n+        protected void performOnPrimary(IndexShard primary, RetentionLeaseSyncAction.Request request,\n+                ActionListener<PrimaryResult> listener) {\n+            ActionListener.completeWith(listener, () -> {\n+                primary.persistRetentionLeases();\n+                return new PrimaryResult(request, new ReplicationResponse());\n+            });\n+        }\n+\n+        @Override\n+        protected void performOnReplica(RetentionLeaseSyncAction.Request request, IndexShard replica) throws Exception {\n+            replica.updateRetentionLeasesOnReplica(request.getRetentionLeases());\n+            replica.persistRetentionLeases();\n+        }\n+    }\n+\n+    public static void performPostWriteActions(final IndexShard indexShard, @Nullable final Translog.Location location) {\n+        final CountDownLatch latch = new CountDownLatch(1);\n+        TransportWriteAction.RespondingWriteResult writerResult = new TransportWriteAction.RespondingWriteResult() {\n+            @Override\n+            public void onSuccess(boolean forcedRefresh) {\n+                latch.countDown();\n+            }\n+\n+            @Override\n+            public void onFailure(Exception ex) {\n+                throw new AssertionError(ex);\n+            }\n+        };\n+        new TransportWriteAction.AsyncAfterWriteAction(indexShard, location, writerResult).run();\n+        try {\n+            latch.await();\n+        } catch (InterruptedException e) {\n+            throw new AssertionError(e);\n+        }\n+    }\n+\n+}"
  },
  {
    "sha": "b4c40af128e1d4ba16825542073acd41644f39c6",
    "filename": "server/src/test/java/org/elasticsearch/index/replication/RecoveryDuringReplicationTests.java",
    "status": "added",
    "additions": 337,
    "deletions": 0,
    "changes": 337,
    "blob_url": "https://github.com/crate/crate/blob/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/test/java/org/elasticsearch/index/replication/RecoveryDuringReplicationTests.java",
    "raw_url": "https://github.com/crate/crate/raw/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/test/java/org/elasticsearch/index/replication/RecoveryDuringReplicationTests.java",
    "contents_url": "https://api.github.com/repos/crate/crate/contents/server/src/test/java/org/elasticsearch/index/replication/RecoveryDuringReplicationTests.java?ref=711161de73ff20b9d7af3ac1df8ee5d310aef77d",
    "patch": "@@ -0,0 +1,337 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.index.replication;\n+\n+import io.crate.common.io.IOUtils;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexableField;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.flush.FlushRequest;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.bytes.BytesArray;\n+import org.elasticsearch.common.lucene.uid.Versions;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.xcontent.XContentType;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.VersionType;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.engine.EngineConfig;\n+import org.elasticsearch.index.engine.EngineFactory;\n+import org.elasticsearch.index.engine.InternalEngineTests;\n+import org.elasticsearch.index.mapper.SourceToParse;\n+import org.elasticsearch.index.seqno.RetentionLeases;\n+import org.elasticsearch.index.seqno.SequenceNumbers;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.store.Store;\n+import org.elasticsearch.index.translog.Translog;\n+import org.elasticsearch.indices.recovery.PeerRecoveryTargetService;\n+import org.elasticsearch.indices.recovery.RecoveryState;\n+import org.elasticsearch.indices.recovery.RecoveryTarget;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.EnumSet;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.is;\n+\n+public class RecoveryDuringReplicationTests extends ESIndexLevelReplicationTestCase {\n+\n+    public void testIndexingDuringFileRecovery() throws Exception {\n+        try (ReplicationGroup shards = createGroup(randomInt(1))) {\n+            shards.startAll();\n+            int docs = shards.indexDocs(randomInt(50));\n+            shards.flush();\n+            IndexShard replica = shards.addReplica();\n+            final CountDownLatch recoveryBlocked = new CountDownLatch(1);\n+            final CountDownLatch releaseRecovery = new CountDownLatch(1);\n+            final RecoveryState.Stage blockOnStage = randomFrom(BlockingTarget.SUPPORTED_STAGES);\n+            final Future<Void> recoveryFuture = shards.asyncRecoverReplica(replica, (indexShard, node) ->\n+                new BlockingTarget(blockOnStage, recoveryBlocked, releaseRecovery, indexShard, node, recoveryListener, logger));\n+\n+            recoveryBlocked.await();\n+            docs += shards.indexDocs(randomInt(20));\n+            releaseRecovery.countDown();\n+            recoveryFuture.get();\n+\n+            shards.assertAllEqual(docs);\n+        }\n+    }\n+\n+    public void testRecoveryOfDisconnectedReplica() throws Exception {\n+        Settings settings = Settings.builder().put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), false).build();\n+        try (ReplicationGroup shards = createGroup(1, settings)) {\n+            shards.startAll();\n+            int docs = shards.indexDocs(randomInt(50));\n+            shards.flush();\n+            final IndexShard originalReplica = shards.getReplicas().get(0);\n+            for (int i = 0; i < randomInt(2); i++) {\n+                final int indexedDocs = shards.indexDocs(randomInt(5));\n+                docs += indexedDocs;\n+\n+                final boolean flush = randomBoolean();\n+                if (flush) {\n+                    originalReplica.flush(new FlushRequest());\n+                }\n+            }\n+\n+            // simulate a background global checkpoint sync at which point we expect the global checkpoint to advance on the replicas\n+            shards.syncGlobalCheckpoint();\n+            long globalCheckpointOnReplica = originalReplica.getLastSyncedGlobalCheckpoint();\n+            Optional<SequenceNumbers.CommitInfo> safeCommitOnReplica =\n+                originalReplica.store().findSafeIndexCommit(globalCheckpointOnReplica);\n+            assertTrue(safeCommitOnReplica.isPresent());\n+            shards.removeReplica(originalReplica);\n+\n+            final int missingOnReplica = shards.indexDocs(randomInt(5));\n+            docs += missingOnReplica;\n+\n+            final boolean translogTrimmed;\n+            if (randomBoolean()) {\n+                shards.flush();\n+                translogTrimmed = randomBoolean();\n+                if (translogTrimmed) {\n+                    final Translog translog = getTranslog(shards.getPrimary());\n+                    translog.getDeletionPolicy().setRetentionAgeInMillis(0);\n+                    translog.trimUnreferencedReaders();\n+                }\n+            } else {\n+                translogTrimmed = false;\n+            }\n+            originalReplica.close(\"disconnected\", false);\n+            IOUtils.close(originalReplica.store());\n+            final IndexShard recoveredReplica =\n+                shards.addReplicaWithExistingPath(originalReplica.shardPath(), originalReplica.routingEntry().currentNodeId());\n+            shards.recoverReplica(recoveredReplica);\n+            if (translogTrimmed && missingOnReplica > 0) {\n+                // replica has something to catch up with, but since we trimmed the primary translog, we should fall back to full recovery\n+                assertThat(recoveredReplica.recoveryState().getIndex().fileDetails().isEmpty(), is(false));\n+            } else {\n+                assertThat(recoveredReplica.recoveryState().getIndex().fileDetails().entrySet(), is(empty()));\n+                assertThat(recoveredReplica.recoveryState().getTranslog().recoveredOperations(),\n+                           equalTo(Math.toIntExact(docs - 1 - safeCommitOnReplica.get().localCheckpoint)));\n+                assertThat(recoveredReplica.recoveryState().getTranslog().totalLocal(),\n+                           equalTo(Math.toIntExact(globalCheckpointOnReplica - safeCommitOnReplica.get().localCheckpoint)));\n+            }\n+\n+            docs += shards.indexDocs(randomInt(5));\n+\n+            shards.assertAllEqual(docs);\n+        }\n+    }\n+\n+    /*\n+     * Simulate a scenario with two replicas where one of the replicas receives an extra document, the other replica is promoted on primary\n+     * failure, the receiving replica misses the primary/replica re-sync and then recovers from the primary. We expect that a\n+     * sequence-number based recovery is performed and the extra document does not remain after recovery.\n+     */\n+    public void testRecoveryToReplicaThatReceivedExtraDocument() throws Exception {\n+        try (ReplicationGroup shards = createGroup(2)) {\n+            shards.startAll();\n+            final int docs = randomIntBetween(0, 16);\n+            shards.indexDocs(docs);\n+\n+            shards.flush();\n+            shards.syncGlobalCheckpoint();\n+\n+            final IndexShard oldPrimary = shards.getPrimary();\n+            final IndexShard promotedReplica = shards.getReplicas().get(0);\n+            final IndexShard remainingReplica = shards.getReplicas().get(1);\n+            // slip the extra document into the replica\n+            remainingReplica.applyIndexOperationOnReplica(\n+                remainingReplica.getLocalCheckpoint() + 1,\n+                remainingReplica.getOperationPrimaryTerm(),\n+                1,\n+                randomNonNegativeLong(),\n+                false,\n+                new SourceToParse(\"index\", \"replica\", new BytesArray(\"{}\"), XContentType.JSON));\n+            shards.promoteReplicaToPrimary(promotedReplica).get();\n+            oldPrimary.close(\"demoted\", randomBoolean());\n+            oldPrimary.store().close();\n+            shards.removeReplica(remainingReplica);\n+            remainingReplica.close(\"disconnected\", false);\n+            remainingReplica.store().close();\n+            // randomly introduce a conflicting document\n+            final boolean extra = randomBoolean();\n+            if (extra) {\n+                promotedReplica.applyIndexOperationOnPrimary(\n+                    Versions.MATCH_ANY,\n+                    VersionType.INTERNAL,\n+                    new SourceToParse(\"index\", \"primary\", new BytesArray(\"{}\"), XContentType.JSON),\n+                    SequenceNumbers.UNASSIGNED_SEQ_NO, 0, -1L,\n+                    false);\n+            }\n+            final IndexShard recoveredReplica =\n+                shards.addReplicaWithExistingPath(remainingReplica.shardPath(), remainingReplica.routingEntry().currentNodeId());\n+            shards.recoverReplica(recoveredReplica);\n+\n+            assertThat(recoveredReplica.recoveryState().getIndex().fileDetails().isEmpty(), is(true));\n+            assertThat(recoveredReplica.recoveryState().getTranslog().recoveredOperations(), equalTo(extra ? 1 : 0));\n+\n+            shards.assertAllEqual(docs + (extra ? 1 : 0));\n+        }\n+    }\n+\n+\n+    public static class BlockingTarget extends RecoveryTarget {\n+\n+        private final CountDownLatch recoveryBlocked;\n+        private final CountDownLatch releaseRecovery;\n+        private final RecoveryState.Stage stageToBlock;\n+        static final EnumSet<RecoveryState.Stage> SUPPORTED_STAGES =\n+            EnumSet.of(RecoveryState.Stage.INDEX, RecoveryState.Stage.TRANSLOG, RecoveryState.Stage.FINALIZE);\n+        private final Logger logger;\n+\n+        public BlockingTarget(RecoveryState.Stage stageToBlock, CountDownLatch recoveryBlocked, CountDownLatch releaseRecovery,\n+                              IndexShard shard, DiscoveryNode sourceNode, PeerRecoveryTargetService.RecoveryListener listener,\n+                              Logger logger) {\n+            super(shard, sourceNode, listener);\n+            this.recoveryBlocked = recoveryBlocked;\n+            this.releaseRecovery = releaseRecovery;\n+            this.stageToBlock = stageToBlock;\n+            this.logger = logger;\n+            if (SUPPORTED_STAGES.contains(stageToBlock) == false) {\n+                throw new UnsupportedOperationException(stageToBlock + \" is not supported\");\n+            }\n+        }\n+\n+        private boolean hasBlocked() {\n+            return recoveryBlocked.getCount() == 0;\n+        }\n+\n+        private void blockIfNeeded(RecoveryState.Stage currentStage) {\n+            if (currentStage == stageToBlock) {\n+                logger.info(\"--> blocking recovery on stage [{}]\", currentStage);\n+                recoveryBlocked.countDown();\n+                try {\n+                    releaseRecovery.await();\n+                    logger.info(\"--> recovery continues from stage [{}]\", currentStage);\n+                } catch (InterruptedException e) {\n+                    throw new RuntimeException(\"blockage released\");\n+                }\n+            }\n+        }\n+\n+        @Override\n+        public void indexTranslogOperations(\n+            final List<Translog.Operation> operations,\n+            final int totalTranslogOps,\n+            final long maxAutoIdTimestamp,\n+            final long maxSeqNoOfUpdates,\n+            final RetentionLeases retentionLeases,\n+            final long mappingVersion,\n+            final ActionListener<Long> listener) {\n+            if (hasBlocked() == false) {\n+                blockIfNeeded(RecoveryState.Stage.TRANSLOG);\n+            }\n+            super.indexTranslogOperations(\n+                operations, totalTranslogOps, maxAutoIdTimestamp, maxSeqNoOfUpdates, retentionLeases, mappingVersion, listener);\n+        }\n+\n+        @Override\n+        public void cleanFiles(int totalTranslogOps, long globalCheckpoint, Store.MetadataSnapshot sourceMetadata,\n+                               ActionListener<Void> listener) {\n+            blockIfNeeded(RecoveryState.Stage.INDEX);\n+            super.cleanFiles(totalTranslogOps, globalCheckpoint, sourceMetadata, listener);\n+        }\n+\n+        @Override\n+        public void finalizeRecovery(long globalCheckpoint, long trimAboveSeqNo, ActionListener<Void> listener) {\n+            if (hasBlocked() == false) {\n+                // it maybe that not ops have been transferred, block now\n+                blockIfNeeded(RecoveryState.Stage.TRANSLOG);\n+            }\n+            blockIfNeeded(RecoveryState.Stage.FINALIZE);\n+            super.finalizeRecovery(globalCheckpoint, trimAboveSeqNo, listener);\n+        }\n+\n+    }\n+\n+    static class BlockingEngineFactory implements EngineFactory, AutoCloseable {\n+\n+        private final List<CountDownLatch> blocks = new ArrayList<>();\n+\n+        private final AtomicReference<CountDownLatch> blockReference = new AtomicReference<>();\n+        private final AtomicReference<CountDownLatch> blockedIndexers = new AtomicReference<>();\n+\n+        public synchronized void latchIndexers(int count) {\n+            final CountDownLatch block = new CountDownLatch(1);\n+            blocks.add(block);\n+            blockedIndexers.set(new CountDownLatch(count));\n+            assert blockReference.compareAndSet(null, block);\n+        }\n+\n+        public void awaitIndexersLatch() throws InterruptedException {\n+            blockedIndexers.get().await();\n+        }\n+\n+        public synchronized void allowIndexing() {\n+            final CountDownLatch previous = blockReference.getAndSet(null);\n+            assert previous == null || blocks.contains(previous);\n+        }\n+\n+        public synchronized void releaseLatchedIndexers() {\n+            allowIndexing();\n+            blocks.forEach(CountDownLatch::countDown);\n+            blocks.clear();\n+        }\n+\n+        @Override\n+        public Engine newReadWriteEngine(final EngineConfig config) {\n+            return InternalEngineTests.createInternalEngine(\n+                (directory, writerConfig) ->\n+                    new IndexWriter(directory, writerConfig) {\n+                        @Override\n+                        public long addDocument(final Iterable<? extends IndexableField> doc) throws IOException {\n+                            final CountDownLatch block = blockReference.get();\n+                            if (block != null) {\n+                                final CountDownLatch latch = blockedIndexers.get();\n+                                if (latch != null) {\n+                                    latch.countDown();\n+                                }\n+                                try {\n+                                    block.await();\n+                                } catch (InterruptedException e) {\n+                                    throw new AssertionError(e);\n+                                }\n+                            }\n+                            return super.addDocument(doc);\n+                        }\n+                    },\n+                null,\n+                null,\n+                config);\n+        }\n+\n+        @Override\n+        public void close() throws Exception {\n+            releaseLatchedIndexers();\n+        }\n+\n+    }\n+\n+}"
  },
  {
    "sha": "2c95e2ecf7f07a14cdd24b1f235d8fc4f1259451",
    "filename": "server/src/test/java/org/elasticsearch/index/replication/RetentionLeasesReplicationTests.java",
    "status": "added",
    "additions": 187,
    "deletions": 0,
    "changes": 187,
    "blob_url": "https://github.com/crate/crate/blob/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/test/java/org/elasticsearch/index/replication/RetentionLeasesReplicationTests.java",
    "raw_url": "https://github.com/crate/crate/raw/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/test/java/org/elasticsearch/index/replication/RetentionLeasesReplicationTests.java",
    "contents_url": "https://api.github.com/repos/crate/crate/contents/server/src/test/java/org/elasticsearch/index/replication/RetentionLeasesReplicationTests.java?ref=711161de73ff20b9d7af3ac1df8ee5d310aef77d",
    "patch": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to Crate under one or more contributor license agreements.\n+ * See the NOTICE file distributed with this work for additional\n+ * information regarding copyright ownership.  Crate licenses this file\n+ * to you under the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.  You may\n+ * obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n+ * implied.  See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ *\n+ * However, if you have executed another commercial license agreement\n+ * with Crate these terms will supersede the license and you may use the\n+ * software solely pursuant to the terms of the relevant commercial\n+ * agreement.\n+ */\n+\n+package org.elasticsearch.index.replication;\n+\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.replication.ReplicationResponse;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.common.Randomness;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.seqno.RetentionLease;\n+import org.elasticsearch.index.seqno.RetentionLeaseSyncAction;\n+import org.elasticsearch.index.seqno.RetentionLeaseUtils;\n+import org.elasticsearch.index.seqno.RetentionLeases;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.test.VersionUtils;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.CountDownLatch;\n+\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+\n+public class RetentionLeasesReplicationTests extends ESIndexLevelReplicationTestCase {\n+\n+    public void testSimpleSyncRetentionLeases() throws Exception {\n+        Settings settings = Settings.builder().put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true).build();\n+        try (ReplicationGroup group = createGroup(between(0, 2), settings)) {\n+            group.startAll();\n+            List<RetentionLease> leases = new ArrayList<>();\n+            int iterations = between(1, 100);\n+            CountDownLatch latch = new CountDownLatch(iterations);\n+            for (int i = 0; i < iterations; i++) {\n+                if (leases.isEmpty() == false && rarely()) {\n+                    RetentionLease leaseToRemove = randomFrom(leases);\n+                    leases.remove(leaseToRemove);\n+                    group.removeRetentionLease(leaseToRemove.id(), ActionListener.wrap(latch::countDown));\n+                } else {\n+                    RetentionLease newLease = group.addRetentionLease(Integer.toString(i), randomNonNegativeLong(), \"test-\" + i,\n+                        ActionListener.wrap(latch::countDown));\n+                    leases.add(newLease);\n+                }\n+            }\n+            RetentionLeases leasesOnPrimary = group.getPrimary().getRetentionLeases();\n+            assertThat(leasesOnPrimary.version(), equalTo(iterations + group.getReplicas().size() + 1L));\n+            assertThat(leasesOnPrimary.primaryTerm(), equalTo(group.getPrimary().getOperationPrimaryTerm()));\n+            assertThat(RetentionLeaseUtils.toMapExcludingPeerRecoveryRetentionLeases(leasesOnPrimary).values(),\n+                containsInAnyOrder(leases.toArray(new RetentionLease[0])));\n+            latch.await();\n+            for (IndexShard replica : group.getReplicas()) {\n+                assertThat(replica.getRetentionLeases(), equalTo(leasesOnPrimary));\n+            }\n+        }\n+    }\n+\n+    public void testOutOfOrderRetentionLeasesRequests() throws Exception {\n+        Settings settings = Settings.builder().put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true).build();\n+        int numberOfReplicas = between(1, 2);\n+        IndexMetadata indexMetadata = buildIndexMetadata(numberOfReplicas, settings);\n+        try (ReplicationGroup group = new ReplicationGroup(indexMetadata) {\n+            @Override\n+            protected void syncRetentionLeases(ShardId shardId, RetentionLeases leases, ActionListener<ReplicationResponse> listener) {\n+                listener.onResponse(new SyncRetentionLeasesResponse(new RetentionLeaseSyncAction.Request(shardId, leases)));\n+            }\n+        }) {\n+            group.startAll();\n+            int numLeases = between(1, 10);\n+            List<RetentionLeaseSyncAction.Request> requests = new ArrayList<>();\n+            for (int i = 0; i < numLeases; i++) {\n+                PlainActionFuture<ReplicationResponse> future = new PlainActionFuture<>();\n+                group.addRetentionLease(Integer.toString(i), randomNonNegativeLong(), \"test-\" + i, future);\n+                requests.add(((SyncRetentionLeasesResponse) future.actionGet()).syncRequest);\n+            }\n+            RetentionLeases leasesOnPrimary = group.getPrimary().getRetentionLeases();\n+            for (IndexShard replica : group.getReplicas()) {\n+                Randomness.shuffle(requests);\n+                requests.forEach(request -> group.executeRetentionLeasesSyncRequestOnReplica(request, replica));\n+                assertThat(replica.getRetentionLeases(), equalTo(leasesOnPrimary));\n+            }\n+        }\n+    }\n+\n+    public void testSyncRetentionLeasesWithPrimaryPromotion() throws Exception {\n+        Settings settings = Settings.builder().put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true).build();\n+        int numberOfReplicas = between(2, 4);\n+        IndexMetadata indexMetadata = buildIndexMetadata(numberOfReplicas, settings);\n+        try (ReplicationGroup group = new ReplicationGroup(indexMetadata) {\n+            @Override\n+            protected void syncRetentionLeases(ShardId shardId, RetentionLeases leases, ActionListener<ReplicationResponse> listener) {\n+                listener.onResponse(new SyncRetentionLeasesResponse(new RetentionLeaseSyncAction.Request(shardId, leases)));\n+            }\n+        }) {\n+            group.startAll();\n+            for (IndexShard replica : group.getReplicas()) {\n+                replica.updateRetentionLeasesOnReplica(group.getPrimary().getRetentionLeases());\n+            }\n+            int numLeases = between(1, 100);\n+            IndexShard newPrimary = randomFrom(group.getReplicas());\n+            RetentionLeases latestRetentionLeasesOnNewPrimary = newPrimary.getRetentionLeases();\n+            for (int i = 0; i < numLeases; i++) {\n+                PlainActionFuture<ReplicationResponse> addLeaseFuture = new PlainActionFuture<>();\n+                group.addRetentionLease(Integer.toString(i), randomNonNegativeLong(), \"test-\" + i, addLeaseFuture);\n+                RetentionLeaseSyncAction.Request request = ((SyncRetentionLeasesResponse) addLeaseFuture.actionGet()).syncRequest;\n+                for (IndexShard replica : randomSubsetOf(group.getReplicas())) {\n+                    group.executeRetentionLeasesSyncRequestOnReplica(request, replica);\n+                    if (newPrimary == replica) {\n+                        latestRetentionLeasesOnNewPrimary = request.getRetentionLeases();\n+                    }\n+                }\n+            }\n+            group.promoteReplicaToPrimary(newPrimary).get();\n+            // we need to make changes to retention leases to sync it to replicas\n+            // since we don't sync retention leases when promoting a new primary.\n+            PlainActionFuture<ReplicationResponse> newLeaseFuture = new PlainActionFuture<>();\n+            group.addRetentionLease(\"new-lease-after-promotion\", randomNonNegativeLong(), \"test\", newLeaseFuture);\n+            RetentionLeases leasesOnPrimary = group.getPrimary().getRetentionLeases();\n+            assertThat(leasesOnPrimary.primaryTerm(), equalTo(group.getPrimary().getOperationPrimaryTerm()));\n+            assertThat(leasesOnPrimary.version(), equalTo(latestRetentionLeasesOnNewPrimary.version() + 1));\n+            assertThat(leasesOnPrimary.leases(), hasSize(latestRetentionLeasesOnNewPrimary.leases().size() + 1));\n+            RetentionLeaseSyncAction.Request request = ((SyncRetentionLeasesResponse) newLeaseFuture.actionGet()).syncRequest;\n+            for (IndexShard replica : group.getReplicas()) {\n+                group.executeRetentionLeasesSyncRequestOnReplica(request, replica);\n+            }\n+            for (IndexShard replica : group.getReplicas()) {\n+                assertThat(replica.getRetentionLeases(), equalTo(leasesOnPrimary));\n+            }\n+        }\n+    }\n+\n+    public void testTurnOffTranslogRetentionAfterAllShardStarted() throws Exception {\n+        final Settings.Builder settings = Settings.builder().put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true);\n+        if (randomBoolean()) {\n+//            settings.put(IndexMetadata.SETTING_VERSION_CREATED, VersionUtils.randomVersion(random()));\n+            settings.put(IndexMetadata.SETTING_VERSION_CREATED, Version.CURRENT);\n+        }\n+        try (ReplicationGroup group = createGroup(between(1, 2), settings.build())) {\n+            group.startAll();\n+            group.indexDocs(randomIntBetween(1, 10));\n+            for (IndexShard shard : group) {\n+                shard.updateShardState(shard.routingEntry(), shard.getOperationPrimaryTerm(), null, 1L,\n+                    group.getPrimary().getReplicationGroup().getInSyncAllocationIds(),\n+                    group.getPrimary().getReplicationGroup().getRoutingTable());\n+            }\n+            group.syncGlobalCheckpoint();\n+            group.flush();\n+            assertBusy(() -> {\n+                // we turn off the translog retention policy using the generic threadPool\n+                for (IndexShard shard : group) {\n+                    int actual = shard.translogStats().estimatedNumberOfOperations();\n+                    assertThat(actual, equalTo(0));\n+                }\n+            });\n+        }\n+    }\n+\n+    static final class SyncRetentionLeasesResponse extends ReplicationResponse {\n+        final RetentionLeaseSyncAction.Request syncRequest;\n+        SyncRetentionLeasesResponse(RetentionLeaseSyncAction.Request syncRequest) {\n+            this.syncRequest = syncRequest;\n+        }\n+    }\n+}"
  },
  {
    "sha": "f587951215abbd2e18a95f6f72509e7ecaeca8a8",
    "filename": "server/src/test/java/org/elasticsearch/index/seqno/ReplicationTrackerTests.java",
    "status": "added",
    "additions": 1137,
    "deletions": 0,
    "changes": 1137,
    "blob_url": "https://github.com/crate/crate/blob/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/test/java/org/elasticsearch/index/seqno/ReplicationTrackerTests.java",
    "raw_url": "https://github.com/crate/crate/raw/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/test/java/org/elasticsearch/index/seqno/ReplicationTrackerTests.java",
    "contents_url": "https://api.github.com/repos/crate/crate/contents/server/src/test/java/org/elasticsearch/index/seqno/ReplicationTrackerTests.java?ref=711161de73ff20b9d7af3ac1df8ee5d310aef77d",
    "patch": "@@ -0,0 +1,1137 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.index.seqno;\n+\n+import io.crate.common.collections.Tuple;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.support.replication.ReplicationResponse;\n+import org.elasticsearch.cluster.routing.AllocationId;\n+import org.elasticsearch.cluster.routing.IndexShardRoutingTable;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.ShardRoutingState;\n+import org.elasticsearch.cluster.routing.TestShardRouting;\n+import org.elasticsearch.common.Randomness;\n+import org.elasticsearch.common.io.stream.BytesStreamOutput;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.util.set.Sets;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.test.IndexSettingsModule;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.BrokenBarrierException;\n+import java.util.concurrent.CyclicBarrier;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.BiConsumer;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.function.LongConsumer;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.Stream;\n+\n+import static java.util.Collections.emptySet;\n+import static org.elasticsearch.index.seqno.SequenceNumbers.NO_OPS_PERFORMED;\n+import static org.elasticsearch.index.seqno.SequenceNumbers.UNASSIGNED_SEQ_NO;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.greaterThanOrEqualTo;\n+import static org.hamcrest.Matchers.hasItem;\n+import static org.hamcrest.Matchers.lessThanOrEqualTo;\n+import static org.hamcrest.Matchers.not;\n+\n+public class ReplicationTrackerTests extends ReplicationTrackerTestCase {\n+\n+    public void testEmptyShards() {\n+        final ReplicationTracker tracker = newTracker(AllocationId.newInitializing());\n+        assertThat(tracker.getGlobalCheckpoint(), equalTo(UNASSIGNED_SEQ_NO));\n+    }\n+\n+    private Map<AllocationId, Long> randomAllocationsWithLocalCheckpoints(int min, int max) {\n+        Map<AllocationId, Long> allocations = new HashMap<>();\n+        for (int i = randomIntBetween(min, max); i > 0; i--) {\n+            allocations.put(AllocationId.newInitializing(), (long) randomInt(1000));\n+        }\n+        return allocations;\n+    }\n+\n+    private static Set<String> ids(Set<AllocationId> allocationIds) {\n+        return allocationIds.stream().map(AllocationId::getId).collect(Collectors.toSet());\n+    }\n+\n+    private void updateLocalCheckpoint(final ReplicationTracker tracker, final String allocationId, final long localCheckpoint) {\n+        tracker.updateLocalCheckpoint(allocationId, localCheckpoint);\n+        assertThat(updatedGlobalCheckpoint.get(), equalTo(tracker.getGlobalCheckpoint()));\n+    }\n+\n+    public void testGlobalCheckpointUpdate() {\n+        final long initialClusterStateVersion = randomNonNegativeLong();\n+        Map<AllocationId, Long> allocations = new HashMap<>();\n+        Map<AllocationId, Long> activeWithCheckpoints = randomAllocationsWithLocalCheckpoints(1, 5);\n+        Set<AllocationId> active = new HashSet<>(activeWithCheckpoints.keySet());\n+        allocations.putAll(activeWithCheckpoints);\n+        Map<AllocationId, Long> initializingWithCheckpoints = randomAllocationsWithLocalCheckpoints(0, 5);\n+        Set<AllocationId> initializing = new HashSet<>(initializingWithCheckpoints.keySet());\n+        allocations.putAll(initializingWithCheckpoints);\n+        assertThat(allocations.size(), equalTo(active.size() + initializing.size()));\n+\n+        // note: allocations can never be empty in practice as we always have at least one primary shard active/in sync\n+        // it is however nice not to assume this on this level and check we do the right thing.\n+        final long minLocalCheckpoint = allocations.values().stream().min(Long::compare).orElse(UNASSIGNED_SEQ_NO);\n+\n+\n+        final AllocationId primaryId = active.iterator().next();\n+        final ReplicationTracker tracker = newTracker(primaryId);\n+        assertThat(tracker.getGlobalCheckpoint(), equalTo(UNASSIGNED_SEQ_NO));\n+\n+        logger.info(\"--> using allocations\");\n+        allocations.keySet().forEach(aId -> {\n+            final String type;\n+            if (active.contains(aId)) {\n+                type = \"active\";\n+            } else if (initializing.contains(aId)) {\n+                type = \"init\";\n+            } else {\n+                throw new IllegalStateException(aId + \" not found in any map\");\n+            }\n+            logger.info(\"  - [{}], local checkpoint [{}], [{}]\", aId, allocations.get(aId), type);\n+        });\n+\n+        tracker.updateFromMaster(initialClusterStateVersion, ids(active), routingTable(initializing, primaryId));\n+        tracker.activatePrimaryMode(NO_OPS_PERFORMED);\n+        assertThat(tracker.getReplicationGroup().getReplicationTargets().size(), equalTo(1));\n+        initializing.forEach(aId -> markAsTrackingAndInSyncQuietly(tracker, aId.getId(), NO_OPS_PERFORMED));\n+        assertThat(tracker.getReplicationGroup().getReplicationTargets().size(), equalTo(1 + initializing.size()));\n+        allocations.keySet().forEach(aId -> updateLocalCheckpoint(tracker, aId.getId(), allocations.get(aId)));\n+\n+        assertThat(tracker.getGlobalCheckpoint(), equalTo(minLocalCheckpoint));\n+\n+        // increment checkpoints\n+        active.forEach(aId -> allocations.put(aId, allocations.get(aId) + 1 + randomInt(4)));\n+        initializing.forEach(aId -> allocations.put(aId, allocations.get(aId) + 1 + randomInt(4)));\n+        allocations.keySet().forEach(aId -> updateLocalCheckpoint(tracker, aId.getId(), allocations.get(aId)));\n+\n+        final long minLocalCheckpointAfterUpdates =\n+            allocations.entrySet().stream().map(Map.Entry::getValue).min(Long::compareTo).orElse(UNASSIGNED_SEQ_NO);\n+\n+        // now insert an unknown active/insync id , the checkpoint shouldn't change but a refresh should be requested.\n+        final AllocationId extraId = AllocationId.newInitializing();\n+\n+        // first check that adding it without the master blessing doesn't change anything.\n+        updateLocalCheckpoint(tracker, extraId.getId(), minLocalCheckpointAfterUpdates + 1 + randomInt(4));\n+        assertNull(tracker.checkpoints.get(extraId.getId()));\n+        expectThrows(IllegalStateException.class, () -> tracker.initiateTracking(extraId.getId()));\n+\n+        Set<AllocationId> newInitializing = new HashSet<>(initializing);\n+        newInitializing.add(extraId);\n+        tracker.updateFromMaster(initialClusterStateVersion + 1, ids(active), routingTable(newInitializing, primaryId));\n+\n+        addPeerRecoveryRetentionLease(tracker, extraId);\n+        tracker.initiateTracking(extraId.getId());\n+\n+        // now notify for the new id\n+        if (randomBoolean()) {\n+            updateLocalCheckpoint(tracker, extraId.getId(), minLocalCheckpointAfterUpdates + 1 + randomInt(4));\n+            markAsTrackingAndInSyncQuietly(tracker, extraId.getId(), randomInt((int) minLocalCheckpointAfterUpdates));\n+        } else {\n+            markAsTrackingAndInSyncQuietly(tracker, extraId.getId(), minLocalCheckpointAfterUpdates + 1 + randomInt(4));\n+        }\n+\n+        // now it should be incremented\n+        assertThat(tracker.getGlobalCheckpoint(), greaterThan(minLocalCheckpoint));\n+    }\n+\n+    public void testUpdateGlobalCheckpointOnReplica() {\n+        final AllocationId active = AllocationId.newInitializing();\n+        final ReplicationTracker tracker = newTracker(active);\n+        final long globalCheckpoint = randomLongBetween(NO_OPS_PERFORMED, Long.MAX_VALUE - 1);\n+        tracker.updateGlobalCheckpointOnReplica(globalCheckpoint, \"test\");\n+        assertThat(updatedGlobalCheckpoint.get(), equalTo(globalCheckpoint));\n+        final long nonUpdate = randomLongBetween(NO_OPS_PERFORMED, globalCheckpoint);\n+        updatedGlobalCheckpoint.set(UNASSIGNED_SEQ_NO);\n+        tracker.updateGlobalCheckpointOnReplica(nonUpdate, \"test\");\n+        assertThat(updatedGlobalCheckpoint.get(), equalTo(UNASSIGNED_SEQ_NO));\n+        final long update = randomLongBetween(globalCheckpoint, Long.MAX_VALUE);\n+        tracker.updateGlobalCheckpointOnReplica(update, \"test\");\n+        assertThat(updatedGlobalCheckpoint.get(), equalTo(update));\n+    }\n+\n+    public void testMarkAllocationIdAsInSync() throws Exception {\n+        final long initialClusterStateVersion = randomNonNegativeLong();\n+        Map<AllocationId, Long> activeWithCheckpoints = randomAllocationsWithLocalCheckpoints(1, 1);\n+        Set<AllocationId> active = new HashSet<>(activeWithCheckpoints.keySet());\n+        Map<AllocationId, Long> initializingWithCheckpoints = randomAllocationsWithLocalCheckpoints(1, 1);\n+        Set<AllocationId> initializing = new HashSet<>(initializingWithCheckpoints.keySet());\n+        final AllocationId primaryId = active.iterator().next();\n+        final AllocationId replicaId = initializing.iterator().next();\n+        final ReplicationTracker tracker = newTracker(primaryId);\n+        tracker.updateFromMaster(initialClusterStateVersion, ids(active), routingTable(initializing, primaryId));\n+        final long localCheckpoint = randomLongBetween(0, Long.MAX_VALUE - 1);\n+        tracker.activatePrimaryMode(localCheckpoint);\n+        addPeerRecoveryRetentionLease(tracker, replicaId);\n+        tracker.initiateTracking(replicaId.getId());\n+        final CyclicBarrier barrier = new CyclicBarrier(2);\n+        final Thread thread = new Thread(() -> {\n+            try {\n+                barrier.await();\n+                tracker.markAllocationIdAsInSync(\n+                    replicaId.getId(),\n+                    randomLongBetween(NO_OPS_PERFORMED, localCheckpoint - 1));\n+                barrier.await();\n+            } catch (BrokenBarrierException | InterruptedException e) {\n+                throw new AssertionError(e);\n+            }\n+        });\n+        thread.start();\n+        barrier.await();\n+        assertBusy(() -> assertTrue(tracker.pendingInSync()));\n+        final long updatedLocalCheckpoint = randomLongBetween(1 + localCheckpoint, Long.MAX_VALUE);\n+        // there is a shard copy pending in sync, the global checkpoint can not advance\n+        updatedGlobalCheckpoint.set(UNASSIGNED_SEQ_NO);\n+        tracker.updateLocalCheckpoint(primaryId.getId(), updatedLocalCheckpoint);\n+        assertThat(updatedGlobalCheckpoint.get(), equalTo(UNASSIGNED_SEQ_NO));\n+        // we are implicitly marking the pending in sync copy as in sync with the current global checkpoint, no advancement should occur\n+        tracker.updateLocalCheckpoint(replicaId.getId(), localCheckpoint);\n+        assertThat(updatedGlobalCheckpoint.get(), equalTo(UNASSIGNED_SEQ_NO));\n+        barrier.await();\n+        thread.join();\n+        // now we expect that the global checkpoint would advance\n+        tracker.markAllocationIdAsInSync(replicaId.getId(), updatedLocalCheckpoint);\n+        assertThat(updatedGlobalCheckpoint.get(), equalTo(updatedLocalCheckpoint));\n+    }\n+\n+    public void testMissingActiveIdsPreventAdvance() {\n+        final Map<AllocationId, Long> active = randomAllocationsWithLocalCheckpoints(2, 5);\n+        final Map<AllocationId, Long> initializing = randomAllocationsWithLocalCheckpoints(0, 5);\n+        final Map<AllocationId, Long> assigned = new HashMap<>();\n+        assigned.putAll(active);\n+        assigned.putAll(initializing);\n+        AllocationId primaryId = active.keySet().iterator().next();\n+        final ReplicationTracker tracker = newTracker(primaryId);\n+        tracker.updateFromMaster(randomNonNegativeLong(), ids(active.keySet()), routingTable(initializing.keySet(), primaryId));\n+        tracker.activatePrimaryMode(NO_OPS_PERFORMED);\n+        randomSubsetOf(initializing.keySet()).forEach(k -> markAsTrackingAndInSyncQuietly(tracker, k.getId(), NO_OPS_PERFORMED));\n+        final AllocationId missingActiveID = randomFrom(active.keySet());\n+        assigned\n+            .entrySet()\n+            .stream()\n+            .filter(e -> !e.getKey().equals(missingActiveID))\n+            .forEach(e -> updateLocalCheckpoint(tracker, e.getKey().getId(), e.getValue()));\n+\n+        if (missingActiveID.equals(primaryId) == false) {\n+            assertThat(tracker.getGlobalCheckpoint(), equalTo(UNASSIGNED_SEQ_NO));\n+            assertThat(updatedGlobalCheckpoint.get(), equalTo(UNASSIGNED_SEQ_NO));\n+        }\n+        // now update all knowledge of all shards\n+        assigned.forEach((aid, localCP) -> updateLocalCheckpoint(tracker, aid.getId(), localCP));\n+        assertThat(tracker.getGlobalCheckpoint(), not(equalTo(UNASSIGNED_SEQ_NO)));\n+        assertThat(updatedGlobalCheckpoint.get(), not(equalTo(UNASSIGNED_SEQ_NO)));\n+    }\n+\n+    public void testMissingInSyncIdsPreventAdvance() {\n+        final Map<AllocationId, Long> active = randomAllocationsWithLocalCheckpoints(1, 5);\n+        final Map<AllocationId, Long> initializing = randomAllocationsWithLocalCheckpoints(2, 5);\n+        logger.info(\"active: {}, initializing: {}\", active, initializing);\n+\n+        AllocationId primaryId = active.keySet().iterator().next();\n+        final ReplicationTracker tracker = newTracker(primaryId);\n+        tracker.updateFromMaster(randomNonNegativeLong(), ids(active.keySet()), routingTable(initializing.keySet(), primaryId));\n+        tracker.activatePrimaryMode(NO_OPS_PERFORMED);\n+        randomSubsetOf(randomIntBetween(1, initializing.size() - 1),\n+                       initializing.keySet()).forEach(aId -> markAsTrackingAndInSyncQuietly(tracker, aId.getId(), NO_OPS_PERFORMED));\n+\n+        active.forEach((aid, localCP) -> updateLocalCheckpoint(tracker, aid.getId(), localCP));\n+\n+        assertThat(tracker.getGlobalCheckpoint(), equalTo(NO_OPS_PERFORMED));\n+        assertThat(updatedGlobalCheckpoint.get(), equalTo(NO_OPS_PERFORMED));\n+\n+        // update again\n+        initializing.forEach((aid, localCP) -> updateLocalCheckpoint(tracker, aid.getId(), localCP));\n+        assertThat(tracker.getGlobalCheckpoint(), not(equalTo(UNASSIGNED_SEQ_NO)));\n+        assertThat(updatedGlobalCheckpoint.get(), not(equalTo(UNASSIGNED_SEQ_NO)));\n+    }\n+\n+    public void testInSyncIdsAreIgnoredIfNotValidatedByMaster() {\n+        final Map<AllocationId, Long> active = randomAllocationsWithLocalCheckpoints(1, 5);\n+        final Map<AllocationId, Long> initializing = randomAllocationsWithLocalCheckpoints(1, 5);\n+        final Map<AllocationId, Long> nonApproved = randomAllocationsWithLocalCheckpoints(1, 5);\n+        final AllocationId primaryId = active.keySet().iterator().next();\n+        final ReplicationTracker tracker = newTracker(primaryId);\n+        tracker.updateFromMaster(randomNonNegativeLong(), ids(active.keySet()), routingTable(initializing.keySet(), primaryId));\n+        tracker.activatePrimaryMode(NO_OPS_PERFORMED);\n+        initializing.keySet().forEach(k -> markAsTrackingAndInSyncQuietly(tracker, k.getId(), NO_OPS_PERFORMED));\n+        nonApproved.keySet().forEach(k ->\n+                                         expectThrows(IllegalStateException.class, () -> markAsTrackingAndInSyncQuietly(tracker, k.getId(), NO_OPS_PERFORMED)));\n+\n+        List<Map<AllocationId, Long>> allocations = Arrays.asList(active, initializing, nonApproved);\n+        Collections.shuffle(allocations, random());\n+        allocations.forEach(a -> a.forEach((aid, localCP) -> updateLocalCheckpoint(tracker, aid.getId(), localCP)));\n+\n+        assertThat(tracker.getGlobalCheckpoint(), not(equalTo(UNASSIGNED_SEQ_NO)));\n+    }\n+\n+    public void testInSyncIdsAreRemovedIfNotValidatedByMaster() {\n+        final long initialClusterStateVersion = randomNonNegativeLong();\n+        final Map<AllocationId, Long> activeToStay = randomAllocationsWithLocalCheckpoints(1, 5);\n+        final Map<AllocationId, Long> initializingToStay = randomAllocationsWithLocalCheckpoints(1, 5);\n+        final Map<AllocationId, Long> activeToBeRemoved = randomAllocationsWithLocalCheckpoints(1, 5);\n+        final Map<AllocationId, Long> initializingToBeRemoved = randomAllocationsWithLocalCheckpoints(1, 5);\n+        final Set<AllocationId> active = Sets.union(activeToStay.keySet(), activeToBeRemoved.keySet());\n+        final Set<AllocationId> initializing = Sets.union(initializingToStay.keySet(), initializingToBeRemoved.keySet());\n+        final Map<AllocationId, Long> allocations = new HashMap<>();\n+        final AllocationId primaryId = active.iterator().next();\n+        if (activeToBeRemoved.containsKey(primaryId)) {\n+            activeToStay.put(primaryId, activeToBeRemoved.remove(primaryId));\n+        }\n+        allocations.putAll(activeToStay);\n+        if (randomBoolean()) {\n+            allocations.putAll(activeToBeRemoved);\n+        }\n+        allocations.putAll(initializingToStay);\n+        if (randomBoolean()) {\n+            allocations.putAll(initializingToBeRemoved);\n+        }\n+        final ReplicationTracker tracker = newTracker(primaryId);\n+        tracker.updateFromMaster(initialClusterStateVersion, ids(active), routingTable(initializing, primaryId));\n+        tracker.activatePrimaryMode(NO_OPS_PERFORMED);\n+        if (randomBoolean()) {\n+            initializingToStay.keySet().forEach(k -> markAsTrackingAndInSyncQuietly(tracker, k.getId(), NO_OPS_PERFORMED));\n+        } else {\n+            initializing.forEach(k -> markAsTrackingAndInSyncQuietly(tracker, k.getId(), NO_OPS_PERFORMED));\n+        }\n+        if (randomBoolean()) {\n+            allocations.forEach((aid, localCP) -> updateLocalCheckpoint(tracker, aid.getId(), localCP));\n+        }\n+\n+        // now remove shards\n+        if (randomBoolean()) {\n+            tracker.updateFromMaster(\n+                initialClusterStateVersion + 1,\n+                ids(activeToStay.keySet()),\n+                routingTable(initializingToStay.keySet(), primaryId));\n+            allocations.forEach((aid, ckp) -> updateLocalCheckpoint(tracker, aid.getId(), ckp + 10L));\n+        } else {\n+            allocations.forEach((aid, ckp) -> updateLocalCheckpoint(tracker, aid.getId(), ckp + 10L));\n+            tracker.updateFromMaster(\n+                initialClusterStateVersion + 2,\n+                ids(activeToStay.keySet()),\n+                routingTable(initializingToStay.keySet(), primaryId));\n+        }\n+\n+        final long checkpoint = Stream.concat(activeToStay.values().stream(), initializingToStay.values().stream())\n+                                    .min(Long::compare).get() + 10; // we added 10 to make sure it's advanced in the second time\n+\n+        assertThat(tracker.getGlobalCheckpoint(), equalTo(checkpoint));\n+    }\n+\n+    public void testWaitForAllocationIdToBeInSync() throws Exception {\n+        final int localCheckpoint = randomIntBetween(1, 32);\n+        final int globalCheckpoint = randomIntBetween(localCheckpoint + 1, 64);\n+        final CyclicBarrier barrier = new CyclicBarrier(2);\n+        final AtomicBoolean complete = new AtomicBoolean();\n+        final AllocationId inSyncAllocationId = AllocationId.newInitializing();\n+        final AllocationId trackingAllocationId = AllocationId.newInitializing();\n+        final ReplicationTracker tracker = newTracker(inSyncAllocationId);\n+        final long clusterStateVersion = randomNonNegativeLong();\n+        tracker.updateFromMaster(clusterStateVersion, Collections.singleton(inSyncAllocationId.getId()),\n+                                 routingTable(Collections.singleton(trackingAllocationId), inSyncAllocationId));\n+        tracker.activatePrimaryMode(globalCheckpoint);\n+        addPeerRecoveryRetentionLease(tracker, trackingAllocationId);\n+        final Thread thread = new Thread(() -> {\n+            try {\n+                // synchronize starting with the test thread\n+                barrier.await();\n+                tracker.initiateTracking(trackingAllocationId.getId());\n+                tracker.markAllocationIdAsInSync(trackingAllocationId.getId(), localCheckpoint);\n+                complete.set(true);\n+                // synchronize with the test thread checking if we are no longer waiting\n+                barrier.await();\n+            } catch (final BrokenBarrierException | InterruptedException e) {\n+                throw new RuntimeException(e);\n+            }\n+        });\n+\n+        thread.start();\n+\n+        // synchronize starting with the waiting thread\n+        barrier.await();\n+\n+        final List<Integer> elements = IntStream.rangeClosed(0, globalCheckpoint - 1).boxed().collect(Collectors.toList());\n+        Randomness.shuffle(elements);\n+        for (int i = 0; i < elements.size(); i++) {\n+            updateLocalCheckpoint(tracker, trackingAllocationId.getId(), elements.get(i));\n+            assertFalse(complete.get());\n+            assertFalse(tracker.getTrackedLocalCheckpointForShard(trackingAllocationId.getId()).inSync);\n+            assertBusy(() -> assertTrue(tracker.pendingInSync.contains(trackingAllocationId.getId())));\n+        }\n+\n+        if (randomBoolean()) {\n+            // normal path, shard catches up\n+            updateLocalCheckpoint(tracker, trackingAllocationId.getId(), randomIntBetween(globalCheckpoint, 64));\n+            // synchronize with the waiting thread to mark that it is complete\n+            barrier.await();\n+            assertTrue(complete.get());\n+            assertTrue(tracker.getTrackedLocalCheckpointForShard(trackingAllocationId.getId()).inSync);\n+        } else {\n+            // master changes its mind and cancels the allocation\n+            tracker.updateFromMaster(clusterStateVersion + 1, Collections.singleton(inSyncAllocationId.getId()),\n+                                     routingTable(emptySet(), inSyncAllocationId));\n+            barrier.await();\n+            assertTrue(complete.get());\n+            assertNull(tracker.getTrackedLocalCheckpointForShard(trackingAllocationId.getId()));\n+        }\n+        assertFalse(tracker.pendingInSync.contains(trackingAllocationId.getId()));\n+        thread.join();\n+    }\n+\n+    private AtomicLong updatedGlobalCheckpoint = new AtomicLong(UNASSIGNED_SEQ_NO);\n+\n+    private ReplicationTracker newTracker(final AllocationId allocationId) {\n+        return newTracker(allocationId, updatedGlobalCheckpoint::set, () -> 0L);\n+    }\n+\n+    public void testWaitForAllocationIdToBeInSyncCanBeInterrupted() throws BrokenBarrierException, InterruptedException {\n+        final int localCheckpoint = randomIntBetween(1, 32);\n+        final int globalCheckpoint = randomIntBetween(localCheckpoint + 1, 64);\n+        final CyclicBarrier barrier = new CyclicBarrier(2);\n+        final AtomicBoolean interrupted = new AtomicBoolean();\n+        final AllocationId inSyncAllocationId = AllocationId.newInitializing();\n+        final AllocationId trackingAllocationId = AllocationId.newInitializing();\n+        final ReplicationTracker tracker = newTracker(inSyncAllocationId);\n+        tracker.updateFromMaster(randomNonNegativeLong(), Collections.singleton(inSyncAllocationId.getId()),\n+                                 routingTable(Collections.singleton(trackingAllocationId), inSyncAllocationId));\n+        tracker.activatePrimaryMode(globalCheckpoint);\n+        addPeerRecoveryRetentionLease(tracker, trackingAllocationId);\n+        final Thread thread = new Thread(() -> {\n+            try {\n+                // synchronize starting with the test thread\n+                barrier.await();\n+            } catch (final BrokenBarrierException | InterruptedException e) {\n+                throw new RuntimeException(e);\n+            }\n+            try {\n+                tracker.initiateTracking(trackingAllocationId.getId());\n+                tracker.markAllocationIdAsInSync(trackingAllocationId.getId(), localCheckpoint);\n+            } catch (final InterruptedException e) {\n+                interrupted.set(true);\n+                // synchronize with the test thread checking if we are interrupted\n+            }\n+            try {\n+                barrier.await();\n+            } catch (final BrokenBarrierException | InterruptedException e) {\n+                throw new RuntimeException(e);\n+            }\n+        });\n+\n+        thread.start();\n+\n+        // synchronize starting with the waiting thread\n+        barrier.await();\n+\n+        thread.interrupt();\n+\n+        // synchronize with the waiting thread to mark that it is complete\n+        barrier.await();\n+\n+        assertTrue(interrupted.get());\n+\n+        thread.join();\n+    }\n+\n+    public void testUpdateAllocationIdsFromMaster() throws Exception {\n+        final long initialClusterStateVersion = randomNonNegativeLong();\n+        final int numberOfActiveAllocationsIds = randomIntBetween(2, 16);\n+        final int numberOfInitializingIds = randomIntBetween(2, 16);\n+        final Tuple<Set<AllocationId>, Set<AllocationId>> activeAndInitializingAllocationIds =\n+            randomActiveAndInitializingAllocationIds(numberOfActiveAllocationsIds, numberOfInitializingIds);\n+        final Set<AllocationId> activeAllocationIds = activeAndInitializingAllocationIds.v1();\n+        final Set<AllocationId> initializingIds = activeAndInitializingAllocationIds.v2();\n+        AllocationId primaryId = activeAllocationIds.iterator().next();\n+        IndexShardRoutingTable routingTable = routingTable(initializingIds, primaryId);\n+        final ReplicationTracker tracker = newTracker(primaryId);\n+        tracker.updateFromMaster(initialClusterStateVersion, ids(activeAllocationIds), routingTable);\n+        tracker.activatePrimaryMode(NO_OPS_PERFORMED);\n+        assertThat(tracker.getReplicationGroup().getInSyncAllocationIds(), equalTo(ids(activeAllocationIds)));\n+        assertThat(tracker.getReplicationGroup().getRoutingTable(), equalTo(routingTable));\n+\n+        // first we assert that the in-sync and tracking sets are set up correctly\n+        assertTrue(activeAllocationIds.stream().allMatch(a -> tracker.getTrackedLocalCheckpointForShard(a.getId()).inSync));\n+        assertTrue(\n+            activeAllocationIds\n+                .stream()\n+                .filter(a -> a.equals(primaryId) == false)\n+                .allMatch(a -> tracker.getTrackedLocalCheckpointForShard(a.getId()).getLocalCheckpoint()\n+                               == SequenceNumbers.UNASSIGNED_SEQ_NO));\n+        assertTrue(initializingIds.stream().noneMatch(a -> tracker.getTrackedLocalCheckpointForShard(a.getId()).inSync));\n+        assertTrue(\n+            initializingIds\n+                .stream()\n+                .filter(a -> a.equals(primaryId) == false)\n+                .allMatch(a -> tracker.getTrackedLocalCheckpointForShard(a.getId()).getLocalCheckpoint()\n+                               == SequenceNumbers.UNASSIGNED_SEQ_NO));\n+\n+        // now we will remove some allocation IDs from these and ensure that they propagate through\n+        final Set<AllocationId> removingActiveAllocationIds = new HashSet<>(randomSubsetOf(activeAllocationIds));\n+        removingActiveAllocationIds.remove(primaryId);\n+        final Set<AllocationId> newActiveAllocationIds =\n+            activeAllocationIds.stream().filter(a -> !removingActiveAllocationIds.contains(a)).collect(Collectors.toSet());\n+        final List<AllocationId> removingInitializingAllocationIds = randomSubsetOf(initializingIds);\n+        final Set<AllocationId> newInitializingAllocationIds =\n+            initializingIds.stream().filter(a -> !removingInitializingAllocationIds.contains(a)).collect(Collectors.toSet());\n+        routingTable = routingTable(newInitializingAllocationIds, primaryId);\n+        tracker.updateFromMaster(initialClusterStateVersion + 1, ids(newActiveAllocationIds), routingTable);\n+        assertTrue(newActiveAllocationIds.stream().allMatch(a -> tracker.getTrackedLocalCheckpointForShard(a.getId()).inSync));\n+        assertTrue(removingActiveAllocationIds.stream().allMatch(a -> tracker.getTrackedLocalCheckpointForShard(a.getId()) == null));\n+        assertTrue(newInitializingAllocationIds.stream().noneMatch(a -> tracker.getTrackedLocalCheckpointForShard(a.getId()).inSync));\n+        assertTrue(removingInitializingAllocationIds.stream().allMatch(a -> tracker.getTrackedLocalCheckpointForShard(a.getId()) == null));\n+        assertThat(tracker.getReplicationGroup().getInSyncAllocationIds(), equalTo(\n+            ids(Sets.difference(Sets.union(activeAllocationIds, newActiveAllocationIds), removingActiveAllocationIds))));\n+        assertThat(tracker.getReplicationGroup().getRoutingTable(), equalTo(routingTable));\n+\n+        /*\n+         * Now we will add an allocation ID to each of active and initializing and ensure they propagate through. Using different lengths\n+         * than we have been using above ensures that we can not collide with a previous allocation ID\n+         */\n+        newInitializingAllocationIds.add(AllocationId.newInitializing());\n+        tracker.updateFromMaster(\n+            initialClusterStateVersion + 2,\n+            ids(newActiveAllocationIds),\n+            routingTable(newInitializingAllocationIds, primaryId));\n+        assertTrue(newActiveAllocationIds.stream().allMatch(a -> tracker.getTrackedLocalCheckpointForShard(a.getId()).inSync));\n+        assertTrue(\n+            newActiveAllocationIds\n+                .stream()\n+                .filter(a -> a.equals(primaryId) == false)\n+                .allMatch(a -> tracker.getTrackedLocalCheckpointForShard(a.getId()).getLocalCheckpoint()\n+                               == SequenceNumbers.UNASSIGNED_SEQ_NO));\n+        assertTrue(newInitializingAllocationIds.stream().noneMatch(a -> tracker.getTrackedLocalCheckpointForShard(a.getId()).inSync));\n+        assertTrue(\n+            newInitializingAllocationIds\n+                .stream()\n+                .allMatch(a -> tracker.getTrackedLocalCheckpointForShard(a.getId()).getLocalCheckpoint()\n+                               == SequenceNumbers.UNASSIGNED_SEQ_NO));\n+\n+        // the tracking allocation IDs should play no role in determining the global checkpoint\n+        final Map<AllocationId, Integer> activeLocalCheckpoints =\n+            newActiveAllocationIds.stream().collect(Collectors.toMap(Function.identity(), a -> randomIntBetween(1, 1024)));\n+        activeLocalCheckpoints.forEach((a, l) -> updateLocalCheckpoint(tracker, a.getId(), l));\n+        final Map<AllocationId, Integer> initializingLocalCheckpoints =\n+            newInitializingAllocationIds.stream().collect(Collectors.toMap(Function.identity(), a -> randomIntBetween(1, 1024)));\n+        initializingLocalCheckpoints.forEach((a, l) -> updateLocalCheckpoint(tracker, a.getId(), l));\n+        assertTrue(\n+            activeLocalCheckpoints\n+                .entrySet()\n+                .stream()\n+                .allMatch(e -> tracker.getTrackedLocalCheckpointForShard(e.getKey().getId()).getLocalCheckpoint() == e.getValue()));\n+        assertTrue(\n+            initializingLocalCheckpoints\n+                .entrySet()\n+                .stream()\n+                .allMatch(e -> tracker.getTrackedLocalCheckpointForShard(e.getKey().getId()).getLocalCheckpoint() == e.getValue()));\n+        final long minimumActiveLocalCheckpoint = activeLocalCheckpoints.values().stream().min(Integer::compareTo).get();\n+        assertThat(tracker.getGlobalCheckpoint(), equalTo(minimumActiveLocalCheckpoint));\n+        assertThat(updatedGlobalCheckpoint.get(), equalTo(minimumActiveLocalCheckpoint));\n+        final long minimumInitailizingLocalCheckpoint = initializingLocalCheckpoints.values().stream().min(Integer::compareTo).get();\n+\n+        // now we are going to add a new allocation ID and bring it in sync which should move it to the in-sync allocation IDs\n+        final long localCheckpoint =\n+            randomIntBetween(0, Math.toIntExact(Math.min(minimumActiveLocalCheckpoint, minimumInitailizingLocalCheckpoint) - 1));\n+\n+        // using a different length than we have been using above ensures that we can not collide with a previous allocation ID\n+        final AllocationId newSyncingAllocationId = AllocationId.newInitializing();\n+        newInitializingAllocationIds.add(newSyncingAllocationId);\n+        tracker.updateFromMaster(\n+            initialClusterStateVersion + 3,\n+            ids(newActiveAllocationIds),\n+            routingTable(newInitializingAllocationIds, primaryId));\n+        addPeerRecoveryRetentionLease(tracker, newSyncingAllocationId);\n+        final CyclicBarrier barrier = new CyclicBarrier(2);\n+        final Thread thread = new Thread(() -> {\n+            try {\n+                barrier.await();\n+                tracker.initiateTracking(newSyncingAllocationId.getId());\n+                tracker.markAllocationIdAsInSync(newSyncingAllocationId.getId(), localCheckpoint);\n+                barrier.await();\n+            } catch (final BrokenBarrierException | InterruptedException e) {\n+                throw new RuntimeException(e);\n+            }\n+        });\n+\n+        thread.start();\n+\n+        barrier.await();\n+\n+        assertBusy(() -> {\n+            assertTrue(tracker.pendingInSync.contains(newSyncingAllocationId.getId()));\n+            assertFalse(tracker.getTrackedLocalCheckpointForShard(newSyncingAllocationId.getId()).inSync);\n+        });\n+\n+        tracker.updateLocalCheckpoint(newSyncingAllocationId.getId(),\n+                                      randomIntBetween(Math.toIntExact(minimumActiveLocalCheckpoint), 1024));\n+\n+        barrier.await();\n+\n+        assertFalse(tracker.pendingInSync.contains(newSyncingAllocationId.getId()));\n+        assertTrue(tracker.getTrackedLocalCheckpointForShard(newSyncingAllocationId.getId()).inSync);\n+\n+        /*\n+         * The new in-sync allocation ID is in the in-sync set now yet the master does not know this; the allocation ID should still be in\n+         * the in-sync set even if we receive a cluster state update that does not reflect this.\n+         *\n+         */\n+        tracker.updateFromMaster(\n+            initialClusterStateVersion + 4,\n+            ids(newActiveAllocationIds),\n+            routingTable(newInitializingAllocationIds, primaryId));\n+        assertTrue(tracker.getTrackedLocalCheckpointForShard(newSyncingAllocationId.getId()).inSync);\n+        assertFalse(tracker.pendingInSync.contains(newSyncingAllocationId.getId()));\n+    }\n+\n+    /**\n+     * If we do not update the global checkpoint in {@link ReplicationTracker#markAllocationIdAsInSync(String, long)} after adding the\n+     * allocation ID to the in-sync set and removing it from pending, the local checkpoint update that freed the thread waiting for the\n+     * local checkpoint to advance could miss updating the global checkpoint in a race if the waiting thread did not add the allocation\n+     * ID to the in-sync set and remove it from the pending set before the local checkpoint updating thread executed the global checkpoint\n+     * update. This test fails without an additional call to {@code ReplicationTracker#updateGlobalCheckpointOnPrimary()} after\n+     * removing the allocation ID from the pending set in {@link ReplicationTracker#markAllocationIdAsInSync(String, long)} (even if a\n+     * call is added after notifying all waiters in {@link ReplicationTracker#updateLocalCheckpoint(String, long)}).\n+     *\n+     * @throws InterruptedException   if the main test thread was interrupted while waiting\n+     * @throws BrokenBarrierException if the barrier was broken while the main test thread was waiting\n+     */\n+    public void testRaceUpdatingGlobalCheckpoint() throws InterruptedException, BrokenBarrierException {\n+\n+        final AllocationId active = AllocationId.newInitializing();\n+        final AllocationId initializing = AllocationId.newInitializing();\n+        final CyclicBarrier barrier = new CyclicBarrier(4);\n+\n+        final int activeLocalCheckpoint = randomIntBetween(0, Integer.MAX_VALUE - 1);\n+        final ReplicationTracker tracker = newTracker(active);\n+        tracker.updateFromMaster(\n+            randomNonNegativeLong(),\n+            Collections.singleton(active.getId()),\n+            routingTable(Collections.singleton(initializing), active));\n+        tracker.activatePrimaryMode(activeLocalCheckpoint);\n+        addPeerRecoveryRetentionLease(tracker, initializing);\n+        final int nextActiveLocalCheckpoint = randomIntBetween(activeLocalCheckpoint + 1, Integer.MAX_VALUE);\n+        final Thread activeThread = new Thread(() -> {\n+            try {\n+                barrier.await();\n+            } catch (final BrokenBarrierException | InterruptedException e) {\n+                throw new RuntimeException(e);\n+            }\n+            tracker.updateLocalCheckpoint(active.getId(), nextActiveLocalCheckpoint);\n+        });\n+\n+        final int initializingLocalCheckpoint = randomIntBetween(0, nextActiveLocalCheckpoint - 1);\n+        final Thread initializingThread = new Thread(() -> {\n+            try {\n+                barrier.await();\n+            } catch (final BrokenBarrierException | InterruptedException e) {\n+                throw new RuntimeException(e);\n+            }\n+            tracker.updateLocalCheckpoint(initializing.getId(), nextActiveLocalCheckpoint);\n+        });\n+\n+        final Thread markingThread = new Thread(() -> {\n+            try {\n+                barrier.await();\n+                tracker.initiateTracking(initializing.getId());\n+                tracker.markAllocationIdAsInSync(initializing.getId(), initializingLocalCheckpoint - 1);\n+            } catch (final BrokenBarrierException | InterruptedException e) {\n+                throw new RuntimeException(e);\n+            }\n+        });\n+\n+        activeThread.start();\n+        initializingThread.start();\n+        markingThread.start();\n+        barrier.await();\n+\n+        activeThread.join();\n+        initializingThread.join();\n+        markingThread.join();\n+\n+        assertThat(tracker.getGlobalCheckpoint(), equalTo((long) nextActiveLocalCheckpoint));\n+    }\n+\n+    public void testPrimaryContextHandoff() throws IOException {\n+        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(\"test\", Settings.EMPTY);\n+        final ShardId shardId = new ShardId(\"test\", \"_na_\", 0);\n+\n+        FakeClusterState clusterState = initialState();\n+        final AllocationId aId = clusterState.routingTable.primaryShard().allocationId();\n+        final LongConsumer onUpdate = updatedGlobalCheckpoint -> {};\n+        final long primaryTerm = randomNonNegativeLong();\n+        final long globalCheckpoint = UNASSIGNED_SEQ_NO;\n+        final BiConsumer<RetentionLeases, ActionListener<ReplicationResponse>> onNewRetentionLease =\n+            (leases, listener) -> {};\n+        ReplicationTracker oldPrimary = new ReplicationTracker(shardId, aId.getId(), indexSettings, primaryTerm, globalCheckpoint,\n+                                                               onUpdate, () -> 0L, onNewRetentionLease, OPS_BASED_RECOVERY_ALWAYS_REASONABLE);\n+        ReplicationTracker newPrimary = new ReplicationTracker(shardId, aId.getRelocationId(), indexSettings, primaryTerm, globalCheckpoint,\n+                                                               onUpdate, () -> 0L, onNewRetentionLease, OPS_BASED_RECOVERY_ALWAYS_REASONABLE);\n+\n+        Set<String> allocationIds = new HashSet<>(Arrays.asList(oldPrimary.shardAllocationId, newPrimary.shardAllocationId));\n+\n+        clusterState.apply(oldPrimary);\n+        clusterState.apply(newPrimary);\n+\n+        oldPrimary.activatePrimaryMode(randomIntBetween(Math.toIntExact(NO_OPS_PERFORMED), 10));\n+        addPeerRecoveryRetentionLease(oldPrimary, newPrimary.shardAllocationId);\n+        newPrimary.updateRetentionLeasesOnReplica(oldPrimary.getRetentionLeases());\n+\n+        final int numUpdates = randomInt(10);\n+        for (int i = 0; i < numUpdates; i++) {\n+            if (rarely()) {\n+                clusterState = randomUpdateClusterState(allocationIds, clusterState);\n+                clusterState.apply(oldPrimary);\n+                clusterState.apply(newPrimary);\n+            }\n+            if (randomBoolean()) {\n+                randomLocalCheckpointUpdate(oldPrimary);\n+            }\n+            if (randomBoolean()) {\n+                randomMarkInSync(oldPrimary, newPrimary);\n+            }\n+        }\n+\n+        // simulate transferring the global checkpoint to the new primary after finalizing recovery before the handoff\n+        markAsTrackingAndInSyncQuietly(\n+            oldPrimary,\n+            newPrimary.shardAllocationId,\n+            Math.max(SequenceNumbers.NO_OPS_PERFORMED, oldPrimary.getGlobalCheckpoint() + randomInt(5)));\n+        oldPrimary.updateGlobalCheckpointForShard(newPrimary.shardAllocationId, oldPrimary.getGlobalCheckpoint());\n+        ReplicationTracker.PrimaryContext primaryContext = oldPrimary.startRelocationHandoff(newPrimary.shardAllocationId);\n+\n+        if (randomBoolean()) {\n+            // cluster state update after primary context handoff\n+            if (randomBoolean()) {\n+                clusterState = randomUpdateClusterState(allocationIds, clusterState);\n+                clusterState.apply(oldPrimary);\n+                clusterState.apply(newPrimary);\n+            }\n+\n+            // abort handoff, check that we can continue updates and retry handoff\n+            oldPrimary.abortRelocationHandoff();\n+\n+            if (rarely()) {\n+                clusterState = randomUpdateClusterState(allocationIds, clusterState);\n+                clusterState.apply(oldPrimary);\n+                clusterState.apply(newPrimary);\n+            }\n+            if (randomBoolean()) {\n+                randomLocalCheckpointUpdate(oldPrimary);\n+            }\n+            if (randomBoolean()) {\n+                randomMarkInSync(oldPrimary, newPrimary);\n+            }\n+\n+            // do another handoff\n+            primaryContext = oldPrimary.startRelocationHandoff(newPrimary.shardAllocationId);\n+        }\n+\n+        // send primary context through the wire\n+        BytesStreamOutput output = new BytesStreamOutput();\n+        primaryContext.writeTo(output);\n+        StreamInput streamInput = output.bytes().streamInput();\n+        primaryContext = new ReplicationTracker.PrimaryContext(streamInput);\n+        switch (randomInt(3)) {\n+            case 0: {\n+                // apply cluster state update on old primary while primary context is being transferred\n+                clusterState = randomUpdateClusterState(allocationIds, clusterState);\n+                clusterState.apply(oldPrimary);\n+                // activate new primary\n+                newPrimary.activateWithPrimaryContext(primaryContext);\n+                // apply cluster state update on new primary so that the states on old and new primary are comparable\n+                clusterState.apply(newPrimary);\n+                break;\n+            }\n+            case 1: {\n+                // apply cluster state update on new primary while primary context is being transferred\n+                clusterState = randomUpdateClusterState(allocationIds, clusterState);\n+                clusterState.apply(newPrimary);\n+                // activate new primary\n+                newPrimary.activateWithPrimaryContext(primaryContext);\n+                // apply cluster state update on old primary so that the states on old and new primary are comparable\n+                clusterState.apply(oldPrimary);\n+                break;\n+            }\n+            case 2: {\n+                // apply cluster state update on both copies while primary context is being transferred\n+                clusterState = randomUpdateClusterState(allocationIds, clusterState);\n+                clusterState.apply(oldPrimary);\n+                clusterState.apply(newPrimary);\n+                newPrimary.activateWithPrimaryContext(primaryContext);\n+                break;\n+            }\n+            case 3: {\n+                // no cluster state update\n+                newPrimary.activateWithPrimaryContext(primaryContext);\n+                break;\n+            }\n+        }\n+\n+        assertTrue(oldPrimary.primaryMode);\n+        assertTrue(newPrimary.primaryMode);\n+        assertThat(newPrimary.appliedClusterStateVersion, equalTo(oldPrimary.appliedClusterStateVersion));\n+        /*\n+         * We can not assert on shared knowledge of the global checkpoint between the old primary and the new primary as the new primary\n+         * will update its global checkpoint state without the old primary learning of it, and the old primary could have updated its\n+         * global checkpoint state after the primary context was transferred.\n+         */\n+        Map<String, ReplicationTracker.CheckpointState> oldPrimaryCheckpointsCopy = new HashMap<>(oldPrimary.checkpoints);\n+        oldPrimaryCheckpointsCopy.remove(oldPrimary.shardAllocationId);\n+        oldPrimaryCheckpointsCopy.remove(newPrimary.shardAllocationId);\n+        Map<String, ReplicationTracker.CheckpointState> newPrimaryCheckpointsCopy = new HashMap<>(newPrimary.checkpoints);\n+        newPrimaryCheckpointsCopy.remove(oldPrimary.shardAllocationId);\n+        newPrimaryCheckpointsCopy.remove(newPrimary.shardAllocationId);\n+        assertThat(newPrimaryCheckpointsCopy, equalTo(oldPrimaryCheckpointsCopy));\n+        // we can however assert that shared knowledge of the local checkpoint and in-sync status is equal\n+        assertThat(\n+            oldPrimary.checkpoints.get(oldPrimary.shardAllocationId).localCheckpoint,\n+            equalTo(newPrimary.checkpoints.get(oldPrimary.shardAllocationId).localCheckpoint));\n+        assertThat(\n+            oldPrimary.checkpoints.get(newPrimary.shardAllocationId).localCheckpoint,\n+            equalTo(newPrimary.checkpoints.get(newPrimary.shardAllocationId).localCheckpoint));\n+        assertThat(\n+            oldPrimary.checkpoints.get(oldPrimary.shardAllocationId).inSync,\n+            equalTo(newPrimary.checkpoints.get(oldPrimary.shardAllocationId).inSync));\n+        assertThat(\n+            oldPrimary.checkpoints.get(newPrimary.shardAllocationId).inSync,\n+            equalTo(newPrimary.checkpoints.get(newPrimary.shardAllocationId).inSync));\n+        assertThat(newPrimary.getGlobalCheckpoint(), equalTo(oldPrimary.getGlobalCheckpoint()));\n+        assertThat(newPrimary.routingTable, equalTo(oldPrimary.routingTable));\n+        assertThat(newPrimary.replicationGroup, equalTo(oldPrimary.replicationGroup));\n+\n+        assertFalse(oldPrimary.relocated);\n+        oldPrimary.completeRelocationHandoff();\n+        assertFalse(oldPrimary.primaryMode);\n+        assertTrue(oldPrimary.relocated);\n+    }\n+\n+    public void testIllegalStateExceptionIfUnknownAllocationId() {\n+        final AllocationId active = AllocationId.newInitializing();\n+        final AllocationId initializing = AllocationId.newInitializing();\n+        final ReplicationTracker tracker = newTracker(active);\n+        tracker.updateFromMaster(randomNonNegativeLong(), Collections.singleton(active.getId()),\n+                                 routingTable(Collections.singleton(initializing), active));\n+        tracker.activatePrimaryMode(NO_OPS_PERFORMED);\n+\n+        expectThrows(IllegalStateException.class, () -> tracker.initiateTracking(randomAlphaOfLength(10)));\n+        expectThrows(IllegalStateException.class, () -> tracker.markAllocationIdAsInSync(randomAlphaOfLength(10), randomNonNegativeLong()));\n+    }\n+\n+    private static class FakeClusterState {\n+        final long version;\n+        final Set<AllocationId> inSyncIds;\n+        final IndexShardRoutingTable routingTable;\n+\n+        private FakeClusterState(long version, Set<AllocationId> inSyncIds, IndexShardRoutingTable routingTable) {\n+            this.version = version;\n+            this.inSyncIds = Collections.unmodifiableSet(inSyncIds);\n+            this.routingTable = routingTable;\n+        }\n+\n+        public Set<AllocationId> allIds() {\n+            return Sets.union(initializingIds(), inSyncIds);\n+        }\n+\n+        public Set<AllocationId> initializingIds() {\n+            return routingTable.getAllInitializingShards().stream()\n+                .map(ShardRouting::allocationId).collect(Collectors.toSet());\n+        }\n+\n+        public void apply(ReplicationTracker gcp) {\n+            gcp.updateFromMaster(version, ids(inSyncIds), routingTable);\n+        }\n+    }\n+\n+    private static FakeClusterState initialState() {\n+        final long initialClusterStateVersion = randomIntBetween(1, Integer.MAX_VALUE);\n+        final int numberOfActiveAllocationsIds = randomIntBetween(1, 8);\n+        final int numberOfInitializingIds = randomIntBetween(0, 8);\n+        final Tuple<Set<AllocationId>, Set<AllocationId>> activeAndInitializingAllocationIds =\n+            randomActiveAndInitializingAllocationIds(numberOfActiveAllocationsIds, numberOfInitializingIds);\n+        final Set<AllocationId> activeAllocationIds = activeAndInitializingAllocationIds.v1();\n+        final Set<AllocationId> initializingAllocationIds = activeAndInitializingAllocationIds.v2();\n+        final AllocationId primaryId = randomFrom(activeAllocationIds);\n+        final AllocationId relocatingId = AllocationId.newRelocation(primaryId);\n+        activeAllocationIds.remove(primaryId);\n+        activeAllocationIds.add(relocatingId);\n+        final ShardId shardId = new ShardId(\"test\", \"_na_\", 0);\n+        final ShardRouting primaryShard =\n+            TestShardRouting.newShardRouting(\n+                shardId,\n+                nodeIdFromAllocationId(relocatingId),\n+                nodeIdFromAllocationId(AllocationId.newInitializing(relocatingId.getRelocationId())),\n+                true, ShardRoutingState.RELOCATING, relocatingId);\n+\n+        return new FakeClusterState(\n+            initialClusterStateVersion,\n+            activeAllocationIds,\n+            routingTable(initializingAllocationIds, primaryShard));\n+    }\n+\n+    private static void randomLocalCheckpointUpdate(ReplicationTracker gcp) {\n+        String allocationId = randomFrom(gcp.checkpoints.keySet());\n+        long currentLocalCheckpoint = gcp.checkpoints.get(allocationId).getLocalCheckpoint();\n+        gcp.updateLocalCheckpoint(allocationId, Math.max(SequenceNumbers.NO_OPS_PERFORMED, currentLocalCheckpoint + randomInt(5)));\n+    }\n+\n+    private static void randomMarkInSync(ReplicationTracker oldPrimary, ReplicationTracker newPrimary) {\n+        final String allocationId = randomFrom(oldPrimary.checkpoints.keySet());\n+        final long newLocalCheckpoint = Math.max(NO_OPS_PERFORMED, oldPrimary.getGlobalCheckpoint() + randomInt(5));\n+        markAsTrackingAndInSyncQuietly(oldPrimary, allocationId, newLocalCheckpoint);\n+        newPrimary.updateRetentionLeasesOnReplica(oldPrimary.getRetentionLeases());\n+    }\n+\n+    private static FakeClusterState randomUpdateClusterState(Set<String> allocationIds, FakeClusterState clusterState) {\n+        final Set<AllocationId> initializingIdsToAdd =\n+            randomAllocationIdsExcludingExistingIds(exclude(clusterState.allIds(), allocationIds), randomInt(2));\n+        final Set<AllocationId> initializingIdsToRemove = new HashSet<>(\n+            exclude(randomSubsetOf(randomInt(clusterState.initializingIds().size()), clusterState.initializingIds()), allocationIds));\n+        final Set<AllocationId> inSyncIdsToRemove = new HashSet<>(\n+            exclude(randomSubsetOf(randomInt(clusterState.inSyncIds.size()), clusterState.inSyncIds), allocationIds));\n+        final Set<AllocationId> remainingInSyncIds = Sets.difference(clusterState.inSyncIds, inSyncIdsToRemove);\n+        final Set<AllocationId> initializingIdsExceptRelocationTargets = exclude(clusterState.initializingIds(),\n+                                                                                 clusterState.routingTable.activeShards().stream().filter(ShardRouting::relocating)\n+                                                                                     .map(s -> s.allocationId().getRelocationId()).collect(Collectors.toSet()));\n+        return new FakeClusterState(\n+            clusterState.version + randomIntBetween(1, 5),\n+            remainingInSyncIds.isEmpty() ? clusterState.inSyncIds : remainingInSyncIds,\n+            routingTable(\n+                Sets.difference(Sets.union(initializingIdsExceptRelocationTargets, initializingIdsToAdd), initializingIdsToRemove),\n+                clusterState.routingTable.primaryShard()));\n+    }\n+\n+    private static Set<AllocationId> exclude(Collection<AllocationId> allocationIds, Set<String> excludeIds) {\n+        return allocationIds.stream().filter(aId -> !excludeIds.contains(aId.getId())).collect(Collectors.toSet());\n+    }\n+\n+    private static Tuple<Set<AllocationId>, Set<AllocationId>> randomActiveAndInitializingAllocationIds(\n+        final int numberOfActiveAllocationsIds,\n+        final int numberOfInitializingIds) {\n+        final Set<AllocationId> activeAllocationIds =\n+            IntStream.range(0, numberOfActiveAllocationsIds).mapToObj(i -> AllocationId.newInitializing()).collect(Collectors.toSet());\n+        final Set<AllocationId> initializingIds = randomAllocationIdsExcludingExistingIds(activeAllocationIds, numberOfInitializingIds);\n+        return Tuple.tuple(activeAllocationIds, initializingIds);\n+    }\n+\n+    private static Set<AllocationId> randomAllocationIdsExcludingExistingIds(final Set<AllocationId> existingAllocationIds,\n+                                                                             final int numberOfAllocationIds) {\n+        return IntStream.range(0, numberOfAllocationIds).mapToObj(i -> {\n+            do {\n+                final AllocationId newAllocationId = AllocationId.newInitializing();\n+                // ensure we do not duplicate an allocation ID\n+                if (!existingAllocationIds.contains(newAllocationId)) {\n+                    return newAllocationId;\n+                }\n+            } while (true);\n+        }).collect(Collectors.toSet());\n+    }\n+\n+    private static void markAsTrackingAndInSyncQuietly(\n+        final ReplicationTracker tracker, final String allocationId, final long localCheckpoint) {\n+        try {\n+            addPeerRecoveryRetentionLease(tracker, allocationId);\n+            tracker.initiateTracking(allocationId);\n+            tracker.markAllocationIdAsInSync(allocationId, localCheckpoint);\n+        } catch (final InterruptedException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    private static void addPeerRecoveryRetentionLease(final ReplicationTracker tracker, final AllocationId allocationId) {\n+        final String nodeId = nodeIdFromAllocationId(allocationId);\n+        if (tracker.getRetentionLeases().contains(ReplicationTracker.getPeerRecoveryRetentionLeaseId(nodeId)) == false) {\n+            tracker.addPeerRecoveryRetentionLease(nodeId, NO_OPS_PERFORMED, ActionListener.wrap(() -> { }));\n+        }\n+    }\n+\n+    private static void addPeerRecoveryRetentionLease(final ReplicationTracker tracker, final String allocationId) {\n+        addPeerRecoveryRetentionLease(tracker, AllocationId.newInitializing(allocationId));\n+    }\n+\n+    public void testPeerRecoveryRetentionLeaseCreationAndRenewal() {\n+\n+        final int numberOfActiveAllocationsIds = randomIntBetween(1, 8);\n+        final int numberOfInitializingIds = randomIntBetween(0, 8);\n+        final Tuple<Set<AllocationId>, Set<AllocationId>> activeAndInitializingAllocationIds =\n+            randomActiveAndInitializingAllocationIds(numberOfActiveAllocationsIds, numberOfInitializingIds);\n+        final Set<AllocationId> activeAllocationIds = activeAndInitializingAllocationIds.v1();\n+        final Set<AllocationId> initializingAllocationIds = activeAndInitializingAllocationIds.v2();\n+\n+        final AllocationId primaryId = activeAllocationIds.iterator().next();\n+\n+        final long initialClusterStateVersion = randomNonNegativeLong();\n+\n+        final AtomicLong currentTimeMillis = new AtomicLong(0L);\n+        final ReplicationTracker tracker = newTracker(primaryId, updatedGlobalCheckpoint::set, currentTimeMillis::get);\n+\n+        final long retentionLeaseExpiryTimeMillis = tracker.indexSettings().getRetentionLeaseMillis();\n+        final long peerRecoveryRetentionLeaseRenewalTimeMillis = retentionLeaseExpiryTimeMillis / 2;\n+\n+        final long maximumTestTimeMillis = 13 * retentionLeaseExpiryTimeMillis;\n+        final long testStartTimeMillis = randomLongBetween(0L, Long.MAX_VALUE - maximumTestTimeMillis);\n+        currentTimeMillis.set(testStartTimeMillis);\n+\n+        final Function<AllocationId, RetentionLease> retentionLeaseFromAllocationId = allocationId\n+            -> new RetentionLease(ReplicationTracker.getPeerRecoveryRetentionLeaseId(nodeIdFromAllocationId(allocationId)),\n+                                  0L, currentTimeMillis.get(), ReplicationTracker.PEER_RECOVERY_RETENTION_LEASE_SOURCE);\n+\n+        final List<RetentionLease> initialLeases = new ArrayList<>();\n+        if (randomBoolean()) {\n+            initialLeases.add(retentionLeaseFromAllocationId.apply(primaryId));\n+        }\n+        for (final AllocationId replicaId : initializingAllocationIds) {\n+            if (randomBoolean()) {\n+                initialLeases.add(retentionLeaseFromAllocationId.apply(replicaId));\n+            }\n+        }\n+        for (int i = randomIntBetween(0, 5); i > 0; i--) {\n+            initialLeases.add(retentionLeaseFromAllocationId.apply(AllocationId.newInitializing()));\n+        }\n+        tracker.updateRetentionLeasesOnReplica(new RetentionLeases(randomNonNegativeLong(), randomNonNegativeLong(), initialLeases));\n+\n+        IndexShardRoutingTable routingTable = routingTable(initializingAllocationIds, primaryId);\n+        tracker.updateFromMaster(initialClusterStateVersion, ids(activeAllocationIds), routingTable);\n+        tracker.activatePrimaryMode(NO_OPS_PERFORMED);\n+        assertTrue(\"primary's retention lease should exist\",\n+                   tracker.getRetentionLeases().contains(ReplicationTracker.getPeerRecoveryRetentionLeaseId(routingTable.primaryShard())));\n+\n+        final Consumer<Runnable> assertAsTimePasses = assertion -> {\n+            final long startTime = currentTimeMillis.get();\n+            while (currentTimeMillis.get() < startTime + retentionLeaseExpiryTimeMillis * 2) {\n+                currentTimeMillis.addAndGet(randomLongBetween(0L, retentionLeaseExpiryTimeMillis * 2));\n+                tracker.renewPeerRecoveryRetentionLeases();\n+                tracker.getRetentionLeases(true);\n+                assertion.run();\n+            }\n+        };\n+\n+        assertAsTimePasses.accept(() -> {\n+            // Leases for assigned replicas do not expire\n+            final RetentionLeases retentionLeases = tracker.getRetentionLeases();\n+            for (final AllocationId replicaId : initializingAllocationIds) {\n+                final String leaseId = retentionLeaseFromAllocationId.apply(replicaId).id();\n+                assertTrue(\"should not have removed lease for \" + replicaId + \" in \" + retentionLeases,\n+                           initialLeases.stream().noneMatch(l -> l.id().equals(leaseId)) || retentionLeases.contains(leaseId));\n+            }\n+        });\n+\n+        // Leases that don't correspond to assigned replicas, however, are expired by this time.\n+        final Set<String> expectedLeaseIds = Stream.concat(Stream.of(primaryId), initializingAllocationIds.stream())\n+            .map(allocationId -> retentionLeaseFromAllocationId.apply(allocationId).id()).collect(Collectors.toSet());\n+        for (final RetentionLease retentionLease : tracker.getRetentionLeases().leases()) {\n+            assertThat(expectedLeaseIds, hasItem(retentionLease.id()));\n+        }\n+\n+        for (AllocationId replicaId : initializingAllocationIds) {\n+            markAsTrackingAndInSyncQuietly(tracker, replicaId.getId(), NO_OPS_PERFORMED);\n+        }\n+\n+        assertThat(tracker.getRetentionLeases().leases().stream().map(RetentionLease::id).collect(Collectors.toSet()),\n+                   equalTo(expectedLeaseIds));\n+\n+        assertAsTimePasses.accept(() -> {\n+            // Leases still don't expire\n+            assertThat(tracker.getRetentionLeases().leases().stream().map(RetentionLease::id).collect(Collectors.toSet()),\n+                       equalTo(expectedLeaseIds));\n+\n+            // Also leases are renewed before reaching half the expiry time\n+            //noinspection OptionalGetWithoutIsPresent\n+            assertThat(tracker.getRetentionLeases() + \" renewed before too long\",\n+                       tracker.getRetentionLeases().leases().stream().mapToLong(RetentionLease::timestamp).min().getAsLong(),\n+                       greaterThanOrEqualTo(currentTimeMillis.get() - peerRecoveryRetentionLeaseRenewalTimeMillis));\n+        });\n+\n+        IndexShardRoutingTable.Builder routingTableBuilder = new IndexShardRoutingTable.Builder(routingTable);\n+        for (ShardRouting replicaShard : routingTable.replicaShards()) {\n+            routingTableBuilder.removeShard(replicaShard);\n+            routingTableBuilder.addShard(replicaShard.moveToStarted());\n+        }\n+        routingTable = routingTableBuilder.build();\n+        activeAllocationIds.addAll(initializingAllocationIds);\n+\n+        tracker.updateFromMaster(initialClusterStateVersion + randomLongBetween(1, 10), ids(activeAllocationIds), routingTable);\n+\n+        assertAsTimePasses.accept(() -> {\n+            // Leases still don't expire\n+            assertThat(tracker.getRetentionLeases().leases().stream().map(RetentionLease::id).collect(Collectors.toSet()),\n+                       equalTo(expectedLeaseIds));\n+            // ... and any extra peer recovery retention leases are expired immediately since the shard is fully active\n+            tracker.addPeerRecoveryRetentionLease(randomAlphaOfLength(10), randomNonNegativeLong(), ActionListener.wrap(() -> {}));\n+        });\n+\n+        tracker.renewPeerRecoveryRetentionLeases();\n+        assertTrue(\"expired extra lease\", tracker.getRetentionLeases(true).v1());\n+\n+        final AllocationId advancingAllocationId\n+            = initializingAllocationIds.isEmpty() || rarely() ? primaryId : randomFrom(initializingAllocationIds);\n+        final String advancingLeaseId = retentionLeaseFromAllocationId.apply(advancingAllocationId).id();\n+\n+        final long initialGlobalCheckpoint\n+            = Math.max(NO_OPS_PERFORMED, tracker.getTrackedLocalCheckpointForShard(advancingAllocationId.getId()).globalCheckpoint);\n+        assertThat(tracker.getRetentionLeases().get(advancingLeaseId).retainingSequenceNumber(), equalTo(initialGlobalCheckpoint + 1));\n+        final long newGlobalCheckpoint = initialGlobalCheckpoint + randomLongBetween(1, 1000);\n+        tracker.updateGlobalCheckpointForShard(advancingAllocationId.getId(), newGlobalCheckpoint);\n+        tracker.renewPeerRecoveryRetentionLeases();\n+        assertThat(\"lease was renewed because the shard advanced its global checkpoint\",\n+                   tracker.getRetentionLeases().get(advancingLeaseId).retainingSequenceNumber(), equalTo(newGlobalCheckpoint + 1));\n+\n+        final long initialVersion = tracker.getRetentionLeases().version();\n+        tracker.renewPeerRecoveryRetentionLeases();\n+        assertThat(\"immediate renewal is a no-op\", tracker.getRetentionLeases().version(), equalTo(initialVersion));\n+\n+        //noinspection OptionalGetWithoutIsPresent\n+        final long millisUntilFirstRenewal\n+            = tracker.getRetentionLeases().leases().stream().mapToLong(RetentionLease::timestamp).min().getAsLong()\n+              + peerRecoveryRetentionLeaseRenewalTimeMillis\n+              - currentTimeMillis.get();\n+\n+        if (millisUntilFirstRenewal != 0) {\n+            final long shorterThanRenewalTime = randomLongBetween(0L, millisUntilFirstRenewal - 1);\n+            currentTimeMillis.addAndGet(shorterThanRenewalTime);\n+            tracker.renewPeerRecoveryRetentionLeases();\n+            assertThat(\"renewal is a no-op after a short time\", tracker.getRetentionLeases().version(), equalTo(initialVersion));\n+            currentTimeMillis.addAndGet(millisUntilFirstRenewal - shorterThanRenewalTime);\n+        }\n+\n+        tracker.renewPeerRecoveryRetentionLeases();\n+        assertThat(\"renewal happens after a sufficiently long time\", tracker.getRetentionLeases().version(), greaterThan(initialVersion));\n+        assertTrue(\"all leases were renewed\",\n+                   tracker.getRetentionLeases().leases().stream().allMatch(l -> l.timestamp() == currentTimeMillis.get()));\n+\n+        assertThat(\"test ran for too long, potentially leading to overflow\",\n+                   currentTimeMillis.get(), lessThanOrEqualTo(testStartTimeMillis + maximumTestTimeMillis));\n+    }\n+\n+}"
  },
  {
    "sha": "ebfef5031ba5576a632c9d77f9919db0557e568e",
    "filename": "server/src/test/java/org/elasticsearch/index/shard/IndexShardTestCase.java",
    "status": "modified",
    "additions": 40,
    "deletions": 37,
    "changes": 77,
    "blob_url": "https://github.com/crate/crate/blob/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/test/java/org/elasticsearch/index/shard/IndexShardTestCase.java",
    "raw_url": "https://github.com/crate/crate/raw/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/test/java/org/elasticsearch/index/shard/IndexShardTestCase.java",
    "contents_url": "https://api.github.com/repos/crate/crate/contents/server/src/test/java/org/elasticsearch/index/shard/IndexShardTestCase.java?ref=711161de73ff20b9d7af3ac1df8ee5d310aef77d",
    "patch": "@@ -21,6 +21,7 @@\n  */\n package org.elasticsearch.index.shard;\n \n+import org.apache.lucene.index.DirectoryReader;\n import org.apache.lucene.index.IndexNotFoundException;\n import org.apache.lucene.store.Directory;\n import org.elasticsearch.Version;\n@@ -377,6 +378,27 @@ protected IndexShard newShard(ShardId shardId,\n         return newShard(shardId, primary, nodeId, indexMetadata, () -> {});\n     }\n \n+    /**\n+     * creates a new initializing shard. The shard will will be put in its proper path under the\n+     * current node id the shard is assigned to.\n+     * @param routing                shard routing to use\n+     * @param indexMetaData          indexMetaData for the shard, including any mapping\n+     * @param indexReaderWrapper     an optional wrapper to be used during search\n+     * @param globalCheckpointSyncer callback for syncing global checkpoints\n+     * @param listeners              an optional set of listeners to add to the shard\n+     */\n+    protected IndexShard newShard(ShardRouting routing, IndexMetadata indexMetaData,\n+                                  @Nullable EngineFactory engineFactory, Runnable globalCheckpointSyncer, RetentionLeaseSyncer retentionLeaseSyncer,\n+                                  IndexingOperationListener... listeners)\n+        throws IOException {\n+        // add node id as name to settings for proper logging\n+        final ShardId shardId = routing.shardId();\n+        final NodeEnvironment.NodePath nodePath = new NodeEnvironment.NodePath(createTempDir());\n+        ShardPath shardPath = new ShardPath(false, nodePath.resolve(shardId), nodePath.resolve(shardId), shardId);\n+        return newShard(routing, shardPath, indexMetaData, null, engineFactory, globalCheckpointSyncer,\n+                        retentionLeaseSyncer, EMPTY_EVENT_LISTENER, listeners);\n+    }\n+\n     /**\n      * creates a new initializing shard. The shard will will be put in its proper path under the\n      * supplied node id.\n@@ -784,21 +806,11 @@ protected final void recoverUnstartedReplica(final IndexShard replica,\n         final long startingSeqNo = recoveryTarget.indexShard().recoverLocallyUpToGlobalCheckpoint();\n         final StartRecoveryRequest request = PeerRecoveryTargetService.getStartRecoveryRequest(\n             logger, rNode, recoveryTarget, startingSeqNo);\n-        final RecoverySourceHandler recovery = new RecoverySourceHandler(\n-            primary,\n-            new AsyncRecoveryTarget(recoveryTarget, threadPool.generic()),\n-            threadPool,\n-            request,\n-            Math.toIntExact(ByteSizeUnit.MB.toBytes(1)),\n-            between(1, 8));\n-        primary.updateShardState(\n-            primary.routingEntry(),\n-            primary.getPendingPrimaryTerm(),\n-            null,\n-            currentClusterStateVersion.incrementAndGet(),\n-            inSyncIds,\n-            routingTable\n-        );\n+        final RecoverySourceHandler recovery = new RecoverySourceHandler(primary,\n+                                                                         new AsyncRecoveryTarget(recoveryTarget, threadPool.generic()), threadPool,\n+                                                                         request, Math.toIntExact(ByteSizeUnit.MB.toBytes(1)), between(1, 8));\n+        primary.updateShardState(primary.routingEntry(), primary.getPendingPrimaryTerm(), null,\n+                                 currentClusterStateVersion.incrementAndGet(), inSyncIds, routingTable);\n         try {\n             PlainActionFuture<RecoveryResponse> future = new PlainActionFuture<>();\n             recovery.recoverToTarget(future);\n@@ -810,6 +822,8 @@ protected final void recoverUnstartedReplica(final IndexShard replica,\n         }\n     }\n \n+\n+\n     protected void startReplicaAfterRecovery(IndexShard replica, IndexShard primary, Set<String> inSyncIds,\n                                              IndexShardRoutingTable routingTable) throws IOException {\n         ShardRouting initializingReplicaRouting = replica.routingEntry();\n@@ -820,34 +834,23 @@ protected void startReplicaAfterRecovery(IndexShard replica, IndexShard primary,\n                     .addShard(replica.routingEntry())\n                     .build() :\n                 new IndexShardRoutingTable.Builder(routingTable)\n-                .removeShard(initializingReplicaRouting)\n-                .addShard(replica.routingEntry())\n-                .build();\n+                    .removeShard(initializingReplicaRouting)\n+                    .addShard(replica.routingEntry())\n+                    .build();\n         Set<String> inSyncIdsWithReplica = new HashSet<>(inSyncIds);\n         inSyncIdsWithReplica.add(replica.routingEntry().allocationId().getId());\n         // update both primary and replica shard state\n-        primary.updateShardState(\n-            primary.routingEntry(),\n-            primary.getPendingPrimaryTerm(),\n-            null,\n-            currentClusterStateVersion.incrementAndGet(),\n-            inSyncIdsWithReplica,\n-            newRoutingTable\n-        );\n-        replica.updateShardState(\n-            replica.routingEntry().moveToStarted(),\n-            replica.getPendingPrimaryTerm(),\n-            null,\n-            currentClusterStateVersion.get(),\n-            inSyncIdsWithReplica,\n-            newRoutingTable\n-        );\n+        primary.updateShardState(primary.routingEntry(), primary.getPendingPrimaryTerm(), null,\n+                                 currentClusterStateVersion.incrementAndGet(), inSyncIdsWithReplica, newRoutingTable);\n+        replica.updateShardState(replica.routingEntry().moveToStarted(), replica.getPendingPrimaryTerm(), null,\n+                                 currentClusterStateVersion.get(), inSyncIdsWithReplica, newRoutingTable);\n     }\n \n \n-    /**\n-     * promotes a replica to primary, incrementing it's term and starting it if needed\n-     */\n+\n+                                 /**\n+                                  * promotes a replica to primary, incrementing it's term and starting it if needed\n+                                  */\n     protected void promoteReplica(IndexShard replica, Set<String> inSyncIds, IndexShardRoutingTable routingTable) throws IOException {\n         assertThat(inSyncIds, contains(replica.routingEntry().allocationId().getId()));\n         final ShardRouting routingEntry = newShardRouting("
  },
  {
    "sha": "91887d07165c3767440b11dbefefb41606bba26f",
    "filename": "server/src/test/java/org/elasticsearch/indices/recovery/RecoveryTests.java",
    "status": "added",
    "additions": 143,
    "deletions": 0,
    "changes": 143,
    "blob_url": "https://github.com/crate/crate/blob/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/test/java/org/elasticsearch/indices/recovery/RecoveryTests.java",
    "raw_url": "https://github.com/crate/crate/raw/711161de73ff20b9d7af3ac1df8ee5d310aef77d/server/src/test/java/org/elasticsearch/indices/recovery/RecoveryTests.java",
    "contents_url": "https://api.github.com/repos/crate/crate/contents/server/src/test/java/org/elasticsearch/indices/recovery/RecoveryTests.java?ref=711161de73ff20b9d7af3ac1df8ee5d310aef77d",
    "patch": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to Crate under one or more contributor license agreements.\n+ * See the NOTICE file distributed with this work for additional\n+ * information regarding copyright ownership.  Crate licenses this file\n+ * to you under the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.  You may\n+ * obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n+ * implied.  See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ *\n+ * However, if you have executed another commercial license agreement\n+ * with Crate these terms will supersede the license and you may use the\n+ * software solely pursuant to the terms of the relevant commercial\n+ * agreement.\n+ */\n+\n+package org.elasticsearch.indices.recovery;\n+\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.index.IndexCommit;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.flush.FlushRequest;\n+import org.elasticsearch.common.bytes.BytesArray;\n+import org.elasticsearch.common.lucene.uid.Versions;\n+import org.elasticsearch.common.xcontent.XContentType;\n+import org.elasticsearch.index.VersionType;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.mapper.SourceToParse;\n+import org.elasticsearch.index.replication.ESIndexLevelReplicationTestCase;\n+import org.elasticsearch.index.seqno.SequenceNumbers;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.store.Store;\n+import org.elasticsearch.index.translog.SnapshotMatchers;\n+import org.elasticsearch.index.translog.Translog;\n+import org.junit.Test;\n+\n+import java.util.List;\n+\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.lessThanOrEqualTo;\n+\n+public class RecoveryTests extends ESIndexLevelReplicationTestCase {\n+\n+    @Test\n+    public void testTranslogHistoryTransferred() throws Exception {\n+        try (ReplicationGroup shards = createGroup(0)) {\n+            shards.startPrimary();\n+            int docs = shards.indexDocs(1);\n+            getTranslog(shards.getPrimary()).rollGeneration();\n+            shards.flush();\n+            int moreDocs = shards.indexDocs(10);\n+            shards.addReplica();\n+            shards.startAll();\n+            final IndexShard replica = shards.getReplicas().get(0);\n+            boolean softDeletesEnabled = replica.indexSettings().isSoftDeleteEnabled();\n+            int actual = getTranslog(replica).totalOperations();\n+            assertThat(actual, equalTo(softDeletesEnabled ? 0 : docs + moreDocs));\n+            shards.assertAllEqual(docs + moreDocs);\n+        }\n+    }\n+\n+    public void testSequenceBasedRecoveryKeepsTranslog() throws Exception {\n+        try (ReplicationGroup shards = createGroup(1)) {\n+            shards.startAll();\n+            final IndexShard replica = shards.getReplicas().get(0);\n+            final int initDocs = scaledRandomIntBetween(0, 20);\n+            int uncommittedDocs = 0;\n+            for (int i = 0; i < initDocs; i++) {\n+                shards.indexDocs(1);\n+                uncommittedDocs++;\n+                if (randomBoolean()) {\n+                    shards.syncGlobalCheckpoint();\n+                    shards.flush();\n+                    uncommittedDocs = 0;\n+                }\n+            }\n+            shards.removeReplica(replica);\n+            final int moreDocs = shards.indexDocs(scaledRandomIntBetween(0, 20));\n+            if (randomBoolean()) {\n+                shards.flush();\n+            }\n+            replica.close(\"test\", randomBoolean());\n+            replica.store().close();\n+            final IndexShard newReplica = shards.addReplicaWithExistingPath(replica.shardPath(), replica.routingEntry().currentNodeId());\n+            shards.recoverReplica(newReplica);\n+\n+            Translog translog = getTranslog(newReplica);\n+            try (Translog.Snapshot snapshot = translog.newSnapshot()) {\n+                if (newReplica.indexSettings().isSoftDeleteEnabled()) {\n+                    assertThat(snapshot.totalOperations(), equalTo(0));\n+                } else {\n+                    assertThat(\"Sequence based recovery should keep existing translog\", snapshot, SnapshotMatchers.size(initDocs + moreDocs));\n+                }\n+            }\n+            assertThat(newReplica.recoveryState().getTranslog().recoveredOperations(), equalTo(uncommittedDocs + moreDocs));\n+            assertThat(newReplica.recoveryState().getIndex().fileDetails().size(), is(0));\n+        }\n+    }\n+\n+    public void testPeerRecoverySendSafeCommitInFileBased() throws Exception {\n+        IndexShard primaryShard = newStartedShard(true);\n+        int numDocs = between(1, 100);\n+        long globalCheckpoint = 0;\n+        for (int i = 0; i < numDocs; i++) {\n+            Engine.IndexResult result = primaryShard.applyIndexOperationOnPrimary(Versions.MATCH_ANY, VersionType.INTERNAL,\n+                                                                                  new SourceToParse(primaryShard.shardId().getIndexName(), \"_doc\",new BytesArray(\"{}\"), XContentType.JSON),\n+                                                                                  SequenceNumbers.UNASSIGNED_SEQ_NO, 0, -1L, false);\n+            assertThat(result.getResultType(), equalTo(Engine.Result.Type.SUCCESS));\n+            if (randomBoolean()) {\n+                globalCheckpoint = randomLongBetween(globalCheckpoint, i);\n+                primaryShard.updateLocalCheckpointForShard(primaryShard.routingEntry().allocationId().getId(), globalCheckpoint);\n+                primaryShard.updateGlobalCheckpointForShard(primaryShard.routingEntry().allocationId().getId(), globalCheckpoint);\n+                primaryShard.flush(new FlushRequest());\n+            }\n+        }\n+        IndexShard replicaShard = newShard(primaryShard.shardId(), false);\n+        updateMappings(replicaShard, primaryShard.indexSettings().getIndexMetadata());\n+        recoverReplica(replicaShard, primaryShard, (r, sourceNode) -> new RecoveryTarget(r, sourceNode, recoveryListener) {\n+            @Override\n+            public void prepareForTranslogOperations(int totalTranslogOps, ActionListener<Void> listener) {\n+                super.prepareForTranslogOperations(totalTranslogOps, listener);\n+                assertThat(replicaShard.getLastKnownGlobalCheckpoint(), equalTo(primaryShard.getLastKnownGlobalCheckpoint()));\n+            }\n+            @Override\n+            public void cleanFiles(int totalTranslogOps, long globalCheckpoint, Store.MetadataSnapshot sourceMetaData,\n+                                   ActionListener<Void> listener) {\n+                assertThat(globalCheckpoint, equalTo(primaryShard.getLastKnownGlobalCheckpoint()));\n+                super.cleanFiles(totalTranslogOps, globalCheckpoint, sourceMetaData, listener);\n+            }\n+        }, true, true);\n+        List<IndexCommit> commits = DirectoryReader.listCommits(replicaShard.store().directory());\n+        long maxSeqNo = Long.parseLong(commits.get(0).getUserData().get(SequenceNumbers.MAX_SEQ_NO));\n+        assertThat(maxSeqNo, lessThanOrEqualTo(globalCheckpoint));\n+        closeShards(primaryShard, replicaShard);\n+    }\n+}"
  }
]
