[
  {
    "sha": "4808b2b6fd63b9b7242a1abea02226ce5411b84c",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/ClusterRupture.java",
    "status": "modified",
    "additions": 130,
    "deletions": 87,
    "changes": 217,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/ClusterRupture.java",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/ClusterRupture.java",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/ClusterRupture.java?ref=541efed3666ce0a47e03a345aada595633351042",
    "patch": "@@ -16,23 +16,22 @@\n import java.util.HashSet;\n import java.util.List;\n import java.util.Map;\n-import java.util.Set;\n \n import org.opensha.sha.earthquake.faultSysSolution.ruptures.FaultSubsectionCluster.JumpStub;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.strategies.ClusterConnectionStrategy;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.util.ComplexRuptureTreeNavigator;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.util.RuptureConnectionSearch;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.util.RuptureTreeNavigator;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.util.SectionDistanceAzimuthCalculator;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.util.SingleStrandRuptureTreeNavigator;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.util.UniqueRupture;\n import org.opensha.sha.faultSurface.FaultSection;\n \n import com.google.common.base.Preconditions;\n import com.google.common.collect.HashBasedTable;\n import com.google.common.collect.HashMultimap;\n-import com.google.common.collect.ImmutableCollection;\n+import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n-import com.google.common.collect.ImmutableMultimap;\n-import com.google.common.collect.ImmutableSet;\n import com.google.common.collect.Iterables;\n import com.google.common.collect.Lists;\n import com.google.common.collect.Multimap;\n@@ -70,9 +69,9 @@\n \t */\n \tpublic final FaultSubsectionCluster[] clusters;\n \t/**\n-\t * Jumps internal to the primary strand, between the primary clusters\n+\t * Jumps internal to the primary strand, between the primary clusters, in order\n \t */\n-\tpublic final ImmutableSet<Jump> internalJumps;\n+\tpublic final ImmutableList<Jump> internalJumps;\n \t/**\n \t * Jumps to each splay sub-rupture\n \t */\n@@ -91,24 +90,31 @@\n \t */\n \tpublic final UniqueRupture internalUnique;\n \t\n+\t/**\n+\t * True if this rupture is a single strand of connected clusters, i.e., each jump occurs between the last section\n+\t * of the prior cluster and the first section of the next.\n+\t */\n+\tpublic final boolean singleStrand;\n+\t\n \t/**\n \t * Initiate a ClusterRupture with the given starting cluster. You can grow it later with the\n \t * take(Jump) method\n \t * @param cluster\n \t */\n \tpublic ClusterRupture(FaultSubsectionCluster cluster) {\n-\t\tthis(new FaultSubsectionCluster[] {cluster}, ImmutableSet.of(),\n-\t\t\t\tImmutableMap.of(), cluster.unique, cluster.unique);\n+\t\tthis(new FaultSubsectionCluster[] {cluster}, ImmutableList.of(),\n+\t\t\t\tImmutableMap.of(), cluster.unique, cluster.unique, true);\n \t}\n \n-\tprotected ClusterRupture(FaultSubsectionCluster[] clusters, ImmutableSet<Jump> internalJumps,\n-\t\t\tImmutableMap<Jump, ClusterRupture> splays, UniqueRupture unique, UniqueRupture internalUnique) {\n+\tprotected ClusterRupture(FaultSubsectionCluster[] clusters, ImmutableList<Jump> internalJumps,\n+\t\t\tImmutableMap<Jump, ClusterRupture> splays, UniqueRupture unique, UniqueRupture internalUnique, boolean singleStrand) {\n \t\tsuper();\n \t\tthis.clusters = clusters;\n \t\tthis.internalJumps = internalJumps;\n \t\tthis.splays = splays;\n \t\tthis.unique = unique;\n \t\tthis.internalUnique = internalUnique;\n+\t\tthis.singleStrand = singleStrand;\n \t\tPreconditions.checkState(internalUnique.size() <= unique.size());\n \t\tint expectedJumps = clusters.length-1;\n \t\tPreconditions.checkState(internalJumps.size() == expectedJumps,\n@@ -198,44 +204,61 @@ public ClusterRupture take(Jump jump) {\n \t\tPreconditions.checkState(!contains(jump.toSection),\n \t\t\t\t\"Cannot take jump because this rupture already has the toSection: %s\", jump);\n \t\t\n-\t\tUniqueRupture newUnique = new UniqueRupture(this.unique, jump.toCluster);\n+\t\tUniqueRupture newUnique = UniqueRupture.add(this.unique, jump.toCluster.unique);\n \t\tint expectedCount = this.unique.size() + jump.toCluster.subSects.size();\n \t\tPreconditions.checkState(newUnique.size() == expectedCount,\n \t\t\t\t\"Duplicate subsections. Have %s unique, %s total\", newUnique.size(), expectedCount);\n \t\t\n+\t\tboolean newSingleStrand = singleStrand && jump.toCluster.subSects.get(0).equals(jump.toSection);\n+\t\t\n \t\tif (containsInternal(jump.fromSection)) {\n \t\t\t// it's on the main strand\n \t\t\tFaultSubsectionCluster lastCluster = clusters[clusters.length-1];\n \t\t\tFaultSubsectionCluster[] newClusters;\n \t\t\tImmutableMap<Jump, ClusterRupture> newSplays;\n-\t\t\tImmutableSet<Jump> newInternalJumps;\n+\t\t\tImmutableList<Jump> newInternalJumps;\n \t\t\tUniqueRupture newInternalUnique;\n \t\t\tif (lastCluster.endSects.contains(jump.fromSection)) {\n \t\t\t\t// regular jump from the end\n //\t\t\t\tSystem.out.println(\"Taking a regular jump to extend a strand\");\n+\t\t\t\tPreconditions.checkState(lastCluster.equals(jump.fromCluster),\n+\t\t\t\t\t\t\"Cannot take jump %s: it's from a section on the last cluster, but fromCluster=%s doesn't match lastCluster=%s\",\n+\t\t\t\t\t\tjump, jump.fromCluster, lastCluster);\n \t\t\t\tnewClusters = Arrays.copyOf(clusters, clusters.length+1);\n \t\t\t\tnewClusters[clusters.length] = jump.toCluster;\n \t\t\t\tnewSplays = splays;\n-\t\t\t\tImmutableSet.Builder<Jump> internalJumpBuild =\n-\t\t\t\t\t\tImmutableSet.builderWithExpectedSize(internalJumps.size()+1);\n+\t\t\t\tImmutableList.Builder<Jump> internalJumpBuild =\n+\t\t\t\t\t\tImmutableList.builderWithExpectedSize(internalJumps.size()+1);\n \t\t\t\tinternalJumpBuild.addAll(internalJumps);\n \t\t\t\tinternalJumpBuild.add(jump);\n \t\t\t\tnewInternalJumps = internalJumpBuild.build();\n-\t\t\t\tnewInternalUnique = new UniqueRupture(internalUnique, jump.toCluster);\n+\t\t\t\tnewInternalUnique = UniqueRupture.add(internalUnique, jump.toCluster.unique);\n+\t\t\t\tnewSingleStrand = newSingleStrand && lastCluster.subSects.get(lastCluster.subSects.size()-1).equals(jump.fromSection);\n \t\t\t} else {\n \t\t\t\t// it's a new splay\n //\t\t\t\tSystem.out.println(\"it's a new splay!\");\n+\t\t\t\tboolean found = false;\n+\t\t\t\tfor (FaultSubsectionCluster cluster : clusters) {\n+\t\t\t\t\tif (cluster.equals(jump.fromCluster)) {\n+\t\t\t\t\t\tfound = true;\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tPreconditions.checkState(found, \"Cannot take jump=%s: fromCluster=%s not found in rupture: %s\",\n+\t\t\t\t\t\tjump, jump.fromCluster, this);\n \t\t\t\tnewClusters = clusters;\n \t\t\t\tImmutableMap.Builder<Jump, ClusterRupture> splayBuilder = ImmutableMap.builder();\n \t\t\t\tsplayBuilder.putAll(splays);\n \t\t\t\tsplayBuilder.put(jump, new ClusterRupture(jump.toCluster));\n \t\t\t\tnewSplays = splayBuilder.build();\n \t\t\t\tnewInternalJumps = internalJumps;\n \t\t\t\tnewInternalUnique = internalUnique;\n+\t\t\t\tnewSingleStrand = false;\n \t\t\t}\n-\t\t\treturn new ClusterRupture(newClusters, newInternalJumps, newSplays, newUnique, newInternalUnique);\n+\t\t\treturn new ClusterRupture(newClusters, newInternalJumps, newSplays, newUnique, newInternalUnique, newSingleStrand);\n \t\t} else {\n \t\t\t// it's on a splay, grow that\n+\t\t\tnewSingleStrand = false;\n \t\t\tboolean found = false;\n \t\t\tImmutableMap.Builder<Jump, ClusterRupture> splayBuilder = ImmutableMap.builder();\n \t\t\tfor (Jump splayJump : splays.keySet()) {\n@@ -254,7 +277,7 @@ public ClusterRupture take(Jump jump) {\n \t\t\tPreconditions.checkState(found,\n \t\t\t\t\t\"From section for jump not found in rupture (including splays): %s\", jump);\n \t\t\treturn new ClusterRupture(clusters, internalJumps, splayBuilder.build(),\n-\t\t\t\t\tnewUnique, internalUnique);\n+\t\t\t\t\tnewUnique, internalUnique, newSingleStrand);\n \t\t}\n \t}\n \t\n@@ -284,31 +307,30 @@ public ClusterRupture take(Jump jump) {\n \t * but the last section of a cluster\n \t */\n \tpublic ClusterRupture reversed() {\n-\t\tPreconditions.checkState(splays.isEmpty(), \"Can't reverse a splayed rupture\");\n+\t\tPreconditions.checkState(singleStrand, \"Can only reverse single strand ruptures\");\n \t\t\n \t\tList<FaultSubsectionCluster> clusterList = new ArrayList<>();\n \t\tfor (int i=clusters.length; --i>=0;)\n \t\t\tclusterList.add(clusters[i].reversed());\n+\t\t\n \t\tTable<FaultSection, FaultSection, Jump> jumpsTable = HashBasedTable.create();\n-\t\tfor (Jump jump : internalJumps) {\n-\t\t\tPreconditions.checkState(\n-\t\t\t\t\tjump.fromSection == jump.fromCluster.subSects.get(jump.fromCluster.subSects.size()-1),\n-\t\t\t\t\t\"Can't reverse a ClusterRupture which contains a non-splay jump from a subsection \"\n-\t\t\t\t\t+ \"which is not the last subsection on that cluster\");\n+\t\tfor (Jump jump : internalJumps)\n \t\t\tjumpsTable.put(jump.fromSection, jump.toSection, jump);\n-\t\t}\n-\t\tImmutableSet.Builder<Jump> jumpsBuilder = ImmutableSet.builder();\n+\t\t\n+\t\tImmutableList.Builder<Jump> jumpsBuilder = ImmutableList.builder();\n \t\tfor (int i=1; i<clusterList.size(); i++) {\n \t\t\tFaultSubsectionCluster fromCluster = clusterList.get(i-1);\n \t\t\tFaultSection fromSection = fromCluster.subSects.get(fromCluster.subSects.size()-1);\n \t\t\tFaultSubsectionCluster toCluster = clusterList.get(i);\n \t\t\tFaultSection toSection = toCluster.startSect;\n \t\t\tJump jump = jumpsTable.get(toSection, fromSection); // get old, non-reversed jump\n+\t\t\tPreconditions.checkNotNull(jump, \"No jump found from %s to %s in rupture:\\n\\t%s\\n\\tAvailable jumps:%s\",\n+\t\t\t\t\ttoSection.getSectionId(), fromSection.getSectionId(), this, jumpsTable);\n \t\t\tjumpsBuilder.add(new Jump(fromSection, fromCluster, toSection, toCluster, jump.distance));\n \t\t}\n \t\t\n \t\treturn new ClusterRupture(clusterList.toArray(new FaultSubsectionCluster[0]), jumpsBuilder.build(),\n-\t\t\t\tImmutableMap.of(), unique, internalUnique);\n+\t\t\t\tImmutableMap.of(), unique, internalUnique, singleStrand);\n \t}\n \n \t/**\n@@ -322,28 +344,10 @@ public ClusterRupture reversed() {\n \tpublic List<ClusterRupture> getPreferredAltRepresentations(RuptureConnectionSearch connSearch) {\n \t\tList<ClusterRupture> inversions = new ArrayList<>();\n \n-\t\tif (splays.isEmpty()) {\n+\t\tif (singleStrand) {\n \t\t\t// simple\n-\t\t\tif (clusters.length == 1) {\n-\t\t\t\tinversions.add(reversed());\n-\t\t\t\treturn inversions;\n-\t\t\t}\n-\n-\t\t\tif (clusters.length > 1) {\n-\t\t\t\t// check if it's truly single strand\n-\t\t\t\tboolean match = true;\n-\t\t\t\tfor (Jump jump : getJumpsIterable()) {\n-\t\t\t\t\tif (jump.fromSection != jump.fromCluster.subSects.get(jump.fromCluster.subSects.size()-1)\n-\t\t\t\t\t\t\t|| jump.toSection != jump.toCluster.startSect) {\n-\t\t\t\t\t\tmatch = false;\n-\t\t\t\t\t\tbreak;\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tif (match) {\n-\t\t\t\t\tinversions.add(reversed());\n-\t\t\t\t\treturn inversions;\n-\t\t\t\t}\n-\t\t\t}\n+\t\t\tinversions.add(reversed());\n+\t\t\treturn inversions;\n \t\t}\n \n \t\t// complex\n@@ -395,7 +399,7 @@ public ClusterRupture reversed() {\n \tpublic List<ClusterRupture> getAllAltRepresentations(ClusterConnectionStrategy connStrat, int maxNumSplays) {\n \t\tList<ClusterRupture> inversions = new ArrayList<>();\n \n-\t\tif (getTotalNumClusters() == 1) {\n+\t\tif (getTotalNumClusters() == 1 || singleStrand) {\n \t\t\tinversions.add(reversed());\n \t\t\treturn inversions;\n \t\t}\n@@ -411,18 +415,18 @@ public ClusterRupture reversed() {\n \t\t\tfor (FaultSubsectionCluster avail : availableClusters)\n \t\t\t\tif (avail != startCluster)\n \t\t\t\t\tnextAvailable.add(avail);\n-\t\t\tbuildInversionRecursive(inversions, new ClusterRupture(startCluster),\n+\t\t\tbuildAltRepsRecursive(inversions, new ClusterRupture(startCluster),\n \t\t\t\t\tnextAvailable, connStrat, maxNumSplays);\n \t\t\tif (startCluster.subSects.size() > 1)\n \t\t\t\t// try it in reverse as well\n-\t\t\t\tbuildInversionRecursive(inversions, new ClusterRupture(startCluster.reversed()),\n+\t\t\t\tbuildAltRepsRecursive(inversions, new ClusterRupture(startCluster.reversed()),\n \t\t\t\t\t\tnextAvailable, connStrat, maxNumSplays);\n \t\t}\n \n \t\treturn inversions;\n \t}\n \n-\tprivate void buildInversionRecursive(List<ClusterRupture> inversions,\n+\tprivate void buildAltRepsRecursive(List<ClusterRupture> inversions,\n \t\t\tClusterRupture curRupture,\n \t\t\tHashSet<FaultSubsectionCluster> availableClusters,\n \t\t\tClusterConnectionStrategy connStrat, int maxNumSplays) {\n@@ -481,7 +485,7 @@ private void buildInversionRecursive(List<ClusterRupture> inversions,\n \t\t\t\t\t\t\tcontinue;\n \t\t\t\t\t\t}\n \n-\t\t\t\t\t\tbuildInversionRecursive(inversions, nextRupture,\n+\t\t\t\t\t\tbuildAltRepsRecursive(inversions, nextRupture,\n \t\t\t\t\t\t\t\tnextAvailable, connStrat, maxNumSplays);\n \t\t\t\t\t}\n \t\t\t\t}\n@@ -494,8 +498,12 @@ private void buildInversionRecursive(List<ClusterRupture> inversions,\n \t * @return Rupture tree navigator (lazily initialized)\n \t */\n \tpublic RuptureTreeNavigator getTreeNavigator() {\n-\t\tif (navigator == null)\n-\t\t\tnavigator = new RuptureTreeNavigator(this);\n+\t\tif (navigator == null) {\n+\t\t\tif (singleStrand)\n+\t\t\t\tnavigator = new SingleStrandRuptureTreeNavigator(this);\n+\t\t\telse\n+\t\t\t\tnavigator = new ComplexRuptureTreeNavigator(this);\n+\t\t}\n \t\treturn navigator;\n \t}\n \t\n@@ -525,7 +533,7 @@ public static ClusterRupture forOrderedSingleStrandRupture(List<? extends FaultS\n \t\t\n \t\tFaultSubsectionCluster[] clusters = clusterList.toArray(new FaultSubsectionCluster[0]);\n \t\t\n-\t\tImmutableSet.Builder<Jump> jumpsBuilder = ImmutableSet.builder();\n+\t\tImmutableList.Builder<Jump> jumpsBuilder = ImmutableList.builder();\n \t\tfor (int i=1; i<clusters.length; i++) {\n \t\t\tFaultSubsectionCluster fromCluster = clusters[i-1];\n \t\t\tFaultSection fromSect = fromCluster.subSects.get(fromCluster.subSects.size()-1);\n@@ -540,14 +548,29 @@ public static ClusterRupture forOrderedSingleStrandRupture(List<? extends FaultS\n \t\t\n \t\tUniqueRupture unique = UniqueRupture.forClusters(clusters);\n \t\t\n-\t\treturn new ClusterRupture(clusters, jumpsBuilder.build(), ImmutableMap.of(), unique, unique);\n+\t\treturn new ClusterRupture(clusters, jumpsBuilder.build(), ImmutableMap.of(), unique, unique, true);\n \t}\n \t\n \t@Override\n \tpublic String toString() {\n \t\tStringBuilder str = new StringBuilder();\n-\t\tfor (FaultSubsectionCluster cluster : clusters)\n-\t\t\tstr.append(cluster.toString());\n+\t\tif (singleStrand) {\n+\t\t\tfor (FaultSubsectionCluster cluster : clusters)\n+\t\t\t\tstr.append(cluster.toString());\n+\t\t} else {\n+\t\t\tRuptureTreeNavigator nav = getTreeNavigator();\n+\t\t\tfor (int i=0; i<clusters.length; i++) {\n+\t\t\t\tFaultSubsectionCluster cluster = clusters[i];\n+\t\t\t\tif (i > 0) {\n+\t\t\t\t\tint toID = nav.getJump(clusters[i-1], cluster).toSection.getSectionId();\n+\t\t\t\t\tif (toID == cluster.subSects.get(0).getSectionId())\n+\t\t\t\t\t\ttoID = -1;\n+\t\t\t\t\tstr.append(cluster.toString(toID));\n+\t\t\t\t} else {\n+\t\t\t\t\tstr.append(cluster.toString());\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n \t\tfor (Jump jump : splays.keySet()) {\n \t\t\tClusterRupture splay = splays.get(jump);\n \t\t\tstr.append(\"\\n\\t--splay from [\").append(jump.fromCluster.parentSectionID);\n@@ -605,7 +628,7 @@ public String toString() {\n \t\n \tpublic static void writeJSON(File jsonFile, List<ClusterRupture> ruptures,\n \t\t\tList<? extends FaultSection> subSects) throws IOException {\n-\t\tGson gson = buildGson(subSects);\n+\t\tGson gson = buildGson(subSects, ruptures.size()<1000);\n \t\tFileWriter fw = new FileWriter(jsonFile);\n \t\tType listType = new TypeToken<List<ClusterRupture>>(){}.getType();\n \t\tgson.toJson(ruptures, listType, fw);\n@@ -624,7 +647,7 @@ public static void writeJSON(File jsonFile, List<ClusterRupture> ruptures,\n \t}\n \t\n \tpublic static List<ClusterRupture> readJSON(Reader json, List<? extends FaultSection> subSects) {\n-\t\tGson gson = buildGson(subSects);\n+\t\tGson gson = buildGson(subSects, false);\n \t\tType listType = new TypeToken<List<ClusterRupture>>(){}.getType();\n \t\tList<ClusterRupture> ruptures = gson.fromJson(json, listType);\n \t\ttry {\n@@ -633,9 +656,10 @@ public static void writeJSON(File jsonFile, List<ClusterRupture> ruptures,\n \t\treturn ruptures;\n \t}\n \t\n-\tprivate static Gson buildGson(List<? extends FaultSection> subSects) {\n+\tprivate static Gson buildGson(List<? extends FaultSection> subSects, boolean pretty) {\n \t\tGsonBuilder builder = new GsonBuilder();\n-//\t\tbuilder.setPrettyPrinting(); // extra whitespace makes these large files large, don't use here\n+\t\tif (pretty)\n+\t\t\tbuilder.setPrettyPrinting(); // extra whitespace makes these large files large\n \t\tbuilder.registerTypeAdapter(ClusterRupture.class, new Adapter(subSects));\n \t\tGson gson = builder.create();\n \t\t\n@@ -645,9 +669,11 @@ private static Gson buildGson(List<? extends FaultSection> subSects) {\n \tpublic static class Adapter extends TypeAdapter<ClusterRupture> {\n \t\t\n \t\tprivate List<? extends FaultSection> subSects;\n+\t\tprivate HashMap<FaultSubsectionCluster, FaultSubsectionCluster> prevClustersMap;\n \n \t\tpublic Adapter(List<? extends FaultSection> subSects) {\n \t\t\tthis.subSects = subSects;\n+\t\t\tthis.prevClustersMap = new HashMap<>();\n \t\t}\n \n \t\t@Override\n@@ -664,6 +690,10 @@ public void write(JsonWriter out, ClusterRupture rupture) throws IOException {\n \t\t\t\t\tif (jump.fromCluster == cluster)\n \t\t\t\t\t\tnewCluster.addConnection(new Jump(jump.fromSection, newCluster,\n \t\t\t\t\t\t\t\tjump.toSection, jump.toCluster, jump.distance));\n+\t\t\t\tfor (Jump jump : rupture.splays.keySet())\n+\t\t\t\t\tif (jump.fromCluster == cluster)\n+\t\t\t\t\t\tnewCluster.addConnection(new Jump(jump.fromSection, newCluster,\n+\t\t\t\t\t\t\t\tjump.toSection, jump.toCluster, jump.distance));\n \t\t\t\tnewCluster.writeJSON(out);\n \t\t\t}\n \t\t\tout.endArray(); // ]\n@@ -688,26 +718,32 @@ public ClusterRupture read(JsonReader in) throws IOException {\n \t\t\t\n \t\t\tin.beginObject(); // {\n \t\t\t\n-\t\t\tswitch (in.nextName()) {\n-\t\t\tcase \"clusters\":\n-\t\t\t\tinternalClusterList = new ArrayList<>();\n-\t\t\t\tin.beginArray();\n-\t\t\t\twhile (in.hasNext())\n-\t\t\t\t\tinternalClusterList.add(FaultSubsectionCluster.readJSON(in, subSects, jumpStubsMap));\n-\t\t\t\tin.endArray();\n-\t\t\t\tbreak;\n-\t\t\tcase \"splays\":\n-\t\t\t\tsplayList = new ArrayList<>();\n-\t\t\t\tin.beginArray();\n-\t\t\t\twhile (in.hasNext())\n-\t\t\t\t\tsplayList.add(read(in));\n-\t\t\t\tin.endArray();\n-\t\t\t\tbreak;\n+\t\t\twhile (in.hasNext()) {\n+\t\t\t\tswitch (in.nextName()) {\n+\t\t\t\tcase \"clusters\":\n+//\t\t\t\t\tSystem.out.println(\"in clsters: \"+in.getPath());\n+\t\t\t\t\tinternalClusterList = new ArrayList<>();\n+\t\t\t\t\tin.beginArray();\n+\t\t\t\t\twhile (in.hasNext())\n+\t\t\t\t\t\tinternalClusterList.add(\n+\t\t\t\t\t\t\t\tFaultSubsectionCluster.readJSON(in, subSects, jumpStubsMap, prevClustersMap));\n+\t\t\t\t\tin.endArray();\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase \"splays\":\n+//\t\t\t\t\tSystem.out.println(\"in splays: \"+in.getPath());\n+\t\t\t\t\tsplayList = new ArrayList<>();\n+\t\t\t\t\tin.beginArray();\n+\t\t\t\t\twhile (in.hasNext())\n+\t\t\t\t\t\tsplayList.add(read(in));\n+\t\t\t\t\tin.endArray();\n+\t\t\t\t\tbreak;\n \n-\t\t\tdefault:\n-\t\t\t\tbreak;\n+\t\t\t\tdefault:\n+\t\t\t\t\tbreak;\n+\t\t\t\t}\n \t\t\t}\n \t\t\t\n+//\t\t\tSystem.out.println(in.getPath());\n \t\t\tin.endObject(); // }\n \t\t\t\n \t\t\t// reconcile all internal jumps\n@@ -722,15 +758,21 @@ public ClusterRupture read(JsonReader in) throws IOException {\n \t\t\t}\n \t\t\tFaultSubsectionCluster.buildJumpsFromStubs(targetClusters, jumpStubsMap);\n \t\t\t\n-\t\t\tImmutableSet.Builder<Jump> internalJumpsBuilder = ImmutableSet.builder();\n+\t\t\tImmutableList.Builder<Jump> internalJumpsBuilder = ImmutableList.builder();\n \t\t\tFaultSubsectionCluster[] clusters = internalClusterList.toArray(new FaultSubsectionCluster[0]);\n+\t\t\tboolean singleStrand = true;\n \t\t\tfor (int i=0; i<clusters.length-1; i++) { // -1 as no internal jumps from last section\n-\t\t\t\tFaultSubsectionCluster cluster = internalClusterList.get(i);\n+\t\t\t\tFaultSubsectionCluster cluster = clusters[i];\n+\t\t\t\tFaultSubsectionCluster nextCluster = clusters[i+1];\n \t\t\t\tCollection<Jump> connections = cluster.getConnectionsTo(clusters[i+1]);\n-\t\t\t\tPreconditions.checkState(connections.size() == 1, \"Internal jump not found?\");\n-\t\t\t\tinternalJumpsBuilder.addAll(connections);\n+\t\t\t\tPreconditions.checkState(connections.size() == 1, \"Expected 1 jump from %s to %s, have %s?\",\n+\t\t\t\t\t\tcluster, nextCluster, connections.size());\n+\t\t\t\tJump jump = connections.iterator().next();\n+\t\t\t\tsingleStrand = singleStrand && jump.fromSection.equals(cluster.subSects.get(cluster.subSects.size()-1))\n+\t\t\t\t\t\t&& jump.toSection.equals(nextCluster.subSects.get(0));\n+\t\t\t\tinternalJumpsBuilder.add(jump);\n \t\t\t}\n-\t\t\tImmutableSet<Jump> internalJumps = internalJumpsBuilder.build();\n+\t\t\tImmutableList<Jump> internalJumps = internalJumpsBuilder.build();\n \t\t\t\n \t\t\tImmutableMap<Jump, ClusterRupture> splays;\n \t\t\tUniqueRupture internalUnique = UniqueRupture.forClusters(clusters);\n@@ -739,8 +781,9 @@ public ClusterRupture read(JsonReader in) throws IOException {\n \t\t\t\tsplays = ImmutableMap.of();\n \t\t\t\tunique = internalUnique;\n \t\t\t} else {\n+\t\t\t\tsingleStrand = false;\n \t\t\t\tImmutableMap.Builder<Jump, ClusterRupture> splayBuilder = ImmutableMap.builder();\n-\t\t\t\t// reverse as we are going to serach for them in reverse order below, so double reverse\n+\t\t\t\t// reverse as we are going to search for them in reverse order below, so double reverse\n \t\t\t\t// ensures that they are loaded in order (which shouldn't matter, but is nice)\n \t\t\t\tCollections.reverse(splayList);\n \t\t\t\tfor (FaultSubsectionCluster cluster : clusters) {\n@@ -763,10 +806,10 @@ public ClusterRupture read(JsonReader in) throws IOException {\n \t\t\t\tsplays = splayBuilder.build();\n \t\t\t\tunique = internalUnique;\n \t\t\t\tfor (ClusterRupture splay : splays.values())\n-\t\t\t\t\tunique = new UniqueRupture(unique, splay.unique);\n+\t\t\t\t\tunique = UniqueRupture.add(unique, splay.unique);\n \t\t\t}\n \t\t\t\n-\t\t\treturn new ClusterRupture(clusters, internalJumps, splays, unique, internalUnique);\n+\t\t\treturn new ClusterRupture(clusters, internalJumps, splays, unique, internalUnique, singleStrand);\n \t\t}\n \t\t\n \t}"
  },
  {
    "sha": "4e1b67ee737385d1b77298e5ffd9c57f897e4bad",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/ClusterRuptureBuilder.java",
    "status": "modified",
    "additions": 708,
    "deletions": 226,
    "changes": 934,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/ClusterRuptureBuilder.java",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/ClusterRuptureBuilder.java",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/ClusterRuptureBuilder.java?ref=541efed3666ce0a47e03a345aada595633351042",
    "patch": "@@ -4,42 +4,52 @@\n import java.io.IOException;\n import java.text.DecimalFormat;\n import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n import java.util.HashSet;\n import java.util.List;\n+import java.util.Map;\n import java.util.concurrent.Callable;\n+import java.util.concurrent.ExecutionException;\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Executors;\n import java.util.concurrent.Future;\n import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n \n+import org.dom4j.Document;\n import org.dom4j.DocumentException;\n+import org.dom4j.Element;\n import org.opensha.commons.util.ExceptionUtils;\n import org.opensha.commons.util.FaultUtils;\n+import org.opensha.commons.util.XMLUtils;\n+import org.opensha.refFaultParamDb.vo.FaultSectionPrefData;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.PlausibilityConfiguration;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.PlausibilityConfiguration.Builder;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.PlausibilityFilter;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.CumulativeProbabilityFilter;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.JumpAzimuthChangeFilter;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.CumulativePenaltyFilter.*;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.CumulativeProbabilityFilter.*;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.strategies.ClusterConnectionStrategy;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.strategies.ClusterPermutationStrategy;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.strategies.DistCutoffClosestSectClusterConnectionStrategy;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.strategies.SectCountAdaptivePermutationStrategy;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.strategies.ConnectionPointsPermutationStrategy;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.strategies.UCERF3ClusterConnectionStrategy;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.strategies.UCERF3ClusterPermuationStrategy;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.*;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.coulomb.*;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.path.*;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.prob.*;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.strategies.*;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.util.FilterDataClusterRupture;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.util.GeoJSONFaultReader;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.util.GeoJSONFaultReader.GeoSlipRateRecord;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.util.SectionDistanceAzimuthCalculator;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.util.UniqueRupture;\n import org.opensha.sha.faultSurface.FaultSection;\n+import org.opensha.sha.simulators.stiffness.AggregatedStiffnessCache;\n+import org.opensha.sha.simulators.stiffness.AggregatedStiffnessCalculator;\n import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator;\n-import org.opensha.sha.simulators.stiffness.RuptureCoulombResult.RupCoulombQuantity;\n-import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator.StiffnessAggregationMethod;\n+import org.opensha.sha.simulators.stiffness.AggregatedStiffnessCalculator.AggregationMethod;\n+import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator.PatchAlignment;\n import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator.StiffnessType;\n \n+import com.google.common.base.Joiner;\n import com.google.common.base.Preconditions;\n import com.google.common.base.Stopwatch;\n+import com.google.common.collect.Range;\n import com.google.common.primitives.Ints;\n \n import scratch.UCERF3.FaultSystemRupSet;\n@@ -101,15 +111,28 @@ public void setDebugCriteria(RupDebugCriteria debugCriteria, boolean stopAfterMa\n \t\tthis.stopAfterDebugMatch = stopAfterMatch;\n \t}\n \t\n-\tprivate class RupSizeTracker {\n+\tprivate class ProgressTracker {\n+\t\t// rupture size & count tracking\n \t\tprivate int largestRup;\n \t\tprivate int largestRupPrintMod = 10;\n+\t\tprivate int rupCountPrintMod = 1000;\n \t\tprivate HashSet<UniqueRupture> allPassedUniques;\n \t\t\n-\t\tpublic RupSizeTracker() {\n+\t\t// start cluster tracking\n+\t\tprivate HashSet<Integer> startClusterIDs = new HashSet<>();\n+\t\tprivate HashMap<Integer, List<Future<?>>> runningStartClusterFutures = new HashMap<>();\n+\t\tprivate HashSet<Integer> completedStartClusters = new HashSet<>();\n+\t\t\n+\t\t// rate tracking\n+\t\tprivate long startTime;\n+\t\tprivate long prevTime;\n+\t\tprivate int prevCount;\n+\t\t\n+\t\tpublic ProgressTracker() {\n \t\t\tthis.largestRup = 0;\n-\t\t\tthis.largestRupPrintMod = 10;\n \t\t\tthis.allPassedUniques = new HashSet<>();\n+\t\t\tthis.startTime = System.currentTimeMillis();\n+\t\t\tthis.prevTime = startTime;\n \t\t}\n \t\t\n \t\tpublic synchronized void processPassedRupture(ClusterRupture rup) {\n@@ -118,14 +141,130 @@ public synchronized void processPassedRupture(ClusterRupture rup) {\n \t\t\t\tint count = rup.getTotalNumSects();\n \t\t\t\tif (count > largestRup) {\n \t\t\t\t\tlargestRup = count;\n-\t\t\t\t\tif (largestRup % largestRupPrintMod == 0)\n-\t\t\t\t\t\tSystem.out.println(\"\\tNew largest rup has \"+largestRup\n+\t\t\t\t\tif (largestRup % largestRupPrintMod == 0) {\n+\t\t\t\t\t\tSystem.out.println(\"New largest rup has \"+largestRup\n \t\t\t\t\t\t\t\t+\" subsections with \"+rup.getTotalNumJumps()+\" jumps and \"\n-\t\t\t\t\t\t\t\t+rup.splays.size()+\" splays. \"+allPassedUniques.size()\n-\t\t\t\t\t\t\t\t+\" total unique passing ruptures found\");\n+\t\t\t\t\t\t\t\t+rup.splays.size()+\" splays.\");\n+\t\t\t\t\t\tprintCountAndStartClusterStatus();\n+\t\t\t\t\t\treturn;\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tint numUnique = allPassedUniques.size();\n+\t\t\t\tif (numUnique % rupCountPrintMod == 0) {\n+\t\t\t\t\tif (rupCountPrintMod <= 1000000) {\n+\t\t\t\t\t\tif (numUnique == 10000)\n+\t\t\t\t\t\t\trupCountPrintMod = 5000;\n+\t\t\t\t\t\tif (numUnique == 50000)\n+\t\t\t\t\t\t\trupCountPrintMod = 10000;\n+\t\t\t\t\t\telse if (numUnique == 100000)\n+\t\t\t\t\t\t\trupCountPrintMod = 25000;\n+\t\t\t\t\t\telse if (numUnique == 500000)\n+\t\t\t\t\t\t\trupCountPrintMod = 50000;\n+\t\t\t\t\t\telse if (numUnique == 1000000)\n+\t\t\t\t\t\t\trupCountPrintMod = 100000;\n+\t\t\t\t\t}\n+\t\t\t\t\tprintCountAndStartClusterStatus();\n \t\t\t\t}\n \t\t\t}\n \t\t}\n+\t\t\n+\t\tpublic synchronized int newStartCluster(int parentSectionID) {\n+\t\t\tstartClusterIDs.add(parentSectionID);\n+\t\t\treturn startClusterIDs.size();\n+\t\t}\n+\t\t\n+\t\tpublic synchronized int startClusterCount() {\n+\t\t\treturn startClusterIDs.size();\n+\t\t}\n+\t\t\n+\t\tpublic synchronized void addStartClusterFuture(int parentSectionID, Future<?> future) {\n+\t\t\tPreconditions.checkNotNull(future);\n+\t\t\tList<Future<?>> futures = runningStartClusterFutures.get(parentSectionID);\n+\t\t\tif (futures == null) {\n+\t\t\t\tfutures = new ArrayList<>();\n+\t\t\t\trunningStartClusterFutures.put(parentSectionID, futures);\n+\t\t\t}\n+\t\t\tfutures.add(future);\n+\t\t}\n+\t\t\n+\t\tpublic synchronized void printCountAndStartClusterStatus() {\n+\t\t\t// see if there are any completed clusters\n+\t\t\tlong curTime = System.currentTimeMillis();\n+\t\t\tList<Integer> newlyCompleted = new ArrayList<>();\n+\t\t\tint futuresOutstanding = 0;\n+\t\t\tfor (Integer parentID : runningStartClusterFutures.keySet()) {\n+\t\t\t\tList<Future<?>> futures = runningStartClusterFutures.get(parentID);\n+\t\t\t\tfor (int i=futures.size(); --i>=0;)\n+\t\t\t\t\tif (futures.get(i).isDone())\n+\t\t\t\t\t\tfutures.remove(i);\n+\t\t\t\tif (futures.isEmpty())\n+\t\t\t\t\tnewlyCompleted.add(parentID);\n+\t\t\t\telse\n+\t\t\t\t\tfuturesOutstanding += futures.size();\n+\t\t\t}\n+\t\t\tint numRups = allPassedUniques.size();\n+\t\t\tif (numRups == prevCount && newlyCompleted.isEmpty())\n+\t\t\t\treturn;\n+\t\t\tfor (Integer parentID : newlyCompleted) {\n+\t\t\t\trunningStartClusterFutures.remove(parentID);\n+\t\t\t\tcompletedStartClusters.add(parentID);\n+\t\t\t}\n+\t\t\tStringBuilder str = new StringBuilder(\"\\t\").append(countDF.format(numRups));\n+\t\t\tstr.append(\" total unique passing ruptures found, longest has \").append(largestRup);\n+\t\t\tstr.append(\" subsections.\\tClusters: \").append(runningStartClusterFutures.size());\n+\t\t\tstr.append(\" running (\").append(futuresOutstanding).append(\" futures), \");\n+\t\t\tstr.append(completedStartClusters.size()).append(\" completed, \");\n+\t\t\tstr.append(startClusterIDs.size()).append(\" total. \");\n+\t\t\t\n+\t\t\tstr.append(\"\\tRate: \").append(rupRate(numRups, curTime - startTime));\n+\t\t\tlong recentMillis = curTime - prevTime;\n+\t\t\tstr.append(\" (\").append(rupRate(numRups - prevCount, recentMillis)).append(\" over last \");\n+\t\t\tdouble recentSecs = (double)recentMillis / 1000d;\n+\t\t\tif (recentSecs > 60d) {\n+\t\t\t\tdouble recentMins = recentSecs / 60d;\n+\t\t\t\tif (recentMins > 60d) {\n+\t\t\t\t\tdouble recentHours = recentMins / 60d;\n+\t\t\t\t\tstr.append(oneDigitDF.format(recentHours)+\"h\");\n+\t\t\t\t} else {\n+\t\t\t\t\tstr.append(oneDigitDF.format(recentMins)+\"m\");\n+\t\t\t\t}\n+\t\t\t} else if (recentSecs > 10){\n+\t\t\t\tstr.append(countDF.format(recentSecs)+\"s\");\n+\t\t\t} else {\n+\t\t\t\tstr.append(oneDigitDF.format(recentSecs)+\"s\");\n+\t\t\t}\n+\t\t\tstr.append(\")\");\n+\t\t\t\n+\t\t\tprevTime = curTime;\n+\t\t\tprevCount = numRups;\n+\t\t\t\n+\t\t\tSystem.out.println(str.toString());\n+\t\t}\n+\t}\n+\t\n+\tprivate static String rupRate(int count, long timeDeltaMillis) {\n+\t\tif (timeDeltaMillis == 0)\n+\t\t\treturn \"N/A rups/s\";\n+\t\tdouble timeDeltaSecs = (double)timeDeltaMillis/1000d;\n+\t\t\n+\t\tdouble perSec = (double)count/timeDeltaSecs;\n+\t\tif (count == 0 || perSec >= 0.1) {\n+\t\t\tif (perSec > 10)\n+\t\t\t\treturn countDF.format(perSec)+\" rups/s\";\n+\t\t\treturn oneDigitDF.format(perSec)+\" rups/s\";\n+\t\t}\n+\t\t// switch to per minute\n+\t\tdouble perMin = perSec*60d;\n+\t\tif (perMin >= 0.1) {\n+\t\t\tif (perMin > 10)\n+\t\t\t\treturn countDF.format(perMin)+\" rups/m\";\n+\t\t\treturn oneDigitDF.format(perMin)+\" rups/m\";\n+\t\t}\n+\t\t// fallback to per hour\n+\t\tdouble perHour = perMin*60d;\n+\t\tif (perHour > 10)\n+\t\t\treturn countDF.format(perHour)+\" rups/hr\";\n+\t\treturn oneDigitDF.format(perHour)+\" rups/hr\";\n \t}\n \t\n \t/**\n@@ -150,18 +289,22 @@ public synchronized void processPassedRupture(ClusterRupture rup) {\n \tpublic List<ClusterRupture> build(ClusterPermutationStrategy permutationStrategy, int numThreads) {\n \t\tList<ClusterRupture> rups = new ArrayList<>();\n \t\tHashSet<UniqueRupture> uniques = new HashSet<>();\n-\t\tRupSizeTracker track = new RupSizeTracker();\n+\t\tProgressTracker track = new ProgressTracker();\n \t\t\n \t\tif (numThreads <= 1) {\n \t\t\tfor (FaultSubsectionCluster cluster : clusters) {\n \t\t\t\tClusterBuildCallable build = new ClusterBuildCallable(\n-\t\t\t\t\t\tpermutationStrategy, cluster, uniques, track);\n+\t\t\t\t\t\tpermutationStrategy, cluster, uniques, track, null);\n \t\t\t\ttry {\n \t\t\t\t\tbuild.call();\n \t\t\t\t} catch (Exception e) {\n \t\t\t\t\tthrow ExceptionUtils.asRuntimeException(e);\n \t\t\t\t}\n-\t\t\t\tbuild.merge(rups);\n+\t\t\t\ttry {\n+\t\t\t\t\tbuild.merge(rups);\n+\t\t\t\t} catch (InterruptedException | ExecutionException e) {\n+\t\t\t\t\tthrow ExceptionUtils.asRuntimeException(e);\n+\t\t\t\t}\n \t\t\t\tif (build.debugStop)\n \t\t\t\t\tbreak;\n //\t\t\t\tfor (FaultSection startSection : cluster.subSects) {\n@@ -213,7 +356,7 @@ public synchronized void processPassedRupture(ClusterRupture rup) {\n \t\t\t\n \t\t\tfor (FaultSubsectionCluster cluster : clusters) {\n \t\t\t\tClusterBuildCallable build = new ClusterBuildCallable(\n-\t\t\t\t\t\tpermutationStrategy, cluster, uniques, track);\n+\t\t\t\t\t\tpermutationStrategy, cluster, uniques, track, exec);\n \t\t\t\tfutures.add(exec.submit(build));\n \t\t\t}\n \t\t\t\n@@ -225,7 +368,11 @@ public synchronized void processPassedRupture(ClusterRupture rup) {\n \t\t\t\t} catch (Exception e) {\n \t\t\t\t\tthrow ExceptionUtils.asRuntimeException(e);\n \t\t\t\t}\n-\t\t\t\tbuild.merge(rups);\n+\t\t\t\ttry {\n+\t\t\t\t\tbuild.merge(rups);\n+\t\t\t\t} catch (InterruptedException | ExecutionException e) {\n+\t\t\t\t\tthrow ExceptionUtils.asRuntimeException(e);\n+\t\t\t\t}\n \t\t\t\tif (build.debugStop) {\n \t\t\t\t\texec.shutdownNow();\n \t\t\t\t\tbreak;\n@@ -239,6 +386,7 @@ public synchronized void processPassedRupture(ClusterRupture rup) {\n \t\treturn rups;\n \t}\n \t\n+\tprivate static DecimalFormat oneDigitDF = new DecimalFormat(\"0.0\");\n \tprivate static DecimalFormat countDF = new DecimalFormat(\"#\");\n \tstatic {\n \t\tcountDF.setGroupingUsed(true);\n@@ -250,21 +398,33 @@ public synchronized void processPassedRupture(ClusterRupture rup) {\n \t\tprivate ClusterPermutationStrategy permutationStrategy;\n \t\tprivate FaultSubsectionCluster cluster;\n \t\tprivate HashSet<UniqueRupture> uniques;\n-\t\tprivate List<ClusterRupture> rups;\n+\t\tprivate List<Future<List<ClusterRupture>>> rupListFutures;\n \t\tprivate boolean debugStop = false;\n-\t\tprivate RupSizeTracker track;\n+\t\tprivate ProgressTracker track;\n+\t\tprivate ExecutorService exec;\n+\t\t\n+\t\tprivate int clusterIndex;\n \n \t\tpublic ClusterBuildCallable(ClusterPermutationStrategy permutationStrategy,\n-\t\t\t\tFaultSubsectionCluster cluster, HashSet<UniqueRupture> uniques, RupSizeTracker track) {\n+\t\t\t\tFaultSubsectionCluster cluster, HashSet<UniqueRupture> uniques, ProgressTracker track,\n+\t\t\t\tExecutorService exec) {\n \t\t\tthis.permutationStrategy = permutationStrategy;\n \t\t\tthis.cluster = cluster;\n \t\t\tthis.uniques = uniques;\n \t\t\tthis.track = track;\n+\t\t\tthis.exec = exec;\n+\t\t\tclusterIndex = track.newStartCluster(cluster.parentSectionID);\n \t\t}\n \n+\t\t@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n \t\t@Override\n \t\tpublic ClusterBuildCallable call() throws Exception {\n-\t\t\tthis.rups = new ArrayList<>();\n+\t\t\tFakeFuture<ClusterBuildCallable> primaryFuture = new FakeFuture<>(this, false);\n+\t\t\ttrack.addStartClusterFuture(cluster.parentSectionID, primaryFuture);\n+\t\t\tif (this.exec != null)\n+\t\t\t\trupListFutures = Collections.synchronizedList(new ArrayList<>());\n+\t\t\telse\n+\t\t\t\trupListFutures = new ArrayList<>();\n \t\t\tfor (FaultSection startSection : cluster.subSects) {\n \t\t\t\tfor (FaultSubsectionCluster permutation : permutationStrategy.getPermutations(\n \t\t\t\t\t\tcluster, startSection)) {\n@@ -277,6 +437,7 @@ public ClusterBuildCallable call() throws Exception {\n \t\t\t\t\t\ttestRup(rup, true);\n \t\t\t\t\t\tif (stopAfterDebugMatch) {\n \t\t\t\t\t\t\tdebugStop = true;\n+\t\t\t\t\t\t\tprimaryFuture.setDone(true);\n \t\t\t\t\t\t\treturn this;\n \t\t\t\t\t\t}\n \t\t\t\t\t}\n@@ -288,36 +449,89 @@ public ClusterBuildCallable call() throws Exception {\n \t\t\t\t\t\ttrack.processPassedRupture(rup);\n \t\t\t\t\t\tif (!uniques.contains(rup.unique))\n \t\t\t\t\t\t\t// this means that this rupture passes and has not yet been processed\n-\t\t\t\t\t\t\trups.add(rup);\n+//\t\t\t\t\t\t\trups.add(rup);\n+\t\t\t\t\t\t\trupListFutures.add(new FakeFuture(Collections.singletonList(rup), true));\n \t\t\t\t\t}\n \t\t\t\t\t// continue to build this rupture\n+\t\t\t\t\tList<ClusterRupture> rups = new ArrayList<>();\n \t\t\t\t\tboolean canContinue = addRuptures(rups, uniques, rup, rup, \n-\t\t\t\t\t\t\tpermutationStrategy, track);\n+\t\t\t\t\t\t\tpermutationStrategy, track, exec, rupListFutures);\n+\t\t\t\t\tif (!rups.isEmpty())\n+\t\t\t\t\t\trupListFutures.add(new FakeFuture(rups, true));\n \t\t\t\t\tif (!canContinue) {\n \t\t\t\t\t\tSystem.out.println(\"Stopping due to debug criteria match with \"+rups.size()+\" ruptures\");\n \t\t\t\t\t\tdebugStop = true;\n+\t\t\t\t\t\tprimaryFuture.setDone(true);\n \t\t\t\t\t\treturn this;\n \t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n+\t\t\tprimaryFuture.setDone(true);\n \t\t\treturn this;\n \t\t}\n \t\t\n-\t\tpublic void merge(List<ClusterRupture> masterRups) {\n+\t\tpublic void merge(List<ClusterRupture> masterRups) throws InterruptedException, ExecutionException {\n \t\t\tint added = 0;\n-\t\t\tfor (ClusterRupture rup : rups) {\n-\t\t\t\tif (!uniques.contains(rup.unique)) {\n-\t\t\t\t\tmasterRups.add(rup);\n-\t\t\t\t\tuniques.add(rup.unique);\n-\t\t\t\t\t// make sure that contains now returns true\n-\t\t\t\t\tPreconditions.checkState(uniques.contains(rup.unique));\n-//\t\t\t\t\tPreconditions.checkState(uniques.contains(rup.reversed().unique));\n-\t\t\t\t\tadded++;\n+\t\t\tint raw = 0;\n+\t\t\tfor (Future<List<ClusterRupture>> future : rupListFutures) {\n+\t\t\t\tfor (ClusterRupture rup : future.get()) {\n+\t\t\t\t\tif (!uniques.contains(rup.unique)) {\n+\t\t\t\t\t\tmasterRups.add(rup);\n+\t\t\t\t\t\tuniques.add(rup.unique);\n+\t\t\t\t\t\t// make sure that contains now returns true\n+\t\t\t\t\t\tPreconditions.checkState(uniques.contains(rup.unique));\n+//\t\t\t\t\t\tPreconditions.checkState(uniques.contains(rup.reversed().unique));\n+\t\t\t\t\t\tadded++;\n+\t\t\t\t\t\traw++;\n+\t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n \t\t\tSystem.out.println(\"Merged in \"+countDF.format(masterRups.size())+\" ruptures after processing \"\n-\t\t\t\t\t+ \"cluster \"+cluster.parentSectionID+\": \"+cluster.parentSectionName\n-\t\t\t\t\t+\" (\"+added+\" new, \"+rups.size()+\" incl. possible duplicates)\");\n+\t\t\t\t\t+ \"start cluster \"+clusterIndex+\"/\"+track.startClusterCount()+\" (id=\"+cluster.parentSectionID+\"): \"\n+\t\t\t\t\t+cluster.parentSectionName+\" (\"+added+\" new, \"+raw+\" incl. possible duplicates).\");\n+\t\t\ttrack.printCountAndStartClusterStatus();\n+\t\t}\n+\t\t\n+\t}\n+\t\n+\tprivate class FakeFuture<E> implements Future<E> {\n+\t\t\n+\t\tprivate E result;\n+\t\tprivate boolean done;\n+\n+\t\tpublic FakeFuture(E result, boolean done) {\n+\t\t\tthis.result = result;\n+\t\t\tthis.done = done;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean cancel(boolean mayInterruptIfRunning) {\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean isCancelled() {\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic boolean isDone() {\n+\t\t\treturn done;\n+\t\t}\n+\t\t\n+\t\tpublic void setDone(boolean done) {\n+\t\t\tthis.done = done;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic E get() throws InterruptedException, ExecutionException {\n+\t\t\treturn result;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic E get(long timeout, TimeUnit unit)\n+\t\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n+\t\t\treturn result;\n \t\t}\n \t\t\n \t}\n@@ -335,41 +549,78 @@ private PlausibilityResult testRup(ClusterRupture rupture, final boolean debug)\n \t\treturn result;\n \t}\n \t\n+\tprivate class AddRupturesCallable implements Callable<List<ClusterRupture>> {\n+\t\t\n+\t\tprivate HashSet<UniqueRupture> uniques;\n+\t\tprivate ClusterRupture currentRupture;\n+\t\tprivate ClusterRupture currentStrand;\n+\t\tprivate ClusterPermutationStrategy permutationStrategy;\n+\t\tprivate ProgressTracker track;\n+\t\tprivate Jump jump;\n+\n+\t\tpublic AddRupturesCallable(HashSet<UniqueRupture> uniques,\n+\t\t\t\tClusterRupture currentRupture, ClusterRupture currentStrand,\n+\t\t\t\tClusterPermutationStrategy permutationStrategy, ProgressTracker track, Jump jump) {\n+\t\t\tthis.uniques = uniques;\n+\t\t\tthis.currentRupture = currentRupture;\n+\t\t\tthis.currentStrand = currentStrand;\n+\t\t\tthis.permutationStrategy = permutationStrategy;\n+\t\t\tthis.track = track;\n+\t\t\tthis.jump = jump;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic List<ClusterRupture> call() {\n+\t\t\tList<ClusterRupture> rups = new ArrayList<>();\n+\t\t\taddJumpPermutations(rups, uniques, currentRupture, currentStrand,\n+\t\t\t\t\tpermutationStrategy, jump, track);\n+\t\t\treturn rups;\n+\t\t}\n+\t\t\n+\t}\n+\t\n \tprivate boolean addRuptures(List<ClusterRupture> rups, HashSet<UniqueRupture> uniques,\n \t\t\tClusterRupture currentRupture, ClusterRupture currentStrand,\n-\t\t\tClusterPermutationStrategy permutationStrategy, RupSizeTracker track) {\n+\t\t\tClusterPermutationStrategy permutationStrategy, ProgressTracker track,\n+\t\t\tExecutorService exec, List<Future<List<ClusterRupture>>> futures) {\n \t\tFaultSubsectionCluster lastCluster = currentStrand.clusters[currentStrand.clusters.length-1];\n \t\tFaultSection firstSection = currentStrand.clusters[0].startSect;\n+\t\t\n+\t\tif (currentStrand == currentRupture && currentRupture.splays.size() < maxNumSplays) {\n+\t\t\t// add splays first, only from the end cluster\n+\t\t\tfor (FaultSection section : lastCluster.subSects) {\n+\t\t\t\tif (section.equals(firstSection))\n+\t\t\t\t\t// can't jump from the first section of the rupture\n+\t\t\t\t\tcontinue;\n+\t\t\t\tif (lastCluster.endSects.contains(section))\n+\t\t\t\t\t// this would be a continuation of the main rupture, not a splay\n+\t\t\t\t\tbreak;\n+\t\t\t\tfor (Jump jump : lastCluster.getConnections(section)) {\n+\t\t\t\t\tif (!currentRupture.contains(jump.toSection)) {\n+\t\t\t\t\t\tboolean canContinue = addJumpPermutations(rups, uniques, currentRupture, currentStrand,\n+\t\t\t\t\t\t\t\tpermutationStrategy, jump, track);\n+\t\t\t\t\t\tif (!canContinue)\n+\t\t\t\t\t\t\treturn false;\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n \n \t\t// try to grow this strand first\n \t\tfor (FaultSection endSection : lastCluster.endSects) {\n \t\t\tfor (Jump jump : lastCluster.getConnections(endSection)) {\n \t\t\t\tif (!currentRupture.contains(jump.toSection)) {\n-\t\t\t\t\tboolean canContinue = addJumpPermutations(rups, uniques, currentRupture, currentStrand,\n-\t\t\t\t\t\t\tpermutationStrategy, jump, track);\n-\t\t\t\t\tif (!canContinue)\n-\t\t\t\t\t\treturn false;\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t\t\n-\t\t// now try to add splays\n-\t\tif (currentStrand == currentRupture && currentRupture.splays.size() < maxNumSplays) {\n-\t\t\tfor (FaultSubsectionCluster cluster : currentRupture.clusters) {\n-\t\t\t\tfor (FaultSection section : cluster.subSects) {\n-\t\t\t\t\tif (section.equals(firstSection))\n-\t\t\t\t\t\t// can't jump from the first section of the rupture\n-\t\t\t\t\t\tcontinue;\n-\t\t\t\t\tif (lastCluster.endSects.contains(section))\n-\t\t\t\t\t\t// this would be a continuation of the main rupture, not a splay\n-\t\t\t\t\t\tbreak;\n-\t\t\t\t\tfor (Jump jump : cluster.getConnections(section)) {\n-\t\t\t\t\t\tif (!currentRupture.contains(jump.toSection)) {\n-\t\t\t\t\t\t\tboolean canContinue = addJumpPermutations(rups, uniques, currentRupture,\n-\t\t\t\t\t\t\t\t\tcurrentStrand, permutationStrategy, jump, track);\n-\t\t\t\t\t\t\tif (!canContinue)\n-\t\t\t\t\t\t\t\treturn false;\n-\t\t\t\t\t\t}\n+\t\t\t\t\tif (exec != null) {\n+\t\t\t\t\t\t// fork it\n+\t\t\t\t\t\tFuture<List<ClusterRupture>> future = exec.submit(new AddRupturesCallable(\n+\t\t\t\t\t\t\t\tuniques, currentRupture, currentStrand, permutationStrategy, track, jump));\n+\t\t\t\t\t\ttrack.addStartClusterFuture(currentRupture.clusters[0].parentSectionID, future);\n+\t\t\t\t\t\tfutures.add(future);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tboolean canContinue = addJumpPermutations(rups, uniques, currentRupture, currentStrand,\n+\t\t\t\t\t\t\t\tpermutationStrategy, jump, track);\n+\t\t\t\t\t\tif (!canContinue)\n+\t\t\t\t\t\t\treturn false;\n \t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n@@ -379,7 +630,7 @@ private boolean addRuptures(List<ClusterRupture> rups, HashSet<UniqueRupture> un\n \n \tprivate boolean addJumpPermutations(List<ClusterRupture> rups, HashSet<UniqueRupture> uniques,\n \t\t\tClusterRupture currentRupture, ClusterRupture currentStrand,\n-\t\t\tClusterPermutationStrategy permutationStrategy, Jump jump, RupSizeTracker track) {\n+\t\t\tClusterPermutationStrategy permutationStrategy, Jump jump, ProgressTracker track) {\n \t\tPreconditions.checkNotNull(jump);\n \t\tfor (FaultSubsectionCluster permutation : permutationStrategy.getPermutations(\n \t\t\t\tcurrentRupture, jump.toCluster, jump.toSection)) {\n@@ -425,26 +676,36 @@ private boolean addJumpPermutations(List<ClusterRupture> rups, HashSet<UniqueRup\n \t\t\t\t\tSystem.out.println(\"We passed but have already processed this rupture, skipping\");\n \t\t\t}\n \t\t\t// continue to build this rupture\n-\t\t\tClusterRupture newCurrentStrand;\n+//\t\t\tClusterRupture newCurrentStrand;\n \t\t\tif (currentStrand == currentRupture) {\n-\t\t\t\tnewCurrentStrand = candidateRupture;\n+\t\t\t\t// we're on the primary strand\n+\t\t\t\tboolean canContinue = addRuptures(rups, uniques, candidateRupture, candidateRupture,\n+\t\t\t\t\t\tpermutationStrategy, track, null, null);\n+\t\t\t\tif (!canContinue)\n+\t\t\t\t\treturn false;\n \t\t\t} else {\n \t\t\t\t// we're building a splay, try to continue that one\n-\t\t\t\tnewCurrentStrand = null;\n+\t\t\t\tClusterRupture splayStrand = null;\n \t\t\t\tfor (ClusterRupture splay : candidateRupture.splays.values()) {\n \t\t\t\t\tif (splay.contains(jump.toSection)) {\n-\t\t\t\t\t\tnewCurrentStrand = splay;\n+\t\t\t\t\t\tsplayStrand = splay;\n \t\t\t\t\t\tbreak;\n \t\t\t\t\t}\n \t\t\t\t}\n-\t\t\t\tPreconditions.checkNotNull(newCurrentStrand);\n-\t\t\t\tFaultSection newLastStart = newCurrentStrand.clusters[newCurrentStrand.clusters.length-1].startSect;\n+\t\t\t\tPreconditions.checkNotNull(splayStrand);\n+\t\t\t\t// try to build out the splay\n+\t\t\t\tFaultSection newLastStart = splayStrand.clusters[splayStrand.clusters.length-1].startSect;\n \t\t\t\tPreconditions.checkState(newLastStart.equals(permutation.startSect));\n+\t\t\t\tboolean canContinue = addRuptures(rups, uniques, candidateRupture, splayStrand,\n+\t\t\t\t\t\tpermutationStrategy, track, null, null);\n+\t\t\t\tif (!canContinue)\n+\t\t\t\t\treturn false;\n+\t\t\t\t// now try to build out the primary strand\n+\t\t\t\tcanContinue = addRuptures(rups, uniques, candidateRupture, candidateRupture,\n+\t\t\t\t\t\tpermutationStrategy, track, null, null);\n+\t\t\t\tif (!canContinue)\n+\t\t\t\t\treturn false;\n \t\t\t}\n-\t\t\tboolean canContinue = addRuptures(rups, uniques, candidateRupture, newCurrentStrand,\n-\t\t\t\t\tpermutationStrategy, track);\n-\t\t\tif (!canContinue)\n-\t\t\t\treturn false;\n \t\t}\n \t\treturn true;\n \t}\n@@ -609,7 +870,7 @@ public boolean appliesTo(PlausibilityResult result) {\n \t\t\n \t}\n \t\n-\tprivate static int[] loadRupString(String rupStr, boolean parents) {\n+\tpublic static int[] loadRupString(String rupStr, boolean parents) {\n \t\tPreconditions.checkState(rupStr.contains(\"[\"));\n \t\tList<Integer> ids = new ArrayList<>();\n \t\twhile (rupStr.contains(\"[\")) {\n@@ -647,7 +908,7 @@ public boolean isMatch(ClusterRupture rup) {\n \n \t\t@Override\n \t\tpublic boolean isMatch(ClusterRupture rup, Jump newJump) {\n-\t\t\treturn !uniques.contains(new UniqueRupture(rup.unique, newJump.toCluster));\n+\t\t\treturn !uniques.contains(UniqueRupture.add(rup.unique, newJump.toCluster.unique));\n \t\t}\n \n \t\t@Override\n@@ -674,7 +935,7 @@ public boolean isMatch(ClusterRupture rup) {\n \n \t\t@Override\n \t\tpublic boolean isMatch(ClusterRupture rup, Jump newJump) {\n-\t\t\treturn uniques.contains(new UniqueRupture(rup.unique, newJump.toCluster));\n+\t\t\treturn uniques.contains(UniqueRupture.add(rup.unique, newJump.toCluster.unique));\n \t\t}\n \n \t\t@Override\n@@ -716,16 +977,72 @@ public boolean appliesTo(PlausibilityResult result) {\n \t@SuppressWarnings(\"unused\")\n \tpublic static void main(String[] args) throws IOException, DocumentException {\n \t\tFile rupSetsDir = new File(\"/home/kevin/OpenSHA/UCERF4/rup_sets\");\n-\t\tFaultModels fm = FaultModels.FM3_1;\n-\t\tFile distAzCacheFile = new File(rupSetsDir, fm.encodeChoiceString().toLowerCase()\n-\t\t\t\t+\"_dist_az_cache.csv\");\n-\t\tDeformationModels dm = fm.getFilterBasis();\n-\t\tScalingRelationships scale = ScalingRelationships.MEAN_UCERF3;\n \t\t\n-\t\tDeformationModelFetcher dmFetch = new DeformationModelFetcher(fm, dm,\n-\t\t\t\tnull, 0.1);\n+\t\t// for UCERF3 fault models\n+//\t\tFaultModels fm = FaultModels.FM3_1;\n+//\t\tString fmPrefix = fm.encodeChoiceString().toLowerCase();\n+//\t\tFile distAzCacheFile = new File(rupSetsDir, fmPrefix+\"_dist_az_cache.csv\");\n+//\t\tDeformationModels dm = fm.getFilterBasis();\n+//\t\tScalingRelationships scale = ScalingRelationships.MEAN_UCERF3;\n+//\t\tDeformationModelFetcher dmFetch = new DeformationModelFetcher(fm, dm, null, 0.1);\n+//\t\tList<? extends FaultSection> subSects = dmFetch.getSubSectionList();\n+\t\t// END U3\n+\t\t\n+\t\t// for NZ tests\n+//\t\tFile xmlFile = new File(rupSetsDir, \"DEMO5_SANSTVZ_crustal_opensha.xml\");\n+//\t\tDocument fsDoc = XMLUtils.loadDocument(xmlFile);\n+//\t\tElement fsEl = fsDoc.getRootElement().element(\"FaultModel\");\n+//\t\tList<FaultSection> sects = FaultSystemIO.fsDataFromXML(fsEl);\n+//\t\tSystem.out.println(\"Loaded \"+sects.size()+\" sections\");\n+//\t\tList<FaultSection> subSects = new ArrayList<>();\n+//\t\tfor (FaultSection sect : sects)\n+//\t\t\tsubSects.addAll(sect.getSubSectionsList(0.5*sect.getOrigDownDipWidth(), subSects.size(), 2));\n+//\t\tSystem.out.println(\"Built \"+subSects.size()+\" subsections\");\n+//\t\tPreconditions.checkState(!subSects.isEmpty());\n+//\t\tString fmPrefix = \"nz_demo5_crustal\";\n+//\t\tFile distAzCacheFile = new File(rupSetsDir, fmPrefix+\"_dist_az_cache.csv\");\n+//\t\tScalingRelationships scale = ScalingRelationships.MEAN_UCERF3;\n+\t\t// END NZ\n+\t\t\n+\t\t// NSHM23 tests\n+\t\tString state = null;\n \t\t\n-\t\tList<? extends FaultSection> subSects = dmFetch.getSubSectionList();\n+\t\tString fmPrefix;\n+\t\tList<FaultSection> sects;\n+\t\tif (state == null) {\n+\t\t\tfmPrefix = \"nshm23_v1_all\";\n+\t\t\tsects = new ArrayList<>();\n+\t\t\tfor (List<FaultSection> stateFaults : GeoJSONFaultReader.readFaultSections(\n+\t\t\t\t\tnew File(\"/home/kevin/Downloads/NSHM2023_FaultSections_v1mod.geojson\"), false).values())\n+\t\t\t\tsects.addAll(stateFaults);\n+\t\t} else {\n+\t\t\tfmPrefix = \"nshm23_v1_\"+state.toLowerCase();\n+\t\t\tsects = GeoJSONFaultReader.readFaultSections(\n+\t\t\t\t\tnew File(\"/home/kevin/Downloads/NSHM2023_FaultSections_v1mod.geojson\"), true).get(state);\n+\t\t}\n+\t\tCollections.sort(sects, new Comparator<FaultSection>() {\n+\n+\t\t\t@Override\n+\t\t\tpublic int compare(FaultSection o1, FaultSection o2) {\n+\t\t\t\treturn o1.getSectionName().compareTo(o2.getSectionName());\n+\t\t\t}\n+\t\t});\n+\t\t\n+\t\tFile distAzCacheFile = new File(rupSetsDir, fmPrefix+\"_dist_az_cache.csv\");\n+\t\tScalingRelationships scale = ScalingRelationships.MEAN_UCERF3;\n+\t\tSystem.out.println(\"Loaded \"+sects.size()+\" sections\");\n+\t\t// add slip rates\n+\t\tMap<Integer, List<GeoSlipRateRecord>> slipRates = GeoJSONFaultReader.readGeoDB(\n+\t\t\t\tnew File(\"/home/kevin/Downloads/NSHM2023_EQGeoDB_v1.geojson\"));\n+\t\tList<FaultSection> fallbacks = new ArrayList<>();\n+\t\tfallbacks.addAll(FaultModels.FM3_1.fetchFaultSections());\n+\t\tfallbacks.addAll(FaultModels.FM3_2.fetchFaultSections());\n+\t\tGeoJSONFaultReader.testMapSlipRates(sects, slipRates, 1d, fallbacks);\n+\t\tList<FaultSection> subSects = new ArrayList<>();\n+\t\tfor (FaultSection sect : sects)\n+\t\t\tsubSects.addAll(sect.getSubSectionsList(0.5*sect.getOrigDownDipWidth(), subSects.size(), 2));\n+\t\tSystem.out.println(\"Built \"+subSects.size()+\" subsections\");\n+\t\t// END NSHM23\n \t\t\n \t\tRupDebugCriteria debugCriteria = null;\n \t\tboolean stopAfterDebug = false;\n@@ -764,131 +1081,307 @@ public static void main(String[] args) throws IOException, DocumentException {\n \t\tint numAzCached = distAzCalc.getNumCachedAzimuths();\n \t\tint numDistCached = distAzCalc.getNumCachedDistances();\n \t\t\n+\t\tint threads = Integer.max(1, Integer.min(31, Runtime.getRuntime().availableProcessors()-2));\n+//\t\tint threads = 1;\n+\t\t\n \t\t/*\n \t\t * =============================\n \t\t * To reproduce UCERF3\n \t\t * =============================\n \t\t */\n //\t\tPlausibilityConfiguration config = PlausibilityConfiguration.getUCERF3(subSects, distAzCalc, fm);\n-//\t\tClusterPermutationStrategy permStrat = new UCERF3ClusterPermuationStrategy();\n-//\t\tString outputName = fm.encodeChoiceString().toLowerCase()+\"_reproduce_ucerf3.zip\";\n-//\t\tSubSectStiffnessCalculator stiffnessCalc = null;\n+//\t\tClusterPermutationStrategy permStrat = new ExhaustiveClusterPermuationStrategy();\n+//\t\tString outputName = fmPrefix+\"_reproduce_ucerf3.zip\";\n+//\t\tAggregatedStiffnessCache stiffnessCache = null;\n //\t\tFile stiffnessCacheFile = null;\n //\t\tint stiffnessCacheSize = 0;\n+//\t\tFile outputDir = rupSetsDir;\n+\t\t\n+\t\t/*\n+\t\t * =============================\n+\t\t * To reproduce UCERF3 with an alternative distance/conn strategy (and calculate missing coulomb)\n+\t\t * =============================\n+\t\t */\n+//\t\tString outputName = fmPrefix+\"_ucerf3\";\n+//\t\tCoulombRates coulombRates = CoulombRates.loadUCERF3CoulombRates(fm);\n+//\t\t\n+//\t\tdouble maxJumpDist = 5d;\n+//\t\tClusterConnectionStrategy connectionStrategy =\n+//\t\t\t\tnew UCERF3ClusterConnectionStrategy(subSects, distAzCalc, maxJumpDist, coulombRates);\n+//\t\toutputName += \"_\"+new DecimalFormat(\"0.#\").format(maxJumpDist)+\"km\";\n+//\t\t\n+////\t\tdouble r0 = 5d;\n+////\t\tdouble rMax = 10d;\n+////\t\tint cMax = -1;\n+////\t\tint sMax = 1;\n+////\t\tClusterConnectionStrategy connectionStrategy =\n+////\t\t\t\tnew AdaptiveDistCutoffClosestSectClusterConnectionStrategy(subSects, distAzCalc, r0, rMax, cMax, sMax);\n+////\t\toutputName += \"_adapt\"+new DecimalFormat(\"0.#\").format(r0)+\"_\"+new DecimalFormat(\"0.#\").format(rMax)+\"km\";\n+////\t\tif (cMax >= 0)\n+////\t\t\toutputName += \"_cMax\"+cMax;\n+////\t\tif (sMax >= 0)\n+////\t\t\toutputName += \"_sMax\"+sMax;\n+//\t\t\n+//\t\tBuilder configBuilder = PlausibilityConfiguration.builder(connectionStrategy, subSects);\n+//\t\tconfigBuilder.minSectsPerParent(2, true, true);\n+//\t\tconfigBuilder.u3Cumulatives();\n+////\t\tconfigBuilder.cumulativeAzChange(560f);\n+//\t\tconfigBuilder.u3Azimuth();\n+//\t\tSubSectStiffnessCalculator stiffnessCalc = new SubSectStiffnessCalculator(\n+//\t\t\t\tsubSects, 1d, 3e4, 3e4, 0.5, PatchAlignment.FILL_OVERLAP, 1d);\n+//\t\tAggregatedStiffnessCache stiffnessCache = stiffnessCalc.getAggregationCache(StiffnessType.CFF);\n+//\t\tFile stiffnessCacheFile = new File(rupSetsDir, stiffnessCache.getCacheFileName());\n+//\t\tint stiffnessCacheSize = 0;\n+//\t\tif (stiffnessCacheFile.exists())\n+//\t\t\tstiffnessCacheSize = stiffnessCache.loadCacheFile(stiffnessCacheFile);\n+////\t\tconfigBuilder.u3Coulomb(coulombRates, stiffnessCalc); outputName += \"_cffFallback\";\n+////\t\toutputName += \"_noCoulomb\";\n+//\t\tconfigBuilder.u3Coulomb(new CoulombRates(fm, new HashMap<>()), stiffnessCalc); outputName += \"_cffReproduce\";\n+//\t\tPlausibilityConfiguration config = configBuilder.build();\n+//\t\tClusterPermutationStrategy permStrat = new UCERF3ClusterPermuationStrategy();\n+//\t\toutputName += \".zip\";\n+//\t\tFile outputDir = rupSetsDir;\n \t\t\n \t\t/*\n \t\t * =============================\n \t\t * For other experiments\n \t\t * =============================\n \t\t */\n \t\t// build stiffness calculator (used for new Coulomb)\n+\t\tdouble stiffGridSpacing = 2d;\n \t\tSubSectStiffnessCalculator stiffnessCalc = new SubSectStiffnessCalculator(\n-\t\t\t\tsubSects, 2d, 3e4, 3e4, 0.5);\n-\t\tFile stiffnessCacheFile = new File(rupSetsDir, stiffnessCalc.getCacheFileName(StiffnessType.CFF));\n+\t\t\t\tsubSects, stiffGridSpacing, 3e4, 3e4, 0.5, PatchAlignment.FILL_OVERLAP, 1d);\n+\t\tAggregatedStiffnessCache stiffnessCache = stiffnessCalc.getAggregationCache(StiffnessType.CFF);\n+\t\tFile stiffnessCacheFile = new File(rupSetsDir, stiffnessCache.getCacheFileName());\n \t\tint stiffnessCacheSize = 0;\n \t\tif (stiffnessCacheFile.exists())\n-\t\t\tstiffnessCacheSize = stiffnessCalc.loadCacheFile(stiffnessCacheFile, StiffnessType.CFF);\n+\t\t\tstiffnessCacheSize = stiffnessCache.loadCacheFile(stiffnessCacheFile);\n+\t\t// common aggregators\n+\t\tAggregatedStiffnessCalculator sumAgg = new AggregatedStiffnessCalculator(StiffnessType.CFF, stiffnessCalc, true,\n+\t\t\t\tAggregationMethod.FLATTEN, AggregationMethod.SUM, AggregationMethod.SUM, AggregationMethod.SUM);\n+\t\tAggregatedStiffnessCalculator fractRpatchPosAgg = new AggregatedStiffnessCalculator(StiffnessType.CFF, stiffnessCalc, true,\n+\t\t\t\tAggregationMethod.SUM, AggregationMethod.PASSTHROUGH, AggregationMethod.RECEIVER_SUM, AggregationMethod.FRACT_POSITIVE);\n+//\t\tAggregatedStiffnessCalculator threeQuarterInts = new AggregatedStiffnessCalculator(StiffnessType.CFF, stiffnessCalc, true,\n+//\t\t\t\tAggregationMethod.NUM_POSITIVE, AggregationMethod.SUM, AggregationMethod.SUM, AggregationMethod.THREE_QUARTER_INTERACTIONS);\n+\t\tAggregatedStiffnessCalculator fractIntsAgg = new AggregatedStiffnessCalculator(StiffnessType.CFF, stiffnessCalc, true,\n+\t\t\t\tAggregationMethod.FLATTEN, AggregationMethod.NUM_POSITIVE, AggregationMethod.SUM, AggregationMethod.NORM_BY_COUNT);\n+\t\tfloat slipRateProb = 0.05f;\t\t// PREF: 0.05\n+\t\tfloat cffFractInts = 0.75f;\t\t// PREF: 0.75\n+\t\tint cffRatioN = 2;\t\t\t\t// PREF: 2\n+\t\tfloat cffRatioThresh = 0.5f;\t// PREF: 0.5\n+\t\tfloat cffRelativeProb = 0.02f;\t// PREF: 0.02\n+\t\tboolean favorableJumps = true;\t// PREF: true\n+//\t\tCoulombSectRatioProb cffRatioProb = new CoulombSectRatioProb(sumAgg, cffRatioN);\n+\t\t\n+\t\tString outputName = fmPrefix;\n+\t\t\n+\t\tif (stiffGridSpacing != 2d)\n+\t\t\toutputName += \"_stiff\"+new DecimalFormat(\"0.#\").format(stiffGridSpacing)+\"km\";\n \t\t\n \t\t/*\n \t\t * Connection strategy: which faults are allowed to connect, and where?\n \t\t */\n \t\t// use this for the exact same connections as UCERF3\n-\t\tdouble minJumpDist = 10d;\n-\t\tClusterConnectionStrategy connectionStrategy =\n-\t\t\t\tnew UCERF3ClusterConnectionStrategy(subSects,\n-\t\t\t\t\t\tdistAzCalc, minJumpDist, CoulombRates.loadUCERF3CoulombRates(fm));\n+//\t\tdouble maxJumpDist = 5d;\n+//\t\tClusterConnectionStrategy connectionStrategy =\n+//\t\t\t\tnew UCERF3ClusterConnectionStrategy(subSects,\n+//\t\t\t\t\t\tdistAzCalc, maxJumpDist, CoulombRates.loadUCERF3CoulombRates(fm));\n+//\t\tif (maxJumpDist != 5d)\n+//\t\t\toutputName += \"_\"+new DecimalFormat(\"0.#\").format(maxJumpDist)+\"km\";\n \t\t// use this for simpler connection rules\n+//\t\tdouble maxJumpDist = 10d;\n+//\t\tClusterConnectionStrategy connectionStrategy =\n+//\t\t\tnew DistCutoffClosestSectClusterConnectionStrategy(subSects, distAzCalc, maxJumpDist);\n+//\t\tif (maxJumpDist != 5d)\n+//\t\t\toutputName += \"_\"+new DecimalFormat(\"0.#\").format(maxJumpDist)+\"km\";\n+\t\t// use this for adaptive distance filter\n+//\t\tdouble r0 = 5d;\n+//\t\tdouble rMax = 10d;\n+//\t\tint cMax = -1;\n+//\t\tint sMax = 1;\n //\t\tClusterConnectionStrategy connectionStrategy =\n-//\t\t\tnew DistCutoffClosestSectClusterConnectionStrategy(subSects, distAzCalc, minJumpDist);\n+//\t\t\t\tnew AdaptiveDistCutoffClosestSectClusterConnectionStrategy(subSects, distAzCalc, r0, rMax, cMax, sMax);\n+//\t\toutputName += \"_adapt\"+new DecimalFormat(\"0.#\").format(r0)+\"_\"+new DecimalFormat(\"0.#\").format(rMax)+\"km\";\n+//\t\tif (cMax >= 0)\n+//\t\t\toutputName += \"_cMax\"+cMax;\n+//\t\tif (sMax >= 0)\n+//\t\t\toutputName += \"_sMax\"+sMax;\n+\t\t// use this to pick connections which agree with your plausibility filters\n+\t\tdouble maxJumpDist = 10d;\n+\t\t// some filters need a connection strategy, use one that only includes immediate neighbors at this step\n+\t\tDistCutoffClosestSectClusterConnectionStrategy neighborsConnStrat =\n+\t\t\t\tnew DistCutoffClosestSectClusterConnectionStrategy(subSects, distAzCalc, 0.1d);\n+\t\tList<PlausibilityFilter> connFilters = new ArrayList<>();\n+\t\tif (cffRatioThresh > 0f) {\n+\t\t\tconnFilters.add(new CumulativeProbabilityFilter(cffRatioThresh, new CoulombSectRatioProb(\n+\t\t\t\t\tsumAgg, cffRatioN, favorableJumps, (float)maxJumpDist, distAzCalc)));\n+\t\t\tif (cffRelativeProb > 0f)\n+\t\t\t\tconnFilters.add(new PathPlausibilityFilter(\n+\t\t\t\t\t\tnew CumulativeProbPathEvaluator(cffRatioThresh, PlausibilityResult.FAIL_HARD_STOP,\n+\t\t\t\t\t\t\t\tnew CoulombSectRatioProb(sumAgg, cffRatioN, favorableJumps, (float)maxJumpDist, distAzCalc)),\n+\t\t\t\t\t\tnew CumulativeProbPathEvaluator(cffRelativeProb, PlausibilityResult.FAIL_HARD_STOP,\n+\t\t\t\t\t\t\t\tnew RelativeCoulombProb(sumAgg, neighborsConnStrat, false, true, favorableJumps, (float)maxJumpDist, distAzCalc))));\n+\t\t} else if (cffRelativeProb > 0f) {\n+\t\t\tconnFilters.add(new CumulativeProbabilityFilter(cffRatioThresh, new RelativeCoulombProb(\n+\t\t\t\t\tsumAgg, neighborsConnStrat, false, true, favorableJumps, (float)maxJumpDist, distAzCalc)));\n+\t\t}\n+\t\tif (cffFractInts > 0f)\n+\t\t\tconnFilters.add(new NetRuptureCoulombFilter(fractIntsAgg, cffFractInts));\n+\t\tSystem.out.println(\"Building plausible connections w/ \"+threads+\" threads...\");\n+\t\tClusterConnectionStrategy connectionStrategy =\n+\t\t\tnew PlausibleClusterConnectionStrategy(subSects, distAzCalc, maxJumpDist,\n+\t\t\t\t\tPlausibleClusterConnectionStrategy.JUMP_SELECTOR_DEFAULT, connFilters);\n+\t\toutputName += \"_plausibleMulti\"+new DecimalFormat(\"0.#\").format(maxJumpDist)+\"km\";\n+\t\tconnectionStrategy.checkBuildThreaded(threads);\n+\t\tSystem.out.println(\"DONE building plausible connections\");\n \t\t\n-\t\tString outputName = fm.encodeChoiceString().toLowerCase();\n-\t\tif (minJumpDist != 5d)\n-\t\t\toutputName += \"_\"+new DecimalFormat(\"0.#\").format(minJumpDist)+\"km\";\n \t\tBuilder configBuilder = PlausibilityConfiguration.builder(connectionStrategy, subSects);\n \t\t\n \t\t/*\n \t\t * Plausibility filters: which ruptures (utilizing those connections) are allowed?\n \t\t */\n-\t\t// UCERF3 filters\n+\t\t\n+\t\t/*\n+\t\t *  UCERF3 filters\n+\t\t */\n //\t\tconfigBuilder.u3All(CoulombRates.loadUCERF3CoulombRates(fm)); outputName += \"_ucerf3\";\n \t\tconfigBuilder.minSectsPerParent(2, true, true); // always do this one\n+\t\tconfigBuilder.noIndirectPaths(); outputName += \"_direct\";\n //\t\tconfigBuilder.u3Cumulatives(); outputName += \"_u3Cml\"; // cml rake and azimuth\n //\t\tconfigBuilder.cumulativeAzChange(560f); outputName += \"_cmlAz\"; // cml azimuth only\n+//\t\tconfigBuilder.cumulativeRakeChange(180f); outputName += \"_cmlRake\"; // cml azimuth only\n //\t\tconfigBuilder.u3Azimuth(); outputName += \"_u3Az\";\n //\t\tconfigBuilder.u3Coulomb(CoulombRates.loadUCERF3CoulombRates(fm)); outputName += \"_u3CFF\";\n \t\t\n-\t\t// new probability-based cumulative filter (cumulatives should always be first for efficiency)\n-\t\tfloat probThresh = 0.005f;\n-\t\toutputName += \"_cmlProb\"+(float)probThresh;\n-//\t\tList<RuptureProbabilityCalc> probCalcs = new ArrayList<>();\n-//\t\tprobCalcs.add(new BiasiWesnousky2016CombJumpDistProb(1d)); outputName += \"-BW16Dist\";\n-//\t\tprobCalcs.add(new BiasiWesnousky2017JumpAzChangeProb(distAzCalc)); outputName += \"-BW17Az\";\n-//\t\tprobCalcs.add(new BiasiWesnousky2017MechChangeProb()); outputName += \"-BW17Mech\";\n-//\t\tconfigBuilder.cumulativeProbability(probThresh, probCalcs.toArray(new RuptureProbabilityCalc[0]));\n-\t\tconfigBuilder.cumulativeProbability(probThresh,\n-\t\t\t\tCumulativeProbabilityFilter.getPrefferedBWCalcs(distAzCalc)); outputName += \"-BW16-17\";\n-\t\t\n-\t\t// new penalty-based cumulative filter (cumulatives should always be first for efficiency)\n-//\t\tfloat penThresh = 10f;\n-//\t\toutputName += \"_cmlPen\"+new DecimalFormat(\"0.#\").format(penThresh);\n-//\t\tboolean noDoubleCount = false;\n-//\t\tif (noDoubleCount)\n-//\t\t\toutputName += \"-noDblCnt\";\n-//\t\tList<Penalty> penalties = new ArrayList<>();\n-////\t\tpenalties.add(new JumpPenalty(3f, 2d, false)); outputName += \"-2xJump3km\";\n-////\t\tpenalties.add(new JumpPenalty(1f, 1d, false)); outputName += \"-jump1km\";\n-//\t\tpenalties.add(new JumpPenalty(0f, 1d, true)); outputName += \"-jumpScalar1x\";\n-//\t\tpenalties.add(new RakeChangePenalty(45f, 2d, false)); outputName += \"-2xrake45\";\n-//\t\tpenalties.add(new DipChangePenalty(20, 1d, false)); outputName += \"-dip20\";\n-////\t\tpenalties.add(new AzimuthChangePenalty(20f, 1d, false,\n-////\t\t\t\tnew JumpAzimuthChangeFilter.SimpleAzimuthCalc(distAzCalc))); outputName += \"-az20\";\n-//\t\tpenalties.add(new AzimuthChangePenalty(0f, 6d/180d, true,\n-//\t\t\t\tnew JumpAzimuthChangeFilter.SimpleAzimuthCalc(distAzCalc))); outputName += \"-azScale6\";\n-//\t\tconfigBuilder.cumulativePenalty(penThresh, noDoubleCount, penalties.toArray(new Penalty[0]));\n-\t\t\n-\t\t// other cumulatives\n-//\t\tconfigBuilder.cumulativeRakeChange(180f); outputName += \"_cmlRake\";\n-\t\t\n-\t\t// new Coulomb filters (path is current preffered)\n-\t\tconfigBuilder.clusterPathCoulomb(stiffnessCalc,\n-\t\t\t\tStiffnessAggregationMethod.MEDIAN, 0f); outputName += \"_cffClusterPathPositive\";\n-//\t\tconfigBuilder.clusterPathCoulomb(stiffnessCalc,\n-//\t\t\t\tStiffnessAggregationMethod.MEDIAN, 0f, 0.5f); outputName += \"_cffClusterHalfPathsPositive\";\n-//\t\tconfigBuilder.parentCoulomb(stiffnessCalc, StiffnessAggregationMethod.MEDIAN,\n-//\t\t\t0f, Directionality.EITHER); outputName += \"_cffParentPositive\";\n-//\t\tconfigBuilder.clusterCoulomb(\n-//\t\tstiffnessCalc, StiffnessAggregationMethod.MEDIAN, 0f); outputName += \"_cffClusterPositive\";\n-//\t\tconfigBuilder.netRupCoulomb(stiffnessCalc, StiffnessAggregationMethod.MEDIAN, 0f,\n-//\t\t\t\tRupCoulombQuantity.SUM_SECT_CFF); outputName += \"_cffRupNetPositive\";\n-//\t\tconfigBuilder.netClusterCoulomb(stiffnessCalc,\n-//\t\t\t\tStiffnessAggregationMethod.MEDIAN, 0f); outputName += \"_cffClusterNetPositive\";\n+\t\t/*\n+\t\t * Regular slip prob\n+\t\t */\n+\t\t// SLIP RATE PROB: only increasing\n+\t\tif (slipRateProb > 0f) {\n+\t\t\tconfigBuilder.cumulativeProbability(slipRateProb, new RelativeSlipRateProb(connectionStrategy, true));\n+\t\t\toutputName += \"_slipP\"+slipRateProb+\"incr\";\n+\t\t}\n+\t\t// END SLIP RATE PROB\n+\t\t\n+\t\t/*\n+\t\t * Regular CFF prob (not currently used)\n+\t\t */\n+\t\t// CFF prob: allow neg, 0.01\n+//\t\tconfigBuilder.cumulativeProbability(0.01f, new RelativeCoulombProb(\n+//\t\t\t\tsumAgg, connectionStrategy, false, true, true));\n+//\t\toutputName += \"_cffP0.01incr\";\n+\t\t// END SLIP RATE PROB\n+\t\t\n+\t\t/*\n+\t\t *  CFF net rupture filters\n+\t\t */\n+\t\t// FRACT INTERACTIONS POSITIVE\n+\t\tif (cffFractInts > 0f) {\n+\t\t\tconfigBuilder.netRupCoulomb(fractIntsAgg,\n+\t\t\t\t\tRange.greaterThan(cffFractInts));\n+\t\t\toutputName += \"_cff\"+cffFractInts+\"IntsPos\";\n+\t\t}\n+\t\t// END MAIN 3/4 INTERACTIONS POSITIVE\n+\t\t\n+\t\t/**\n+\t\t * Path filters\n+\t\t */\n+\t\tList<NucleationClusterEvaluator> combPathEvals = new ArrayList<>();\n+\t\tList<String> combPathPrefixes = new ArrayList<>();\n+\t\tfloat fractPathsThreshold = 0f; String fractPathsStr = \"\";\n+\t\tfloat favorableDist = Float.max((float)maxJumpDist, 10f);\n+\t\tString favStr = \"\";\n+\t\tif (favorableJumps) {\n+\t\t\tfavStr = \"Fav\";\n+\t\t\tif (favorableDist != (float)maxJumpDist)\n+\t\t\t\tfavStr += (int)favorableDist;\n+\t\t}\n+\t\t// SLIP RATE PROB: as a path, only increasing NOT CURRENTLY PREFERRED\n+//\t\tfloat pathSlipProb = 0.1f;\n+//\t\tCumulativeJumpProbPathEvaluator slipEval = new CumulativeJumpProbPathEvaluator(\n+//\t\t\t\tpathSlipProb, PlausibilityResult.FAIL_HARD_STOP, new RelativeSlipRateProb(connectionStrategy, true));\n+//\t\tcombPathEvals.add(slipEval); combPathPrefixes.add(\"slipP\"+pathSlipProb+\"incr\");\n+////\t\tconfigBuilder.path(slipEval); outputName += \"_slipPathP\"+pathSlipProb+\"incr\"; // do it separately\n+\t\t// END SLIP RATE PROB\n+\t\t// CFF PROB: as a path, allow negative, 0.01\n+\t\tif (cffRelativeProb > 0f) {\n+\t\t\tRelativeCoulombProb cffProbCalc = new RelativeCoulombProb(\n+\t\t\t\t\tsumAgg, connectionStrategy, false, true, favorableJumps, favorableDist, distAzCalc);\n+\t\t\tCumulativeProbPathEvaluator cffProbPathEval = new CumulativeProbPathEvaluator(\n+\t\t\t\t\tcffRelativeProb, PlausibilityResult.FAIL_HARD_STOP, cffProbCalc);\n+\t\t\tcombPathEvals.add(cffProbPathEval); combPathPrefixes.add(\"cff\"+favStr+\"P\"+cffRelativeProb);\n+\t\t}\n+//\t\tconfigBuilder.path(cffProbPathEval); outputName += \"_cffPathP0.01\"; // do it separately\n+\t\t// CFF SECT PATH: relBest, 15km\n+//\t\tSectCoulombPathEvaluator prefCFFSectPathEval = new SectCoulombPathEvaluator(\n+//\t\t\t\tsumAgg, Range.atLeast(0f), PlausibilityResult.FAIL_HARD_STOP, true, 15f, distAzCalc);\n+//\t\tcombPathEvals.add(prefCFFSectPathEval); combPathPrefixes.add(\"cffSPathFav15\");\n+////\t\tconfigBuilder.path(prefCFFSectPathEval); outputName += \"_cffSPathFav15\"; // do it separately\n+\t\t// END CFF SECT PATH\n+\t\t// CFF CLUSTER PATH: half RPatches positive\n+//\t\tClusterCoulombPathEvaluator prefCFFRPatchEval = new ClusterCoulombPathEvaluator(\n+//\t\t\t\tfractRpatchPosAgg, Range.atLeast(0.5f), PlausibilityResult.FAIL_HARD_STOP);\n+//\t\tcombPathEvals.add(prefCFFRPatchEval); combPathPrefixes.add(\"cffCPathRPatchHalfPos\");\n+////\t\tconfigBuilder.path(prefCFFRPatchEval); outputName += \"_cffCPathRPatchHalfPos\"; // do it separately\n+\t\t// END CFF CLUSTER PATH\n+\t\t// CFF RATIO PATH: N=2, relBest, 15km\n+\t\tif (cffRatioThresh > 0f) {\n+\t\t\tCumulativeProbPathEvaluator cffRatioPatchEval = new CumulativeProbPathEvaluator(cffRatioThresh,\n+\t\t\t\t\tPlausibilityResult.FAIL_HARD_STOP,\n+\t\t\t\t\tnew CoulombSectRatioProb(sumAgg, cffRatioN, favorableJumps, favorableDist, distAzCalc));\n+\t\t\tcombPathEvals.add(cffRatioPatchEval);\n+\t\t\tcombPathPrefixes.add(\"cff\"+favStr+\"RatioN\"+cffRatioN+\"P\"+cffRatioThresh);\n+\t\t}\n+//\t\tconfigBuilder.path(prefCFFRPatchEval); outputName += \"_cffCPathRPatchHalfPos\"; // do it separately\n+\t\t// END CFF RATIO PATH\n+\t\t// add them\n+\t\tPreconditions.checkState(combPathEvals.size() == combPathPrefixes.size());\n+\t\tif (!combPathEvals.isEmpty()) {\n+\t\t\tconfigBuilder.path(fractPathsThreshold, combPathEvals.toArray(new NucleationClusterEvaluator[0]));\n+\t\t\toutputName += \"_\";\n+\t\t\tif (combPathEvals.size() > 1)\n+\t\t\t\toutputName += \"comb\"+combPathEvals.size();\n+\t\t\toutputName += fractPathsStr;\n+\t\t\tif (fractPathsStr.isEmpty() && combPathEvals.size() == 1) {\n+\t\t\t\toutputName += \"path\";\n+\t\t\t} else {\n+\t\t\t\toutputName += \"Path\";\n+\t\t\t\tif (combPathEvals.size() > 1)\n+\t\t\t\t\toutputName += \"s\";\n+\t\t\t}\n+\t\t\toutputName += \"_\"+Joiner.on(\"_\").join(combPathPrefixes);\n+\t\t}\n \n \t\t// Check connectivity only (maximum 2 clusters per rupture)\n //\t\tconfigBuilder.maxNumClusters(2); outputName += \"_connOnly\";\n \t\t\n+//\t\tFile outputDir = new File(rupSetsDir, \"fm3_1_plausible10km_slipP0.05incr_cff3_4_IntsPos_comb2Paths_cffP0.05_cffRatioN2P0.5_sectFractPerm0.05_comp\");\n+//\t\tPreconditions.checkState(outputDir.exists() || outputDir.mkdir());\n+\t\tFile outputDir = rupSetsDir;\n+\t\t\n \t\t/*\n \t\t * Splay constraints\n \t\t */\n \t\tconfigBuilder.maxSplays(0); // default, no splays\n //\t\tconfigBuilder.maxSplays(1); outputName += \"_max1Splays\";\n ////\t\tconfigBuilder.splayLength(0.1, true, true); outputName += \"_splayLenFract0.1\";\n-//\t\tconfigBuilder.splayLength(50, false, true); outputName += \"_splayLen50km\";\n+//\t\tconfigBuilder.splayLength(100, false, true); outputName += \"_splayLen100km\";\n \t\t\n \t\t/*\n \t\t * Permutation strategies: how should faults be broken up into permutations, combinations of which\n \t\t * will be used to construct ruptures\n \t\t */\n \t\t// regular (UCERF3) permutation strategy\n-\t\tClusterPermutationStrategy permStrat = new UCERF3ClusterPermuationStrategy();\n+//\t\tClusterPermutationStrategy permStrat = new UCERF3ClusterPermuationStrategy();\n \t\t// only permutate at connection points or subsection end points\n \t\t// (for strict segmentation or testing connection points)\n //\t\tClusterPermutationStrategy permStrat = new ConnectionPointsPermutationStrategy();\n-//\t\toutputName += \"_connPointsPerm\"\n+//\t\toutputName += \"_connPointsPerm\";\n \t\t// build permutations adaptively (skip over some end points for larger ruptures)\n-//\t\tfloat sectFract = 0.1f;\n-//\t\tSectCountAdaptivePermutationStrategy permStrat = new SectCountAdaptivePermutationStrategy(sectFract, true);\n-//\t\tconfigBuilder.add(permStrat.buildConnPointCleanupFilter(connectionStrategy));\n-//\t\toutputName += \"_sectFractPerm\"+sectFract;\n+\t\tfloat sectFract = 0.05f;\n+\t\tSectCountAdaptivePermutationStrategy permStrat = new SectCountAdaptivePermutationStrategy(sectFract, true);\n+\t\tconfigBuilder.add(permStrat.buildConnPointCleanupFilter(connectionStrategy));\n+\t\toutputName += \"_sectFractPerm\"+sectFract;\n \t\t\n \t\t// build our configuration\n \t\tPlausibilityConfiguration config = configBuilder.build();\n@@ -917,81 +1410,21 @@ public static void main(String[] args) throws IOException, DocumentException {\n \t\t\n \t\tif (debugCriteria != null)\n \t\t\tbuilder.setDebugCriteria(debugCriteria, stopAfterDebug);\n-\t\t\n-\t\tint threads = Runtime.getRuntime().availableProcessors()-2;\n-//\t\tint threads = 1;\n \t\tSystem.out.println(\"Building ruptures with \"+threads+\" threads...\");\n \t\tStopwatch watch = Stopwatch.createStarted();\n \t\tList<ClusterRupture> rups = builder.build(permStrat, threads);\n \t\twatch.stop();\n-\t\tdouble secs = watch.elapsed(TimeUnit.MILLISECONDS)/1000d;\n+\t\tlong millis = watch.elapsed(TimeUnit.MILLISECONDS);\n+\t\tdouble secs = millis/1000d;\n \t\tdouble mins = (secs / 60d);\n \t\tDecimalFormat timeDF = new DecimalFormat(\"0.00\");\n \t\tSystem.out.println(\"Built \"+countDF.format(rups.size())+\" ruptures in \"+timeDF.format(secs)\n-\t\t\t+\" secs = \"+timeDF.format(mins)+\" mins\");\n+\t\t\t+\" secs = \"+timeDF.format(mins)+\" mins. Total rate: \"+rupRate(rups.size(), millis));\n \t\t\n \t\tif (writeRupSet) {\n-\t\t\t// write out test rup set\n-//\t\t\tdouble[] mags = new double[rups.size()];\n-//\t\t\tdouble[] lenghts = new double[rups.size()];\n-//\t\t\tdouble[] rakes = new double[rups.size()];\n-//\t\t\tdouble[] rupAreas = new double[rups.size()];\n-//\t\t\tList<List<Integer>> sectionForRups = new ArrayList<>();\n-//\t\t\tfor (int r=0; r<rups.size(); r++) {\n-//\t\t\t\tList<FaultSection> sects = rups.get(r).buildOrderedSectionList();\n-//\t\t\t\tList<Integer> ids = new ArrayList<>();\n-//\t\t\t\tfor (FaultSection sect : sects)\n-//\t\t\t\t\tids.add(sect.getSectionId());\n-//\t\t\t\tsectionForRups.add(ids);\n-//\t\t\t\tmags[r] = Double.NaN;\n-//\t\t\t\trakes[r] = Double.NaN;\n-//\t\t\t\trupAreas[r] = Double.NaN;\n-//\t\t\t}\n-\t\t\tdouble[] sectSlipRates = new double[subSects.size()];\n-\t\t\tdouble[] sectAreasReduced = new double[subSects.size()];\n-\t\t\tdouble[] sectAreasOrig = new double[subSects.size()];\n-\t\t\tfor (int s=0; s<sectSlipRates.length; s++) {\n-\t\t\t\tFaultSection sect = subSects.get(s);\n-\t\t\t\tsectAreasReduced[s] = sect.getArea(true);\n-\t\t\t\tsectAreasOrig[s] = sect.getArea(false);\n-\t\t\t\tsectSlipRates[s] = sect.getReducedAveSlipRate()*1e-3; // mm/yr => m/yr\n-\t\t\t}\n-\t\t\tdouble[] rupMags = new double[rups.size()];\n-\t\t\tdouble[] rupRakes = new double[rups.size()];\n-\t\t\tdouble[] rupAreas = new double[rups.size()];\n-\t\t\tdouble[] rupLengths = new double[rups.size()];\n-\t\t\tList<List<Integer>> rupsIDsList = new ArrayList<>();\n-\t\t\tfor (int r=0; r<rups.size(); r++) {\n-\t\t\t\tClusterRupture rup = rups.get(r);\n-\t\t\t\tList<FaultSection> rupSects = rup.buildOrderedSectionList();\n-\t\t\t\tList<Integer> sectIDs = new ArrayList<>();\n-\t\t\t\tdouble totLength = 0d;\n-\t\t\t\tdouble totArea = 0d;\n-\t\t\t\tdouble totOrigArea = 0d; // not reduced for aseismicity\n-\t\t\t\tList<Double> sectAreas = new ArrayList<>();\n-\t\t\t\tList<Double> sectRakes = new ArrayList<>();\n-\t\t\t\tfor (FaultSection sect : rupSects) {\n-\t\t\t\t\tsectIDs.add(sect.getSectionId());\n-\t\t\t\t\tdouble length = sect.getTraceLength()*1e3;\t// km --> m\n-\t\t\t\t\ttotLength += length;\n-\t\t\t\t\tdouble area = sectAreasReduced[sect.getSectionId()];\t// sq-m\n-\t\t\t\t\ttotArea += area;\n-\t\t\t\t\ttotOrigArea += sectAreasOrig[sect.getSectionId()];\t// sq-m\n-\t\t\t\t\tsectAreas.add(area);\n-\t\t\t\t\tsectRakes.add(sect.getAveRake());\n-\t\t\t\t}\n-\t\t\t\trupAreas[r] = totArea;\n-\t\t\t\trupLengths[r] = totLength;\n-\t\t\t\trupRakes[r] = FaultUtils.getInRakeRange(FaultUtils.getScaledAngleAverage(sectAreas, sectRakes));\n-\t\t\t\tdouble origDDW = totOrigArea/totLength;\n-\t\t\t\trupMags[r] = scale.getMag(totArea, origDDW);\n-\t\t\t\trupsIDsList.add(sectIDs);\n-\t\t\t}\n-\t\t\tFaultSystemRupSet rupSet = new FaultSystemRupSet(subSects, sectSlipRates, null, sectAreasReduced, \n-\t\t\t\t\trupsIDsList, rupMags, rupRakes, rupAreas, rupLengths, \"\");\n-\t\t\trupSet.setPlausibilityConfiguration(config);\n-\t\t\trupSet.setClusterRuptures(rups);\n-\t\t\tFaultSystemIO.writeRupSet(rupSet, new File(rupSetsDir, outputName));\n+\t\t\tFile outputFile = new File(outputDir, outputName);\n+\t\t\tFaultSystemRupSet rupSet = buildClusterRupSet(scale, subSects, config, rups);\n+\t\t\tFaultSystemIO.writeRupSet(rupSet, outputFile);\n \t\t}\n \n \t\tif (numAzCached < distAzCalc.getNumCachedAzimuths()\n@@ -1001,12 +1434,61 @@ public static void main(String[] args) throws IOException, DocumentException {\n \t\t\tSystem.out.println(\"DONE writing dist/az cache\");\n \t\t}\n \t\t\n-\t\tif (stiffnessCalc != null && stiffnessCacheFile != null\n-\t\t\t\t&& stiffnessCacheSize < stiffnessCalc.calcCacheSize()) {\n+\t\tif (stiffnessCache != null && stiffnessCacheFile != null\n+\t\t\t\t&& stiffnessCacheSize < stiffnessCache.calcCacheSize()) {\n \t\t\tSystem.out.println(\"Writing stiffness cache to \"+stiffnessCacheFile.getAbsolutePath());\n-\t\t\tstiffnessCalc.writeCacheFile(stiffnessCacheFile, StiffnessType.CFF);\n+\t\t\tstiffnessCache.writeCacheFile(stiffnessCacheFile);\n \t\t\tSystem.out.println(\"DONE writing stiffness cache\");\n \t\t}\n \t}\n \n+\tpublic static FaultSystemRupSet buildClusterRupSet(ScalingRelationships scale, List<? extends FaultSection> subSects,\n+\t\t\tPlausibilityConfiguration config, List<ClusterRupture> rups) {\n+\t\tdouble[] sectSlipRates = new double[subSects.size()];\n+\t\tdouble[] sectAreasReduced = new double[subSects.size()];\n+\t\tdouble[] sectAreasOrig = new double[subSects.size()];\n+\t\tfor (int s=0; s<sectSlipRates.length; s++) {\n+\t\t\tFaultSection sect = subSects.get(s);\n+\t\t\tsectAreasReduced[s] = sect.getArea(true);\n+\t\t\tsectAreasOrig[s] = sect.getArea(false);\n+\t\t\tsectSlipRates[s] = sect.getReducedAveSlipRate()*1e-3; // mm/yr => m/yr\n+\t\t}\n+\t\tdouble[] rupMags = new double[rups.size()];\n+\t\tdouble[] rupRakes = new double[rups.size()];\n+\t\tdouble[] rupAreas = new double[rups.size()];\n+\t\tdouble[] rupLengths = new double[rups.size()];\n+\t\tList<List<Integer>> rupsIDsList = new ArrayList<>();\n+\t\tfor (int r=0; r<rups.size(); r++) {\n+\t\t\tClusterRupture rup = rups.get(r);\n+\t\t\tList<FaultSection> rupSects = rup.buildOrderedSectionList();\n+\t\t\tList<Integer> sectIDs = new ArrayList<>();\n+\t\t\tdouble totLength = 0d;\n+\t\t\tdouble totArea = 0d;\n+\t\t\tdouble totOrigArea = 0d; // not reduced for aseismicity\n+\t\t\tList<Double> sectAreas = new ArrayList<>();\n+\t\t\tList<Double> sectRakes = new ArrayList<>();\n+\t\t\tfor (FaultSection sect : rupSects) {\n+\t\t\t\tsectIDs.add(sect.getSectionId());\n+\t\t\t\tdouble length = sect.getTraceLength()*1e3;\t// km --> m\n+\t\t\t\ttotLength += length;\n+\t\t\t\tdouble area = sectAreasReduced[sect.getSectionId()];\t// sq-m\n+\t\t\t\ttotArea += area;\n+\t\t\t\ttotOrigArea += sectAreasOrig[sect.getSectionId()];\t// sq-m\n+\t\t\t\tsectAreas.add(area);\n+\t\t\t\tsectRakes.add(sect.getAveRake());\n+\t\t\t}\n+\t\t\trupAreas[r] = totArea;\n+\t\t\trupLengths[r] = totLength;\n+\t\t\trupRakes[r] = FaultUtils.getInRakeRange(FaultUtils.getScaledAngleAverage(sectAreas, sectRakes));\n+\t\t\tdouble origDDW = totOrigArea/totLength;\n+\t\t\trupMags[r] = scale.getMag(totArea, origDDW);\n+\t\t\trupsIDsList.add(sectIDs);\n+\t\t}\n+\t\tFaultSystemRupSet rupSet = new FaultSystemRupSet(subSects, sectSlipRates, null, sectAreasReduced, \n+\t\t\t\trupsIDsList, rupMags, rupRakes, rupAreas, rupLengths, \"\");\n+\t\trupSet.setPlausibilityConfiguration(config);\n+\t\trupSet.setClusterRuptures(rups);\n+\t\treturn rupSet;\n+\t}\n+\n }"
  },
  {
    "sha": "94cf88d24660ddb7645e7f9b66548e266f792ceb",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/FaultSubsectionCluster.java",
    "status": "modified",
    "additions": 25,
    "deletions": 3,
    "changes": 28,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/FaultSubsectionCluster.java",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/FaultSubsectionCluster.java",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/FaultSubsectionCluster.java?ref=541efed3666ce0a47e03a345aada595633351042",
    "patch": "@@ -99,6 +99,11 @@ public void addConnection(Jump jump) {\n \t\tPreconditions.checkState(jump.fromCluster == this);\n \t\tPreconditions.checkState(jump.fromSection.getParentSectionId() == parentSectionID);\n \t\tPreconditions.checkState(contains(jump.fromSection));\n+\t\tfor (Jump existing : possibleJumps)\n+\t\t\tif (existing.toCluster.equals(jump.toCluster) && existing.toSection.equals(jump.toSection)\n+\t\t\t\t\t&& existing.fromSection.equals(jump.fromSection))\n+\t\t\t\t// this is a duplicate, skip adding\n+\t\t\t\treturn;\n \t\tpossibleJumps.add(jump);\n \t}\n \t\n@@ -141,7 +146,8 @@ else if (jump.distance < prevDist)\n \t}\n \t\n \tpublic List<Jump> getConnections() {\n-\t\treturn ImmutableList.copyOf(possibleJumps);\n+\t\treturn Collections.unmodifiableList(possibleJumps);\n+//\t\treturn ImmutableList.copyOf(possibleJumps);\n \t}\n \t\n \tpublic Collection<Jump> getConnections(FaultSection exitPoint) {\n@@ -154,12 +160,17 @@ else if (jump.distance < prevDist)\n \t}\n \t@Override\n \tpublic String toString() {\n+\t\treturn toString(-1);\n+\t}\n+\tpublic String toString(int jumpToID) {\n \t\tStringBuilder str = null;\n \t\tfor (FaultSection sect : subSects) {\n \t\t\tif (str == null)\n \t\t\t\tstr = new StringBuilder(\"[\").append(parentSectionID).append(\":\");\n \t\t\telse\n \t\t\t\tstr.append(\",\");\n+\t\t\tif (sect.getSectionId() == jumpToID)\n+\t\t\t\tstr.append(\"->\");\n \t\t\tstr.append(sect.getSectionId());\n \t\t}\n \t\treturn str.append(\"]\").toString();\n@@ -278,16 +289,21 @@ private static FaultSection getSect(List<? extends FaultSection> allSects, int s\n \t * This will read the next FaultSubsectionCluster from a JsonReader. This gets complicated because\n \t * jumps cannot be setup until all clusters have been read, so instead you must supply a map\n \t * of FaultSubsectionClusters to a list of JumpStub instances. Once all clusters have been read\n-\t * from JSON, you can build/add all of the ruptures with the buildJumpsFromStubs(..) method\n+\t * from JSON, you can build/add all of the ruptures with the buildJumpsFromStubs(..) method.\n+\t * \n+\t * If prevClustersMap is supplied, then previous instances of the exact same cluster will be reused in\n+\t * order to conserve memory.\n \t * \n \t * @param in\n \t * @param allSects\n \t * @param jumpStubsMap\n+\t * @param prevClustersMap\n \t * @return\n \t * @throws IOException\n \t */\n \tpublic static FaultSubsectionCluster readJSON(JsonReader in, List<? extends FaultSection> allSects,\n-\t\t\tMap<FaultSubsectionCluster, List<JumpStub>> jumpStubsMap) throws IOException {\n+\t\t\tMap<FaultSubsectionCluster, List<JumpStub>> jumpStubsMap,\n+\t\t\tMap<FaultSubsectionCluster, FaultSubsectionCluster> prevClustersMap) throws IOException {\n \t\tin.beginObject();\n \t\t\n \t\tInteger parentID = null;\n@@ -367,6 +383,12 @@ public static FaultSubsectionCluster readJSON(JsonReader in, List<? extends Faul\n \t\t\tcluster = new FaultSubsectionCluster(subSects, endSects);\n \t\telse\n \t\t\tcluster = new FaultSubsectionCluster(subSects);\n+\t\tif (prevClustersMap != null) {\n+\t\t\tif (prevClustersMap.containsKey(cluster))\n+\t\t\t\tcluster = prevClustersMap.get(cluster);\n+\t\t\telse\n+\t\t\t\tprevClustersMap.put(cluster, cluster);\n+\t\t}\n \t\tif (jumps != null) {\n \t\t\tif (jumpStubsMap == null)\n \t\t\t\tSystem.err.println(\"WARNING: skipping loading all jumps (jumpStubsMap is null)\");"
  },
  {
    "sha": "9e162f8ea661c4d6696731bcb390049c1337f0c5",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/Jump.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/Jump.java",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/Jump.java",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/Jump.java?ref=541efed3666ce0a47e03a345aada595633351042",
    "patch": "@@ -72,7 +72,7 @@ public String toString() {\n \t}\n \t\n \t/**\n-\t * @return reversed view of this jump\n+\t * @return reversed view of this jump (note that this doesn't reverse the clusters themselves)\n \t */\n \tpublic Jump reverse() {\n \t\treturn new Jump(toSection, toCluster, fromSection, fromCluster, distance);"
  },
  {
    "sha": "e019bfa863ba5c90eae0e2096e61665cec77058e",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/PlausibilityConfiguration.java",
    "status": "modified",
    "additions": 360,
    "deletions": 90,
    "changes": 450,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/PlausibilityConfiguration.java",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/PlausibilityConfiguration.java",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/PlausibilityConfiguration.java?ref=541efed3666ce0a47e03a345aada595633351042",
    "patch": "@@ -16,37 +16,32 @@\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.ClusterRupture;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.Jump;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.PlausibilityFilter.PlausibilityFilterTypeAdapter;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.ClusterCoulombCompatibilityFilter;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.ClusterPathCoulombCompatibilityFilter;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.CumulativeAzimuthChangeFilter;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.CumulativePenaltyFilter;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.*;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.CumulativePenaltyFilter.Penalty;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.CumulativeProbabilityFilter;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.CumulativeProbabilityFilter.RuptureProbabilityCalc;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.CumulativeRakeChangeFilter;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.JumpAzimuthChangeFilter;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.JumpAzimuthChangeFilter.AzimuthCalc;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.MinSectsPerParentFilter;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.NetClusterCoulombFilter;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.NetRuptureCoulombFilter;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.NumClustersFilter;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.ParentCoulombCompatibilityFilter;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.ParentCoulombCompatibilityFilter.Directionality;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.coulomb.*;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.coulomb.ParentCoulombCompatibilityFilter.Directionality;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.path.*;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.path.PathPlausibilityFilter.*;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.prob.*;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.SplayLengthFilter;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.TotalAzimuthChangeFilter;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.U3CompatibleCumulativeRakeChangeFilter;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.U3CoulombJunctionFilter;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.strategies.ClusterConnectionStrategy;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.strategies.ClusterConnectionStrategy.ConnStratTypeAdapter;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.strategies.DistCutoffClosestSectClusterConnectionStrategy;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.strategies.UCERF3ClusterConnectionStrategy;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.util.SectionDistanceAzimuthCalculator;\n import org.opensha.sha.faultSurface.FaultSection;\n+import org.opensha.sha.simulators.stiffness.AggregatedStiffnessCalculator;\n import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator;\n-import org.opensha.sha.simulators.stiffness.RuptureCoulombResult.RupCoulombQuantity;\n-import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator.StiffnessAggregationMethod;\n+import org.opensha.sha.simulators.stiffness.AggregatedStiffnessCalculator.AggregationMethod;\n+import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator.PatchAlignment;\n+import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator.StiffnessType;\n \n import com.google.common.base.Preconditions;\n+import com.google.common.collect.BoundType;\n+import com.google.common.collect.Range;\n import com.google.gson.Gson;\n import com.google.gson.GsonBuilder;\n import com.google.gson.TypeAdapter;\n@@ -146,7 +141,8 @@ public Builder u3All(CoulombRates coulombRates) {\n \t\t\tu3Azimuth();\n \t\t\tu3Cumulatives();\n \t\t\tminSectsPerParent(2, true, true);\n-\t\t\tu3Coulomb(coulombRates);\n+\t\t\tif (coulombRates != null)\n+\t\t\t\tu3Coulomb(coulombRates);\n \t\t\treturn this;\n \t\t}\n \t\t\n@@ -173,9 +169,16 @@ public Builder u3Cumulatives() {\n \t\t}\n \t\t\n \t\tpublic Builder u3Coulomb(CoulombRates coulombRates) {\n+\t\t\treturn u3Coulomb(coulombRates, null);\n+\t\t}\n+\t\t\n+\t\tpublic Builder u3Coulomb(CoulombRates coulombRates, SubSectStiffnessCalculator fallbackCalc) {\n \t\t\tCoulombRatesTester coulombTester = new CoulombRatesTester(\n \t\t\t\t\tTestType.COULOMB_STRESS, 0.04, 0.04, 1.25d, true, true);\n-\t\t\tfilters.add(new U3CoulombJunctionFilter(coulombTester, coulombRates));\n+\t\t\tU3CoulombJunctionFilter filter = new U3CoulombJunctionFilter(coulombTester, coulombRates);\n+\t\t\tif (fallbackCalc != null)\n+\t\t\t\tfilter.setFallbackCalculator(fallbackCalc, connectionStrategy);\n+\t\t\tfilters.add(filter);\n \t\t\treturn this;\n \t\t}\n \t\t\n@@ -185,40 +188,78 @@ public Builder minSectsPerParent(int minPerParent, boolean allowIfNoDirect, bool\n \t\t\treturn this;\n \t\t}\n \t\t\n-\t\tpublic Builder clusterCoulomb(SubSectStiffnessCalculator subSectCalc,\n-\t\t\t\tStiffnessAggregationMethod aggMethod, float threshold) {\n-\t\t\tfilters.add(new ClusterCoulombCompatibilityFilter(subSectCalc, aggMethod, threshold));\n+\t\tpublic Builder noIndirectPaths() {\n+\t\t\tfilters.add(new DirectPathPlausibilityFilter(connectionStrategy));\n+\t\t\treturn this;\n+\t\t}\n+\t\t\n+\t\tpublic Builder clusterCoulomb(AggregatedStiffnessCalculator aggCalc, float threshold) {\n+\t\t\tfilters.add(new ClusterCoulombCompatibilityFilter(aggCalc, threshold));\n+\t\t\treturn this;\n+\t\t}\n+\t\t\n+\t\tpublic Builder clusterPathCoulomb(AggregatedStiffnessCalculator aggCalc, float threshold) {\n+\t\t\treturn clusterPathCoulomb(aggCalc, Range.atLeast(threshold));\n+\t\t}\n+\t\t\n+\t\tpublic Builder clusterPathCoulomb(AggregatedStiffnessCalculator aggCalc, Range<Float> acceptableRange) {\n+\t\t\treturn clusterPathCoulomb(aggCalc, acceptableRange, 0f, false);\n+\t\t}\n+\t\t\n+\t\tpublic Builder clusterPathCoulomb(AggregatedStiffnessCalculator aggCalc, Range<Float> acceptableRange,\n+\t\t\t\tfloat fractPathsThreshold, boolean failFuturePossible) {\n+\t\t\tPlausibilityResult failureType = failFuturePossible ? PlausibilityResult.FAIL_FUTURE_POSSIBLE : PlausibilityResult.FAIL_HARD_STOP;\n+\t\t\treturn path(fractPathsThreshold, new ClusterCoulombPathEvaluator(aggCalc, acceptableRange, failureType));\n+\t\t}\n+\t\t\n+\t\tpublic Builder sectPathCoulomb(AggregatedStiffnessCalculator aggCalc, float threshold) {\n+\t\t\treturn sectPathCoulomb(aggCalc, Range.atLeast(threshold), false, 0f);\n+\t\t}\n+\t\t\n+\t\tpublic Builder sectPathCoulomb(AggregatedStiffnessCalculator aggCalc, Range<Float> acceptableRange,\n+\t\t\t\tboolean jumpToMostFavorable, float maxJumpDist) {\n+\t\t\treturn sectPathCoulomb(aggCalc, acceptableRange, 0f, jumpToMostFavorable, maxJumpDist, false);\n+\t\t}\n+\t\t\n+\t\tpublic Builder sectPathCoulomb(AggregatedStiffnessCalculator aggCalc, Range<Float> acceptableRange,\n+\t\t\t\tfloat fractPassThreshold, boolean jumpToMostFavorable, float maxJumpDist, boolean failFuturePossible) {\n+\t\t\tPlausibilityResult failureType = failFuturePossible ? PlausibilityResult.FAIL_FUTURE_POSSIBLE : PlausibilityResult.FAIL_HARD_STOP;\n+\t\t\tpath(fractPassThreshold, new SectCoulombPathEvaluator(aggCalc, acceptableRange, failureType, jumpToMostFavorable, maxJumpDist, distAzCalc));\n+\t\t\treturn this;\n+\t\t}\n+\t\t\n+\t\tpublic Builder path(NucleationClusterEvaluator... evaluators) {\n+\t\t\tfilters.add(new PathPlausibilityFilter(evaluators));\n \t\t\treturn this;\n \t\t}\n \t\t\n-\t\tpublic Builder clusterPathCoulomb(SubSectStiffnessCalculator subSectCalc,\n-\t\t\t\tStiffnessAggregationMethod aggMethod, float threshold) {\n-\t\t\tfilters.add(new ClusterPathCoulombCompatibilityFilter(subSectCalc, aggMethod, threshold));\n+\t\tpublic Builder path(float fractPassThreshold, NucleationClusterEvaluator... evaluators) {\n+\t\t\tfilters.add(new PathPlausibilityFilter(fractPassThreshold, evaluators));\n \t\t\treturn this;\n \t\t}\n \t\t\n-\t\tpublic Builder clusterPathCoulomb(SubSectStiffnessCalculator subSectCalc,\n-\t\t\t\tStiffnessAggregationMethod aggMethod, float threshold, float fractPathsThreshold) {\n-\t\t\tfilters.add(new ClusterPathCoulombCompatibilityFilter(\n-\t\t\t\t\tsubSectCalc, aggMethod, threshold, fractPathsThreshold));\n+\t\tpublic Builder path(float fractPassThreshold, boolean logicalOr, NucleationClusterEvaluator... evaluators) {\n+\t\t\tfilters.add(new PathPlausibilityFilter(fractPassThreshold, logicalOr, evaluators));\n \t\t\treturn this;\n \t\t}\n \t\t\n-\t\tpublic Builder parentCoulomb(SubSectStiffnessCalculator subSectCalc,\n-\t\t\t\tStiffnessAggregationMethod aggMethod, float threshold, Directionality directionality) {\n-\t\t\tfilters.add(new ParentCoulombCompatibilityFilter(subSectCalc, aggMethod, threshold, directionality));\n+\t\tpublic Builder parentCoulomb(AggregatedStiffnessCalculator aggCalc, float threshold, Directionality directionality) {\n+\t\t\tfilters.add(new ParentCoulombCompatibilityFilter(aggCalc, threshold, directionality));\n \t\t\treturn this;\n \t\t}\n \t\t\n-\t\tpublic Builder netRupCoulomb(SubSectStiffnessCalculator subSectCalc,\n-\t\t\t\tStiffnessAggregationMethod aggMethod, float threshold, RupCoulombQuantity quantity) {\n-\t\t\tfilters.add(new NetRuptureCoulombFilter(subSectCalc, aggMethod, quantity, threshold));\n+\t\tpublic Builder netRupCoulomb(AggregatedStiffnessCalculator aggCalc, float threshold) {\n+\t\t\tfilters.add(new NetRuptureCoulombFilter(aggCalc, threshold));\n \t\t\treturn this;\n \t\t}\n \t\t\n-\t\tpublic Builder netClusterCoulomb(SubSectStiffnessCalculator subSectCalc,\n-\t\t\t\tStiffnessAggregationMethod aggMethod, float threshold) {\n-\t\t\tfilters.add(new NetClusterCoulombFilter(subSectCalc, aggMethod, threshold));\n+\t\tpublic Builder netRupCoulomb(AggregatedStiffnessCalculator aggCalc, Range<Float> acceptableRange) {\n+\t\t\tfilters.add(new NetRuptureCoulombFilter(aggCalc, acceptableRange));\n+\t\t\treturn this;\n+\t\t}\n+\t\t\n+\t\tpublic Builder netClusterCoulomb(AggregatedStiffnessCalculator aggCalc, float threshold) {\n+\t\t\tfilters.add(new NetClusterCoulombFilter(aggCalc, threshold));\n \t\t\treturn this;\n \t\t}\n \t\t\n@@ -426,6 +467,7 @@ private static Gson buildGson(List<? extends FaultSection> subSects,\n \t\tPlausibilityFilterAdapter filterAdapter = new PlausibilityFilterAdapter(\n \t\t\t\tnull, connStrat, distAzCalc);\n \t\tbuilder.registerTypeHierarchyAdapter(PlausibilityFilterRecord.class, filterAdapter);\n+\t\tbuilder.registerTypeAdapter(TypeToken.getParameterized(Range.class, Float.class).getType(), new FloatRangeTypeAdapter());\n \t\tGson gson = builder.create();\n \t\tconfigAdapter.setGson(gson);\n \t\tfilterAdapter.setGson(gson);\n@@ -601,15 +643,19 @@ public PlausibilityFilterRecord read(JsonReader in) throws IOException {\n \t\t\t\t\tbreak;\n \t\t\t\tcase \"adapter\":\n \t\t\t\t\tPreconditions.checkState(filter == null, \"adapter must be before filter in JSON\");\n+\t\t\t\t\tString adapterClassName = in.nextString();\n \t\t\t\t\ttry {\n \t\t\t\t\t\tClass<TypeAdapter<PlausibilityFilter>> adapterClass =\n-\t\t\t\t\t\t\t\tgetDeclaredTypeClass(in.nextString());\n+\t\t\t\t\t\t\t\tgetDeclaredTypeClass(adapterClassName);\n \t\t\t\t\t\tConstructor<TypeAdapter<PlausibilityFilter>> constructor = adapterClass.getConstructor();\n \t\t\t\t\t\tadapter = constructor.newInstance();\n-\t\t\t\t\t} catch (Exception e) {\n-\t\t\t\t\t\te.printStackTrace();\n+\t\t\t\t\t} catch (ClassNotFoundException e) {\n \t\t\t\t\t\tSystem.err.println(\"Warning: adapter specified but class not found, \"\n \t\t\t\t\t\t\t\t+ \"will attempt default serialization\");\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\t} catch (Exception e) {\n+\t\t\t\t\t\tSystem.err.println(\"Warning: error calling no-arg constructor to instantiate adapter\");\n+\t\t\t\t\t\te.printStackTrace();\n //\t\t\t\t\t\tthrow ExceptionUtils.asRuntimeException(e);\n \t\t\t\t\t\tbreak;\n \t\t\t\t\t}\n@@ -630,22 +676,22 @@ public PlausibilityFilterRecord read(JsonReader in) throws IOException {\n \t\t\t\t\t\tskipUntilEndObject(in, startPath);\n \t\t\t\t\t} else {\n \t\t\t\t\t\tPreconditions.checkNotNull(type, \"filter must be last in json object\");\n-\t\t\t\t\t\tif (adapter == null) {\n-\t\t\t\t\t\t\ttry {\n+\t\t\t\t\t\ttry {\n+\t\t\t\t\t\t\tif (adapter == null) {\n \t\t\t\t\t\t\t\t// use Gson default\n \t\t\t\t\t\t\t\tfilter = gson.fromJson(in, type);\n-\t\t\t\t\t\t\t} catch (Exception e) {\n-\t\t\t\t\t\t\t\te.printStackTrace();\n-\t\t\t\t\t\t\t\tSystem.err.println(\"Warning: couldn't de-serialize filter \"\n-\t\t\t\t\t\t\t\t\t\t+ \"(using stub instead): \"+type.getName());\n-\t\t\t\t\t\t\t\tSystem.err.flush();\n-//\t\t\t\t\t\t\t\tSystem.out.println(\"PATH after read: \"+in.getPath());\n-\t\t\t\t\t\t\t\tfilter = new PlausibilityFilterStub(name, shortName);\n-\t\t\t\t\t\t\t\tskipUntilEndObject(in, startPath);\n+\t\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\t\t// use specified adapter\n+\t\t\t\t\t\t\t\tfilter = adapter.read(in);\n \t\t\t\t\t\t\t}\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\t// use specified adapter\n-\t\t\t\t\t\t\tfilter = adapter.read(in);\n+\t\t\t\t\t\t} catch (Exception e) {\n+\t\t\t\t\t\t\te.printStackTrace();\n+\t\t\t\t\t\t\tSystem.err.println(\"Warning: couldn't de-serialize filter \"\n+\t\t\t\t\t\t\t\t\t+ \"(using stub instead): \"+type.getName());\n+\t\t\t\t\t\t\tSystem.err.flush();\n+//\t\t\t\t\t\t\tSystem.out.println(\"PATH after read: \"+in.getPath());\n+\t\t\t\t\t\t\tfilter = new PlausibilityFilterStub(name, shortName);\n+\t\t\t\t\t\t\tskipUntilEndObject(in, startPath);\n \t\t\t\t\t\t}\n \t\t\t\t\t}\n \t\t\t\t\tbreak;\n@@ -673,12 +719,24 @@ private static void skipUntilEndObject(JsonReader in, String startPath) throws I\n //\t\tSystem.out.println(\"Looking to back out to: \"+startPath);\n \t\twhile (true) {\n \t\t\tString path = in.getPath();\n+//\t\t\tSystem.out.println(\"Path: \"+path);\n \t\t\tJsonToken peek = in.peek();\n+\t\t\tif (peek == JsonToken.BEGIN_OBJECT && path.equals(startPath)) {\n+\t\t\t\t// phew, we haven't gone in yet. just skip over it\n+//\t\t\t\tSystem.out.println(\"DONE: hadn't yet descended into object, can skip\");\n+\t\t\t\tin.skipValue();\n+\t\t\t\tbreak;\n+\t\t\t}\n \t\t\tif (peek == JsonToken.END_OBJECT && path.equals(startPath)) {\n \t\t\t\t// we're ready to break\n //\t\t\t\tSystem.out.println(\"DONE, new path: \"+in.getPath());\n \t\t\t\tbreak;\n \t\t\t}\n+\t\t\tif (peek == JsonToken.END_DOCUMENT) {\n+\t\t\t\t// we've gone too far, end with an error\n+\t\t\t\tin.close();\n+\t\t\t\tthrow new IllegalStateException(\"Failed to skipUnilEndObject to \"+startPath+\", encountered END_DOCUMENT\");\n+\t\t\t}\n //\t\t\tSystem.out.println(\"Still in the thick of it at: \"+path);\n \t\t\tif (peek == JsonToken.END_ARRAY) {\n //\t\t\t\tSystem.out.println(\"\\tending array\");\n@@ -694,13 +752,8 @@ private static void skipUntilEndObject(JsonReader in, String startPath) throws I\n \t}\n \t\n \t@SuppressWarnings(\"unchecked\")\n-\tpublic static <T> Class<T> getDeclaredTypeClass(String className) {\n-\t\tClass<?> raw;\n-\t\ttry {\n-\t\t\traw = Class.forName(className);\n-\t\t} catch (ClassNotFoundException e) {\n-\t\t\tthrow ExceptionUtils.asRuntimeException(e);\n-\t\t}\n+\tpublic static <T> Class<T> getDeclaredTypeClass(String className) throws ClassNotFoundException {\n+\t\tClass<?> raw = Class.forName(className);\n \t\treturn (Class<T>)raw;\n \t}\n \t\n@@ -764,17 +817,34 @@ public DistAzCalcTypeAdapter(SectionDistanceAzimuthCalculator distAzCalc) {\n \n \t\t@Override\n \t\tpublic void write(JsonWriter out, SectionDistanceAzimuthCalculator value) throws IOException {\n-\t\t\tout.beginObject();\n-\t\t\tout.name(\"numSects\").value(value.getSubSections().size());\n-\t\t\tout.endObject();\n+\t\t\tif (value == null) {\n+\t\t\t\t//if the writer was not allowed to write null values\n+\t\t\t\t//do it only for this field\n+\t\t\t\tif (!out.getSerializeNulls()) {\n+\t\t\t\t\tout.setSerializeNulls(true);\n+\t\t\t\t\tout.nullValue();\n+\t\t\t\t\tout.setSerializeNulls(false);\n+\t\t\t\t} else {\n+\t\t\t\t\tout.nullValue();\n+\t\t\t\t}\n+\t\t\t} else {\n+\t\t\t\tout.beginObject();\n+\t\t\t\tout.name(\"numSects\").value(value.getSubSections().size());\n+\t\t\t\tout.endObject();\n+\t\t\t}\n \t\t}\n \n \t\t@Override\n \t\tpublic SectionDistanceAzimuthCalculator read(JsonReader in) throws IOException {\n+\t\t\tif(in.peek() == JsonToken.NULL) {\n+\t\t\t\tin.nextNull();\n+\t\t\t\treturn null;\n+\t\t\t}\n \t\t\tin.beginObject();\n \t\t\tin.nextName();\n \t\t\tint numSects = in.nextInt();\n-\t\t\tPreconditions.checkState(numSects == distAzCalc.getSubSections().size());\n+\t\t\tPreconditions.checkState(numSects == distAzCalc.getSubSections().size(),\n+\t\t\t\t\t\"JSON says %s sects, expected %s\", numSects, distAzCalc.getSubSections().size());\n \t\t\tin.endObject();\n \t\t\treturn distAzCalc;\n \t\t}\n@@ -797,6 +867,9 @@ public void write(JsonWriter out, SubSectStiffnessCalculator calc) throws IOExce\n \t\t\tout.name(\"lameLambda\").value(calc.getLameLambda());\n \t\t\tout.name(\"lameMu\").value(calc.getLameMu());\n \t\t\tout.name(\"coeffOfFriction\").value(calc.getCoeffOfFriction());\n+\t\t\tout.name(\"patchAlignment\").value(calc.getPatchAlignment().name());\n+\t\t\tout.name(\"selfStiffnessCap\").value(calc.getSelfStiffnessCap());\n+//\t\t\tSystem.out.println(\"writing sub sect stiffness calc! cap=\"+calc.getSelfStiffnessCap());\n \t\t\tout.endObject();\n \t\t}\n \n@@ -807,6 +880,8 @@ public SubSectStiffnessCalculator read(JsonReader in) throws IOException {\n \t\t\tDouble lambda = null;\n \t\t\tDouble coeffOfFriction = null;\n \t\t\tDouble gridSpacing = null;\n+\t\t\tdouble selfStiffnessCap = 0d;\n+\t\t\tPatchAlignment alignment = SubSectStiffnessCalculator.alignment_default;\n \t\t\twhile (in.hasNext()) {\n \t\t\t\tswitch (in.nextName()) {\n \t\t\t\tcase \"lameMu\":\n@@ -821,6 +896,12 @@ public SubSectStiffnessCalculator read(JsonReader in) throws IOException {\n \t\t\t\tcase \"gridSpacing\":\n \t\t\t\t\tgridSpacing = in.nextDouble();\n \t\t\t\t\tbreak;\n+\t\t\t\tcase \"patchAlignment\":\n+\t\t\t\t\talignment = PatchAlignment.valueOf(in.nextString());\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase \"selfStiffnessCap\":\n+\t\t\t\t\tselfStiffnessCap = in.nextDouble();\n+\t\t\t\t\tbreak;\n \n \t\t\t\tdefault:\n \t\t\t\t\tbreak;\n@@ -830,19 +911,77 @@ public SubSectStiffnessCalculator read(JsonReader in) throws IOException {\n \t\t\tif (prevCalc != null) {\n \t\t\t\t// see if it's the same\n \t\t\t\tif (gridSpacing == prevCalc.getGridSpacing() && lambda == prevCalc.getLameLambda()\n-\t\t\t\t\t\t&& mu == prevCalc.getLameMu() && coeffOfFriction == prevCalc.getCoeffOfFriction()) {\n+\t\t\t\t\t\t&& mu == prevCalc.getLameMu() && coeffOfFriction == prevCalc.getCoeffOfFriction()\n+\t\t\t\t\t\t&& selfStiffnessCap == prevCalc.getSelfStiffnessCap()) {\n \t\t\t\t\treturn prevCalc;\n \t\t\t\t}\n \t\t\t}\n \t\t\tSubSectStiffnessCalculator calc = new SubSectStiffnessCalculator(\n-\t\t\t\t\tsubSects, gridSpacing, lambda, mu, coeffOfFriction);\n+\t\t\t\t\tsubSects, gridSpacing, lambda, mu, coeffOfFriction, alignment, selfStiffnessCap);\n \t\t\tprevCalc = calc;\n \t\t\treturn calc;\n \t\t}\n \t\t\n \t}\n \t\n+\tprivate static class FloatRangeTypeAdapter extends TypeAdapter<Range<Float>> {\n+\n+\t\t@Override\n+\t\tpublic void write(JsonWriter out, Range<Float> value) throws IOException {\n+\t\t\tout.beginObject();\n+\t\t\tif (value.hasLowerBound())\n+\t\t\t\tout.name(\"lower\").value(value.lowerEndpoint()).name(\"lowerType\").value(value.lowerBoundType().name());\n+\t\t\tif (value.hasUpperBound())\n+\t\t\t\tout.name(\"upper\").value(value.upperEndpoint()).name(\"upperType\").value(value.upperBoundType().name());\n+\t\t\tout.endObject();\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Range<Float> read(JsonReader in) throws IOException {\n+\t\t\tFloat lower = null;\n+\t\t\tBoundType lowerType = null;\n+\t\t\tFloat upper = null;\n+\t\t\tBoundType upperType = null;\n+\t\t\tin.beginObject();\n+\t\t\t\n+\t\t\twhile (in.hasNext()) {\n+\t\t\t\tString name = in.nextName();\n+\t\t\t\tswitch (name) {\n+\t\t\t\tcase \"lower\":\n+\t\t\t\t\tlower = (float)in.nextDouble();\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase \"lowerType\":\n+\t\t\t\t\tlowerType = BoundType.valueOf(in.nextString());\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase \"upper\":\n+\t\t\t\t\tupper = (float)in.nextDouble();\n+\t\t\t\t\tbreak;\n+\t\t\t\tcase \"upperType\":\n+\t\t\t\t\tupperType = BoundType.valueOf(in.nextString());\n+\t\t\t\t\tbreak;\n+\n+\t\t\t\tdefault:\n+\t\t\t\t\tthrow new IllegalStateException(\"unexpected json name: \"+name);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\t\n+\t\t\tin.endObject();\n+\t\t\tPreconditions.checkState(lower != null || upper != null);\n+\t\t\tif (lower != null)\n+\t\t\t\tPreconditions.checkNotNull(lowerType, \"lower bound supplied without type\");\n+\t\t\tif (upper != null)\n+\t\t\t\tPreconditions.checkNotNull(upperType, \"upper bound supplied without type\");\n+\t\t\tif (lower == null)\n+\t\t\t\treturn Range.upTo(upper, upperType);\n+\t\t\tif (upper == null)\n+\t\t\t\treturn Range.downTo(lower, lowerType);\n+\t\t\treturn Range.range(lower, lowerType, upper, upperType);\n+\t\t}\n+\t\t\n+\t}\n+\t\n \tpublic static void main(String[] args) throws IOException {\n+\t\t\n \t\tFaultModels fm = FaultModels.FM3_1;\n \t\tDeformationModels dm = fm.getFilterBasis();\n \t\t\n@@ -851,31 +990,162 @@ public static void main(String[] args) throws IOException {\n \t\t\n \n \t\tList<? extends FaultSection> subSects = dmFetch.getSubSectionList();\n+\t\tSectionDistanceAzimuthCalculator distAzCalc = new SectionDistanceAzimuthCalculator(subSects);\n \t\t// small subset\n-//\t\tsubSects = subSects.subList(0, 30);\n-//\t\tdouble maxDist = 50d;\n+\t\tsubSects = subSects.subList(0, 30);\n+\t\tdouble maxDist = 50d;\n \t\t\n-\t\tdouble maxDist = 5d;\n-\t\t\n-\t\tSectionDistanceAzimuthCalculator distAzCalc = new SectionDistanceAzimuthCalculator(subSects);\n+//\t\tdouble maxDist = 5d;\n \t\t\n \t\tDistCutoffClosestSectClusterConnectionStrategy connStrat =\n \t\t\t\tnew DistCutoffClosestSectClusterConnectionStrategy(subSects, distAzCalc, maxDist);\n \t\t\n-\t\tBuilder builder = builder(connStrat, subSects);\n+//\t\treadFiltersJSON(new File(\"/home/kevin/OpenSHA/UCERF4/rup_sets/bad_parse_filter.json\"), connStrat, distAzCalc);\n+//\t\tSystem.exit(0);\n+\t\t\n+\t\tBuilder builder = builder(connStrat, distAzCalc);\n //\t\tbuilder.u3Azimuth();\n //\t\tbuilder.u3Cumulatives();\n //\t\tbuilder.minSectsPerParent(2, true, true);\n \t\tSubSectStiffnessCalculator stiffnessCalc =\n-\t\t\t\tnew SubSectStiffnessCalculator(subSects, 2d, 3e4, 3e4, 0.5);\n-//\t\tbuilder.parentCoulomb(stiffnessCalc, StiffnessAggregationMethod.MEDIAN, 0f, Directionality.EITHER);\n-//\t\tbuilder.clusterCoulomb(stiffnessCalc, StiffnessAggregationMethod.MEDIAN, 0f);\n-\t\tbuilder.clusterPathCoulomb(stiffnessCalc, StiffnessAggregationMethod.MEDIAN, 0f);\n-//\t\tbuilder.clusterPathCoulomb(stiffnessCalc, StiffnessAggregationMethod.MEDIAN, 0f, 0.5f);\n-//\t\tbuilder.clusterPathCoulomb(stiffnessCalc, StiffnessAggregationMethod.MEDIAN, 0f, 1f/3f);\n-//\t\tbuilder.clusterPathCoulomb(stiffnessCalc, StiffnessAggregationMethod.MEDIAN, 0f, 2f/3f);\n-//\t\tbuilder.netRupCoulomb(stiffnessCalc, StiffnessAggregationMethod.MEDIAN, 0f, RupCoulombQuantity.SUM_SECT_CFF);\n-//\t\tbuilder.netClusterCoulomb(stiffnessCalc, StiffnessAggregationMethod.MEDIAN, 0f);\n+\t\t\t\tnew SubSectStiffnessCalculator(subSects, 2d, 3e4, 3e4, 0.5, PatchAlignment.FILL_OVERLAP, 1d);\n+\t\t\n+\t\t// common aggregators:\n+\t\tAggregatedStiffnessCalculator medSumAgg = new AggregatedStiffnessCalculator(StiffnessType.CFF, stiffnessCalc, false,\n+\t\t\t\tAggregationMethod.FLATTEN, AggregationMethod.MEDIAN, AggregationMethod.SUM, AggregationMethod.SUM);\n+\t\tAggregatedStiffnessCalculator sumAgg = new AggregatedStiffnessCalculator(StiffnessType.CFF, stiffnessCalc, true,\n+\t\t\t\tAggregationMethod.SUM, AggregationMethod.SUM, AggregationMethod.SUM, AggregationMethod.SUM);\n+\t\tAggregatedStiffnessCalculator fractRpatchPosAgg = new AggregatedStiffnessCalculator(StiffnessType.CFF, stiffnessCalc, true,\n+\t\t\t\tAggregationMethod.SUM, AggregationMethod.PASSTHROUGH, AggregationMethod.RECEIVER_SUM, AggregationMethod.FRACT_POSITIVE);\n+\t\t\n+\t\t/*\n+\t\t * cluster path\n+\t\t */\n+//////\t\tbuilder.clusterPathCoulomb(medSumAgg, 0f);\n+////\t\tbuilder.clusterPathCoulomb(sumAgg, 0f);\n+////\t\tClusterCoulombPathEvaluator prefCFFRPatchEval = new ClusterCoulombPathEvaluator(\n+////\t\t\t\tfractRpatchPosAgg, Range.atLeast(0.5f), PlausibilityResult.FAIL_FUTURE_POSSIBLE);\n+////\t\tbuilder.path(prefCFFRPatchEval);\n+//\t\t/*\n+//\t\t * section path\n+//\t\t */\n+////\t\tbuilder.sectPathCoulomb(sumAgg, 0f);\n+////\t\tSectCoulombPathEvaluator prefCFFSectPathEval = new SectCoulombPathEvaluator(\n+////\t\t\t\tsumAgg, Range.atLeast(0f), PlausibilityResult.FAIL_HARD_STOP, true, 15f, distAzCalc);\n+////\t\tbuilder.path(prefCFFSectPathEval);\n+////\t\tbuilder.path(1f/3f, prefCFFSectPathEval);\n+//\t\t/*\n+//\t\t * Net rupture\n+//\t\t */\n+////\t\t// fraction of all receiver patches that are net positive (summed over all sources)\n+////\t\tbuilder.netRupCoulomb(fractRpatchPosAgg, 0.95f);\n+////\t\t// 3/4 of all interactions positive\n+//\t\tbuilder.netRupCoulomb(new AggregatedStiffnessCalculator(StiffnessType.CFF, stiffnessCalc, true,\n+//\t\t\t\tAggregationMethod.NUM_POSITIVE, AggregationMethod.SUM, AggregationMethod.SUM, AggregationMethod.THREE_QUARTER_INTERACTIONS), 0f);\n+//\t\t/*\n+//\t\t * Jump Cluster\n+//\t\t */\n+//\t\t// fraction of receiver patches on the opposite side of a jump that are net positive with the prior rupture as source\n+//\t\tbuilder.clusterCoulomb(fractRpatchPosAgg, 0.5f);\n+//\t\t// fraction of receiver patches on the opposite side of a jump that have >1/2 interactions positive with the prior rupture as source\n+////\t\tbuilder.clusterCoulomb(new AggregatedStiffnessCalculator(StiffnessType.CFF, stiffnessCalc, false,\n+////\t\t\t\tAggregationMethod.NUM_POSITIVE, AggregationMethod.SUM, AggregationMethod.HALF_INTERACTIONS, AggregationMethod.FRACT_POSITIVE), 0.5f);\n+//\t\t/**\n+//\t\t * CFF probability\n+//\t\t */\n+//\t\t// no negative, cluster-by-cluster\n+////\t\tbuilder.cumulativeProbability(0.01f, new RelativeCoulombProb(sumAgg, connStrat, false, false));\n+//\t\t// allow negative, cluster-by-cluster\n+////\t\tbuilder.cumulativeProbability(0.01f, new RelativeCoulombProb(sumAgg, connStrat, true, false));\n+//\t\t// no negative, sect-by-sect\n+////\t\tbuilder.cumulativeProbability(0.01f, new RelativeCoulombProb(sumAgg, connStrat, false, true));\n+//\t\t// no negative, sect-by-sect, favorable jumps up to 15km\n+//\t\tRelativeCoulombProb prefFavCFFProb = new RelativeCoulombProb(sumAgg, connStrat, false, true, true, 10f, distAzCalc);\n+////\t\tbuilder.cumulativeProbability(0.01f, prefFavCFFProb);\n+////\t\tbuilder.cumulativeProbability(0.02f, prefFavCFFProb);\n+//\t\tbuilder.cumulativeProbability(0.05f, prefFavCFFProb);\n+//\t\t// no negative, sect-by-sect, favorable jumps up to 15km\n+//\t\tRelativeCoulombProb prefCFFProb = new RelativeCoulombProb(sumAgg, connStrat, false, true);\n+////\t\tbuilder.cumulativeProbability(0.01f, prefCFFProb);\n+////\t\tbuilder.cumulativeProbability(0.02f, prefCFFProb);\n+//\t\tbuilder.cumulativeProbability(0.05f, prefCFFProb);\n+//\t\t// same but as a path option\n+////\t\tCumulativeProbPathEvaluator cffProbPathEval = new CumulativeProbPathEvaluator(\n+////\t\t\t\t0.01f, PlausibilityResult.FAIL_HARD_STOP, prefCFFProb);\n+////\t\tbuilder.path(cffProbPathEval);\n+//\t\t/**\n+//\t\t * CFF ratio probability\n+//\t\t */\n+//\t\tCoulombSectRatioProb cffFavRatioProb = new CoulombSectRatioProb(sumAgg, 2, true, 10f, distAzCalc);\n+//\t\tbuilder.cumulativeProbability(0.5f, cffFavRatioProb);\n+//\t\t// same but as a path option\n+////\t\tCumulativeProbPathEvaluator cffRatioProbPathEval = new CumulativeProbPathEvaluator(\n+////\t\t\t\t0.01f, PlausibilityResult.FAIL_HARD_STOP, cffRatioProb);\n+////\t\tbuilder.path(cffRatioProbPathEval);\n+//\t\tCoulombSectRatioProb cffRatioProb = new CoulombSectRatioProb(sumAgg, 2);\n+//\t\tbuilder.cumulativeProbability(0.5f, cffRatioProb);\n+//\t\t// same but as a path option\n+////\t\tcffRatioProbPathEval = new CumulativeProbPathEvaluator(\n+////\t\t\t\t0.1f, PlausibilityResult.FAIL_HARD_STOP, cffRatioProb);\n+////\t\tbuilder.path(cffRatioProbPathEval);\n+//\t\t/**\n+//\t\t * Slip probability\n+//\t\t */\n+////\t\t// regular, 0.01\n+////\t\tbuilder.cumulativeProbability(0.01f, new RelativeSlipRateProb(connStrat, false));\n+//\t\t// only increasing, 0.01\n+//\t\tbuilder.cumulativeProbability(0.05f, new RelativeSlipRateProb(connStrat, true));\n+////\t\t// regular, 0.01\n+////\t\tbuilder.cumulativeProbability(0.1f, new RelativeSlipRateProb(connStrat, false));\n+//\t\t// only increasing, 0.1\n+////\t\tbuilder.cumulativeProbability(0.1f, new RelativeSlipRateProb(connStrat, true));\n+//\t\t// as a path, only increasing, 0.01\n+////\t\tCumulativeJumpProbPathEvaluator prefSlipEval = new CumulativeJumpProbPathEvaluator(\n+////\t\t\t\t0.01f, PlausibilityResult.FAIL_HARD_STOP, new RelativeSlipRateProb(connStrat, true));\n+////\t\tbuilder.add(new ScalarPathPlausibilityFilter<>(prefSlipEval));\n+//\t\t/**\n+//\t\t * Combined path filter\n+//\t\t */\n+////\t\tbuilder.path(prefSlipEval, cffProbPathEval, prefCFFSectPathEval, prefCFFRPatchEval);\n+////\t\tbuilder.path(cffProbPathEval, prefCFFSectPathEval, prefCFFRPatchEval);\n+////\t\tbuilder.path(cffProbPathEval, prefCFFRPatchEval);\n+//\t\tbuilder.path(new CumulativeProbPathEvaluator(0.05f, PlausibilityResult.FAIL_HARD_STOP, prefCFFProb),\n+//\t\t\t\tnew CumulativeProbPathEvaluator(0.5f, PlausibilityResult.FAIL_HARD_STOP, cffRatioProb));\n+//\t\tbuilder.path(new CumulativeProbPathEvaluator(0.02f, PlausibilityResult.FAIL_HARD_STOP, prefCFFProb),\n+//\t\t\t\tnew CumulativeProbPathEvaluator(0.5f, PlausibilityResult.FAIL_HARD_STOP, cffRatioProb));\n+//\t\tbuilder.path(new CumulativeProbPathEvaluator(0.02f, PlausibilityResult.FAIL_HARD_STOP, prefFavCFFProb),\n+//\t\t\t\tnew CumulativeProbPathEvaluator(0.5f, PlausibilityResult.FAIL_HARD_STOP, cffFavRatioProb));\n+//\t\t\n+//\t\tString destFileName = \"alt_filters.json\";\n+\t\t\n+\t\t// current best\n+\t\tbuilder.cumulativeProbability(0.05f, new RelativeSlipRateProb(connStrat, true));\n+\t\tbuilder.netRupCoulomb(new AggregatedStiffnessCalculator(StiffnessType.CFF, stiffnessCalc, true,\n+\t\t\t\tAggregationMethod.NUM_POSITIVE, AggregationMethod.SUM, AggregationMethod.SUM, AggregationMethod.THREE_QUARTER_INTERACTIONS), 0f);\n+//\t\tRelativeCoulombProb prefCFFProb = new RelativeCoulombProb(sumAgg, connStrat, false, true);\n+\t\tRelativeCoulombProb prefCFFProb = new RelativeCoulombProb(sumAgg, connStrat, false, true, true, 10f, distAzCalc);\n+\t\tCumulativeProbPathEvaluator cffProbPathEval = new CumulativeProbPathEvaluator(\n+\t\t\t\t0.02f, PlausibilityResult.FAIL_HARD_STOP, prefCFFProb);\n+//\t\tSectCoulombPathEvaluator prefCFFSectPathEval = new SectCoulombPathEvaluator(\n+//\t\t\t\tsumAgg, Range.atLeast(0f), PlausibilityResult.FAIL_HARD_STOP, true, 15f, distAzCalc);\n+\t\tCoulombSectRatioProb cffRatioProb = new CoulombSectRatioProb(sumAgg, 2, true, 10f, distAzCalc);\n+//\t\tCoulombSectRatioProb cffRatioProb = new CoulombSectRatioProb(sumAgg, 2);\n+\t\tCumulativeProbPathEvaluator cffRatioProbPathEval = new CumulativeProbPathEvaluator(\n+\t\t\t\t0.2f, PlausibilityResult.FAIL_HARD_STOP, cffRatioProb);\n+//\t\tClusterCoulombPathEvaluator prefCFFRPatchEval = new ClusterCoulombPathEvaluator(\n+//\t\t\t\tfractRpatchPosAgg, Range.atLeast(0.5f), PlausibilityResult.FAIL_FUTURE_POSSIBLE);\n+//\t\tbuilder.path(cffProbPathEval, cffRatioProbPathEval, prefCFFRPatchEval);\n+\t\tbuilder.path(cffProbPathEval, cffRatioProbPathEval);\n+\t\tString destFileName = \"cur_pref_filters.json\";\n+\t\t\n+\t\t// UCERF3 non-construction\n+//\t\tbuilder.u3Azimuth();\n+//\t\tbuilder.u3Cumulatives();\n+//\t\tSubSectStiffnessCalculator u3StiffnessCalc =\n+//\t\t\t\tnew SubSectStiffnessCalculator(subSects, 1d, 3e4, 3e4, 0.5, PatchAlignment.FILL_OVERLAP, 1d);\n+//\t\tu3StiffnessCalc.setPatchAlignment(PatchAlignment.FILL_OVERLAP);\n+//\t\tbuilder.u3Coulomb(CoulombRates.loadUCERF3CoulombRates(fm), u3StiffnessCalc);\n+//\t\tString destFileName = \"u3_az_cff_cmls.json\";\n \t\t\n //\t\tPenalty[] penalties = {\n //\t\t\t\tnew CumulativePenaltyFilter.JumpPenalty(0f, 1d, true),\n@@ -886,19 +1156,19 @@ public static void main(String[] args) throws IOException {\n //\t\t};\n //\t\tbuilder.cumulativePenalty(10f, false, penalties);\n \t\t\n-\t\tRuptureProbabilityCalc[] probCalcs = CumulativeProbabilityFilter.getPrefferedBWCalcs(distAzCalc);\n-\t\tbuilder.cumulativeProbability(0.01f, probCalcs);\n-\t\tbuilder.cumulativeProbability(0.0075f, probCalcs);\n-\t\tbuilder.cumulativeProbability(0.005f, probCalcs);\n-\t\tbuilder.cumulativeProbability(0.0025f, probCalcs);\n-\t\tbuilder.cumulativeProbability(0.001f, probCalcs);\n-\t\tbuilder.cumulativeProbability(0.0005f, probCalcs);\n+//\t\tRuptureProbabilityCalc[] probCalcs = CumulativeProbabilityFilter.getPrefferedBWCalcs(distAzCalc);\n+//\t\tbuilder.cumulativeProbability(0.01f, probCalcs);\n+//\t\tbuilder.cumulativeProbability(0.0075f, probCalcs);\n+//\t\tbuilder.cumulativeProbability(0.005f, probCalcs);\n+//\t\tbuilder.cumulativeProbability(0.0025f, probCalcs);\n+//\t\tbuilder.cumulativeProbability(0.001f, probCalcs);\n+//\t\tbuilder.cumulativeProbability(0.0005f, probCalcs);\n \t\t\n \t\tPlausibilityConfiguration config = builder.build();\n \t\t\n //\t\tconfig.writeFiltersJSON(new File(\"/home/kevin/OpenSHA/UCERF4/rup_sets/new_coulomb_filters.json\"));\n //\t\tconfig.writeFiltersJSON(new File(\"/home/kevin/OpenSHA/UCERF4/rup_sets/new_cumulative_prob_filters.json\"));\n-\t\tconfig.writeFiltersJSON(new File(\"/home/kevin/OpenSHA/UCERF4/rup_sets/alt_filters.json\"));\n+\t\tconfig.writeFiltersJSON(new File(\"/home/kevin/OpenSHA/UCERF4/rup_sets/\"+destFileName));\n \t\t\n \t\tGson gson = buildGson(subSects);\n \t\t"
  },
  {
    "sha": "1fd2eca10c9b8345daf653e416e73ff68ef46642",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/PlausibilityFilter.java",
    "status": "modified",
    "additions": 0,
    "deletions": 4,
    "changes": 4,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/PlausibilityFilter.java",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/PlausibilityFilter.java",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/PlausibilityFilter.java?ref=541efed3666ce0a47e03a345aada595633351042",
    "patch": "@@ -1,13 +1,9 @@\n package org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility;\n \n-import java.util.List;\n-\n import org.opensha.commons.data.ShortNamed;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.ClusterRupture;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.Jump;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.strategies.ClusterConnectionStrategy;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.util.SectionDistanceAzimuthCalculator;\n-import org.opensha.sha.faultSurface.FaultSection;\n \n import com.google.gson.Gson;\n import com.google.gson.TypeAdapter;"
  },
  {
    "sha": "0677614245613b3319940f143f65b9315ac80f51",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/ScalarCoulombPlausibilityFilter.java",
    "status": "modified",
    "additions": 12,
    "deletions": 5,
    "changes": 17,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/ScalarCoulombPlausibilityFilter.java",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/ScalarCoulombPlausibilityFilter.java",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/ScalarCoulombPlausibilityFilter.java?ref=541efed3666ce0a47e03a345aada595633351042",
    "patch": "@@ -1,18 +1,25 @@\n package org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility;\n \n-import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator;\n-import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator.StiffnessType;\n+import java.util.Comparator;\n+\n+import org.opensha.sha.simulators.stiffness.AggregatedStiffnessCalculator;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.BoundType;\n+import com.google.common.collect.Range;\n \n public interface ScalarCoulombPlausibilityFilter extends ScalarValuePlausibiltyFilter<Float> {\n \t\n-\tpublic SubSectStiffnessCalculator getStiffnessCalc();\n+\tpublic AggregatedStiffnessCalculator getAggregator();\n \t\n \tpublic default String getScalarName() {\n-\t\treturn StiffnessType.CFF.getName();\n+\t\treturn getAggregator().getScalarName();\n \t}\n \t\n \tpublic default String getScalarUnits() {\n-\t\treturn StiffnessType.CFF.getUnits();\n+\t\tif (getAggregator().hasUnits())\n+\t\t\treturn getAggregator().getType().getUnits();\n+\t\treturn null;\n \t}\n \n }"
  },
  {
    "sha": "4468ebc015de2c98839fc6a43573e818763f9961",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/ScalarValuePlausibiltyFilter.java",
    "status": "modified",
    "additions": 203,
    "deletions": 0,
    "changes": 203,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/ScalarValuePlausibiltyFilter.java",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/ScalarValuePlausibiltyFilter.java",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/ScalarValuePlausibiltyFilter.java?ref=541efed3666ce0a47e03a345aada595633351042",
    "patch": "@@ -1,10 +1,16 @@\n package org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility;\n \n+import java.util.Comparator;\n+\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.ClusterRupture;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.Jump;\n \n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.BoundType;\n import com.google.common.collect.Range;\n \n+import scratch.UCERF3.inversion.laughTest.PlausibilityResult;\n+\n /**\n  * Interface for a plausibility filter which is produces a scalar value (e.g., integer, double, float)\n  * for each rupture. This is mostly a helper interface to allow diagnostic plots when comparing rupture\n@@ -36,5 +42,202 @@\n \t * @return units of this scalar value\n \t */\n \tpublic String getScalarUnits();\n+\t\n+\t/**\n+\t * @param val1\n+\t * @param val2\n+\t * @return the value that is worse. see isValueBetter for implementation details\n+\t */\n+\tpublic default E getWorseValue(E val1, E val2) {\n+\t\tif (isValueBetter(val1, val2))\n+\t\t\treturn val2;\n+\t\treturn val1;\n+\t}\n+\n+\t/**\n+\t * \n+\t * @return a comparator to sort values from worst to best (see isValueBetter for implementation details)\n+\t */\n+\tpublic default Comparator<E> worstToBestComparator() {\n+\t\treturn new Comparator<E>() {\n+\n+\t\t\t@Override\n+\t\t\tpublic int compare(E val1, E val2) {\n+\t\t\t\tif (val1.equals(val2))\n+\t\t\t\t\treturn 0;\n+\t\t\t\tif (isValueBetter(val1, val2))\n+\t\t\t\t\treturn 1;\n+\t\t\t\treturn -1;\n+\t\t\t}\n+\t\t};\n+\t}\n+\t\n+\t/**\n+\t * \n+\t * @param val1\n+\t * @param val2\n+\t * @return the better of the two given values. see isValueBetter for implementation details\n+\t */\n+\tpublic default E getBestValue(E val1, E val2) {\n+\t\tif (isValueBetter(val1, val2))\n+\t\t\treturn val1;\n+\t\treturn val2;\n+\t}\n+\t\n+\t/**\n+\t * This tests two values to determine if the test value (argument 1) is better than the reference value (argument 2).\n+\t * \n+\t * It starts by computing the distance (to double precision) from the acceptable range for each value. This will be 0\n+\t * if the acceptable range contains the value. If the distance for either value is nonzero, then the value with the\n+\t * smallest distance is taken to be the better value. If that distance is zero for both values, then the algorithm\n+\t * is as follows: if there is an upper bound, choose the smallest value as best. if there is no upper bound, choose\n+\t * the largest\n+\t * \n+\t * @param testVal\n+\t * @param refVal\n+\t * @range\n+\t * @return true if testVal is better than refVal\n+\t */\n+\tpublic default boolean isValueBetter(E testVal, E refVal) {\n+\t\treturn isValueBetter(testVal, refVal, getAcceptableRange());\n+\t}\n+\t\n+\t/**\n+\t * This tests two values to determine if the test value (argument 1) is better than the reference value (argument 2).\n+\t * \n+\t * It starts by computing the distance (to double precision) from the acceptable range for each value. This will be 0\n+\t * if the acceptable range contains the value. If the distance for either value is nonzero, then the value with the\n+\t * smallest distance is taken to be the better value. If that distance is zero for both values, then the algorithm\n+\t * is as follows: if there is an upper bound, choose the smallest value as best. if there is no upper bound, choose\n+\t * the largest\n+\t * \n+\t * @param testVal\n+\t * @param refVal\n+\t * @param range\n+\t * @return true if testVal is better than refVal\n+\t */\n+\tpublic static <E extends Number & Comparable<E>> boolean isValueBetter(E testVal, E refVal, Range<E> range) {\n+\t\tdouble dist1 = distFromRange(testVal, range);\n+\t\tdouble dist2 = distFromRange(refVal, range);\n+\t\tif ((float)dist1 == 0f && (float)dist2 == 0f) {\n+\t\t\t// both are acceptable, choose better\n+\t\t\tif (!range.hasLowerBound())\n+//\t\t\t\treturn testVal < refVal;\n+\t\t\t\treturn testVal.compareTo(refVal) < 0;\n+\t\t\treturn testVal.compareTo(refVal) > 0;\n+\t\t}\n+\t\tif (dist1 < dist2)\n+\t\t\treturn true;\n+\t\treturn false;\n+\t}\n+\t\n+\t/**\n+\t * The distance (always positive) of the given value from the given range, or zero if the range contains the value.\n+\t * @param val\n+\t * @param range\n+\t * @return\n+\t */\n+\tpublic static <E extends Number & Comparable<E>> double distFromRange(E val, Range<E> range) {\n+\t\tif (range.contains(val))\n+\t\t\treturn 0d;\n+\t\tif (range.hasUpperBound() && val.compareTo(range.upperEndpoint()) > 0)\n+\t\t\treturn val.doubleValue() - range.upperEndpoint().doubleValue();\n+\t\treturn range.lowerEndpoint().doubleValue() - val.doubleValue();\n+\t}\n+\t\n+\t/**\n+\t * \n+\t * @return string representation of the acceptable range\n+\t */\n+\tpublic default String getRangeStr() {\n+\t\treturn getRangeStr(getAcceptableRange());\n+\t}\n+\t\n+\t/**\n+\t * \n+\t * @return string representation of the acceptable range\n+\t */\n+\tpublic static <E extends Number & Comparable<E>> String getRangeStr(Range<E> acceptableRange) {\n+\t\tPreconditions.checkState(acceptableRange.hasLowerBound() || acceptableRange.hasUpperBound());\n+\t\tif (!acceptableRange.hasLowerBound()) {\n+\t\t\tE upper = acceptableRange.upperEndpoint();\n+\t\t\tchar ineq = acceptableRange.upperBoundType() == BoundType.CLOSED ? '\\u2264' : '<'; // u2264 is less than or equal\n+\t\t\tif (upper.floatValue() == 0f)\n+\t\t\t\treturn ineq+\"0\";\n+\t\t\treturn ineq+(upper+\"\");\n+\t\t} else if (!acceptableRange.hasUpperBound()) {\n+\t\t\tE lower = acceptableRange.lowerEndpoint();\n+\t\t\tchar ineq = acceptableRange.lowerBoundType() == BoundType.CLOSED ? '\\u2265' : '>'; // u2265 is greater than or equal\n+\t\t\tif (lower.floatValue() == 0f)\n+\t\t\t\treturn ineq+\"0\";\n+\t\t\treturn ineq+(lower+\"\");\n+\t\t}\n+\t\tString ret = \"\";\n+\t\tret += (acceptableRange.lowerBoundType() == BoundType.CLOSED) ? \"[\" : \"(\";\n+\t\tret += acceptableRange.lowerEndpoint()+\",\"+acceptableRange.upperEndpoint();\n+\t\tret += (acceptableRange.upperBoundType() == BoundType.CLOSED) ? \"]\" : \")\";\n+\t\treturn ret;\n+\t}\n+\t\n+\tpublic static class DoubleWrapper implements ScalarValuePlausibiltyFilter<Double> {\n+\t\t\n+\t\tprivate ScalarValuePlausibiltyFilter<?> filter;\n+\t\tprivate Range<Double> doubleRange;\n+\n+\t\tpublic DoubleWrapper(ScalarValuePlausibiltyFilter<?> filter) {\n+\t\t\tthis.filter = filter;\n+\t\t\tRange<?> origRange = filter.getAcceptableRange();\n+\t\t\tif (origRange.hasLowerBound() && origRange.hasUpperBound())\n+\t\t\t\tdoubleRange = Range.range(numberToDouble(origRange.lowerEndpoint()), origRange.lowerBoundType(),\n+\t\t\t\t\t\tnumberToDouble(origRange.upperEndpoint()), origRange.upperBoundType());\n+\t\t\telse if (origRange.hasLowerBound())\n+\t\t\t\tdoubleRange = Range.downTo(numberToDouble(origRange.lowerEndpoint()), origRange.lowerBoundType());\n+\t\t\telse\n+\t\t\t\tdoubleRange = Range.upTo(numberToDouble(origRange.upperEndpoint()), origRange.upperBoundType());\n+\t\t}\n+\t\t\n+\t\tprivate double numberToDouble(Object obj) {\n+\t\t\treturn ((Number)obj).doubleValue();\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic PlausibilityResult apply(ClusterRupture rupture, boolean verbose) {\n+\t\t\treturn filter.apply(rupture, verbose);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic String getShortName() {\n+\t\t\treturn filter.getShortName();\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic String getName() {\n+\t\t\treturn filter.getName();\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Double getValue(ClusterRupture rupture) {\n+\t\t\tNumber value = filter.getValue(rupture);\n+\t\t\tif (value == null)\n+\t\t\t\treturn null;\n+\t\t\treturn value.doubleValue();\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Range<Double> getAcceptableRange() {\n+\t\t\treturn doubleRange;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic String getScalarName() {\n+\t\t\treturn filter.getScalarName();\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic String getScalarUnits() {\n+\t\t\treturn filter.getScalarUnits();\n+\t\t}\n+\t\t\n+\t}\n \n }"
  },
  {
    "sha": "94b3ea1515a16ebeff7a0d966cd7f22e8bbf761c",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/README.md",
    "status": "added",
    "additions": 45,
    "deletions": 0,
    "changes": 45,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/README.md",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/README.md",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/README.md?ref=541efed3666ce0a47e03a345aada595633351042",
    "patch": "@@ -0,0 +1,45 @@\n+# Plausibility Filter Examples\n+\n+Example figures showing new proposed plausibility filters.\n+\n+## Relative Slip Rate Probability\n+\n+This filter penalizes jumps taken to lower slip-rate faults when a higher slip rate alternative was available (which could be continuing on the same fault). The probability at a junction is defined as the taken slip rate divided by maximum available slip rate, and this probability is multipled through each jump taken in a rupture.\n+\n+This is only applied at slip-rate increases, so ruptures can die out on low slip-rate faults without penalty; the penalty is only applied at the next slip-rate increase. This is shown by the gray probabilites that are not counted in the two middle panels of the example below.\n+\n+![Slip Rate Prob](slip_prob_demo.png)\n+\n+## Relative Coulomb Probability\n+\n+This filter is the same as the slip rate probability filter, but instead uses coulomb values. Negative jumps are never taken, and unlike the slip rate filter no junctions are skipped. As Coulomb calculations are not symmetric, every cluster of a rupture is tried as the nucleation cluster to see if any pass with a probability above the supplied threshold.\n+\n+![Coulomb Prob](cff_prob_demo.png)\n+\n+## Coulomb Favorability Ratio\n+\n+This filter tests the Coulomb favorability of each jump taken in a rupture. First, every subsection in a rupture thus far is used as a source to compute Coulomb stress change on the candidate receiver subsection. Those are then sorted in decreasing order, and normalized by the contribution from the largest two source subsections:\n+\n+`R = Sum[CFF_1...CFF_N]/|CFF_1+CFF_2|`\n+\n+That ratio is bounded in the range [0,1] and treated as a probability, and multiplied out as each subsection is added to a rupture. Coulomb sums are dominated by the nearest intaraction, so the normalization by the top two penalizes additions that are largely favorable at the jumping point but are unfavorable to the rest of the rupture. The absolute value in the denominator also ensures that the sign of the interaction is kept, so negative interactions will never be allowed.\n+\n+This algorithm need not use the exact jumping point defined by the connection strategy, however. The example below shows a rupture with some stress-shadowing overlap where the closest subsection does not work as a jumping point (left), but the rupture can easily jump to the next subsection and spread bilaterally (right). This example rupture only passes if the favorable path (up to a maximum distance, usually 10km) option is enabled.\n+\n+As with the Coulomb probability filter, every cluster of a rupture is tried as the nucleation cluster to see if any pass.\n+\n+| ![Exhaustive](cff_ratio_overlapping_ss_0.png) | ![Connection Points](cff_ratio_overlapping_ss_favjump_0.png) |\n+|-----|-----|\n+\n+## Fraction of Coulomb Interactions Positive\n+\n+This filter tests the far-field Coulomb compatibility of a rupture in order to filter out overly complicated ruptures where each addition is Coulomb positive but the entire rupture is nonsensical. Here, Coulomb stress change is computed between every patch of every subsection in a rupture (including within a subsection) and the fraction that are positive are tested against a threshold (0.75 is a common threshold). There can be many millions of interactions computed to evaluate a large rupture, and all are weighted equally for this test (only their sign is considered). The example below shows the evolution of a rupture that might slip through the other Coulomb filters (as each addition is strongly near-field compatible at jumping points), but is intuitively unphysical.\n+\n+![Fraction of Interactions](cff_interact_demo.png)\n+\n+## No Indirect Connections\n+\n+This filter prevents a rupture from taking a small excursion that could have been skipped on one fault and then immediately jumping back to another fault that was directly connected to the prior rupture. This case is encountered where a fault network branches. We want to be able to take both branches independently (if they pass other filters), but not both.\n+\n+![No Indirect](indirect_demo.png)\n+"
  },
  {
    "sha": "cda2efacb40c07e1c5f305820e0b95bfe2df1412",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/cff_interact_demo.png",
    "status": "added",
    "additions": 0,
    "deletions": 0,
    "changes": 0,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/cff_interact_demo.png",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/cff_interact_demo.png",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/cff_interact_demo.png?ref=541efed3666ce0a47e03a345aada595633351042"
  },
  {
    "sha": "5a81449af9df50a7df8c90dc44fc1b39dfa9c2fa",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/cff_prob_demo.png",
    "status": "added",
    "additions": 0,
    "deletions": 0,
    "changes": 0,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/cff_prob_demo.png",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/cff_prob_demo.png",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/cff_prob_demo.png?ref=541efed3666ce0a47e03a345aada595633351042"
  },
  {
    "sha": "743a870f9bf66cf30e7930491edea01247417444",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/cff_ratio_overlapping_ss_0.png",
    "status": "added",
    "additions": 0,
    "deletions": 0,
    "changes": 0,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/cff_ratio_overlapping_ss_0.png",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/cff_ratio_overlapping_ss_0.png",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/cff_ratio_overlapping_ss_0.png?ref=541efed3666ce0a47e03a345aada595633351042"
  },
  {
    "sha": "10a319b938040b6530dc67abf06e9416ddf41be5",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/cff_ratio_overlapping_ss_1.png",
    "status": "added",
    "additions": 0,
    "deletions": 0,
    "changes": 0,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/cff_ratio_overlapping_ss_1.png",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/cff_ratio_overlapping_ss_1.png",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/cff_ratio_overlapping_ss_1.png?ref=541efed3666ce0a47e03a345aada595633351042"
  },
  {
    "sha": "d106f728c02b548702d316c7937652486f14e948",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/cff_ratio_overlapping_ss_favjump_0.png",
    "status": "added",
    "additions": 0,
    "deletions": 0,
    "changes": 0,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/cff_ratio_overlapping_ss_favjump_0.png",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/cff_ratio_overlapping_ss_favjump_0.png",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/cff_ratio_overlapping_ss_favjump_0.png?ref=541efed3666ce0a47e03a345aada595633351042"
  },
  {
    "sha": "d7ef15b7b839906cbd3b7beb496c2f24d4600152",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/cff_ratio_overlapping_ss_favjump_1.png",
    "status": "added",
    "additions": 0,
    "deletions": 0,
    "changes": 0,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/cff_ratio_overlapping_ss_favjump_1.png",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/cff_ratio_overlapping_ss_favjump_1.png",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/cff_ratio_overlapping_ss_favjump_1.png?ref=541efed3666ce0a47e03a345aada595633351042"
  },
  {
    "sha": "c4f29c76abefcb8ac8d039e958825faa7d7a76c2",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/indirect_demo.png",
    "status": "added",
    "additions": 0,
    "deletions": 0,
    "changes": 0,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/indirect_demo.png",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/indirect_demo.png",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/indirect_demo.png?ref=541efed3666ce0a47e03a345aada595633351042"
  },
  {
    "sha": "111d401bb7d33989a4cb61554e81df40333b5258",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/slip_prob_demo.png",
    "status": "added",
    "additions": 0,
    "deletions": 0,
    "changes": 0,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/slip_prob_demo.png",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/slip_prob_demo.png",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/doc/slip_prob_demo.png?ref=541efed3666ce0a47e03a345aada595633351042"
  },
  {
    "sha": "616d6acde05924a032ab8d9a3cd43238d345455e",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/ClusterPathCoulombCompatibilityFilter.java",
    "status": "removed",
    "additions": 0,
    "deletions": 334,
    "changes": 334,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/9e0f2c8ac3a20f3e4ce6dc8814fa8709d85cdd79/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/ClusterPathCoulombCompatibilityFilter.java",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/9e0f2c8ac3a20f3e4ce6dc8814fa8709d85cdd79/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/ClusterPathCoulombCompatibilityFilter.java",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/ClusterPathCoulombCompatibilityFilter.java?ref=9e0f2c8ac3a20f3e4ce6dc8814fa8709d85cdd79",
    "patch": "@@ -1,334 +0,0 @@\n-package org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl;\n-\n-import java.util.ArrayList;\n-import java.util.Collections;\n-import java.util.HashSet;\n-import java.util.List;\n-\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.ClusterRupture;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.FaultSubsectionCluster;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.Jump;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.ScalarCoulombPlausibilityFilter;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.util.FilterDataClusterRupture;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.util.RuptureTreeNavigator;\n-import org.opensha.sha.faultSurface.FaultSection;\n-import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator;\n-import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator.StiffnessAggregationMethod;\n-import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator.StiffnessResult;\n-import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator.StiffnessType;\n-\n-import com.google.common.base.Preconditions;\n-import com.google.common.collect.Lists;\n-import com.google.common.collect.Range;\n-\n-import scratch.UCERF3.inversion.laughTest.PlausibilityResult;\n-\n-/**\n- * This filter tests the Coulomb compatibility of each possible path through the given rupture. It tries each\n- * cluster as the nucleation point, and builds outwards to each rupture endpoint. Ruptures pass if at least\n- * one nucleation point is viable (or if at least the supplied fraction of paths pass).\n- * \n- * @author kevin\n- *\n- */\n-public class ClusterPathCoulombCompatibilityFilter implements ScalarCoulombPlausibilityFilter {\n-\t\n-\tprivate SubSectStiffnessCalculator stiffnessCalc;\n-\tprivate StiffnessAggregationMethod aggMethod;\n-\tprivate float threshold;\n-\tprivate float fractPassThreshold = 0; // default pass to if 1 or more paths pass\n-\n-\tpublic ClusterPathCoulombCompatibilityFilter(SubSectStiffnessCalculator stiffnessCalc,\n-\t\t\tStiffnessAggregationMethod aggMethod, float threshold) {\n-\t\tthis(stiffnessCalc, aggMethod, threshold, 0f);\n-\t}\n-\n-\tpublic ClusterPathCoulombCompatibilityFilter(SubSectStiffnessCalculator stiffnessCalc,\n-\t\t\tStiffnessAggregationMethod aggMethod, float threshold, float fractPassThreshold) {\n-\t\tthis.stiffnessCalc = stiffnessCalc;\n-\t\tthis.aggMethod = aggMethod;\n-\t\tthis.threshold = threshold;\n-\t\tPreconditions.checkState(fractPassThreshold <= 1f);\n-\t\tthis.fractPassThreshold = fractPassThreshold;\n-\t}\n-\n-\t@Override\n-\tpublic PlausibilityResult apply(ClusterRupture rupture, boolean verbose) {\n-\t\tif (rupture.getTotalNumJumps() == 0)\n-\t\t\treturn PlausibilityResult.PASS;\n-\t\tRuptureTreeNavigator navigator = rupture.getTreeNavigator();\n-\t\tList<FaultSubsectionCluster> clusters = Lists.newArrayList(rupture.getClustersIterable());\n-\t\tint numPaths = clusters.size();\n-\t\tint numPasses = 0;\n-\t\tint numNeeded = 1;\n-\t\tif (fractPassThreshold > 0f)\n-\t\t\tnumNeeded = Integer.max(1, (int)Math.ceil(fractPassThreshold*numPaths));\n-\t\tHashSet<FaultSubsectionCluster> skipClusters = null;\n-\t\tif (rupture instanceof FilterDataClusterRupture) {\n-\t\t\tFilterDataClusterRupture fdRupture = (FilterDataClusterRupture)rupture;\n-\t\t\tObject filterData = fdRupture.getFilterData(this);\n-\t\t\tif (filterData != null && filterData instanceof HashSet<?>)\n-\t\t\t\tskipClusters = new HashSet<>((HashSet<FaultSubsectionCluster>)filterData); \n-\t\t\telse\n-\t\t\t\tskipClusters = new HashSet<>();\n-\t\t\tfdRupture.addFilterData(this, skipClusters);\n-\t\t}\n-\t\tfor (FaultSubsectionCluster nucleationCluster : clusters) {\n-\t\t\tif (skipClusters != null && skipClusters.contains(nucleationCluster))\n-\t\t\t\t// we can skip this one because it already failed in a subset of this rupture so it will\n-\t\t\t\t// never pass here\n-\t\t\t\tcontinue;\n-\t\t\tfloat val = testNucleationPoint(navigator, nucleationCluster, !verbose, verbose);\n-\t\t\tif (verbose)\n-\t\t\t\tSystem.out.println(getShortName()+\": Nucleation point \"+nucleationCluster\n-\t\t\t\t\t\t+\", val=\"+val+\", result: \"+(val >= threshold));\n-\t\t\tif (val >= threshold)\n-\t\t\t\tnumPasses++;\n-\t\t\telse if (skipClusters != null)\n-\t\t\t\tskipClusters.add(nucleationCluster);\n-\t\t\tif (!verbose && numPasses >= numNeeded)\n-\t\t\t\treturn PlausibilityResult.PASS;\n-\t\t}\n-\t\tif (verbose)\n-\t\t\tSystem.out.println(getShortName()+\": \"+numPasses+\"/\"+numPaths+\" pass, \"+numNeeded+\" needed\");\n-\t\tif (numPasses >= numNeeded)\n-\t\t\treturn PlausibilityResult.PASS;\n-\t\treturn PlausibilityResult.FAIL_FUTURE_POSSIBLE;\n-\t}\n-\n-\t@Override\n-\tpublic Float getValue(ClusterRupture rupture) {\n-\t\tif (rupture.getTotalNumJumps()  == 0)\n-\t\t\treturn null;\n-\t\tRuptureTreeNavigator navigator = rupture.getTreeNavigator();\n-\t\tList<Float> vals = new ArrayList<>();\n-\t\tfor (FaultSubsectionCluster nucleationCluster : rupture.getClustersIterable()) {\n-\t\t\tfloat val = testNucleationPoint(navigator, nucleationCluster, false, false);\n-\t\t\tvals.add(val);\n-\t\t}\n-\t\tif (fractPassThreshold > 0f) {\n-\t\t\t// if we need N paths to pass, return the Nth largest value outside\n-\t\t\t// (such that if and only if that value passes, the rupture passes)\n-\t\t\tint numPaths = vals.size();\n-\t\t\tint numNeeded = Integer.max(1, (int)Math.ceil(fractPassThreshold*numPaths));\n-\t\t\tCollections.sort(vals);\n-\t\t\treturn vals.get(vals.size()-numNeeded);\n-\t\t}\n-\t\tfloat max = Float.NEGATIVE_INFINITY;\n-\t\tfor (Float val : vals)\n-\t\t\tmax = Float.max(val, max);\n-\t\treturn max;\n-\t}\n-\t\n-\tprivate float testNucleationPoint(RuptureTreeNavigator navigator,\n-\t\t\tFaultSubsectionCluster nucleationCluster, boolean shortCircuit, boolean verbose) {\n-\t\tHashSet<FaultSubsectionCluster> curClusters = new HashSet<>();\n-\t\tcurClusters.add(nucleationCluster);\n-\t\tFaultSubsectionCluster predecessor = navigator.getPredecessor(nucleationCluster);\n-\t\tFloat minVal = Float.POSITIVE_INFINITY;\n-\t\tif (verbose)\n-\t\t\tSystem.out.println(\"Testing strand(s) with start=\"+nucleationCluster);\n-\t\tif (predecessor != null) {\n-\t\t\tif (verbose)\n-\t\t\t\tSystem.out.println(\"\\tTesting to predecessor=\"+predecessor);\n-\t\t\tList<FaultSection> strandSects = getAddSectsToward(\n-\t\t\t\t\tnavigator, null, null, nucleationCluster, predecessor);\n-\t\t\tfloat strandVal = testStrand(navigator, curClusters, strandSects, nucleationCluster, predecessor,\n-\t\t\t\t\tshortCircuit, verbose);\n-\t\t\tif (verbose)\n-\t\t\t\tSystem.out.println(\"\\tPredecessor strand val=\"+strandVal);\n-\t\t\tminVal = Float.min(minVal, strandVal);\n-\t\t\tif (shortCircuit && minVal < threshold)\n-\t\t\t\treturn minVal;\n-\t\t}\n-\t\t\n-\t\tfor (FaultSubsectionCluster descendant : navigator.getDescendants(nucleationCluster)) {\n-\t\t\tif (verbose)\n-\t\t\t\tSystem.out.println(\"\\tTesting to descendant=\"+descendant);\n-\t\t\tList<FaultSection> strandSects = getAddSectsToward(\n-\t\t\t\t\tnavigator, null, null, nucleationCluster, descendant);\n-\t\t\tfloat strandVal = testStrand(navigator, curClusters, strandSects, nucleationCluster, descendant,\n-\t\t\t\t\tshortCircuit, verbose);\n-\t\t\tif (verbose)\n-\t\t\t\tSystem.out.println(\"\\tDescendant strand val=\"+strandVal);\n-\t\t\tminVal = Float.min(minVal, strandVal);\n-\t\t\tif (shortCircuit && minVal < threshold)\n-\t\t\t\treturn minVal;\n-\t\t}\n-\t\t\n-\t\treturn minVal;\n-\t}\n-\t\n-\tprivate float testStrand(RuptureTreeNavigator navigator, HashSet<FaultSubsectionCluster> strandClusters,\n-\t\t\tList<FaultSection> strandSections, FaultSubsectionCluster prevAddition,\n-\t\t\tFaultSubsectionCluster addition, boolean shortCircuit,\n-\t\t\tboolean verbose) {\n-\t\tfloat minVal = Float.POSITIVE_INFINITY;\n-\t\tif (!strandSections.isEmpty()) {\n-//\t\t\tStiffnessResult stiffness = stiffnessCalc.calcAggClustersToClusterStiffness(\n-//\t\t\t\t\tStiffnessType.CFF, strandClusters, addition);\n-\t\t\tStiffnessResult stiffness = stiffnessCalc.calcAggStiffness(StiffnessType.CFF,\n-\t\t\t\t\tstrandSections, addition.subSects, -1, addition.parentSectionID);\n-\t\t\tdouble val = stiffness.getValue(aggMethod);\n-\t\t\tif (verbose)\n-\t\t\t\tSystem.out.println(\"\\t\\tval=\"+val+\" for \"+strandClusters.size()+\" clusters (\"\n-\t\t\t\t\t\t+strandSections.size()+\" sects) to \"+addition);\n-\t\t\tminVal = Float.min(minVal, (float)val);\n-\t\t\tif (shortCircuit && minVal < threshold)\n-\t\t\t\treturn (float)val;\n-\t\t}\n-\t\t\n-\t\t// this addition passed, continue downstream\n-\t\tHashSet<FaultSubsectionCluster> newStrandClusters = new HashSet<>(strandClusters);\n-\t\tnewStrandClusters.add(addition);\n-\t\t\n-\t\t// check predecessor of this strand\n-\t\tFaultSubsectionCluster predecessor = navigator.getPredecessor(addition);\n-\t\tif (predecessor != null && !strandClusters.contains(predecessor)) {\n-\t\t\t// go down that path\n-\t\t\tList<FaultSection> newStrandSects = getAddSectsToward(navigator, strandSections,\n-\t\t\t\t\tprevAddition, addition, predecessor);\n-\t\t\tfloat val = testStrand(navigator, newStrandClusters, newStrandSects, addition, predecessor,\n-\t\t\t\t\tshortCircuit, verbose);\n-\t\t\tminVal = Float.min(minVal, (float)val);\n-\t\t\tif (shortCircuit && minVal < threshold)\n-\t\t\t\treturn (float)val;\n-\t\t}\n-\t\t\n-\t\t// check descendants of this strand\n-\t\tfor (FaultSubsectionCluster descendant : navigator.getDescendants(addition)) {\n-\t\t\tif (strandClusters.contains(descendant))\n-\t\t\t\tcontinue;\n-\t\t\t// go down that path\n-\t\t\tList<FaultSection> newStrandSects = getAddSectsToward(navigator, strandSections,\n-\t\t\t\t\tprevAddition, addition, descendant);\n-\t\t\tfloat val = testStrand(navigator, newStrandClusters, newStrandSects, addition, descendant,\n-\t\t\t\t\tshortCircuit, verbose);\n-\t\t\tminVal = Float.min(minVal, (float)val);\n-\t\t\tif (shortCircuit && minVal < threshold)\n-\t\t\t\treturn (float)val;\n-\t\t}\n-\t\t\n-\t\t// if we m)ade it here, this either the end of the line or all downstream strand extensions pass\n-\t\treturn minVal;\n-\t}\n-\t\n-\tprivate List<FaultSection> getAddSectsToward(RuptureTreeNavigator navigator, List<FaultSection> prevSects,\n-\t\t\tFaultSubsectionCluster prevLastCluster, FaultSubsectionCluster newLastCluster,\n-\t\t\tFaultSubsectionCluster destination) {\n-\t\tList<FaultSection> sects = prevSects == null ? new ArrayList<>() : new ArrayList<>(prevSects);\n-\t\t\n-\t\t// jump to the destination\n-\t\tJump nextJump = navigator.getJump(newLastCluster, destination);\n-\t\tPreconditions.checkState(nextJump.fromCluster == newLastCluster);\n-\t\tPreconditions.checkState(nextJump.toCluster == destination);\n-\t\tFaultSection destSect = nextJump.fromSection;\n-\t\t\n-\t\tFaultSection startSect;\n-\t\tif (prevLastCluster == null) {\n-\t\t\t// this is the first cluster, use the longest strand of sections up to this jump\n-\t\t\tint jumpIndex = newLastCluster.subSects.indexOf(nextJump.fromSection);\n-\t\t\tPreconditions.checkState(jumpIndex >= 0);\n-\t\t\tint numFromStart = jumpIndex+1;\n-\t\t\tint numFromEnd = newLastCluster.subSects.size()-jumpIndex;\n-\t\t\tif (numFromEnd > numFromStart)\n-\t\t\t\tstartSect = newLastCluster.subSects.get(newLastCluster.subSects.size()-1);\n-\t\t\telse\n-\t\t\t\tstartSect = newLastCluster.subSects.get(0);\n-//\t\t\tSystem.out.println(newLastCluster.parentSectionID+\" is a start, using longest strand to jump \"\n-//\t\t\t\t\t+ \"starting at \"+startSect.getSectionId());\n-\t\t} else {\n-\t\t\t// just the portion of this cluster on the direct path\n-\t\t\tJump prevJump = navigator.getJump(prevLastCluster, newLastCluster);\n-\t\t\tPreconditions.checkState(prevJump.fromCluster == prevLastCluster);\n-\t\t\tPreconditions.checkState(prevJump.toCluster == newLastCluster);\n-\t\t\tstartSect = prevJump.toSection;\n-//\t\t\tSystem.out.println(\"Using startSect=\"+startSect.getSectionId()+\" via jump from prevLast=\"+prevJump);\n-\t\t}\n-\t\t\n-\t\tif (newLastCluster.endSects.contains(destSect) && newLastCluster.subSects.get(0).equals(startSect)\n-\t\t\t\t|| newLastCluster.endSects.contains(startSect) && newLastCluster.subSects.get(0).equals(destSect)) {\n-\t\t\t// simple case, it's a jump from the end, include all sections\n-\t\t\tsects.addAll(newLastCluster.subSects);\n-//\t\t\tSystem.out.println(\"added full \"+newLastCluster.subSects.size()+\" sects on \"\n-//\t\t\t\t\t+newLastCluster.parentSectionID+\" between \"+startSect.getSectionId()\n-//\t\t\t\t\t+\" and \"+destSect.getSectionId());\n-\t\t} else if (destSect.equals(startSect)) {\n-//\t\t\tSystem.out.println(\"added single sect on \"+newLastCluster.parentSectionID\n-//\t\t\t\t\t+\", start=dest=\"+startSect.getSectionId());\n-\t\t\tsects.add(startSect);\n-\t\t} else {\n-\t\t\t// only include sections up to and including fromSection\n-\t\t\tboolean inside = false;\n-\t\t\tint added = 0;\n-\t\t\tfor (FaultSection sect : newLastCluster.subSects) {\n-\t\t\t\tif (sect.equals(startSect) || sect.equals(destSect)) {\n-\t\t\t\t\tinside = !inside;\n-\t\t\t\t\tif (!inside)\n-\t\t\t\t\t\t// we've encountered a start and an end, stop searching\n-\t\t\t\t\t\tbreak;\n-\t\t\t\t}\n-\t\t\t\tif (inside) {\n-\t\t\t\t\tsects.add(sect);\n-\t\t\t\t\tadded++;\n-\t\t\t\t}\n-\t\t\t}\n-//\t\t\tSystem.out.println(\"added \"+added+\" sects on \"+newLastCluster.parentSectionID+\" between \"\n-//\t\t\t\t\t+startSect.getSectionId()+\" and \"+destSect.getSectionId());\n-\t\t}\n-\t\treturn sects;\n-\t}\n-\n-\t@Override\n-\tpublic String getShortName() {\n-\t\tString threshStr = threshold == 0f ? \"0\" : threshold+\"\";\n-\t\tif (fractPassThreshold > 0f) {\n-\t\t\tif (fractPassThreshold == 0.5f)\n-\t\t\t\treturn \"HalfPathsCFF\"+threshStr;\n-\t\t\tif (fractPassThreshold == 1f/3f)\n-\t\t\t\treturn \"1/3PathsCFF\"+threshStr;\n-\t\t\tif (fractPassThreshold == 2f/3f)\n-\t\t\t\treturn \"2/3PathsCFF\"+threshStr;\n-\t\t\tif (fractPassThreshold == 0.25f)\n-\t\t\t\treturn \"1/4PathsCFF\"+threshStr;\n-\t\t\tif (fractPassThreshold == 0.75f)\n-\t\t\t\treturn \"3/4PathsCFF\"+threshStr;\n-\t\t\treturn fractPassThreshold+\"PathsCFF\"+threshStr;\n-\t\t}\n-\t\treturn \"PathCFF\"+threshStr;\n-\t}\n-\n-\t@Override\n-\tpublic String getName() {\n-\t\tif (fractPassThreshold > 0f) {\n-\t\t\tif (fractPassThreshold == 0.5f)\n-\t\t\t\treturn \"Cluster Half Paths Coulomb   \"+(float)threshold;\n-\t\t\tif (fractPassThreshold == 1f/3f)\n-\t\t\t\treturn \"Cluster 1/3 Paths Coulomb   \"+(float)threshold;\n-\t\t\tif (fractPassThreshold == 2f/3f)\n-\t\t\t\treturn \"Cluster 2/3 Paths Coulomb   \"+(float)threshold;\n-\t\t\tif (fractPassThreshold == 0.25f)\n-\t\t\t\treturn \"Cluster 1/4 Paths Coulomb   \"+(float)threshold;\n-\t\t\tif (fractPassThreshold == 0.75f)\n-\t\t\t\treturn \"Cluster 3/4 Paths Coulomb   \"+(float)threshold;\n-\t\t\treturn \"Cluster \"+fractPassThreshold+\"x Paths Coulomb   \"+(float)threshold;\n-\t\t}\n-\t\treturn \"Cluster Path Coulomb   \"+(float)threshold;\n-\t}\n-\n-\t@Override\n-\tpublic Range<Float> getAcceptableRange() {\n-\t\treturn Range.atLeast((float)threshold);\n-\t}\n-\n-\t@Override\n-\tpublic SubSectStiffnessCalculator getStiffnessCalc() {\n-\t\treturn stiffnessCalc;\n-\t}\n-\n-\t@Override\n-\tpublic boolean isDirectional(boolean splayed) {\n-\t\treturn splayed;\n-\t}\n-\n-}"
  },
  {
    "sha": "8391097afd0670ec1dd8e5d4f38e8fe591c1c1ab",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/CumulativePenaltyFilter.java",
    "status": "modified",
    "additions": 6,
    "deletions": 1,
    "changes": 7,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/CumulativePenaltyFilter.java",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/CumulativePenaltyFilter.java",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/CumulativePenaltyFilter.java?ref=541efed3666ce0a47e03a345aada595633351042",
    "patch": "@@ -4,6 +4,7 @@\n import java.util.ArrayList;\n \n import org.opensha.commons.data.Named;\n+import org.opensha.commons.util.ExceptionUtils;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.ClusterRupture;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.Jump;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.PlausibilityConfiguration;\n@@ -320,7 +321,11 @@ public PlausibilityFilter read(JsonReader in) throws IOException {\n \t\t\t\t\t\twhile (in.hasNext()) {\n \t\t\t\t\t\t\tswitch (in.nextName()) {\n \t\t\t\t\t\t\tcase \"class\":\n-\t\t\t\t\t\t\t\ttype = PlausibilityConfiguration.getDeclaredTypeClass(in.nextString());\n+\t\t\t\t\t\t\t\ttry {\n+\t\t\t\t\t\t\t\t\ttype = PlausibilityConfiguration.getDeclaredTypeClass(in.nextString());\n+\t\t\t\t\t\t\t\t} catch (ClassNotFoundException e) {\n+\t\t\t\t\t\t\t\t\tthrow ExceptionUtils.asRuntimeException(e);\n+\t\t\t\t\t\t\t\t}\n \t\t\t\t\t\t\t\tbreak;\n \t\t\t\t\t\t\tcase \"value\":\n \t\t\t\t\t\t\t\tPreconditions.checkNotNull(type, \"Class must preceed value in Penalty JSON\");"
  },
  {
    "sha": "559ee76eb5029a2eca17a4baf579b854fda8cd95",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/DirectPathPlausibilityFilter.java",
    "status": "added",
    "additions": 125,
    "deletions": 0,
    "changes": 125,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/DirectPathPlausibilityFilter.java",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/DirectPathPlausibilityFilter.java",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/DirectPathPlausibilityFilter.java?ref=541efed3666ce0a47e03a345aada595633351042",
    "patch": "@@ -0,0 +1,125 @@\n+package org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+\n+import org.opensha.commons.util.IDPairing;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.ClusterRupture;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.Jump;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.PlausibilityFilter;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.strategies.ClusterConnectionStrategy;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.util.SectionDistanceAzimuthCalculator;\n+import org.opensha.sha.faultSurface.FaultSection;\n+\n+import com.google.common.collect.Lists;\n+import com.google.gson.Gson;\n+import com.google.gson.TypeAdapter;\n+import com.google.gson.stream.JsonReader;\n+import com.google.gson.stream.JsonWriter;\n+\n+import scratch.UCERF3.inversion.laughTest.PlausibilityResult;\n+\n+public class DirectPathPlausibilityFilter implements PlausibilityFilter {\n+\t\n+\tprivate transient HashSet<IDPairing> sectJumps;\n+\n+\tpublic DirectPathPlausibilityFilter(ClusterConnectionStrategy connStrat) {\n+\t\tsectJumps = new HashSet<>();\n+\t\tfor (Jump jump : connStrat.getAllPossibleJumps())\n+\t\t\tsectJumps.add(pairing(jump.fromSection, jump.toSection));\n+\t}\n+\t\n+\tprivate IDPairing pairing(FaultSection from, FaultSection to) {\n+\t\tint id1 = from.getSectionId();\n+\t\tint id2 = to.getSectionId();\n+\t\tif (id1 < id2)\n+\t\t\treturn new IDPairing(id1, id2);\n+\t\treturn new IDPairing(id2, id1);\n+\t}\n+\n+\t@Override\n+\tpublic String getShortName() {\n+\t\treturn \"NoIndirect\";\n+\t}\n+\n+\t@Override\n+\tpublic String getName() {\n+\t\treturn \"No Indirect Connections\";\n+\t}\n+\n+\t@Override\n+\tpublic PlausibilityResult apply(ClusterRupture rupture, boolean verbose) {\n+\t\tPlausibilityResult result = evaluate(rupture, new ArrayList<>(), verbose);\n+\t\tfor (Jump splayJump : rupture.splays.keySet()) {\n+\t\t\tClusterRupture splay = rupture.splays.get(splayJump);\n+\t\t\tresult = result.logicalAnd(evaluate(splay, Lists.newArrayList(splayJump.fromSection), verbose));\n+\t\t}\n+\t\treturn result;\n+\t}\n+\t\n+\tprivate PlausibilityResult evaluate(ClusterRupture rupture, List<FaultSection> prevFroms, boolean verbose) {\n+\t\tPlausibilityResult result = PlausibilityResult.PASS;\n+\t\tfor (Jump jump : rupture.internalJumps) {\n+\t\t\tfor (FaultSection prevFrom : prevFroms) {\n+\t\t\t\tIDPairing pair = pairing(prevFrom, jump.toSection);\n+\t\t\t\tif (sectJumps.contains(pair)) {\n+\t\t\t\t\tresult = PlausibilityResult.FAIL_HARD_STOP;\n+\t\t\t\t\tif (verbose)\n+\t\t\t\t\t\tSystem.out.println(getShortName()+\": loopback detected. Could have jumped directly from \"\n+\t\t\t\t\t\t\t\t+prevFrom.getSectionId()+\" to \"+jump.toSection.getSectionId());\n+\t\t\t\t\telse\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t}\n+\t\t\t\tif (prevFrom.getParentSectionId() == jump.toSection.getParentSectionId() && pair.getID2() == pair.getID1()+1) {\n+\t\t\t\t\tresult = PlausibilityResult.FAIL_HARD_STOP;\n+\t\t\t\t\tif (verbose)\n+\t\t\t\t\t\tSystem.out.println(getShortName()+\": loopback detected. \"+prevFrom.getSectionId()+\" to \"\n+\t\t\t\t\t\t\t\t+jump.toSection.getSectionId()+\" are neighbors on the same parent, but we took a circuitous route.\");\n+\t\t\t\t\telse\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tprevFroms.add(jump.fromSection);\n+\t\t}\n+\t\treturn result;\n+\t}\n+\n+\t@Override\n+\tpublic boolean isDirectional(boolean splayed) {\n+\t\t// only directional if splayed\n+\t\treturn splayed;\n+\t}\n+\n+\t@Override\n+\tpublic TypeAdapter<PlausibilityFilter> getTypeAdapter() {\n+\t\treturn new Adapter();\n+\t}\n+\t\n+\tpublic static class Adapter extends PlausibilityFilterTypeAdapter {\n+\n+\t\tprivate ClusterConnectionStrategy connStrategy;\n+\n+\t\t@Override\n+\t\tpublic void init(ClusterConnectionStrategy connStrategy,\n+\t\t\t\tSectionDistanceAzimuthCalculator distAzCalc, Gson gson) {\n+\t\t\tthis.connStrategy = connStrategy;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void write(JsonWriter out, PlausibilityFilter value) throws IOException {\n+\t\t\tout.beginObject();\n+\t\t\tout.endObject();\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic PlausibilityFilter read(JsonReader in) throws IOException {\n+\t\t\tin.beginObject();\n+\t\t\tin.endObject();\n+\t\t\treturn new DirectPathPlausibilityFilter(connStrategy);\n+\t\t}\n+\t\t\n+\t}\n+\n+}"
  },
  {
    "sha": "3fefcbe3f53804c24610c2403492ad9df3ba2f8e",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/JumpAzimuthChangeFilter.java",
    "status": "modified",
    "additions": 100,
    "deletions": 25,
    "changes": 125,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/JumpAzimuthChangeFilter.java",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/JumpAzimuthChangeFilter.java",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/JumpAzimuthChangeFilter.java?ref=541efed3666ce0a47e03a345aada595633351042",
    "patch": "@@ -4,13 +4,16 @@\n import java.util.Collection;\n import java.util.HashSet;\n \n+import org.opensha.commons.geo.Location;\n+import org.opensha.commons.geo.LocationUtils;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.ClusterRupture;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.Jump;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.JumpPlausibilityFilter;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.ScalarValuePlausibiltyFilter;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.util.RuptureTreeNavigator;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.util.SectionDistanceAzimuthCalculator;\n import org.opensha.sha.faultSurface.FaultSection;\n+import org.opensha.sha.faultSurface.FaultTrace;\n \n import com.google.common.base.Preconditions;\n import com.google.common.collect.BoundType;\n@@ -29,6 +32,7 @@\n \tprivate float threshold;\n \t\n \tprivate transient boolean errOnCantEval = false;\n+\tprivate boolean useEndpointsForSingle = true;\n \n \tpublic JumpAzimuthChangeFilter(AzimuthCalc calc, float threshold) {\n \t\tthis.azCalc = calc;\n@@ -81,7 +85,7 @@ public PlausibilityResult apply(ClusterRupture rupture, boolean verbose) {\n \tpublic PlausibilityResult testJump(ClusterRupture rupture, Jump jump, boolean verbose) {\n \t\tRuptureTreeNavigator navigator = rupture.getTreeNavigator();\n \t\tFaultSection before1 = navigator.getPredecessor(jump.fromSection);\n-\t\tif (before1 == null) {\n+\t\tif (before1 == null && !useEndpointsForSingle) {\n \t\t\t// fewer than 2 sections before the first jump, will never work\n \t\t\tif (errOnCantEval)\n \t\t\t\tthrow new IllegalStateException(getShortName()+\": erring because fewer than \"\n@@ -97,51 +101,122 @@ public PlausibilityResult testJump(ClusterRupture rupture, Jump jump, boolean ve\n \t\t\t\t\t\t+ \"2 sects after a jump\");\n \t\t\treturn PlausibilityResult.FAIL_FUTURE_POSSIBLE;\n \t\t}\n-\t\tif (value > threshold)\n+\t\tif (value > threshold) {\n+\t\t\tif (useEndpointsForSingle && navigator.getDescendants(jump.toCluster).isEmpty())\n+\t\t\t\treturn PlausibilityResult.FAIL_FUTURE_POSSIBLE;\n \t\t\treturn PlausibilityResult.FAIL_HARD_STOP;\n+\t\t}\n \t\treturn PlausibilityResult.PASS;\n \t}\n \t\n+\tpublic static double horzDistToTrace(Location loc, FaultTrace trace) {\n+\t\tdouble minDist = Double.POSITIVE_INFINITY;\n+\t\tfor (Location loc2 : trace)\n+\t\t\tminDist = Math.min(minDist, LocationUtils.horzDistanceFast(loc, loc2));\n+\t\treturn minDist;\n+\t}\n+\t\n \tprivate Float calc(ClusterRupture rupture, Jump jump, boolean verbose) {\n \t\tRuptureTreeNavigator navigator = rupture.getTreeNavigator();\n \t\tFaultSection before1 = navigator.getPredecessor(jump.fromSection);\n-\t\tif (before1 == null) {\n-\t\t\t// fewer than 2 sections before the first jump, will never work\n-\t\t\tif (verbose)\n-\t\t\t\tSystem.out.println(getShortName()+\": failing because fewer than 2 before 1st jump\");\n-\t\t\treturn null;\n-\t\t}\n \t\tFaultSection before2 = jump.fromSection;\n-\t\tdouble beforeAz = azCalc.calcAzimuth(before1, before2);\n-\t\t\n \t\tFaultSection after1 = jump.toSection;\n-\t\tCollection<FaultSection> after2s;\n-\t\tif (rupture.contains(after1)) {\n-\t\t\t// this is a preexisting jump and can be a fork with multiple second sections after the jump\n-\t\t\t// we will pass only if they all pass\n-\t\t\tafter2s = navigator.getDescendants(after1);\n-\t\t} else {\n-\t\t\t// we're testing a new possible jump\n-\t\t\tif (jump.toCluster.subSects.size() < 2) {\n-\t\t\t\t// it's a jump to a single-section cluster\n+\t\t\n+\t\tint beforeID1;\n+\t\tdouble beforeAz;\n+\t\tif (before1 == null) {\n+\t\t\tif (useEndpointsForSingle) {\n+\t\t\t\tbeforeID1 = -1;\n+\t\t\t\tLocation startLoc = before2.getFaultTrace().first();\n+\t\t\t\tLocation endLoc = before2.getFaultTrace().last();\n+\t\t\t\t// we want this azimuth vector to go toward the destination\n+\t\t\t\tif (horzDistToTrace(startLoc, after1.getFaultTrace()) < horzDistToTrace(endLoc, after1.getFaultTrace())) {\n+\t\t\t\t\t// reverse it\n+\t\t\t\t\tLocation tmp = startLoc;\n+\t\t\t\t\tstartLoc = endLoc;\n+\t\t\t\t\tendLoc = tmp;\n+\t\t\t\t}\n+\t\t\t\tif (azCalc instanceof HardCodedLeftLateralFlipAzimuthCalc) {\n+\t\t\t\t\tif (((HardCodedLeftLateralFlipAzimuthCalc)azCalc).parentIDs.contains(before2.getParentSectionId())) {\n+\t\t\t\t\t\t// reverse it\n+\t\t\t\t\t\tLocation tmp = startLoc;\n+\t\t\t\t\t\tstartLoc = endLoc;\n+\t\t\t\t\t\tendLoc = tmp;\n+\t\t\t\t\t}\n+\t\t\t\t} else if (azCalc instanceof LeftLateralFlipAzimuthCalc) {\n+\t\t\t\t\tif (((LeftLateralFlipAzimuthCalc)azCalc).rakeRange.contains(before2.getAveRake())) {\n+\t\t\t\t\t\t// reverse it\n+\t\t\t\t\t\tLocation tmp = startLoc;\n+\t\t\t\t\t\tstartLoc = endLoc;\n+\t\t\t\t\t\tendLoc = tmp;\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tbeforeAz = LocationUtils.azimuth(startLoc, endLoc);\n+\t\t\t\tif (verbose)\n+\t\t\t\t\tSystem.out.println(getShortName()+\": using endpoint azimuth for beforeAz=\"+beforeAz);\n+\t\t\t} else {\n+\t\t\t\t// fewer than 2 sections before the first jump, will never work\n \t\t\t\tif (verbose)\n-\t\t\t\t\tSystem.out.println(getShortName()+\": jump to single-section cluster\");\n+\t\t\t\t\tSystem.out.println(getShortName()+\": failing because fewer than 2 before 1st jump\");\n \t\t\t\treturn null;\n \t\t\t}\n-\t\t\tafter2s = Lists.newArrayList(jump.toCluster.subSects.get(1));\n+\t\t} else {\n+\t\t\tbeforeID1 = before1.getSectionId();\n+\t\t\tbeforeAz = azCalc.calcAzimuth(before1, before2);\n \t\t}\n+\t\t\n+\t\tPreconditions.checkState(rupture.contains(after1));\n+\t\t// this is a preexisting jump and can be a fork with multiple second sections after the jump\n+\t\t// we will pass only if they all pass\n+\t\tCollection<FaultSection> after2s = navigator.getDescendants(after1);\n \t\tif (after2s.isEmpty()) {\n-\t\t\tif (verbose)\n-\t\t\t\tSystem.out.println(getShortName()+\": jump to single-section cluster & nothing downstream\");\n-\t\t\treturn null;\n+\t\t\tif (useEndpointsForSingle) {\n+\t\t\t\tLocation startLoc = after1.getFaultTrace().first();\n+\t\t\t\tLocation endLoc = after1.getFaultTrace().last();\n+\t\t\t\t// we want this azimuth vector to go away from the before section\n+\t\t\t\tif (horzDistToTrace(startLoc, before2.getFaultTrace()) > horzDistToTrace(endLoc, before2.getFaultTrace())) {\n+\t\t\t\t\t// reverse it\n+\t\t\t\t\tLocation tmp = startLoc;\n+\t\t\t\t\tstartLoc = endLoc;\n+\t\t\t\t\tendLoc = tmp;\n+\t\t\t\t}\n+\t\t\t\tif (azCalc instanceof HardCodedLeftLateralFlipAzimuthCalc) {\n+\t\t\t\t\tif (((HardCodedLeftLateralFlipAzimuthCalc)azCalc).parentIDs.contains(after1.getParentSectionId())) {\n+\t\t\t\t\t\t// reverse it\n+\t\t\t\t\t\tLocation tmp = startLoc;\n+\t\t\t\t\t\tstartLoc = endLoc;\n+\t\t\t\t\t\tendLoc = tmp;\n+\t\t\t\t\t}\n+\t\t\t\t} else if (azCalc instanceof LeftLateralFlipAzimuthCalc) {\n+\t\t\t\t\tif (((LeftLateralFlipAzimuthCalc)azCalc).rakeRange.contains(after1.getAveRake())) {\n+\t\t\t\t\t\t// reverse it\n+\t\t\t\t\t\tLocation tmp = startLoc;\n+\t\t\t\t\t\tstartLoc = endLoc;\n+\t\t\t\t\t\tendLoc = tmp;\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tdouble afterAz = LocationUtils.azimuth(startLoc, endLoc);\n+\t\t\t\tif (verbose)\n+\t\t\t\t\tSystem.out.println(getShortName()+\": using endpoint azimuth for afterAz=\"+afterAz);\n+\t\t\t\tdouble diff = getAzimuthDifference(beforeAz, afterAz);\n+\t\t\t\tPreconditions.checkState(Double.isFinite(diff));\n+\t\t\t\tif (verbose)\n+\t\t\t\t\tSystem.out.println(getShortName()+\": [\"+beforeID1+\",\"+before2.getSectionId()+\"]=\"\n+\t\t\t\t\t\t\t+beforeAz+\" => [\"+after1.getSectionId()+\",-1\"+\"]=\"+afterAz+\" = \"+diff);\n+\t\t\t\treturn (float)Math.abs(diff);\n+\t\t\t} else {\n+\t\t\t\tif (verbose)\n+\t\t\t\t\tSystem.out.println(getShortName()+\": jump to single-section cluster & nothing downstream\");\n+\t\t\t\treturn null;\n+\t\t\t}\n \t\t}\n \t\tfloat maxVal = 0f;\n \t\tfor (FaultSection after2 : after2s) {\n \t\t\tdouble afterAz = azCalc.calcAzimuth(after1, after2);\n \t\t\tdouble diff = getAzimuthDifference(beforeAz, afterAz);\n \t\t\tPreconditions.checkState(Double.isFinite(diff));\n \t\t\tif (verbose)\n-\t\t\t\tSystem.out.println(getShortName()+\": [\"+before1.getSectionId()+\",\"+before2.getSectionId()+\"]=\"\n+\t\t\t\tSystem.out.println(getShortName()+\": [\"+beforeID1+\",\"+before2.getSectionId()+\"]=\"\n \t\t\t\t\t\t+beforeAz+\" => [\"+after1.getSectionId()+\",\"+after2.getSectionId()+\"]=\"+afterAz+\" = \"+diff);\n \t\t\tmaxVal = Float.max(maxVal, (float)Math.abs(diff));\n //\t\t\tif ((float)Math.abs(diff) > threshold) {"
  },
  {
    "sha": "a0c02d147562692088f59c9525f0d74f8ee00262",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/MultiDirectionalPlausibilityFilter.java",
    "status": "modified",
    "additions": 6,
    "deletions": 22,
    "changes": 28,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/MultiDirectionalPlausibilityFilter.java",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/MultiDirectionalPlausibilityFilter.java",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/MultiDirectionalPlausibilityFilter.java?ref=541efed3666ce0a47e03a345aada595633351042",
    "patch": "@@ -44,6 +44,10 @@ public MultiDirectionalPlausibilityFilter(PlausibilityFilter filter,\n \t\tthis.connSearch = connSearch;\n \t\tthis.onlyWhenSplayed = onlyWhenSplayed;\n \t}\n+\t\n+\tpublic PlausibilityFilter getFilter() {\n+\t\treturn filter;\n+\t}\n \n \t@Override\n \tpublic String getShortName() {\n@@ -81,6 +85,7 @@ public PlausibilityResult apply(ClusterRupture rupture, boolean verbose) {\n \t\t\t\t\tSystem.out.println(\"MultiDirectional \"+getShortName()\n \t\t\t\t\t\t+\": inversion=\"+altRupture);\n \t\t\t\tresult = filter.apply(altRupture, verbose);\n+//\t\t\t\tresult = filter.apply(altRupture, false);\n \t\t\t\tif (verbose)\n \t\t\t\t\tSystem.out.println(\"MultiDirectional \"+getShortName()\n \t\t\t\t\t\t+\": inversion result=\"+result);\n@@ -148,34 +153,13 @@ public E getValue(ClusterRupture rupture) {\n \t\t\tfor (ClusterRupture inversion : super.getInversions(rupture)) {\n \t\t\t\t// see if there's a better version available\n \t\t\t\tE altScalar = filter.getValue(inversion);\n-\t\t\t\tboolean better = isScalarBetter(altScalar, scalar);\n+\t\t\t\tboolean better = altScalar != null && (scalar == null || isValueBetter(altScalar, scalar));\n \t\t\t\tif (altScalar != null && better)\n \t\t\t\t\t// this one is better\n \t\t\t\t\tscalar = altScalar;\n \t\t\t}\n \t\t\treturn scalar;\n \t\t}\n-\t\t\n-\t\tprivate boolean isScalarBetter(E curNumber, E prevNumber) {\n-\t\t\tPreconditions.checkState(upper != null || lower != null);\n-\t\t\tif (curNumber == null)\n-\t\t\t\treturn false;\n-\t\t\tif (prevNumber == null)\n-\t\t\t\treturn true;\n-\t\t\tdouble curVal = curNumber.doubleValue();\n-\t\t\tdouble prevVal = prevNumber.doubleValue();\n-\t\t\tif (upper == null) {\n-\t\t\t\t// we only have a lower bound, so higher values are better\n-\t\t\t\treturn curVal > prevVal;\n-\t\t\t}\n-\t\t\tif (lower == null) {\n-\t\t\t\t// we only have an upper bound, so lower values are better\n-\t\t\t\treturn curVal < prevVal;\n-\t\t\t}\n-\t\t\t// we have both, use distance to center (lower being better)\n-\t\t\tdouble center = 0.5*(lower + upper);\n-\t\t\treturn Math.abs(curVal - center) < Math.abs(prevVal - center);\n-\t\t}\n \n \t\t@Override\n \t\tpublic Range<E> getAcceptableRange() {"
  },
  {
    "sha": "2215b12fe45e911bf53281d9a57bbb9daae40b3b",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/NetRuptureCoulombFilter.java",
    "status": "removed",
    "additions": 0,
    "deletions": 86,
    "changes": 86,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/9e0f2c8ac3a20f3e4ce6dc8814fa8709d85cdd79/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/NetRuptureCoulombFilter.java",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/9e0f2c8ac3a20f3e4ce6dc8814fa8709d85cdd79/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/NetRuptureCoulombFilter.java",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/NetRuptureCoulombFilter.java?ref=9e0f2c8ac3a20f3e4ce6dc8814fa8709d85cdd79",
    "patch": "@@ -1,86 +0,0 @@\n-package org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl;\n-\n-import java.util.ArrayList;\n-import java.util.List;\n-\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.ClusterRupture;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.FaultSubsectionCluster;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.Jump;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.ScalarCoulombPlausibilityFilter;\n-import org.opensha.sha.faultSurface.FaultSection;\n-import org.opensha.sha.simulators.stiffness.RuptureCoulombResult;\n-import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator;\n-import org.opensha.sha.simulators.stiffness.RuptureCoulombResult.RupCoulombQuantity;\n-import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator.StiffnessAggregationMethod;\n-\n-import com.google.common.collect.Range;\n-\n-import scratch.UCERF3.inversion.laughTest.PlausibilityResult;\n-\n-/**\n- * This filter tests the net Coulomb compatibility of a rupture. For each participating section, it computes\n- * Coulomb with all other sections as a source that section as the receiver. It then aggregates across all\n- * sections according to the specified RupCoulombQuantity, e.g., RupCoulombQuantity.SUM_SECT_CFF to sum\n- * the value across all subsections.\n- * \n- * @author kevin\n- *\n- */\n-public class NetRuptureCoulombFilter implements ScalarCoulombPlausibilityFilter {\n-\t\n-\tprivate SubSectStiffnessCalculator stiffnessCalc;\n-\tprivate StiffnessAggregationMethod aggMethod;\n-\tprivate RupCoulombQuantity quantity;\n-\tprivate float threshold;\n-\n-\tpublic NetRuptureCoulombFilter(SubSectStiffnessCalculator stiffnessCalc, StiffnessAggregationMethod aggMethod,\n-\t\t\tRupCoulombQuantity quantity, float threshold) {\n-\t\tsuper();\n-\t\tthis.stiffnessCalc = stiffnessCalc;\n-\t\tthis.aggMethod = aggMethod;\n-\t\tthis.quantity = quantity;\n-\t\tthis.threshold = threshold;\n-\t}\n-\n-\t@Override\n-\tpublic PlausibilityResult apply(ClusterRupture rupture, boolean verbose) {\n-\t\tif (rupture.getTotalNumSects() == 1)\n-\t\t\treturn PlausibilityResult.PASS;\n-\t\tfloat val = getValue(rupture);\n-\t\tPlausibilityResult result = val < threshold ?\n-\t\t\t\tPlausibilityResult.FAIL_HARD_STOP : PlausibilityResult.PASS;\n-\t\tif (verbose)\n-\t\t\tSystem.out.println(getShortName()+\": val=\"+val+\", result=\"+result);\n-\t\treturn result;\n-\t}\n-\n-\t@Override\n-\tpublic String getShortName() {\n-\t\tif (threshold == 0f)\n-\t\t\treturn \"NetRupCFF0\";\n-\t\treturn \"NetRupCFF\"+(float)threshold;\n-\t}\n-\n-\t@Override\n-\tpublic String getName() {\n-\t\treturn \"Net Rupture Coulomb   \"+(float)threshold;\n-\t}\n-\n-\t@Override\n-\tpublic Float getValue(ClusterRupture rupture) {\n-\t\tif (rupture.getTotalNumSects() == 1)\n-\t\t\treturn null;\n-\t\treturn (float)new RuptureCoulombResult(rupture, stiffnessCalc, aggMethod).getValue(quantity);\n-\t}\n-\n-\t@Override\n-\tpublic Range<Float> getAcceptableRange() {\n-\t\treturn Range.atLeast(threshold);\n-\t}\n-\n-\t@Override\n-\tpublic SubSectStiffnessCalculator getStiffnessCalc() {\n-\t\treturn stiffnessCalc;\n-\t}\n-\n-}"
  },
  {
    "sha": "bf36606ed82ac0bd3dd34aff4dda010a3027362a",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/TotalAzimuthChangeFilter.java",
    "status": "modified",
    "additions": 7,
    "deletions": 17,
    "changes": 24,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/TotalAzimuthChangeFilter.java",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/TotalAzimuthChangeFilter.java",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/TotalAzimuthChangeFilter.java?ref=541efed3666ce0a47e03a345aada595633351042",
    "patch": "@@ -62,19 +62,18 @@ public PlausibilityResult apply(ClusterRupture rupture, boolean verbose) {\n \t\t}\n \t\tRuptureTreeNavigator navigator = rupture.getTreeNavigator();\n \t\tPlausibilityResult result = apply(rupture.clusters[0],\n-\t\t\t\trupture.clusters[rupture.clusters.length-1], navigator, null, verbose);\n+\t\t\t\trupture.clusters[rupture.clusters.length-1], navigator, verbose);\n \t\tif (verbose)\n \t\t\tSystem.out.println(getShortName()+\": primary starnd result=\"+result);\n \t\tfor (ClusterRupture splay : rupture.splays.values())\n \t\t\tresult = result.logicalAnd(apply(rupture.clusters[0],\n-\t\t\t\t\tsplay.clusters[splay.clusters.length-1], navigator, null, verbose));\n+\t\t\t\t\tsplay.clusters[splay.clusters.length-1], navigator, verbose));\n \t\treturn result;\n \t}\n \t\n \tprivate PlausibilityResult apply(FaultSubsectionCluster startCluster,\n-\t\t\tFaultSubsectionCluster endCluster, RuptureTreeNavigator navigator, Jump newJump,\n-\t\t\tboolean verbose) {\n-\t\tdouble maxDiff = getValue(startCluster, endCluster, navigator, newJump, verbose);\n+\t\t\tFaultSubsectionCluster endCluster, RuptureTreeNavigator navigator, boolean verbose) {\n+\t\tdouble maxDiff = getValue(startCluster, endCluster, navigator, verbose);\n \t\tif ((float)maxDiff <= threshold)\n \t\t\treturn PlausibilityResult.PASS;\n \t\tif (verbose)\n@@ -86,16 +85,12 @@ private PlausibilityResult apply(FaultSubsectionCluster startCluster,\n \t}\n \t\n \tprivate double getValue(FaultSubsectionCluster startCluster,\n-\t\t\tFaultSubsectionCluster endCluster, RuptureTreeNavigator navigator, Jump newJump,\n-\t\t\tboolean verbose) {\n+\t\t\tFaultSubsectionCluster endCluster, RuptureTreeNavigator navigator, boolean verbose) {\n \t\tFaultSection before1 = startCluster.startSect;\n \t\tList<FaultSection> before2s = new ArrayList<>();\n \t\tif (startCluster.subSects.size() == 1) {\n \t\t\t// use the first section of the next cluster\n \t\t\tbefore2s.addAll(navigator.getDescendants(before1));\n-\t\t\tif (newJump != null && newJump.fromCluster == startCluster)\n-\t\t\t\t// testing a new jump from the first section, use the jumping point\n-\t\t\t\tbefore2s.add(newJump.toSection);\n \t\t} else {\n \t\t\tbefore2s.add(startCluster.subSects.get(1));\n \t\t}\n@@ -108,12 +103,7 @@ private double getValue(FaultSubsectionCluster startCluster,\n \t\t\t\t// need to use the last section of the previous cluster\n \t\t\t\t\n \t\t\t\tFaultSection after2 = endCluster.subSects.get(0);\n-\t\t\t\tFaultSection after1;\n-\t\t\t\tif (newJump != null)\n-\t\t\t\t\t// it's a jump, use the fromSection\n-\t\t\t\t\tafter1 = newJump.fromSection;\n-\t\t\t\telse\n-\t\t\t\t\tafter1 = navigator.getPredecessor(after2);\n+\t\t\t\tFaultSection after1 = navigator.getPredecessor(after2);\n \t\t\t\tdouble afterAz = azCalc.calcAzimuth(after1, after2);\n \t\t\t\t\n \t\t\t\tdouble diff = JumpAzimuthChangeFilter.getAzimuthDifference(beforeAz, afterAz);\n@@ -156,7 +146,7 @@ public Float getValue(FaultSubsectionCluster startCluster, ClusterRupture ruptur\n \t\tif (rupture.getTotalNumSects() < 3)\n \t\t\treturn null;\n \t\tfloat maxVal = (float)getValue(startCluster,\n-\t\t\t\trupture.clusters[rupture.clusters.length-1], navigator, null, false);\n+\t\t\t\trupture.clusters[rupture.clusters.length-1], navigator, false);\n \t\tfor (ClusterRupture splay : rupture.splays.values()) {\n \t\t\tFloat splayVal = getValue(startCluster, splay, navigator);\n \t\t\tif (splayVal == null)"
  },
  {
    "sha": "c24cd6a7dbed7474f689a59dc332d181364377e6",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/U3CoulombJunctionFilter.java",
    "status": "removed",
    "additions": 0,
    "deletions": 125,
    "changes": 125,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/9e0f2c8ac3a20f3e4ce6dc8814fa8709d85cdd79/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/U3CoulombJunctionFilter.java",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/9e0f2c8ac3a20f3e4ce6dc8814fa8709d85cdd79/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/U3CoulombJunctionFilter.java",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/U3CoulombJunctionFilter.java?ref=9e0f2c8ac3a20f3e4ce6dc8814fa8709d85cdd79",
    "patch": "@@ -1,125 +0,0 @@\n-package org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl;\n-\n-import java.util.ArrayList;\n-import java.util.Collection;\n-import java.util.List;\n-\n-import org.opensha.commons.util.IDPairing;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.ClusterRupture;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.Jump;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.PlausibilityFilter;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.util.RuptureTreeNavigator;\n-import org.opensha.sha.faultSurface.FaultSection;\n-\n-import com.google.common.base.Preconditions;\n-import com.google.common.collect.HashMultimap;\n-import com.google.common.collect.Multimap;\n-\n-import scratch.UCERF3.inversion.coulomb.CoulombRates;\n-import scratch.UCERF3.inversion.coulomb.CoulombRatesRecord;\n-import scratch.UCERF3.inversion.coulomb.CoulombRatesTester;\n-import scratch.UCERF3.inversion.laughTest.PlausibilityResult;\n-\n-public class U3CoulombJunctionFilter implements PlausibilityFilter {\n-\t\n-\tprivate CoulombRatesTester tester;\n-\tprivate CoulombRates coulombRates;\n-\n-\tpublic U3CoulombJunctionFilter(CoulombRatesTester tester, CoulombRates coulombRates) {\n-\t\tthis.tester = tester;\n-\t\tthis.coulombRates = coulombRates;\n-\t}\n-\t\n-\t@Override\n-\tpublic PlausibilityResult apply(ClusterRupture rupture, boolean verbose) {\n-\t\tif (rupture.getTotalNumJumps() == 0)\n-\t\t\treturn PlausibilityResult.PASS;\n-\t\t\n-\t\tList<List<IDPairing>> paths = new ArrayList<>();\n-\t\tfindPaths(rupture.getTreeNavigator(), paths, new ArrayList<>(), rupture.clusters[0].startSect);\n-\t\t\n-\t\treturn testPaths(paths, verbose);\n-\t}\n-\t\n-\tprivate void findPaths(RuptureTreeNavigator navigator,\n-\t\t\tList<List<IDPairing>> fullPaths, List<IDPairing> curPath, FaultSection curSect) {\n-\t\tCollection<FaultSection> descendants = navigator.getDescendants(curSect);\n-\t\t\n-\t\twhile (descendants.size() == 1) {\n-\t\t\tFaultSection destSect = descendants.iterator().next();\n-\t\t\tif (curSect.getParentSectionId() != destSect.getParentSectionId())\n-\t\t\t\t// it's a jump\n-\t\t\t\tcurPath.add(new IDPairing(curSect.getSectionId(), destSect.getSectionId()));\n-\t\t\t\n-\t\t\tcurSect = destSect;\n-\t\t\tdescendants = navigator.getDescendants(curSect);\n-\t\t}\n-\t\t\n-\t\tif (descendants.isEmpty()) {\n-\t\t\t// we're at the end of a chain\n-\t\t\tfullPaths.add(curPath);\n-\t\t} else {\n-\t\t\t// we're at a branching point\n-\t\t\tfor (FaultSection destSect : descendants) {\n-\t\t\t\tList<IDPairing> branchPath = new ArrayList<>(curPath);\n-\t\t\t\tif (curSect.getParentSectionId() != destSect.getParentSectionId())\n-\t\t\t\t\t// it's a jump\n-\t\t\t\t\tbranchPath.add(new IDPairing(curSect.getSectionId(), destSect.getSectionId()));\n-\t\t\t\t\n-\t\t\t\tfindPaths(navigator, fullPaths, branchPath, destSect);\n-\t\t\t}\n-\t\t}\n-\t}\n-\t\n-\tprivate PlausibilityResult testPaths(List<List<IDPairing>> paths, boolean verbose) {\n-\t\tif (verbose)\n-\t\t\tSystem.out.println(getShortName()+\": found \"+paths.size()+\" paths\");\n-\t\tPreconditions.checkState(paths.size() >= 0);\n-\t\tfor (List<IDPairing> path : paths) {\n-\t\t\tif (verbose)\n-\t\t\t\tSystem.out.println(getShortName()+\": testing a path with \"+path.size()+\" jumps\");\n-\t\t\tif (path.isEmpty())\n-\t\t\t\tcontinue;\n-\t\t\tList<CoulombRatesRecord> forwardRates = new ArrayList<>();\n-\t\t\tList<CoulombRatesRecord> backwardRates = new ArrayList<>();\n-\t\t\t\n-\t\t\tfor (IDPairing pair : path) {\n-\t\t\t\tCoulombRatesRecord forwardRate = coulombRates.get(pair);\n-\t\t\t\tPreconditions.checkNotNull(forwardRate, \"No coulomb rates for %s\", pair);\n-\t\t\t\tCoulombRatesRecord backwardRate = coulombRates.get(pair.getReversed());\n-\t\t\t\tPreconditions.checkNotNull(backwardRate, \"No coulomb rates for reversed %s\", pair);\n-\t\t\t\tif (verbose) {\n-\t\t\t\t\tSystem.out.println(getShortName()+\": \"+pair.getID1()+\" => \"+pair.getID2());\n-\t\t\t\t\tSystem.out.println(\"\\tForward rate: \"+forwardRate);\n-\t\t\t\t\tSystem.out.println(\"\\tBackward rate: \"+backwardRate);\n-\t\t\t\t}\n-\t\t\t\tforwardRates.add(forwardRate);\n-\t\t\t\tbackwardRates.add(0, backwardRate);\n-\t\t\t}\n-\t\t\t\n-\t\t\tboolean passes = tester.doesRupturePass(forwardRates, backwardRates);\n-\t\t\tif (verbose)\n-\t\t\t\tSystem.out.println(getShortName()+\": test with \"+forwardRates.size()+\" jumps. passes ? \"+passes);\n-\t\t\tif (!passes)\n-\t\t\t\treturn PlausibilityResult.FAIL_HARD_STOP;\n-\t\t}\n-\t\treturn PlausibilityResult.PASS;\n-\t}\n-\n-\t@Override\n-\tpublic String getShortName() {\n-\t\treturn \"Coulomb\";\n-\t}\n-\n-\t@Override\n-\tpublic String getName() {\n-\t\treturn \"Coulomb Jump Filter\";\n-\t}\n-\n-\t@Override\n-\tpublic boolean isDirectional(boolean splayed) {\n-\t\t// only directional if splayed (different inversions could take different jumping points)\n-\t\treturn splayed;\n-\t}\n-\n-}"
  },
  {
    "sha": "9d867bdb9697997b92499e90c76d7abfbda9dd5f",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/coulomb/ClusterCoulombCompatibilityFilter.java",
    "status": "renamed",
    "additions": 29,
    "deletions": 34,
    "changes": 63,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/coulomb/ClusterCoulombCompatibilityFilter.java",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/coulomb/ClusterCoulombCompatibilityFilter.java",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/coulomb/ClusterCoulombCompatibilityFilter.java?ref=541efed3666ce0a47e03a345aada595633351042",
    "patch": "@@ -1,17 +1,14 @@\n-package org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl;\n+package org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.coulomb;\n \n import java.util.ArrayList;\n import java.util.List;\n \n import org.opensha.sha.earthquake.faultSysSolution.ruptures.ClusterRupture;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.FaultSubsectionCluster;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.Jump;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.ScalarCoulombPlausibilityFilter;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.util.RuptureTreeNavigator;\n-import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator;\n-import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator.StiffnessAggregationMethod;\n-import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator.StiffnessResult;\n-import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator.StiffnessType;\n+import org.opensha.sha.faultSurface.FaultSection;\n+import org.opensha.sha.simulators.stiffness.AggregatedStiffnessCalculator;\n \n import com.google.common.collect.Range;\n \n@@ -28,23 +25,23 @@\n  */\n public class ClusterCoulombCompatibilityFilter implements ScalarCoulombPlausibilityFilter {\n \t\n-\tprivate SubSectStiffnessCalculator stiffnessCalc;\n-\tprivate StiffnessAggregationMethod aggMethod;\n-\tprivate float threshold;\n+\tprivate AggregatedStiffnessCalculator aggCalc;\n+\tprivate Range<Float> acceptableRange;\n \n-\tpublic ClusterCoulombCompatibilityFilter(SubSectStiffnessCalculator subSectCalc,\n-\t\t\tStiffnessAggregationMethod aggMethod, float threshold) {\n-\t\tthis.stiffnessCalc = subSectCalc;\n-\t\tthis.aggMethod = aggMethod;\n-\t\tthis.threshold = threshold;\n+\tpublic ClusterCoulombCompatibilityFilter(AggregatedStiffnessCalculator aggCalc, float threshold) {\n+\t\tthis(aggCalc, Range.atLeast(threshold));\n+\t}\n+\tpublic ClusterCoulombCompatibilityFilter(AggregatedStiffnessCalculator aggCalc, Range<Float> acceptableRange) {\n+\t\tthis.aggCalc = aggCalc;\n+\t\tthis.acceptableRange = acceptableRange;\n \t}\n \n \t@Override\n \tpublic PlausibilityResult apply(ClusterRupture rupture, boolean verbose) {\n \t\tdouble worstVal = doTest(new ArrayList<>(), rupture.clusters[0], rupture.getTreeNavigator(),\n \t\t\t\tverbose, !verbose);\n \t\tPlausibilityResult result =\n-\t\t\t\t(float)worstVal >= threshold ? PlausibilityResult.PASS : PlausibilityResult.FAIL_HARD_STOP;\n+\t\t\t\tacceptableRange.contains((float)worstVal) ? PlausibilityResult.PASS : PlausibilityResult.FAIL_HARD_STOP;\n \t\tif (verbose)\n \t\t\tSystem.out.println(getShortName()+\": worst val=\"+worstVal+\"\\tresult=\"+result.name());\n \t\treturn result;\n@@ -58,27 +55,25 @@ public Float getValue(ClusterRupture rupture) {\n \t\t\t\tfalse, false);\n \t}\n \t\n-\tprivate double doTest(List<FaultSubsectionCluster> curClusters, FaultSubsectionCluster nextCluster,\n+\tprivate float doTest(List<FaultSection> curSects, FaultSubsectionCluster nextCluster,\n \t\t\tRuptureTreeNavigator navigator, boolean verbose, boolean shortCircuit) {\n-\t\tdouble val = Double.POSITIVE_INFINITY;\n-\t\tif (!curClusters.isEmpty()) {\n+\t\tfloat val = Float.POSITIVE_INFINITY;\n+\t\tif (!curSects.isEmpty()) {\n \t\t\t// check rupture so far\n-\t\t\tStiffnessResult stiffness = stiffnessCalc.calcAggClustersToClusterStiffness(\n-\t\t\t\t\tStiffnessType.CFF, curClusters, nextCluster);\n-\t\t\tval = stiffness.getValue(aggMethod);\n+\t\t\tval = (float)aggCalc.calc(curSects, nextCluster.subSects);\n \t\t\tif (verbose)\n-\t\t\t\tSystem.out.println(getShortName()+\": \"+curClusters.size()+\" clusters to \"\n+\t\t\t\tSystem.out.println(getShortName()+\": \"+curSects.size()+\" sects to \"\n \t\t\t\t\t\t+nextCluster+\", val=\"+val);\n-\t\t\telse if ((float)val < threshold)\n+\t\t\telse if (!acceptableRange.contains((float)val))\n \t\t\t\treturn val;\n \t\t}\n \t\t\n \t\tif (navigator != null) {\n \t\t\tfor (FaultSubsectionCluster descendant : navigator.getDescendants(nextCluster)) {\n-\t\t\t\tList<FaultSubsectionCluster> newClusters = new ArrayList<>(curClusters);\n-\t\t\t\tnewClusters.add(nextCluster);\n-\t\t\t\tval = Math.min(val, doTest(newClusters, descendant, navigator, verbose, shortCircuit));\n-\t\t\t\tif (!verbose && (float)val < threshold)\n+\t\t\t\tList<FaultSection> newSects = new ArrayList<>(curSects);\n+\t\t\t\tnewSects.addAll(nextCluster.subSects);\n+\t\t\t\tval = getWorseValue((float)val, (float)doTest(newSects, descendant, navigator, verbose, shortCircuit));\n+\t\t\t\tif (!verbose && !acceptableRange.contains((float)val))\n \t\t\t\t\tbreak;\n \t\t\t}\n \t\t}\n@@ -88,14 +83,14 @@ else if ((float)val < threshold)\n \n \t@Override\n \tpublic String getShortName() {\n-\t\tif (threshold == 0f)\n-\t\t\treturn \"JumpClusterCFF0\";\n-\t\treturn \"JumpClusterCFF\"+(float)threshold;\n+\t\tString type = \"[\"+aggCalc.getScalarShortName()+\"]\";\n+\t\treturn \"JumpCluster\"+type+getRangeStr();\n \t}\n \n \t@Override\n \tpublic String getName() {\n-\t\treturn \"Jump Cluster Coulomb   \"+(float)threshold;\n+\t\tString type = \"[\"+aggCalc.getScalarName()+\"]\";\n+\t\treturn \"Jump Cluster \"+type+\" \"+getRangeStr();\n \t}\n \t\n \t@Override\n@@ -105,12 +100,12 @@ public boolean isDirectional(boolean splayed) {\n \n \t@Override\n \tpublic Range<Float> getAcceptableRange() {\n-\t\treturn Range.atLeast((float)threshold);\n+\t\treturn acceptableRange;\n \t}\n \n \t@Override\n-\tpublic SubSectStiffnessCalculator getStiffnessCalc() {\n-\t\treturn stiffnessCalc;\n+\tpublic AggregatedStiffnessCalculator getAggregator() {\n+\t\treturn aggCalc;\n \t}\n \n }",
    "previous_filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/ClusterCoulombCompatibilityFilter.java"
  },
  {
    "sha": "b19813e31630da3f13804de2b5d8ca91c003dd13",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/coulomb/NetClusterCoulombFilter.java",
    "status": "renamed",
    "additions": 16,
    "deletions": 17,
    "changes": 33,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/coulomb/NetClusterCoulombFilter.java",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/coulomb/NetClusterCoulombFilter.java",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/coulomb/NetClusterCoulombFilter.java?ref=541efed3666ce0a47e03a345aada595633351042",
    "patch": "@@ -1,16 +1,14 @@\n-package org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl;\n+package org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.coulomb;\n \n import java.util.ArrayList;\n import java.util.List;\n+import java.util.stream.Collectors;\n \n import org.opensha.sha.earthquake.faultSysSolution.ruptures.ClusterRupture;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.FaultSubsectionCluster;\n-import org.opensha.sha.earthquake.faultSysSolution.ruptures.Jump;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.ScalarCoulombPlausibilityFilter;\n-import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator;\n-import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator.StiffnessAggregationMethod;\n-import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator.StiffnessResult;\n-import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator.StiffnessType;\n+import org.opensha.sha.faultSurface.FaultSection;\n+import org.opensha.sha.simulators.stiffness.AggregatedStiffnessCalculator;\n \n import com.google.common.collect.Range;\n \n@@ -26,15 +24,12 @@\n  */\n public class NetClusterCoulombFilter implements ScalarCoulombPlausibilityFilter {\n \t\n-\tprivate SubSectStiffnessCalculator stiffnessCalc;\n-\tprivate StiffnessAggregationMethod aggMethod;\n+\tprivate AggregatedStiffnessCalculator aggCalc;\n \tprivate float threshold;\n \n-\tpublic NetClusterCoulombFilter(SubSectStiffnessCalculator stiffnessCalc, StiffnessAggregationMethod aggMethod,\n-\t\t\tfloat threshold) {\n+\tpublic NetClusterCoulombFilter(AggregatedStiffnessCalculator aggCalc, float threshold) {\n \t\tsuper();\n-\t\tthis.stiffnessCalc = stiffnessCalc;\n-\t\tthis.aggMethod = aggMethod;\n+\t\tthis.aggCalc = aggCalc;\n \t\tthis.threshold = threshold;\n \t}\n \n@@ -63,11 +58,15 @@ public String getName() {\n \t}\n \t\n \tprivate float getMinValue(List<FaultSubsectionCluster> clusters) {\n+\t\tList<FaultSection> allSects = new ArrayList<>();\n+\t\tfor (FaultSubsectionCluster cluster : clusters)\n+\t\t\tallSects.addAll(cluster.subSects);\n \t\tfloat minVal = Float.POSITIVE_INFINITY;\n \t\tfor (FaultSubsectionCluster cluster : clusters) {\n-\t\t\tStiffnessResult val = stiffnessCalc.calcAggClustersToClusterStiffness(\n-\t\t\t\t\tStiffnessType.CFF, clusters, cluster);\n-\t\t\tminVal = Float.min(minVal, (float)val.getValue(aggMethod));\n+\t\t\t// get sublist of source sects: all sects not on this cluster\n+\t\t\tList<FaultSection> sources = allSects.stream().filter(s -> !cluster.contains(s)).collect(Collectors.toList());\n+\t\t\tdouble val = aggCalc.calc(sources, cluster.subSects);\n+\t\t\tminVal = Float.min(minVal, (float)val);\n \t\t}\n \t\treturn minVal;\n \t}\n@@ -92,8 +91,8 @@ public Float getValue(ClusterRupture rupture) {\n \t}\n \n \t@Override\n-\tpublic SubSectStiffnessCalculator getStiffnessCalc() {\n-\t\treturn stiffnessCalc;\n+\tpublic AggregatedStiffnessCalculator getAggregator() {\n+\t\treturn aggCalc;\n \t}\n \n }",
    "previous_filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/NetClusterCoulombFilter.java"
  },
  {
    "sha": "ecab924fcc50ce6d3ffd3eef700c336d2a8d42ae",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/coulomb/NetRuptureCoulombFilter.java",
    "status": "added",
    "additions": 137,
    "deletions": 0,
    "changes": 137,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/coulomb/NetRuptureCoulombFilter.java",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/coulomb/NetRuptureCoulombFilter.java",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/coulomb/NetRuptureCoulombFilter.java?ref=541efed3666ce0a47e03a345aada595633351042",
    "patch": "@@ -0,0 +1,137 @@\n+package org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.coulomb;\n+\n+import java.util.ArrayList;\n+import java.util.EnumSet;\n+import java.util.List;\n+\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.ClusterRupture;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.FaultSubsectionCluster;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.Jump;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.ScalarCoulombPlausibilityFilter;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.util.FilterDataClusterRupture;\n+import org.opensha.sha.faultSurface.FaultSection;\n+import org.opensha.sha.simulators.stiffness.AggregatedStiffnessCalculator;\n+import org.opensha.sha.simulators.stiffness.AggregatedStiffnessCalculator.AggregationMethod;\n+import org.opensha.sha.simulators.stiffness.AggregatedStiffnessCalculator.StiffnessAggregation;\n+import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.BoundType;\n+import com.google.common.collect.Range;\n+\n+import scratch.UCERF3.inversion.laughTest.PlausibilityResult;\n+\n+/**\n+ * This filter tests the net Coulomb compatibility of a rupture. For each participating section, it computes\n+ * Coulomb with all other sections as a source that section as the receiver.\n+ * \n+ * @author kevin\n+ *\n+ */\n+public class NetRuptureCoulombFilter implements ScalarCoulombPlausibilityFilter {\n+\t\n+\tprivate AggregatedStiffnessCalculator aggCalc;\n+\tprivate Range<Float> acceptableRange;\n+\t\n+\t// can't use the fitler data shortcut for anything where the final aggregation step involves a median\n+\tprivate static EnumSet<AggregationMethod> filterDataAggMethods = EnumSet.complementOf(\n+\t\t\tEnumSet.of(AggregationMethod.GREATER_MEAN_MEDIAN, AggregationMethod.GREATER_SUM_MEDIAN, AggregationMethod.MEDIAN));\n+\n+\tpublic NetRuptureCoulombFilter(AggregatedStiffnessCalculator aggCalc, float threshold) {\n+\t\tthis(aggCalc, Range.atLeast(threshold));\n+\t}\n+\n+\tpublic NetRuptureCoulombFilter(AggregatedStiffnessCalculator aggCalc, Range<Float> acceptableRange) {\n+\t\tsuper();\n+\t\tthis.aggCalc = aggCalc;\n+\t\tPreconditions.checkArgument(acceptableRange.hasLowerBound() || acceptableRange.hasUpperBound());\n+\t\tthis.acceptableRange = acceptableRange;\n+\t}\n+\n+\t@Override\n+\tpublic PlausibilityResult apply(ClusterRupture rupture, boolean verbose) {\n+\t\tif (rupture.getTotalNumSects() == 1)\n+\t\t\treturn PlausibilityResult.PASS;\n+\t\tfloat val = getValue(rupture);\n+\t\tPlausibilityResult result = acceptableRange.contains(val) ?\n+\t\t\t\tPlausibilityResult.PASS : PlausibilityResult.FAIL_HARD_STOP;\n+\t\tif (verbose)\n+\t\t\tSystem.out.println(getShortName()+\": val=\"+val+\", result=\"+result);\n+\t\treturn result;\n+\t}\n+\n+\t@Override\n+\tpublic String getShortName() {\n+\t\tString name = aggCalc.getScalarShortName();\n+\t\treturn name+getRangeStr();\n+\t}\n+\n+\t@Override\n+\tpublic String getName() {\n+\t\tString name = \"Net Rupture [\"+aggCalc.getScalarName()+\"]\";\n+\t\treturn name+getRangeStr();\n+\t}\n+\n+\t@Override\n+\tpublic Float getValue(ClusterRupture rupture) {\n+\t\tif (rupture.getTotalNumSects() == 1)\n+\t\t\treturn null;\n+\t\t\n+\t\t// TODO think about filterdata approach to speed it up\n+//\t\tif (rupture instanceof FilterDataClusterRupture && filterDataAggMethods.contains(aggCalc.getSectsToSectsAggMethod())) {\n+//\t\t\tFilterDataClusterRupture fdRupture = (FilterDataClusterRupture)rupture;\n+//\t\t\tObject data = fdRupture.getFilterData(this);\n+//\t\t\tStiffnessAggregation aggregation = null;\n+//\t\t\tif (data != null && data instanceof FilterData) {\n+//\t\t\t\tFilterData filterData = (FilterData)data;\n+//\t\t\t\tList<FaultSection> newSects = new ArrayList<>();\n+//\t\t\t\tfor (FaultSubsectionCluster cluster : rupture.getClustersIterable()) {\n+//\t\t\t\t\tfor (FaultSection sect : cluster.subSects) {\n+//\t\t\t\t\t\tPreconditions.checkState(rupture.contains(sect));\n+//\t\t\t\t\t\tif (!filterData.prevRupture.contains(sect))\n+//\t\t\t\t\t\t\tnewSects.add(sect);\n+//\t\t\t\t\t}\n+//\t\t\t\t}\n+//\t\t\t\tif (newSects.isEmpty()) {\n+//\t\t\t\t\tPreconditions.checkState(rupture.unique.equals(filterData.prevRupture.unique));\n+//\t\t\t\t\taggregation = filterData.prevAggregation;\n+//\t\t\t\t} else {\n+//\t\t\t\t\tresult = new RuptureCoulombResult(filterData.prevResult, newSects, stiffnessCalc, aggMethod);\n+//\t\t\t\t}\n+//\t\t\t} else {\n+//\t\t\t\tList<FaultSection> allSects = new ArrayList<>();\n+//\t\t\t\tfor (FaultSubsectionCluster cluster : rupture.getClustersIterable())\n+//\t\t\t\t\tallSects.addAll(cluster.subSects);\n+//\t\t\t\taggregation = aggCalc.getSectsToSectsAggregation(allSects, allSects);\n+//\t\t\t}\n+//\t\t\tfdRupture.addFilterData(this, new FilterData(rupture, aggregation));\n+//\t\t\treturn (float)aggregation.get(aggCalc.getSectsToSectsAggMethod());\n+//\t\t}\n+\t\tList<FaultSection> allSects = new ArrayList<>();\n+\t\tfor (FaultSubsectionCluster cluster : rupture.getClustersIterable())\n+\t\t\tallSects.addAll(cluster.subSects);\n+\t\treturn (float)aggCalc.calc(allSects, allSects);\n+\t}\n+\n+\t@Override\n+\tpublic Range<Float> getAcceptableRange() {\n+\t\treturn acceptableRange;\n+\t}\n+\t\n+\tprivate static class FilterData {\n+\t\tprivate final ClusterRupture prevRupture;\n+\t\tprivate final StiffnessAggregation prevAggregation;\n+\t\t\n+\t\tpublic FilterData(ClusterRupture prevRupture,StiffnessAggregation prevAggregation) {\n+\t\t\tsuper();\n+\t\t\tthis.prevRupture = prevRupture;\n+\t\t\tthis.prevAggregation = prevAggregation;\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic AggregatedStiffnessCalculator getAggregator() {\n+\t\treturn aggCalc;\n+\t}\n+\n+}"
  },
  {
    "sha": "d241010c74c697de3c49c6c2da5d1994c00809a9",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/coulomb/ParentCoulombCompatibilityFilter.java",
    "status": "renamed",
    "additions": 34,
    "deletions": 18,
    "changes": 52,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/coulomb/ParentCoulombCompatibilityFilter.java",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/coulomb/ParentCoulombCompatibilityFilter.java",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/coulomb/ParentCoulombCompatibilityFilter.java?ref=541efed3666ce0a47e03a345aada595633351042",
    "patch": "@@ -1,42 +1,47 @@\n-package org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl;\n+package org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.coulomb;\n \n import java.util.HashMap;\n+import java.util.List;\n import java.util.Map;\n+import java.util.stream.Collectors;\n \n import org.opensha.commons.util.IDPairing;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.ClusterRupture;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.Jump;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.JumpPlausibilityFilter;\n import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.ScalarCoulombPlausibilityFilter;\n-import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator;\n-import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator.StiffnessAggregationMethod;\n-import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator.StiffnessResult;\n-import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator.StiffnessType;\n+import org.opensha.sha.faultSurface.FaultSection;\n+import org.opensha.sha.simulators.stiffness.AggregatedStiffnessCalculator;\n \n import com.google.common.collect.Range;\n \n import scratch.UCERF3.inversion.laughTest.PlausibilityResult;\n \n+/**\n+ * Filter that evaluates the Coulomb compatibility of whole parent sections (indpendnet of specific ruptures)\n+ * \n+ * @author kevin\n+ *\n+ */\n public class ParentCoulombCompatibilityFilter extends JumpPlausibilityFilter\n implements ScalarCoulombPlausibilityFilter {\n \t\n-\tprivate SubSectStiffnessCalculator stiffnessCalc;\n-\tprivate StiffnessAggregationMethod aggMethod;\n+\tprivate AggregatedStiffnessCalculator aggCalc;\n \tprivate float threshold;\n \tprivate Directionality directionality;\n \t\n \tprivate transient Map<IDPairing, Boolean> passCache;\n+\tprivate transient Map<Integer, List<FaultSection>> parentSectsMap;\n \t\n \tpublic enum Directionality {\n \t\tEITHER,\n \t\tBOTH,\n \t\tSUM\n \t}\n \n-\tpublic ParentCoulombCompatibilityFilter(SubSectStiffnessCalculator stiffnessCalc,\n-\t\t\tStiffnessAggregationMethod aggMethod, float threshold, Directionality directionality) {\n-\t\tthis.stiffnessCalc = stiffnessCalc;\n-\t\tthis.aggMethod = aggMethod;\n+\tpublic ParentCoulombCompatibilityFilter(AggregatedStiffnessCalculator aggCalc,\n+\t\t\tfloat threshold, Directionality directionality) {\n+\t\tthis.aggCalc = aggCalc;\n \t\tthis.threshold = threshold;\n \t\tthis.directionality = directionality;\n \t}\n@@ -98,9 +103,20 @@ public PlausibilityResult testJump(ClusterRupture rupture, Jump newJump, boolean\n \t\treturn result ? PlausibilityResult.PASS : PlausibilityResult.FAIL_HARD_STOP;\n \t}\n \t\n+\tprivate synchronized List<FaultSection> getSectsForParent(int parentID) {\n+\t\tif (parentSectsMap == null)\n+\t\t\tparentSectsMap = new HashMap<>();\n+\t\tList<FaultSection> sects = parentSectsMap.get(parentID);\n+\t\tif (sects == null) {\n+\t\t\tsects = aggCalc.getCalc().getSubSects().stream().filter(\n+\t\t\t\t\ts -> s.getParentSectionId() == parentID).collect(Collectors.toList());\n+\t\t\tparentSectsMap.put(parentID, sects);\n+\t\t}\n+\t\treturn sects;\n+\t}\n+\t\n \tprivate double calc(int sourceID, int receiverID) {\n-\t\tStiffnessResult stiffness = stiffnessCalc.calcParentStiffness(StiffnessType.CFF, sourceID, receiverID);\n-\t\treturn stiffness.getValue(aggMethod);\n+\t\treturn aggCalc.calc(getSectsForParent(sourceID), getSectsForParent(receiverID));\n \t}\n \n \t@Override\n@@ -150,15 +166,15 @@ private Float getValue(ClusterRupture rupture, Jump newJump) {\n \t\treturn Range.atLeast(threshold);\n \t}\n \n-\t@Override\n-\tpublic SubSectStiffnessCalculator getStiffnessCalc() {\n-\t\treturn stiffnessCalc;\n-\t}\n-\n \t@Override\n \tpublic boolean isDirectional(boolean splayed) {\n \t\t// only directional if splayed\n \t\treturn splayed;\n \t}\n \n+\t@Override\n+\tpublic AggregatedStiffnessCalculator getAggregator() {\n+\t\treturn aggCalc;\n+\t}\n+\n }",
    "previous_filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/ParentCoulombCompatibilityFilter.java"
  },
  {
    "sha": "75dd678b557621b752819502e12101d441c88f41",
    "filename": "src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/coulomb/U3CoulombJunctionFilter.java",
    "status": "added",
    "additions": 255,
    "deletions": 0,
    "changes": 255,
    "blob_url": "https://github.com/GNS-Science/opensha-ucerf3/blob/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/coulomb/U3CoulombJunctionFilter.java",
    "raw_url": "https://github.com/GNS-Science/opensha-ucerf3/raw/541efed3666ce0a47e03a345aada595633351042/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/coulomb/U3CoulombJunctionFilter.java",
    "contents_url": "https://api.github.com/repos/GNS-Science/opensha-ucerf3/contents/src/org/opensha/sha/earthquake/faultSysSolution/ruptures/plausibility/impl/coulomb/U3CoulombJunctionFilter.java?ref=541efed3666ce0a47e03a345aada595633351042",
    "patch": "@@ -0,0 +1,255 @@\n+package org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.impl.coulomb;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+\n+import org.opensha.commons.util.IDPairing;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.ClusterRupture;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.Jump;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.plausibility.PlausibilityFilter;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.strategies.ClusterConnectionStrategy;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.util.RuptureTreeNavigator;\n+import org.opensha.sha.earthquake.faultSysSolution.ruptures.util.SectionDistanceAzimuthCalculator;\n+import org.opensha.sha.faultSurface.FaultSection;\n+import org.opensha.sha.simulators.stiffness.AggregatedStiffnessCalculator;\n+import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator;\n+import org.opensha.sha.simulators.stiffness.AggregatedStiffnessCalculator.AggregationMethod;\n+import org.opensha.sha.simulators.stiffness.SubSectStiffnessCalculator.StiffnessType;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.HashMultimap;\n+import com.google.common.collect.Multimap;\n+import com.google.gson.Gson;\n+import com.google.gson.TypeAdapter;\n+import com.google.gson.stream.JsonReader;\n+import com.google.gson.stream.JsonWriter;\n+\n+import scratch.UCERF3.inversion.coulomb.CoulombRates;\n+import scratch.UCERF3.inversion.coulomb.CoulombRatesRecord;\n+import scratch.UCERF3.inversion.coulomb.CoulombRatesTester;\n+import scratch.UCERF3.inversion.coulomb.CoulombRatesTester.TestType;\n+import scratch.UCERF3.inversion.laughTest.PlausibilityResult;\n+\n+public class U3CoulombJunctionFilter implements PlausibilityFilter {\n+\t\n+\tprivate CoulombRatesTester tester;\n+\tprivate CoulombRates coulombRates;\n+\t\n+\tprivate AggregatedStiffnessCalculator tauCalc;\n+\tprivate AggregatedStiffnessCalculator cffCalc;\n+\tprivate transient ClusterConnectionStrategy connStrat;\n+\n+\tpublic U3CoulombJunctionFilter(CoulombRatesTester tester, CoulombRates coulombRates) {\n+\t\tthis.tester = tester;\n+\t\tthis.coulombRates = coulombRates;\n+\t}\n+\t\n+\t@Override\n+\tpublic PlausibilityResult apply(ClusterRupture rupture, boolean verbose) {\n+\t\tif (rupture.getTotalNumJumps() == 0)\n+\t\t\treturn PlausibilityResult.PASS;\n+\t\t\n+\t\tList<List<IDPairing>> paths = new ArrayList<>();\n+\t\tfindPaths(rupture.getTreeNavigator(), paths, new ArrayList<>(), rupture.clusters[0].startSect);\n+\t\t\n+\t\treturn testPaths(paths, verbose);\n+\t}\n+\t\n+\tprivate void findPaths(RuptureTreeNavigator navigator,\n+\t\t\tList<List<IDPairing>> fullPaths, List<IDPairing> curPath, FaultSection curSect) {\n+\t\tCollection<FaultSection> descendants = navigator.getDescendants(curSect);\n+\t\t\n+\t\twhile (descendants.size() == 1) {\n+\t\t\tFaultSection destSect = descendants.iterator().next();\n+\t\t\tif (curSect.getParentSectionId() != destSect.getParentSectionId())\n+\t\t\t\t// it's a jump\n+\t\t\t\tcurPath.add(new IDPairing(curSect.getSectionId(), destSect.getSectionId()));\n+\t\t\t\n+\t\t\tcurSect = destSect;\n+\t\t\tdescendants = navigator.getDescendants(curSect);\n+\t\t}\n+\t\t\n+\t\tif (descendants.isEmpty()) {\n+\t\t\t// we're at the end of a chain\n+\t\t\tfullPaths.add(curPath);\n+\t\t} else {\n+\t\t\t// we're at a branching point\n+\t\t\tfor (FaultSection destSect : descendants) {\n+\t\t\t\tList<IDPairing> branchPath = new ArrayList<>(curPath);\n+\t\t\t\tif (curSect.getParentSectionId() != destSect.getParentSectionId())\n+\t\t\t\t\t// it's a jump\n+\t\t\t\t\tbranchPath.add(new IDPairing(curSect.getSectionId(), destSect.getSectionId()));\n+\t\t\t\t\n+\t\t\t\tfindPaths(navigator, fullPaths, branchPath, destSect);\n+\t\t\t}\n+\t\t}\n+\t}\n+\t\n+\tprivate PlausibilityResult testPaths(List<List<IDPairing>> paths, boolean verbose) {\n+\t\tif (verbose)\n+\t\t\tSystem.out.println(getShortName()+\": found \"+paths.size()+\" paths\");\n+\t\tPreconditions.checkState(paths.size() >= 0);\n+\t\tfor (List<IDPairing> path : paths) {\n+\t\t\tif (verbose)\n+\t\t\t\tSystem.out.println(getShortName()+\": testing a path with \"+path.size()+\" jumps\");\n+\t\t\tif (path.isEmpty())\n+\t\t\t\tcontinue;\n+\t\t\tList<CoulombRatesRecord> forwardRates = new ArrayList<>();\n+\t\t\tList<CoulombRatesRecord> backwardRates = new ArrayList<>();\n+\t\t\t\n+\t\t\tfor (IDPairing pair : path) {\n+\t\t\t\tCoulombRatesRecord forwardRate = getCoulombRates(pair);\n+\t\t\t\tPreconditions.checkNotNull(forwardRate, \"No coulomb rates for %s\", pair);\n+\t\t\t\tCoulombRatesRecord backwardRate = getCoulombRates(pair.getReversed());\n+\t\t\t\tPreconditions.checkNotNull(backwardRate, \"No coulomb rates for reversed %s\", pair);\n+\t\t\t\tif (verbose) {\n+\t\t\t\t\tSystem.out.println(getShortName()+\": \"+pair.getID1()+\" => \"+pair.getID2());\n+\t\t\t\t\tSystem.out.println(\"\\tForward rate: \"+forwardRate);\n+\t\t\t\t\tSystem.out.println(\"\\tBackward rate: \"+backwardRate);\n+\t\t\t\t}\n+\t\t\t\tforwardRates.add(forwardRate);\n+\t\t\t\tbackwardRates.add(0, backwardRate);\n+\t\t\t}\n+\t\t\t\n+\t\t\tboolean passes = tester.doesRupturePass(forwardRates, backwardRates);\n+\t\t\tif (verbose)\n+\t\t\t\tSystem.out.println(getShortName()+\": test with \"+forwardRates.size()+\" jumps. passes ? \"+passes);\n+\t\t\tif (!passes)\n+\t\t\t\treturn PlausibilityResult.FAIL_HARD_STOP;\n+\t\t}\n+\t\treturn PlausibilityResult.PASS;\n+\t}\n+\t\n+\tpublic void setFallbackCalculator(SubSectStiffnessCalculator calc, ClusterConnectionStrategy connStrat) {\n+\t\tPreconditions.checkState(calc.getGridSpacing() == 1d, \"Should be 1km grid spacing for fallback calculation\");\n+\t\tthis.connStrat = connStrat;\n+\t\tTestType testType = tester.getTestType();\n+\t\tif (testType == TestType.SHEAR_STRESS)\n+\t\t\tthis.tauCalc = new AggregatedStiffnessCalculator(StiffnessType.TAU, calc, false, AggregationMethod.MAX, AggregationMethod.MAX);\n+\t\telse if (testType == TestType.COULOMB_STRESS)\n+\t\t\tthis.cffCalc = new AggregatedStiffnessCalculator(StiffnessType.CFF, calc, false, AggregationMethod.MAX, AggregationMethod.MAX);\n+\t\telse {\n+\t\t\tthis.tauCalc = new AggregatedStiffnessCalculator(StiffnessType.TAU, calc, false, AggregationMethod.MAX, AggregationMethod.MAX);\n+\t\t\tthis.cffCalc = new AggregatedStiffnessCalculator(StiffnessType.CFF, calc, false, AggregationMethod.MAX, AggregationMethod.MAX);\n+\t\t}\n+\t}\n+\t\n+\tprivate CoulombRatesRecord getCoulombRates(IDPairing pair) {\n+\t\tCoulombRatesRecord rates;\n+\t\tsynchronized (this) {\n+\t\t\trates = coulombRates.get(pair);\n+\t\t}\n+\t\tif (rates == null && (cffCalc != null || tauCalc != null)) {\n+\t\t\t// calculate it ourselves\n+\t\t\tPreconditions.checkNotNull(connStrat, \"have stiffness calculator but not connection strategy\");\n+\t\t\tList<? extends FaultSection> subSects = cffCalc.getCalc().getSubSects();\n+\t\t\tdouble ds=0, pds=0, dcff=0, pdcff=0;\n+\t\t\tStiffnessType[] types;\n+\t\t\tswitch (tester.getTestType()) {\n+\t\t\tcase COULOMB_STRESS:\n+\t\t\t\ttypes = new StiffnessType[] { StiffnessType.CFF };\n+\t\t\t\tbreak;\n+\t\t\tcase SHEAR_STRESS:\n+\t\t\t\ttypes = new StiffnessType[] { StiffnessType.TAU };\n+\t\t\t\tbreak;\n+\t\t\tdefault:\n+\t\t\t\ttypes = new StiffnessType[] { StiffnessType.TAU, StiffnessType.CFF };\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t\tfor (StiffnessType type : types) {\n+\t\t\t\tAggregatedStiffnessCalculator stiffnessCalc = type == StiffnessType.CFF ? cffCalc : tauCalc;\n+\t\t\t\tPreconditions.checkNotNull(type, \"Don't have a stiffness calculator for %s\", type);\n+\t\t\t\tFaultSection source = subSects.get(pair.getID1());\n+\t\t\t\tPreconditions.checkState(source.getSectionId() == pair.getID1());\n+\t\t\t\tFaultSection receiver = subSects.get(pair.getID2());\n+\t\t\t\tPreconditions.checkState(receiver.getSectionId() == pair.getID2());\n+\t\t\t\t// stiffness calculator uses 1m displacement and gives results in MPa\n+\t\t\t\t// CoulombRuptureRates assumes 0.1m displacement and bars\n+\t\t\t\t// these two cancel out exactly (multiply by 0.1 to fix displacement, then by 10 to fix units)\n+\t\t\t\tdouble val = Math.max(0, stiffnessCalc.calc(source, receiver));\n+\t\t\t\t// scalar to correct for method differences\n+\t\t\t\t// median ratio of the value calculated previously divided by that calucated from our method\n+\t\t\t\tval *= 1.5945456137624658;\n+\t\t\t\t// now calculate pdcff, first by summing across all DCFF values\n+\t\t\t\tdouble sum = val;\n+\t\t\t\t// check other possible connecting sections\n+\t\t\t\tfor (Jump jump : connStrat.getJumpsFrom(source)) {\n+\t\t\t\t\tif (jump.toCluster.parentSectionID != receiver.getParentSectionId()) {\n+\t\t\t\t\t\t// it's to a different parent section, include it\n+\t\t\t\t\t\tsum += Math.max(0, stiffnessCalc.calc(source, jump.toSection));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\t// check neighbors on the same fault\n+\t\t\t\tif (source.getSectionId() > 0 && source.getParentSectionId() == subSects.get(source.getSectionId()-1).getParentSectionId())\n+\t\t\t\t\t// section before is a target\n+\t\t\t\t\tsum += Math.max(0, stiffnessCalc.calc(source, subSects.get(source.getSectionId()-1)));\n+\t\t\t\tif (source.getSectionId() < subSects.size()-1 && source.getParentSectionId() == subSects.get(source.getSectionId()+1).getParentSectionId())\n+\t\t\t\t\tsum += Math.max(0, stiffnessCalc.calc(source, subSects.get(source.getSectionId()+1)));\n+\t\t\t\tdouble prob = sum > 0 ? val/sum : 0;\n+\t\t\t\tif (type == StiffnessType.CFF) {\n+\t\t\t\t\tdcff = val;\n+\t\t\t\t\tpdcff = prob;\n+\t\t\t\t} else {\n+\t\t\t\t\tds = val;\n+\t\t\t\t\tpds = prob;\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\trates = new CoulombRatesRecord(pair, ds, pds, dcff, pdcff);\n+\t\t\tsynchronized (this) {\n+\t\t\t\tcoulombRates.put(pair, rates);\n+\t\t\t}\n+\t\t}\n+\t\treturn rates;\n+\t}\n+\n+\t@Override\n+\tpublic String getShortName() {\n+\t\treturn \"Coulomb\";\n+\t}\n+\n+\t@Override\n+\tpublic String getName() {\n+\t\treturn \"Coulomb Jump Filter\";\n+\t}\n+\n+\t@Override\n+\tpublic boolean isDirectional(boolean splayed) {\n+\t\t// only directional if splayed (different inversions could take different jumping points)\n+\t\treturn splayed;\n+\t}\n+\n+\t@Override\n+\tpublic TypeAdapter<PlausibilityFilter> getTypeAdapter() {\n+\t\treturn new Adapter();\n+\t}\n+\t\n+\tpublic static class Adapter extends PlausibilityFilterTypeAdapter {\n+\n+\t\tprivate ClusterConnectionStrategy connStrategy;\n+\t\tprivate Gson gson;\n+\n+\t\t@Override\n+\t\tpublic void init(ClusterConnectionStrategy connStrategy,\n+\t\t\t\tSectionDistanceAzimuthCalculator distAzCalc, Gson gson) {\n+\t\t\tthis.connStrategy = connStrategy;\n+\t\t\tthis.gson = gson;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void write(JsonWriter out, PlausibilityFilter value) throws IOException {\n+\t\t\tU3CoulombJunctionFilter filter = (U3CoulombJunctionFilter)value;\n+\t\t\tgson.toJson(filter, filter.getClass(), out);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic PlausibilityFilter read(JsonReader in) throws IOException {\n+\t\t\tU3CoulombJunctionFilter filter = gson.fromJson(in, U3CoulombJunctionFilter.class);\n+\t\t\tfilter.connStrat = connStrategy;\n+\t\t\treturn filter;\n+\t\t}\n+\t\t\n+\t}\n+\n+}"
  }
]
