[
  {
    "sha": "5e1062adc305032e05617c97bfd34db9fb2bd754",
    "filename": "resources/schema.edn",
    "status": "modified",
    "additions": 18,
    "deletions": 12,
    "changes": 30,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/resources/schema.edn",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/resources/schema.edn",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/resources/schema.edn?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -40,18 +40,22 @@\n  {:JWT {:fields {:accessToken {:type (non-null String)}}}\n \n   :ContinuousTreeParserStatus {:description \"Return id and status. Mostly for querying the workers progress but also for kicking off new analysis.\"\n-                               :fields      {:id     {:type (non-null ID)}\n-                                             :status {:type (non-null :ContinuousTreeStatus)}}}\n+                               :fields      {:id       {:type (non-null ID)}\n+                                             :progress {:type (non-null Float)}\n+                                             :status   {:type (non-null :ContinuousTreeStatus)}}}\n \n   :DiscreteTreeParserStatus {:description \"Return id and status. Mostly for querying the workers progress but also for kicking off new analysis.\"\n-                             :fields      {:id     {:type (non-null ID)}\n-                                           :status {:type (non-null :DiscreteTreeStatus)}}}\n+                             :fields      {:id       {:type (non-null ID)}\n+                                           :progress {:type (non-null Float)}\n+                                           :status   {:type (non-null :DiscreteTreeStatus)}}}\n \n-  :TimeSlicerParserStatus {:fields {:id     {:type (non-null ID)}\n-                                    :status {:type (non-null :TimeSlicerStatus)}}}\n+  :TimeSlicerParserStatus {:fields {:id       {:type (non-null ID)}\n+                                    :progress {:type (non-null Float)}\n+                                    :status   {:type (non-null :TimeSlicerStatus)}}}\n \n-  :BayesFactorParserStatus {:fields {:id     {:type (non-null ID)}\n-                                     :status {:type (non-null :BayesFactorStatus)}}}\n+  :BayesFactorParserStatus {:fields {:id       {:type (non-null ID)}\n+                                     :progress {:type (non-null Float)}\n+                                     :status   {:type (non-null :BayesFactorStatus)}}}\n \n   :User {:fields {:id    {:type (non-null ID)}\n                   :email {:type (non-null String)}}}\n@@ -70,6 +74,7 @@\n                                  :timescaleMultiplier      {:type Float}\n                                  :mostRecentSamplingDate   {:type String}\n                                  :status                   {:type :ContinuousTreeStatus}\n+                                 :progress                 {:type Float}\n                                  :attributeNames           {:type    (list String)\n                                                             :resolve :resolve/continuous-tree->attributes}\n                                  :hpdLevels                {:type    (list String)\n@@ -88,6 +93,7 @@\n                                :timescaleMultiplier    {:type Float}\n                                :mostRecentSamplingDate {:type String}\n                                :status                 {:type :DiscreteTreeStatus}\n+                               :progress               {:type Float}\n                                :attributeNames         {:type    (list String)\n                                                         :resolve :resolve/discrete-tree->attributes}\n                                :outputFileUrl          {:type String}}}\n@@ -111,6 +117,7 @@\n                              :timescaleMultiplier                {:type Float}\n                              :mostRecentSamplingDate             {:type String}\n                              :status                             {:type :TimeSlicerStatus}\n+                             :progress                           {:type Float}\n                              :attributeNames                     {:type    (list String)\n                                                                   :resolve :resolve/time-slicer->attributes}\n                              :outputFileUrl                      {:type String}}}\n@@ -130,6 +137,7 @@\n                                       :logFileUrl       {:type (non-null String)}\n                                       :locationsFileUrl {:type String}\n                                       :status           {:type :BayesFactorStatus}\n+                                      :progress         {:type Float}\n                                       :burnIn           {:type Float}\n                                       :outputFileUrl    {:type String}\n                                       :bayesFactors     {:type    (list :BayesFactor)\n@@ -138,9 +146,7 @@\n  :queries\n  {:getAuthorizedUser {:type        :User\n                       :description \"Returns a current user. Does not need ID which is read from the Authorization header\"\n-                      ;; :args {:id {:type (non-null ID)}}\n-                      :resolve     :query/getAuthorizedUser\n-                      }\n+                      :resolve     :query/getAuthorizedUser}\n   :getContinuousTree {:type        :ContinuousTree\n                       :description \"Returns a ContinuousTree entity by id.\"\n                       :args        {:id {:type (non-null ID)}}\n@@ -191,7 +197,7 @@\n \n   :getUploadUrls {:type        (list String)\n                   :description \"Returns one-time POST urls.\"\n-                  :args        {:files {:type (list :File)}}\n+                  :args        {:files {:type (non-null (list :File))}}\n                   :resolve     :mutation/getUploadUrls}\n \n   :uploadContinuousTree {:type        :ContinuousTreeParserStatus"
  },
  {
    "sha": "fe9c58854124feeb79f421b1c96445e0d2c7ffcb",
    "filename": "resources/sql/bayes_factor.sql",
    "status": "modified",
    "additions": 26,
    "deletions": 9,
    "changes": 35,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/resources/sql/bayes_factor.sql",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/resources/sql/bayes_factor.sql",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/resources/sql/bayes_factor.sql?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -7,7 +7,6 @@ log_file_url,\n locations_file_url,\n number_of_locations,\n burn_in,\n-status,\n readable_name\n )\n VALUES (\n@@ -17,7 +16,6 @@ VALUES (\n :locations-file-url,\n :number-of-locations,\n :burn-in,\n-:status,\n :readable-name\n )\n ON DUPLICATE KEY UPDATE\n@@ -26,15 +24,13 @@ log_file_url = IF(:log-file-url IS NOT NULL, :log-file-url, log_file_url),\n locations_file_url = IF(:locations-file-url IS NOT NULL, :locations-file-url, locations_file_url),\n number_of_locations = IF(:number-of-locations IS NOT NULL, :number-of-locations, number_of_locations),\n burn_in = IF(:burn-in IS NOT NULL, :burn-in, burn_in),\n-status = :status,\n readable_name = :readable-name\n \n -- :name update-bayes-factor-analysis :! :n\n -- :doc Updates a continuous tree\n \n UPDATE bayes_factor_analysis\n SET\n-status = :status,\n readable_name = IF(:readable-name IS NOT NULL, :readable-name, readable_name),\n burn_in = IF(:burn-in IS NOT NULL, :burn-in, burn_in),\n number_of_locations = IF(:number-of-locations IS NOT NULL, :number-of-locations, number_of_locations),\n@@ -58,9 +54,13 @@ log_file_url,\n locations_file_url,\n burn_in,\n status,\n+progress,\n output_file_url,\n-readable_name\n+readable_name,\n+status,\n+progress\n FROM bayes_factor_analysis\n+JOIN bayes_factor_analysis_status ON bayes_factor_analysis_status.bayes_factor_analysis_id = bayes_factor_analysis.id\n WHERE id = :id\n \n -- :name insert-bayes-factors :! :n\n@@ -81,12 +81,29 @@ bayes_factors\n FROM bayes_factors\n WHERE bayes_factor_analysis_id = :bayes-factor-analysis-id\n \n+-- :name upsert-status :! :n\n+-- :doc Upsert a continuous tree status\n+\n+INSERT INTO bayes_factor_analysis_status(\n+bayes_factor_analysis_id,\n+status,\n+progress\n+)\n+VALUES (\n+:bayes-factor-analysis-id,\n+:status,\n+:progress\n+)\n+ON DUPLICATE KEY UPDATE\n+status = IF(:status IS NOT NULL, :status, status),\n+progress = IF(:progress IS NOT NULL, :progress, progress)\n \n -- :name get-status :? :1\n -- :doc Get analysis status by id\n \n SELECT\n-id,\n-status\n-FROM bayes_factor\n-WHERE :id = id\n+bayes_factor_analysis_id,\n+status,\n+progress\n+FROM bayes_factor_status\n+WHERE :bayes-factor-analysis-id = bayes_factor_analysis_id"
  },
  {
    "sha": "2526bdd291a38225fdfb2b08c2648d3758d28f96",
    "filename": "resources/sql/continuous_tree.sql",
    "status": "modified",
    "additions": 26,
    "deletions": 10,
    "changes": 36,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/resources/sql/continuous_tree.sql",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/resources/sql/continuous_tree.sql",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/resources/sql/continuous_tree.sql?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -5,28 +5,24 @@ INSERT INTO continuous_tree(\n id,\n user_id,\n tree_file_url,\n-status,\n readable_name\n )\n VALUES (\n :id,\n :user-id,\n :tree-file-url,\n-:status,\n :readable-name\n )\n ON DUPLICATE KEY UPDATE\n user_id = user_id,\n tree_file_url = IF(:tree-file-url IS NOT NULL, :tree-file-url, tree_file_url),\n-status = :status,\n readable_name = :readable-name\n \n -- :name update-tree :! :n\n -- :doc Updates a continuous tree\n \n UPDATE continuous_tree\n SET\n-status = :status,\n readable_name = IF(:readable-name IS NOT NULL, :readable-name, readable_name),\n x_coordinate_attribute_name = IF(:x-coordinate-attribute-name IS NOT NULL, :x-coordinate-attribute-name, x_coordinate_attribute_name),\n y_coordinate_attribute_name = IF(:y-coordinate-attribute-name IS NOT NULL, :y-coordinate-attribute-name, y_coordinate_attribute_name),\n@@ -89,17 +85,37 @@ hpd_level,\n has_external_annotations,\n timescale_multiplier,\n most_recent_sampling_date,\n-status,\n output_file_url,\n-readable_name\n+readable_name,\n+status,\n+progress\n FROM continuous_tree\n+JOIN continuous_tree_status ON continuous_tree_status.tree_id = continuous_tree.id\n WHERE :id = id\n \n+-- :name upsert-status :! :n\n+-- :doc Upsert a continuous tree status\n+\n+INSERT INTO continuous_tree_status(\n+tree_id,\n+status,\n+progress\n+)\n+VALUES (\n+:tree-id,\n+:status,\n+:progress\n+)\n+ON DUPLICATE KEY UPDATE\n+status = IF(:status IS NOT NULL, :status, status),\n+progress = IF(:progress IS NOT NULL, :progress, progress)\n+\n -- :name get-status :? :1\n -- :doc Get analysis status by id\n \n SELECT\n-id,\n-status\n-FROM continuous_tree\n-WHERE :id = id\n+tree_id,\n+status,\n+progress\n+FROM continuous_tree_status\n+WHERE tree_id = :tree-id"
  },
  {
    "sha": "a5284f3212345ff71224c51b68a9081c7eae5c54",
    "filename": "resources/sql/discrete_tree.sql",
    "status": "modified",
    "additions": 26,
    "deletions": 10,
    "changes": 36,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/resources/sql/discrete_tree.sql",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/resources/sql/discrete_tree.sql",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/resources/sql/discrete_tree.sql?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -6,30 +6,26 @@ id,\n user_id,\n tree_file_url,\n locations_file_url,\n-status,\n readable_name\n )\n VALUES (\n :id,\n :user-id,\n :tree-file-url,\n :locations-file-url,\n-:status,\n :readable-name\n )\n ON DUPLICATE KEY UPDATE\n user_id = user_id,\n tree_file_url = IF(:tree-file-url IS NOT NULL, :tree-file-url, tree_file_url),\n locations_file_url = IF(:locations-file-url IS NOT NULL, :locations-file-url, locations_file_url),\n-status = :status,\n readable_name = :readable-name\n \n -- :name update-tree :! :n\n -- :doc Updates a continuous tree\n \n UPDATE discrete_tree\n SET\n-status = :status,\n readable_name = IF(:readable-name IS NOT NULL, :readable-name, readable_name),\n location_attribute_name = IF(:location-attribute-name IS NOT NULL, :location-attribute-name, location_attribute_name),\n timescale_multiplier = IF(:timescale-multiplier IS NOT NULL, :timescale-multiplier, timescale_multiplier),\n@@ -71,17 +67,37 @@ locations_file_url,\n location_attribute_name,\n timescale_multiplier,\n most_recent_sampling_date,\n-status,\n output_file_url,\n-readable_name\n+readable_name,\n+status,\n+progress\n FROM discrete_tree\n+JOIN discrete_tree_status ON discrete_tree_status.tree_id = discrete_tree.id\n WHERE :id = id\n \n+-- :name upsert-status :! :n\n+-- :doc Upsert a continuous tree status\n+\n+INSERT INTO discrete_tree_status(\n+tree_id,\n+status,\n+progress\n+)\n+VALUES (\n+:tree-id,\n+:status,\n+:progress\n+)\n+ON DUPLICATE KEY UPDATE\n+status = IF(:status IS NOT NULL, :status, status),\n+progress = IF(:progress IS NOT NULL, :progress, progress)\n+\n -- :name get-status :? :1\n -- :doc Get analysis status by id\n \n SELECT\n-id,\n-status\n-FROM discrete_tree\n-WHERE :id = id\n+tree_id,\n+status,\n+progress\n+FROM discrete_tree_status\n+WHERE tree_id = :tree-id"
  },
  {
    "sha": "21eb3794314395e90a1b7986c8d451b804b7c90b",
    "filename": "resources/sql/time_slicer.sql",
    "status": "modified",
    "additions": 26,
    "deletions": 10,
    "changes": 36,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/resources/sql/time_slicer.sql",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/resources/sql/time_slicer.sql",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/resources/sql/time_slicer.sql?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -6,30 +6,26 @@ id,\n user_id,\n trees_file_url,\n slice_heights_file_url,\n-status,\n readable_name\n )\n VALUES (\n :id,\n :user-id,\n :trees-file-url,\n :slice-heights-file-url,\n-:status,\n :readable-name\n )\n ON DUPLICATE KEY UPDATE\n user_id = user_id,\n trees_file_url = IF(:trees-file-url IS NOT NULL, :trees-file-url, trees_file_url),\n slice_heights_file_url = IF(:slice-heights-file-url IS NOT NULL, :slice-heights-file-url, slice_heights_file_url),\n-status = :status,\n readable_name = :readable-name\n \n -- :name update-time-slicer :! :n\n -- :doc Updates a time-slicer entity\n \n UPDATE time_slicer\n SET\n-status = :status,\n readable_name = IF(:readable-name IS NOT NULL, :readable-name, readable_name),\n burn_in = IF(:burn-in IS NOT NULL, :burn-in, burn_in),\n number_of_intervals = IF(:number-of-intervals IS NOT NULL, :number-of-intervals, number_of_intervals),\n@@ -51,7 +47,6 @@ id,\n user_id,\n trees_file_url,\n slice_heights_file_url,\n-status,\n readable_name,\n burn_in,\n number_of_intervals,\n@@ -62,8 +57,11 @@ hpd_level,\n timescale_multiplier,\n most_recent_sampling_date,\n output_file_url,\n-trees_count\n+trees_count,\n+status,\n+progress\n FROM time_slicer\n+JOIN time_slicer_status ON time_slicer_status.time_slicer_id = time_slicer.id\n WHERE :id = id\n \n -- :name insert-attribute :! :n\n@@ -89,11 +87,29 @@ DELETE\n FROM time_slicer\n WHERE id = :id\n \n+-- :name upsert-status :! :n\n+-- :doc Upsert a continuous tree status\n+\n+INSERT INTO time_slicer_status(\n+time_slicer_id,\n+status,\n+progress\n+)\n+VALUES (\n+:time-slicer-id,\n+:status,\n+:progress\n+)\n+ON DUPLICATE KEY UPDATE\n+status = IF(:status IS NOT NULL, :status, status),\n+progress = IF(:progress IS NOT NULL, :progress, progress)\n+\n -- :name get-status :? :1\n -- :doc Get analysis status by id\n \n SELECT\n-id,\n-status\n-FROM time_slicer\n-WHERE :id = id\n+time_slicer_id,\n+status,\n+progress\n+FROM time_slicer_status\n+WHERE :time-slicer-id = time_slicer_id"
  },
  {
    "sha": "92286ce77311c1f82ca510c1d4703c576732910a",
    "filename": "services/db-migration/src/main/resources/liquibase/db.changelog-2.0.xml",
    "status": "modified",
    "additions": 33,
    "deletions": 11,
    "changes": 44,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/services/db-migration/src/main/resources/liquibase/db.changelog-2.0.xml",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/services/db-migration/src/main/resources/liquibase/db.changelog-2.0.xml",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/services/db-migration/src/main/resources/liquibase/db.changelog-2.0.xml?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -14,11 +14,8 @@\n       <column name=\"tree_file_url\" type=\"VARCHAR(200)\">\n         <constraints nullable=\"false\" />\n       </column>\n-      <column name=\"status\" type=\"VARCHAR(50)\">\n-        <constraints nullable=\"false\" />\n-      </column>\n-      <column name=\"output_file_url\" type=\"VARCHAR(200)\"/>      \n-      <column name=\"readable_name\" type=\"VARCHAR(200)\"/>      \n+      <column name=\"output_file_url\" type=\"VARCHAR(200)\"/>\n+      <column name=\"readable_name\" type=\"VARCHAR(200)\"/>\n       <column name=\"x_coordinate_attribute_name\" type=\"VARCHAR(50)\"/>\n       <column name=\"y_coordinate_attribute_name\" type=\"VARCHAR(50)\"/>\n       <column name=\"hpd_level\" type=\"VARCHAR(3)\"/>\n@@ -27,7 +24,7 @@\n       <column name=\"most_recent_sampling_date\" type=\"VARCHAR(20)\"/>\n     </createTable>\n   </changeSet>\n-  \n+\n   <changeSet author=\"filip\" id=\"2.0-2\">\n     <createIndex indexName=\"continuous_tree_SLASH_user_id_INDEX\" tableName=\"continuous_tree\">\n       <column name=\"user_id\"/>\n@@ -46,7 +43,7 @@\n                              initiallyDeferred=\"false\"\n                              validate=\"true\"/>\n   </changeSet>\n-  \n+\n   <changeSet author=\"filip\" id=\"2.0-4\">\n     <createTable tableName=\"continuous_tree_attributes\">\n       <column name=\"tree_id\" type=\"VARCHAR(36)\">\n@@ -72,9 +69,9 @@\n     <addUniqueConstraint\n         columnNames=\"tree_id, attribute_name\"\n         constraintName=\"unique_tree_attribute_names\"\n-        tableName=\"continuous_tree_attributes\"/>    \n+        tableName=\"continuous_tree_attributes\"/>\n   </changeSet>\n-  \n+\n   <changeSet author=\"filip\" id=\"2.0-6\">\n     <createTable tableName=\"continuous_tree_hpd_levels\">\n       <column name=\"tree_id\" type=\"VARCHAR(36)\">\n@@ -100,7 +97,32 @@\n     <addUniqueConstraint\n         columnNames=\"tree_id, level\"\n         constraintName=\"unique_tree_hpd_level\"\n-        tableName=\"continuous_tree_hpd_levels\"/>    \n+        tableName=\"continuous_tree_hpd_levels\"/>\n+  </changeSet>\n+\n+  <changeSet author=\"filip\" id=\"2.0-8\">\n+    <createTable tableName=\"continuous_tree_status\">\n+      <column name=\"tree_id\" type=\"VARCHAR(36)\">\n+        <constraints nullable=\"false\" primaryKey=\"true\"/>\n+      </column>\n+      <column name=\"status\" type=\"VARCHAR(50)\">\n+        <constraints nullable=\"false\" />\n+      </column>\n+      <column name=\"progress\" type=\"float\" defaultValueNumeric=\"0\"/>\n+    </createTable>\n   </changeSet>\n-  \n+\n+  <changeSet author=\"filip\" id=\"2.0-9\">\n+    <addForeignKeyConstraint constraintName=\"continuous_tree_status_SLASH_continuous_tree_FK\"\n+                             baseColumnNames=\"tree_id\"\n+                             baseTableName=\"continuous_tree_status\"\n+                             referencedColumnNames=\"id\"\n+                             referencedTableName=\"continuous_tree\"\n+                             onDelete=\"CASCADE\"\n+                             onUpdate=\"RESTRICT\"\n+                             deferrable=\"false\"\n+                             initiallyDeferred=\"false\"\n+                             validate=\"true\"/>\n+  </changeSet>\n+\n </databaseChangeLog>"
  },
  {
    "sha": "3813cf0f5b52a22aca2aaa90ce31f8ac4d68dc72",
    "filename": "services/db-migration/src/main/resources/liquibase/db.changelog-3.0.xml",
    "status": "modified",
    "additions": 33,
    "deletions": 11,
    "changes": 44,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/services/db-migration/src/main/resources/liquibase/db.changelog-3.0.xml",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/services/db-migration/src/main/resources/liquibase/db.changelog-3.0.xml",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/services/db-migration/src/main/resources/liquibase/db.changelog-3.0.xml?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -16,13 +16,10 @@\n       </column>\n       <column name=\"locations_file_url\" type=\"VARCHAR(200)\">\n         <constraints nullable=\"false\" />\n-      </column>      \n-      <column name=\"status\" type=\"VARCHAR(50)\">\n-        <constraints nullable=\"false\" />\n       </column>\n-      <column name=\"output_file_url\" type=\"VARCHAR(200)\"/>      \n-      <column name=\"readable_name\" type=\"VARCHAR(200)\"/>      \n-      <column name=\"location_attribute_name\" type=\"VARCHAR(50)\"/>      \n+      <column name=\"output_file_url\" type=\"VARCHAR(200)\"/>\n+      <column name=\"readable_name\" type=\"VARCHAR(200)\"/>\n+      <column name=\"location_attribute_name\" type=\"VARCHAR(50)\"/>\n       <column name=\"timescale_multiplier\" type=\"float\"/>\n       <column name=\"most_recent_sampling_date\" type=\"VARCHAR(20)\"/>\n     </createTable>\n@@ -33,7 +30,7 @@\n       <column name=\"user_id\"/>\n     </createIndex>\n   </changeSet>\n-  \n+\n   <changeSet author=\"filip\" id=\"3.0-3\">\n     <addForeignKeyConstraint constraintName=\"discrete_tree_SLASH_user_id_FK\"\n                              baseColumnNames=\"user_id\"\n@@ -47,7 +44,7 @@\n                              validate=\"true\"/>\n   </changeSet>\n \n-  <changeSet author=\"filip\" id=\"2.0-4\">\n+  <changeSet author=\"filip\" id=\"3.0-4\">\n     <createTable tableName=\"discrete_tree_attributes\">\n       <column name=\"tree_id\" type=\"VARCHAR(36)\">\n         <constraints nullable=\"false\" primaryKey=\"true\"/>\n@@ -58,7 +55,7 @@\n     </createTable>\n   </changeSet>\n \n-  <changeSet author=\"filip\" id=\"2.0-5\">\n+  <changeSet author=\"filip\" id=\"3.0-5\">\n     <addForeignKeyConstraint constraintName=\"discrete_tree_attributes_SLASH_discrete_tree_FK\"\n                              baseColumnNames=\"tree_id\"\n                              baseTableName=\"discrete_tree_attributes\"\n@@ -72,7 +69,32 @@\n     <addUniqueConstraint\n         columnNames=\"tree_id, attribute_name\"\n         constraintName=\"unique_tree_attribute_names\"\n-        tableName=\"discrete_tree_attributes\"/>    \n+        tableName=\"discrete_tree_attributes\"/>\n+  </changeSet>\n+\n+  <changeSet author=\"filip\" id=\"3.0-6\">\n+    <createTable tableName=\"discrete_tree_status\">\n+      <column name=\"tree_id\" type=\"VARCHAR(36)\">\n+        <constraints nullable=\"false\" primaryKey=\"true\"/>\n+      </column>\n+      <column name=\"status\" type=\"VARCHAR(50)\">\n+        <constraints nullable=\"false\" />\n+      </column>\n+      <column name=\"progress\" type=\"float\" defaultValueNumeric=\"0\"/>\n+    </createTable>\n   </changeSet>\n-  \n+\n+  <changeSet author=\"filip\" id=\"3.0-7\">\n+    <addForeignKeyConstraint constraintName=\"discrete_tree_status_SLASH_discrete_tree_FK\"\n+                             baseColumnNames=\"tree_id\"\n+                             baseTableName=\"discrete_tree_status\"\n+                             referencedColumnNames=\"id\"\n+                             referencedTableName=\"discrete_tree\"\n+                             onDelete=\"CASCADE\"\n+                             onUpdate=\"RESTRICT\"\n+                             deferrable=\"false\"\n+                             initiallyDeferred=\"false\"\n+                             validate=\"true\"/>\n+  </changeSet>\n+\n </databaseChangeLog>"
  },
  {
    "sha": "c7e0b89a2e4cb0dbdc9ab950a40fa0bbc9b31131",
    "filename": "services/db-migration/src/main/resources/liquibase/db.changelog-4.0.xml",
    "status": "modified",
    "additions": 27,
    "deletions": 5,
    "changes": 32,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/services/db-migration/src/main/resources/liquibase/db.changelog-4.0.xml",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/services/db-migration/src/main/resources/liquibase/db.changelog-4.0.xml",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/services/db-migration/src/main/resources/liquibase/db.changelog-4.0.xml?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -15,11 +15,8 @@\n         <constraints nullable=\"false\" />\n       </column>\n       <column name=\"slice_heights_file_url\" type=\"VARCHAR(200)\"/>\n-      <column name=\"status\" type=\"VARCHAR(50)\">\n-        <constraints nullable=\"false\" />\n-      </column>\n       <column name=\"burn_in\" type=\"INT\"/>\n-      <column name=\"number_of_intervals\" type=\"INT\"/>      \n+      <column name=\"number_of_intervals\" type=\"INT\"/>\n       <column name=\"output_file_url\" type=\"VARCHAR(200)\"/>\n       <column name=\"readable_name\" type=\"VARCHAR(200)\"/>\n       <column name=\"trait_attribute_name\" type=\"VARCHAR(50)\"/>\n@@ -62,7 +59,7 @@\n     </createTable>\n   </changeSet>\n \n-  <changeSet author=\"filip\" id=\"2.0-5\">\n+  <changeSet author=\"filip\" id=\"4.0-5\">\n     <addForeignKeyConstraint constraintName=\"time_slicer_attributes_SLASH_timeslicer_FK\"\n                              baseColumnNames=\"time_slicer_id\"\n                              baseTableName=\"time_slicer_attributes\"\n@@ -79,4 +76,29 @@\n         tableName=\"time_slicer_attributes\"/>\n   </changeSet>\n \n+  <changeSet author=\"filip\" id=\"4.0-6\">\n+    <createTable tableName=\"time_slicer_status\">\n+      <column name=\"time_slicer_id\" type=\"VARCHAR(36)\">\n+        <constraints nullable=\"false\" primaryKey=\"true\"/>\n+      </column>\n+      <column name=\"status\" type=\"VARCHAR(50)\">\n+        <constraints nullable=\"false\"/>\n+      </column>\n+      <column name=\"progress\" type=\"float\" defaultValueNumeric=\"0\"/>\n+    </createTable>\n+  </changeSet>\n+\n+  <changeSet author=\"filip\" id=\"4.0-7\">\n+    <addForeignKeyConstraint constraintName=\"time_slicer_status_SLASH_time_slicer_FK\"\n+                             baseColumnNames=\"time_slicer_id\"\n+                             baseTableName=\"time_slicer_status\"\n+                             referencedColumnNames=\"id\"\n+                             referencedTableName=\"time_slicer\"\n+                             onDelete=\"CASCADE\"\n+                             onUpdate=\"RESTRICT\"\n+                             deferrable=\"false\"\n+                             initiallyDeferred=\"false\"\n+                             validate=\"true\"/>\n+  </changeSet>\n+\n </databaseChangeLog>"
  },
  {
    "sha": "9603adb50799dd062baada3e6888a30e9634f000",
    "filename": "services/db-migration/src/main/resources/liquibase/db.changelog-5.0.xml",
    "status": "modified",
    "additions": 31,
    "deletions": 11,
    "changes": 42,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/services/db-migration/src/main/resources/liquibase/db.changelog-5.0.xml",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/services/db-migration/src/main/resources/liquibase/db.changelog-5.0.xml",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/services/db-migration/src/main/resources/liquibase/db.changelog-5.0.xml?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -3,7 +3,7 @@\n \n   <property name=\"RealBooleanType\" value=\"TINYINT(1)\" dbms=\"mysql\"/>\n \n-  <changeSet author=\"filip\" id=\"3.0-1\">\n+  <changeSet author=\"filip\" id=\"5.0-1\">\n     <createTable tableName=\"bayes_factor_analysis\">\n       <column name=\"id\" type=\"VARCHAR(36)\">\n         <constraints nullable=\"false\" primaryKey=\"true\"/>\n@@ -15,25 +15,20 @@\n         <constraints nullable=\"false\" />\n       </column>\n       <column name=\"locations_file_url\" type=\"VARCHAR(200)\"/>\n-      <!--   <constraints nullable=\"true\" /> -->\n-      <!-- </column> -->\n-      <column name=\"status\" type=\"VARCHAR(50)\">\n-        <constraints nullable=\"false\" />\n-      </column>\n       <column name=\"output_file_url\" type=\"VARCHAR(200)\"/>\n       <column name=\"readable_name\" type=\"VARCHAR(200)\"/>\n       <column name=\"burn_in\" type=\"FLOAT\"/>\n-      <column name=\"number_of_locations\" type=\"INT\"/>      \n+      <column name=\"number_of_locations\" type=\"INT\"/>\n     </createTable>\n   </changeSet>\n \n-  <changeSet author=\"filip\" id=\"3.0-2\">\n+  <changeSet author=\"filip\" id=\"5.0-2\">\n     <createIndex indexName=\"bayes_factor_SLASH_user_id_INDEX\" tableName=\"bayes_factor_analysis\">\n       <column name=\"user_id\"/>\n     </createIndex>\n   </changeSet>\n \n-  <changeSet author=\"filip\" id=\"3.0-3\">\n+  <changeSet author=\"filip\" id=\"5.0-3\">\n     <addForeignKeyConstraint constraintName=\"bayes_factor_analysis_SLASH_user_id_FK\"\n                              baseColumnNames=\"user_id\"\n                              baseTableName=\"bayes_factor_analysis\"\n@@ -46,7 +41,7 @@\n                              validate=\"true\"/>\n   </changeSet>\n \n-  <changeSet author=\"filip\" id=\"3.0-4\">\n+  <changeSet author=\"filip\" id=\"5.0-4\">\n     <createTable tableName=\"bayes_factors\">\n       <column name=\"bayes_factor_analysis_id\" type=\"VARCHAR(36)\">\n         <constraints nullable=\"false\" primaryKey=\"true\"/>\n@@ -57,7 +52,7 @@\n     </createTable>\n   </changeSet>\n \n-  <changeSet author=\"filip\" id=\"2.0-5\">\n+  <changeSet author=\"filip\" id=\"5.0-5\">\n     <addForeignKeyConstraint constraintName=\"bayes_factors_SLASH_bayes_factor_analysis_FK\"\n                              baseColumnNames=\"bayes_factor_analysis_id\"\n                              baseTableName=\"bayes_factors\"\n@@ -70,4 +65,29 @@\n                              validate=\"true\"/>\n   </changeSet>\n \n+  <changeSet author=\"filip\" id=\"5.0-6\">\n+    <createTable tableName=\"bayes_factor_analysis_status\">\n+      <column name=\"bayes_factor_analysis_id\" type=\"VARCHAR(36)\">\n+        <constraints nullable=\"false\" primaryKey=\"true\"/>\n+      </column>\n+      <column name=\"status\" type=\"VARCHAR(50)\">\n+        <constraints nullable=\"false\" />\n+      </column>\n+      <column name=\"progress\" type=\"float\" defaultValueNumeric=\"0\"/>\n+    </createTable>\n+  </changeSet>\n+\n+  <changeSet author=\"filip\" id=\"5.0-7\">\n+    <addForeignKeyConstraint constraintName=\"bayes_factor_analysis_status_SLASH_bayes_factor_analysis_FK\"\n+                             baseColumnNames=\"bayes_factor_analysis_id\"\n+                             baseTableName=\"bayes_factor_analysis_status\"\n+                             referencedColumnNames=\"id\"\n+                             referencedTableName=\"bayes_factor_analysis\"\n+                             onDelete=\"CASCADE\"\n+                             onUpdate=\"RESTRICT\"\n+                             deferrable=\"false\"\n+                             initiallyDeferred=\"false\"\n+                             validate=\"true\"/>\n+  </changeSet>\n+\n </databaseChangeLog>"
  },
  {
    "sha": "e2db79c4b8e6bfeb7ec442495f44ebcff2d0e598",
    "filename": "src/clj/api/models/bayes_factor.clj",
    "status": "modified",
    "additions": 10,
    "deletions": 7,
    "changes": 17,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/src/clj/api/models/bayes_factor.clj",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/src/clj/api/models/bayes_factor.clj",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/src/clj/api/models/bayes_factor.clj?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -9,6 +9,7 @@\n (declare delete-bayes-factor-analysis)\n (declare insert-bayes-factors)\n (declare get-bayes-factors)\n+(declare upsert-status)\n (declare get-status)\n \n (hugsql/def-db-fns \"sql/bayes_factor.sql\")\n@@ -21,21 +22,23 @@\n    :log-file-url        nil\n    :locations-file-url  nil\n    :number-of-locations nil\n-   :status              nil\n    :readable-name       nil\n    :burn-in             nil\n    :output-file-url     nil})\n \n (defn upsert! [db analysis]\n-  (let [analysis (->> analysis\n-                      (merge nil-bayes-factor-analysis)\n-                      (#(update % :status name)))]\n+  (let [analysis (merge nil-bayes-factor-analysis analysis)]\n     (log/debug \"upsert!\" analysis)\n     (upsert-bayes-factor-analysis db analysis)))\n \n (defn update! [db analysis]\n-  (let [analysis (->> analysis\n-                      (merge nil-bayes-factor-analysis)\n-                      (#(update % :status name)))]\n+  (let [analysis (merge nil-bayes-factor-analysis analysis)]\n     (log/debug \"update!\" analysis)\n     (update-bayes-factor-analysis db analysis)))\n+\n+(defn upsert-status! [db status]\n+  (let [status (->> status\n+                    (merge {:bayes-factor-analysis-id nil :status nil :progress nil})\n+                    (#(update % :status name)))]\n+    (log/debug \"upsert-status!\" status)\n+    (upsert-status db status)))"
  },
  {
    "sha": "2daefce638ad087e6983f371637f13abf9f6ddcc",
    "filename": "src/clj/api/models/continuous_tree.clj",
    "status": "modified",
    "additions": 19,
    "deletions": 16,
    "changes": 35,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/src/clj/api/models/continuous_tree.clj",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/src/clj/api/models/continuous_tree.clj",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/src/clj/api/models/continuous_tree.clj?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -11,37 +11,33 @@\n (declare delete-tree)\n (declare get-attributes)\n (declare get-hpd-levels)\n+(declare upsert-status)\n (declare get-status)\n \n (hugsql/def-db-fns \"sql/continuous_tree.sql\")\n (hugsql/def-sqlvec-fns \"sql/continuous_tree.sql\")\n \n ;; TODO: remove this when we figure out https://github.com/layerware/hugsql/issues/116\n (def ^:private nil-tree\n-  {:id nil\n-   :user-id nil\n-   :tree-file-url nil\n-   :status nil\n-   :readable-name nil\n+  {:id                          nil\n+   :user-id                     nil\n+   :tree-file-url               nil\n+   :readable-name               nil\n    :x-coordinate-attribute-name nil\n    :y-coordinate-attribute-name nil\n-   :hpd-level nil\n-   :has-external-annotations nil\n-   :timescale-multiplier nil\n-   :most-recent-sampling-date nil\n-   :output-file-url nil})\n+   :hpd-level                   nil\n+   :has-external-annotations    nil\n+   :timescale-multiplier        nil\n+   :most-recent-sampling-date   nil\n+   :output-file-url             nil})\n \n (defn upsert! [db tree]\n-  (let [tree (->> tree\n-                  (merge nil-tree)\n-                  (#(update % :status name)))]\n+  (let [tree (merge nil-tree tree)]\n     (log/debug \"upsert-tree!\" tree)\n     (upsert-tree db tree)))\n \n (defn update! [db tree]\n-  (let [tree (->> tree\n-                  (merge nil-tree)\n-                  (#(update % :status name)))]\n+  (let [tree (merge nil-tree tree)]\n     (log/debug \"update-tree!\" tree)\n     (update-tree db tree)))\n \n@@ -52,3 +48,10 @@\n (defn insert-hpd-levels! [db tree-id levels]\n   (doseq [lvl levels ]\n     (insert-hpd-level db {:tree-id tree-id :level lvl})))\n+\n+(defn upsert-status! [db status]\n+  (let [status (->> status\n+                    (merge {:tree-id nil :status nil :progress nil})\n+                    (#(update % :status name)))]\n+    (log/debug \"upsert-status!\" status)\n+    (upsert-status db status)))"
  },
  {
    "sha": "d152c2b1b3e34d125e698175eed53121da193de9",
    "filename": "src/clj/api/models/discrete_tree.clj",
    "status": "modified",
    "additions": 18,
    "deletions": 15,
    "changes": 33,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/src/clj/api/models/discrete_tree.clj",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/src/clj/api/models/discrete_tree.clj",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/src/clj/api/models/discrete_tree.clj?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -9,38 +9,41 @@\n (declare get-tree)\n (declare delete-tree)\n (declare get-attributes)\n+(declare upsert-status)\n (declare get-status)\n \n (hugsql/def-db-fns \"sql/discrete_tree.sql\")\n (hugsql/def-sqlvec-fns \"sql/discrete_tree.sql\")\n \n ;; TODO: remove this when we figure out https://github.com/layerware/hugsql/issues/116\n (def ^:private nil-tree\n-  {:id nil\n-   :user-id nil\n-   :tree-file-url nil\n-   :locations-file-url nil\n-   :status nil\n-   :readable-name nil\n-   :location-attribute-name nil\n-   :timescale-multiplier nil\n+  {:id                        nil\n+   :user-id                   nil\n+   :tree-file-url             nil\n+   :locations-file-url        nil\n+   :readable-name             nil\n+   :location-attribute-name   nil\n+   :timescale-multiplier      nil\n    :most-recent-sampling-date nil\n-   :output-file-url nil})\n+   :output-file-url           nil})\n \n (defn upsert! [db tree]\n-  (let [tree (->> tree\n-                  (merge nil-tree)\n-                  (#(update % :status name)))]\n+  (let [tree (merge nil-tree tree)]\n     (log/debug \"upsert!\" tree)\n     (upsert-tree db tree)))\n \n (defn update! [db tree]\n-  (let [tree (->> tree\n-                  (merge nil-tree)\n-                  (#(update % :status name)))]\n+  (let [tree (merge nil-tree tree)]\n     (log/debug \"update!\" tree)\n     (update-tree db tree)))\n \n (defn insert-attributes! [db tree-id attributes]\n   (doseq [att attributes]\n     (insert-attribute db {:tree-id tree-id :attribute-name att})))\n+\n+(defn upsert-status! [db status]\n+  (let [status (->> status\n+                    (merge {:tree-id nil :status nil :progress nil})\n+                    (#(update % :status name)))]\n+    (log/debug \"upsert-status!\" status)\n+    (upsert-status db status)))"
  },
  {
    "sha": "95dd0ebd93cfba7a741a70ad72d93ca6e70a552e",
    "filename": "src/clj/api/models/time_slicer.clj",
    "status": "modified",
    "additions": 24,
    "deletions": 20,
    "changes": 44,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/src/clj/api/models/time_slicer.clj",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/src/clj/api/models/time_slicer.clj",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/src/clj/api/models/time_slicer.clj?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -8,45 +8,49 @@\n (declare update-time-slicer)\n (declare insert-attribute)\n (declare get-attributes)\n+(declare upsert-status)\n (declare get-status)\n \n (hugsql/def-db-fns \"sql/time_slicer.sql\")\n (hugsql/def-sqlvec-fns \"sql/time_slicer.sql\")\n \n ;; TODO: remove this when we figure out https://github.com/layerware/hugsql/issues/116\n (def ^:private nil-time-slicer\n-  {:id nil\n-   :user-id nil\n-   :trees-file-url nil\n-   :slice-heights-file-url nil\n-   :status nil\n-   :readable-name nil\n-   :burn-in nil\n-   :number-of-intervals nil\n+  {:id                                      nil\n+   :user-id                                 nil\n+   :trees-file-url                          nil\n+   :slice-heights-file-url                  nil\n+   :readable-name                           nil\n+   :burn-in                                 nil\n+   :number-of-intervals                     nil\n    :relaxed-random-walk-rate-attribute-name nil\n-   :trait-attribute-name nil\n-   :hpd-level nil\n-   :contouring-grid-size nil\n-   :timescale-multiplier nil\n-   :most-recent-sampling-date nil\n-   :output-file-url nil\n-   :trees-count nil\n-   })\n+   :trait-attribute-name                    nil\n+   :hpd-level                               nil\n+   :contouring-grid-size                    nil\n+   :timescale-multiplier                    nil\n+   :most-recent-sampling-date               nil\n+   :output-file-url                         nil\n+   :trees-count                             nil})\n \n (defn upsert! [db time-slicer]\n   (let [time-slicer (->> time-slicer\n-                         (merge nil-time-slicer)\n-                         (#(update % :status name)))]\n+                         (merge nil-time-slicer))]\n     (log/debug \"upsert-time-slicer!\" time-slicer)\n     (upsert-time-slicer db time-slicer)))\n \n (defn update! [db time-slicer]\n   (let [time-slicer (->> time-slicer\n-                         (merge nil-time-slicer)\n-                         (#(update % :status name)))]\n+                         (merge nil-time-slicer))]\n     (log/debug \"update-time-slicer!\" time-slicer)\n     (update-time-slicer db time-slicer)))\n \n (defn insert-attributes! [db time-slicer-id attributes]\n   (doseq [att attributes]\n     (insert-attribute db {:time-slicer-id time-slicer-id :attribute-name att})))\n+\n+(defn upsert-status! [db status]\n+  (let [status (->> status\n+                    (merge {:time-slicer-id nil :status nil :progress nil})\n+                    (#(update % :status name)))]\n+    (log/debug \"upsert-status!\" status)\n+    (upsert-status db status)))"
  },
  {
    "sha": "c0577267d67c716f36e51fbb0e3b63a704a12988",
    "filename": "src/clj/api/mutations.clj",
    "status": "modified",
    "additions": 90,
    "deletions": 76,
    "changes": 166,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/src/clj/api/mutations.clj",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/src/clj/api/mutations.clj",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/src/clj/api/mutations.clj?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -58,15 +58,16 @@\n                                :as           args} _]\n   (log/info \"upload-continuous-tree\" {:user/id authed-user-id\n                                       :args    args})\n-  (let [id              (s3-url->id tree-file-url authed-user-id)\n-        status          :TREE_UPLOADED\n-        continuous-tree {:id            id\n-                         :readable-name readable-name\n-                         :user-id       authed-user-id\n-                         :tree-file-url tree-file-url\n-                         :status        status}]\n+  (let [id     (s3-url->id tree-file-url authed-user-id)\n+        status :TREE_UPLOADED]\n     (try\n-      (continuous-tree-model/upsert! db continuous-tree)\n+      ;; TODO : in a transaction\n+      (continuous-tree-model/upsert! db {:id            id\n+                                         :readable-name readable-name\n+                                         :user-id       authed-user-id\n+                                         :tree-file-url tree-file-url})\n+      (continuous-tree-model/upsert-status! db {:tree-id id\n+                                                :status  status})\n       ;; sends message to worker to parse hpd levels and attributes\n       (aws-sqs/send-message sqs workers-queue-url {:message/type :continuous-tree-upload\n                                                    :id           id\n@@ -75,8 +76,8 @@\n        :status status}\n       (catch Exception e\n         (log/error \"Exception occured\" {:error e})\n-        (continuous-tree-model/update! db {:id     id\n-                                           :status :ERROR})))))\n+        (continuous-tree-model/upsert-status! db {:tree-id id\n+                                                  :status  :ERROR})))))\n \n (defn update-continuous-tree\n   [{:keys [authed-user-id db]} {id                          :id\n@@ -94,21 +95,23 @@\n                                       :args    args})\n   (try\n     (let [status :PARSER_ARGUMENTS_SET]\n+      ;; TODO : in a transaction\n       (continuous-tree-model/update! db {:id                          id\n                                          :readable-name               readable-name\n                                          :x-coordinate-attribute-name x-coordinate-attribute-name\n                                          :y-coordinate-attribute-name y-coordinate-attribute-name\n                                          :hpd-level                   hpd-level\n                                          :has-external-annotations    has-external-annotations\n                                          :timescale-multiplier        timescale-multiplier\n-                                         :most-recent-sampling-date   most-recent-sampling-date\n-                                         :status                      status})\n+                                         :most-recent-sampling-date   most-recent-sampling-date})\n+      (continuous-tree-model/upsert-status! db {:tree-id id\n+                                                :status  status})\n       {:id     id\n        :status status})\n     (catch Exception e\n       (log/error \"Exception occured\" {:error e})\n-      (continuous-tree-model/update! db {:id     id\n-                                         :status :ERROR}))))\n+      (continuous-tree-model/upsert-status! db {:tree-id id\n+                                                :status  :ERROR}))))\n \n (defn start-continuous-tree-parser\n   [{:keys [db sqs workers-queue-url]} {id :id :as args} _]\n@@ -117,13 +120,14 @@\n     (try\n       (aws-sqs/send-message sqs workers-queue-url {:message/type :parse-continuous-tree\n                                                    :id           id})\n-      (continuous-tree-model/update! db {:id id :status status})\n+      (continuous-tree-model/upsert-status! db {:tree-id id\n+                                                :status  status})\n       {:id     id\n        :status status}\n       (catch Exception e\n         (log/error \"Exception when sending message to worker\" {:error e})\n-        (continuous-tree-model/update! db {:id     id\n-                                           :status :ERROR})))))\n+        (continuous-tree-model/upsert-status! db {:tree-id id\n+                                                  :status  :ERROR})))))\n \n (defn upload-discrete-tree [{:keys [sqs workers-queue-url authed-user-id db]}\n                             {tree-file-url      :treeFileUrl\n@@ -132,18 +136,19 @@\n                              :as                args} _]\n   (log/info \"upload-discrete-tree\" {:user/id authed-user-id\n                                     :args    args})\n-  (let [id            (s3-url->id tree-file-url authed-user-id)\n+  (let [id     (s3-url->id tree-file-url authed-user-id)\n         ;; NOTE: uploads mutation generates different ids for each uploaded file\n         ;; _ (assert (= id (s3-url->id locations-file-url bucket-name authed-user-id)))\n-        status        :TREE_AND_LOCATIONS_UPLOADED\n-        discrete-tree {:id                 id\n-                       :readable-name      readable-name\n-                       :user-id            authed-user-id\n-                       :tree-file-url      tree-file-url\n-                       :locations-file-url locations-file-url\n-                       :status             status}]\n+        status :TREE_AND_LOCATIONS_UPLOADED]\n     (try\n-      (discrete-tree-model/upsert! db discrete-tree)\n+      ;; TODO : in a transaction\n+      (discrete-tree-model/upsert! db {:id                 id\n+                                       :readable-name      readable-name\n+                                       :user-id            authed-user-id\n+                                       :tree-file-url      tree-file-url\n+                                       :locations-file-url locations-file-url})\n+      (discrete-tree-model/upsert-status! db {:tree-id id\n+                                              :status  status})\n       ;; sends message to worker to parse attributes\n       (aws-sqs/send-message sqs workers-queue-url {:message/type :discrete-tree-upload\n                                                    :id           id\n@@ -152,8 +157,8 @@\n        :status status}\n       (catch Exception e\n         (log/error \"Exception occured\" {:error e})\n-        (discrete-tree-model/update! db {:id     id\n-                                         :status :ERROR})))))\n+        (discrete-tree-model/upsert-status! db {:tree-id id\n+                                                :status  :ERROR})))))\n \n (defn update-discrete-tree\n   [{:keys [authed-user-id db]} {id                        :id\n@@ -171,14 +176,15 @@\n                                        :readable-name             readable-name\n                                        :location-attribute-name   location-attribute-name\n                                        :timescale-multiplier      timescale-multiplier\n-                                       :most-recent-sampling-date most-recent-sampling-date\n-                                       :status                    status})\n+                                       :most-recent-sampling-date most-recent-sampling-date})\n+      (discrete-tree-model/upsert-status! db {:tree-id id\n+                                              :status  status})\n       {:id     id\n        :status status})\n     (catch Exception e\n       (log/error \"Exception occured\" {:error e})\n-      (discrete-tree-model/update! db {:id     id\n-                                       :status :ERROR}))))\n+      (discrete-tree-model/upsert-status! db {:tree-id id\n+                                              :status  :ERROR}))))\n \n (defn start-discrete-tree-parser\n   [{:keys [db sqs workers-queue-url]} {id :id :as args} _]\n@@ -187,30 +193,32 @@\n     (try\n       (aws-sqs/send-message sqs workers-queue-url {:message/type :parse-discrete-tree\n                                                    :id           id})\n-      (discrete-tree-model/update! db {:id id :status status})\n+      (discrete-tree-model/upsert-status! db {:tree-id id :status status})\n       {:id     id\n        :status status}\n       (catch Exception e\n         (log/error \"Exception when sending message to worker\" {:error e})\n-        (discrete-tree-model/update! db {:id     id\n-                                         :status :ERROR})))))\n+        (discrete-tree-model/upsert-status! db {:tree-id id\n+                                                :status  :ERROR})))))\n \n (defn upload-time-slicer [{:keys [sqs workers-queue-url authed-user-id db]}\n-                          {trees-file-url         :treesFileUrl readable-name :readableName\n+                          {trees-file-url         :treesFileUrl\n+                           readable-name :readableName\n                            slice-heights-file-url :sliceHeightsFileUrl\n                            :as                    args} _]\n   (log/info \"upload-time-slicer\" {:user/id authed-user-id\n                                   :args    args})\n-  (let [id          (s3-url->id trees-file-url authed-user-id)\n-        status      :TREES_UPLOADED\n-        time-slicer {:id                     id\n-                     :readable-name          readable-name\n-                     :user-id                authed-user-id\n-                     :trees-file-url         trees-file-url\n-                     :slice-heights-file-url slice-heights-file-url\n-                     :status                 status}]\n+  (let [id     (s3-url->id trees-file-url authed-user-id)\n+        status :TREES_UPLOADED]\n     (try\n-      (time-slicer-model/upsert! db time-slicer)\n+      ;; TODO : in a transaction\n+      (time-slicer-model/upsert! db {:id                     id\n+                                     :readable-name          readable-name\n+                                     :user-id                authed-user-id\n+                                     :trees-file-url         trees-file-url\n+                                     :slice-heights-file-url slice-heights-file-url})\n+      (time-slicer-model/upsert-status! db {:time-slicer-id id\n+                                            :status         status})\n       ;; sends message to the worker to parse attributes\n       (aws-sqs/send-message sqs workers-queue-url {:message/type :time-slicer-upload\n                                                    :id           id\n@@ -219,8 +227,8 @@\n        :status status}\n       (catch Exception e\n         (log/error \"Exception occured\" {:error e})\n-        (time-slicer-model/update! db {:id     id\n-                                       :status :ERROR})))))\n+        (time-slicer-model/upsert-status! db {:time-slicer-id id\n+                                              :status         :ERROR})))))\n \n (defn update-time-slicer\n   [{:keys [authed-user-id db]} {id                                      :id\n@@ -241,6 +249,7 @@\n                                   :args    args})\n   (try\n     (let [status :PARSER_ARGUMENTS_SET]\n+      ;; TODO : in a transaction\n       (time-slicer-model/update! db {:id                                      id\n                                      :readable-name                           readable-name\n                                      :burn-in                                 burn-in\n@@ -250,14 +259,15 @@\n                                      :contouring-grid-size                    contouring-grid-size\n                                      :hpd-level                               hpd-level\n                                      :timescale-multiplier                    timescale-multiplier\n-                                     :most-recent-sampling-date               most-recent-sampling-date\n-                                     :status                                  status})\n+                                     :most-recent-sampling-date               most-recent-sampling-date})\n+      (time-slicer-model/upsert-status! db {:time-slicer-id id\n+                                            :status         status})\n       {:id     id\n        :status status})\n     (catch Exception e\n       (log/error \"Exception occured\" {:error e})\n-      (time-slicer-model/update! db {:id     id\n-                                     :status :ERROR}))))\n+      (time-slicer-model/upsert-status! db {:time-slicer-id id\n+                                            :status         :ERROR}))))\n \n (defn start-time-slicer-parser\n   [{:keys [db sqs workers-queue-url]} {id :id :as args} _]\n@@ -266,13 +276,13 @@\n     (try\n       (aws-sqs/send-message sqs workers-queue-url {:message/type :parse-time-slicer\n                                                    :id           id})\n-      (time-slicer-model/update! db {:id id :status status})\n+      (time-slicer-model/upsert-status! db {:time-slicer-id id :status status})\n       {:id     id\n        :status status}\n       (catch Exception e\n         (log/error \"Exception when sending message to worker\" {:error e})\n-        (time-slicer-model/update! db {:id     id\n-                                       :status :ERROR})))))\n+        (time-slicer-model/upsert-status! db {:time-slicer-id id\n+                                              :status         :ERROR})))))\n \n (defn upload-bayes-factor-analysis [{:keys [authed-user-id db]}\n                                     {log-file-url        :logFileUrl\n@@ -284,26 +294,27 @@\n                                      :as                 args} _]\n   (log/info \"upload-bayes-factor\" {:user/id authed-user-id\n                                    :args    args})\n-  (let [id       (s3-url->id log-file-url authed-user-id)\n+  (let [id     (s3-url->id log-file-url authed-user-id)\n         ;; NOTE: uploads mutation generates different ids for each uploaded file\n         ;; _ (assert (= id (s3-url->id locations-file-url bucket-name authed-user-id)))\n-        status   :DATA_UPLOADED\n-        analysis {:id                  id\n-                  :readable-name       readable-name\n-                  :user-id             authed-user-id\n-                  :log-file-url        log-file-url\n-                  :locations-file-url  locations-file-url\n-                  :number-of-locations number-of-locations\n-                  :burn-in             burn-in\n-                  :status              status}]\n+        status :DATA_UPLOADED]\n     (try\n-      (bayes-factor-model/upsert! db analysis)\n+      ;; TODO : in a transaction\n+      (bayes-factor-model/upsert! db {:id                  id\n+                                      :readable-name       readable-name\n+                                      :user-id             authed-user-id\n+                                      :log-file-url        log-file-url\n+                                      :locations-file-url  locations-file-url\n+                                      :number-of-locations number-of-locations\n+                                      :burn-in             burn-in})\n+      (bayes-factor-model/upsert-status! db {:bayes-factor-analysis-id id\n+                                             :status                   status})\n       {:id     id\n        :status status}\n       (catch Exception e\n         (log/error \"Exception occured\" {:error e})\n-        (bayes-factor-model/update! db {:id     id\n-                                        :status :ERROR})))))\n+        (bayes-factor-model/upsert-status! db {:bayes-factor-analysis-id id\n+                                               :status                   :ERROR})))))\n \n (defn update-bayes-factor-analysis\n   [{:keys [authed-user-id db]} {id                  :id\n@@ -313,21 +324,23 @@\n                                 burn-in             :burnIn\n                                 :or                 {burn-in 0.1}\n                                 :as                 args} _]\n-  (log/info \"update discrete tree\" {:user/id authed-user-id\n-                                    :args    args})\n+  (log/info \"update bayes factor analysis\" {:user/id authed-user-id\n+                                            :args    args})\n   (try\n     (let [status :PARSER_ARGUMENTS_SET]\n+      ;; TODO : in a transaction\n       (discrete-tree-model/update! db {:id                  id\n                                        :readable-name       readable-name\n                                        :number-of-locations number-of-locations\n-                                       :burn-in             burn-in\n-                                       :status              status})\n+                                       :burn-in             burn-in})\n+      (bayes-factor-model/upsert-status! db {:bayes-factor-analysis-id id\n+                                             :status                   status})\n       {:id     id\n        :status status})\n     (catch Exception e\n       (log/error \"Exception occured\" {:error e})\n-      (bayes-factor-model/update! db {:id     id\n-                                      :status :ERROR}))))\n+      (bayes-factor-model/upsert-status! db {:bayes-factor-analysis-id id\n+                                             :status                   :ERROR}))))\n \n (defn start-bayes-factor-parser\n   [{:keys [db sqs workers-queue-url]} {id :id :as args} _]\n@@ -336,10 +349,11 @@\n     (try\n       (aws-sqs/send-message sqs workers-queue-url {:message/type :parse-bayes-factors\n                                                    :id           id})\n-      (bayes-factor-model/update! db {:id id :status status})\n+      (bayes-factor-model/upsert-status! db {:bayes-factor-analysis-id id\n+                                             :status                   status})\n       {:id     id\n        :status status}\n       (catch Exception e\n         (log/error \"Exception when sending message to worker\" {:error e})\n-        (bayes-factor-model/update! db {:id     id\n-                                        :status :ERROR})))))\n+        (bayes-factor-model/upsert-status! db {:bayes-factor-analysis-id id\n+                                               :status                   :ERROR})))))"
  },
  {
    "sha": "cb682a0237ed71e73e6c4dc012374860ba8f1e2f",
    "filename": "src/clj/api/subscriptions.clj",
    "status": "modified",
    "additions": 8,
    "deletions": 7,
    "changes": 15,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/src/clj/api/subscriptions.clj",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/src/clj/api/subscriptions.clj",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/src/clj/api/subscriptions.clj?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -15,9 +15,10 @@\n                                         :token       access-token})\n     ;; create the subscription\n     (let [subscription (go-loop []\n-                         (when-let [{:keys [status]} (callback db id)]\n-                           (source-stream (clj->gql {:id     id\n-                                                     :status status}))\n+                         (when-let [{:keys [status progress]} (callback db id)]\n+                           (source-stream (clj->gql {:id       id\n+                                                     :status   status\n+                                                     :progress (or progress 0)}))\n                            (<! (timeout 1000))\n                            (recur)))]\n       ;; return a function to cleanup the subscription\n@@ -29,16 +30,16 @@\n \n (defn create-continuous-tree-parser-status-sub []\n   (create-status-subscription \"continuous-tree\" (fn [db id]\n-                                                  (continuous-tree-model/get-status db {:id id}))))\n+                                                  (continuous-tree-model/get-status db {:tree-id id}))))\n \n (defn create-discrete-tree-parser-status-sub []\n   (create-status-subscription \"discrete-tree\" (fn [db id]\n-                                                (discrete-tree-model/get-status db {:id id}))))\n+                                                (discrete-tree-model/get-status db {:tree-id id}))))\n \n (defn create-bayes-factor-parser-status-sub []\n   (create-status-subscription \"bayes-factor\" (fn [db id]\n-                                               (bayes-factor-model/get-status db {:id id}))))\n+                                               (bayes-factor-model/get-status db {:bayes-factor-analysis-id id}))))\n \n (defn create-time-slicer-parser-status-sub []\n   (create-status-subscription \"time-slicer\" (fn [db id]\n-                                              (time-slicer-model/get-status db {:id id}))))\n+                                              (time-slicer-model/get-status db {:time-slicer-id id}))))"
  },
  {
    "sha": "ffa1a841a12e66161778d0b339fab5f8e5cacda3",
    "filename": "src/clj/shared/utils.clj",
    "status": "modified",
    "additions": 6,
    "deletions": 0,
    "changes": 6,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/src/clj/shared/utils.clj",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/src/clj/shared/utils.clj",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/src/clj/shared/utils.clj?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -58,6 +58,12 @@\n   [path]\n   (.exists (io/file path)))\n \n+(defn round\n+  \"Round a double to the given precision\"\n+  [precision d]\n+  (let [factor (Math/pow 10 precision)]\n+    (/ (Math/round (* d factor)) factor)))\n+\n (comment\n   (decode-transit (encode-transit {:a 1}))\n   (clj->gql {:tree-id \"fubar\"}))"
  },
  {
    "sha": "ee6db3cfb3dfa0c1f79cdd2cfb9dea41aaac6eac",
    "filename": "src/clj/tests/integration/bayes_factor_test.clj",
    "status": "modified",
    "additions": 9,
    "deletions": 5,
    "changes": 14,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/src/clj/tests/integration/bayes_factor_test.clj",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/src/clj/tests/integration/bayes_factor_test.clj",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/src/clj/tests/integration/bayes_factor_test.clj?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -21,7 +21,9 @@\n     (loop [current-status (query-status id)]\n       (if (= status current-status)\n         current-status\n-        (recur (query-status id))))))\n+        (do\n+          (Thread/sleep 1000)\n+          (recur (query-status id)))))))\n \n (deftest continuous-tree-test\n   (let [[log-url locations-url] (get-in (run-query {:query\n@@ -82,11 +84,12 @@\n \n         _ (block-on-status id :SUCCEEDED)\n \n-        {:keys [id status outputFileUrl bayesFactors]} (get-in (run-query {:query\n-                                                              \"query GetResults($id: ID!) {\n+        {:keys [id status progress outputFileUrl bayesFactors]} (get-in (run-query {:query\n+                                                                                    \"query GetResults($id: ID!) {\n                                                                        getBayesFactorAnalysis(id: $id) {\n                                                                          id\n                                                                          status\n+                                                                         progress\n                                                                          outputFileUrl\n                                                                              bayesFactors {\n                                                                                from\n@@ -96,10 +99,11 @@\n                                                                              }\n                                                                        }\n                                                                      }\"\n-                                                              :variables {:id id}})\n-                                                  [:data :getBayesFactorAnalysis])]\n+                                                                                    :variables {:id id}})\n+                                                                        [:data :getBayesFactorAnalysis])]\n     (log/debug \"response\" {:id id :status status :bayes-factors bayesFactors})\n     (is (sequential? bayesFactors))\n     (is (= 21 (count bayesFactors)))\n     (is #{\"from\" \"to\" \"bayesFactors\" \"posteriorProbability\"} (-> bayesFactors first keys set))\n+    (is (= 1.0 progress))\n     (is outputFileUrl)))"
  },
  {
    "sha": "495d084424b732b92e4307f6ecb10055ad1962ba",
    "filename": "src/clj/tests/integration/continuous_tree_test.clj",
    "status": "modified",
    "additions": 16,
    "deletions": 11,
    "changes": 27,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/src/clj/tests/integration/continuous_tree_test.clj",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/src/clj/tests/integration/continuous_tree_test.clj",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/src/clj/tests/integration/continuous_tree_test.clj?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -21,14 +21,16 @@\n     (loop [current-status (query-status id)]\n       (if (= status current-status)\n         current-status\n-        (recur (query-status id))))))\n+        (do\n+          (Thread/sleep 1000)\n+          (recur (query-status id)))))))\n \n (deftest continuous-tree-test\n   (let [[url _] (get-in (run-query {:query\n                                     \"mutation GetUploadUrl($files: [File]) {\n                                        getUploadUrls(files: $files)\n                                      }\"\n-                                    :variables {:files [{:name \"speciesDiffusion.MCC\"\n+                                    :variables {:files [{:name      \"speciesDiffusion.MCC\"\n                                                          :extension \"tree\"}]}})\n                         [:data :getUploadUrls])\n \n@@ -76,10 +78,10 @@\n                                                      status\n                                                    }\n                                               }\"\n-                                             :variables {:id id\n-                                                         :x \"trait2\"\n-                                                         :y \"trait1\"\n-                                                         :hpd \"80\"\n+                                             :variables {:id   id\n+                                                         :x    \"trait2\"\n+                                                         :y    \"trait1\"\n+                                                         :hpd  \"80\"\n                                                          :mrsd \"2019/02/12\"}})\n                                  [:data :updateContinuousTree])\n \n@@ -98,18 +100,19 @@\n \n         _ (block-on-status id :SUCCEEDED)\n \n-        {:keys [id status outputFileUrl]} (get-in (run-query {:query\n-                                                              \"query GetTree($id: ID!) {\n+        {:keys [id status progress outputFileUrl]} (get-in (run-query {:query\n+                                                                       \"query GetTree($id: ID!) {\n                                                                             getContinuousTree(id: $id) {\n                                                                               id\n                                                                               status\n+                                                                              progress\n                                                                               outputFileUrl\n                                                                             }\n                                                                           }\"\n-                                                              :variables {:id id}})\n-                                                  [:data :getContinuousTree])]\n+                                                                       :variables {:id id}})\n+                                                           [:data :getContinuousTree])]\n \n-    (log/debug \"response\" {:id id :status status})\n+    (log/debug \"response\" {:id id :status status :progress progress})\n \n     (is #{\"height\" \"height_95%_HPD\" \"height_median\" \"height_range\"\n           \"length\" \"length_95%_HPD\" \"length_median\" \"length_range\"\n@@ -126,4 +129,6 @@\n \n     (is #{\"80\"} (set hpdLevels))\n \n+    (is (= 1.0 progress))\n+\n     (is outputFileUrl)))"
  },
  {
    "sha": "f3f2e92f0e657659718e871279df88fdb0a199bb",
    "filename": "src/clj/tests/integration/discrete_tree_test.clj",
    "status": "modified",
    "additions": 27,
    "deletions": 21,
    "changes": 48,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/src/clj/tests/integration/discrete_tree_test.clj",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/src/clj/tests/integration/discrete_tree_test.clj",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/src/clj/tests/integration/discrete_tree_test.clj?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -21,35 +21,37 @@\n     (loop [current-status (query-status id)]\n       (if (= status current-status)\n         current-status\n-        (recur (query-status id))))))\n+        (do\n+          (Thread/sleep 1000)\n+          (recur (query-status id)))))))\n \n (deftest discrete-tree-test\n   (let [[tree-url locations-url] (get-in (run-query {:query\n                                                      \"mutation GetUploadUrls($files: [File]) {\n                                                         getUploadUrls(files: $files)\n                                                       }\"\n-                                                     :variables {:files [{:name \"H5N1_HA_discrete_MCC\"\n+                                                     :variables {:files [{:name      \"H5N1_HA_discrete_MCC\"\n                                                                           :extension \"tree\"}\n-                                                                         {:name \"locationCoordinates_H5N1\"\n+                                                                         {:name      \"locationCoordinates_H5N1\"\n                                                                           :extension \"txt\"}]}})\n                                          [:data :getUploadUrls])\n-        _ (http/put tree-url {:body (io/file \"src/test/resources/discrete/H5N1_HA_discrete_MCC.tree\")})\n-        _ (http/put locations-url {:body (io/file \"src/test/resources/discrete/locationCoordinates_H5N1\")})\n-        {:keys [id status]} (get-in (run-query {:query\n-                                                \"mutation UploadTree($treeUrl: String!,\n+        _                        (http/put tree-url {:body (io/file \"src/test/resources/discrete/H5N1_HA_discrete_MCC.tree\")})\n+        _                        (http/put locations-url {:body (io/file \"src/test/resources/discrete/locationCoordinates_H5N1\")})\n+        {:keys [id status]}      (get-in (run-query {:query\n+                                                     \"mutation UploadTree($treeUrl: String!,\n                                                                      $locationsUrl: String!) {\n                                                    uploadDiscreteTree(treeFileUrl: $treeUrl,\n                                                                       locationsFileUrl: $locationsUrl) {\n                                                      id\n                                                      status\n                                                    }\n                                                 }\"\n-                                                :variables {:treeUrl (-> tree-url\n-                                                                         (string/split  #\"\\?\")\n-                                                                         first)\n-                                                            :locationsUrl (-> locations-url\n+                                                     :variables {:treeUrl      (-> tree-url\n                                                                               (string/split  #\"\\?\")\n-                                                                              first)}})\n+                                                                              first)\n+                                                                 :locationsUrl (-> locations-url\n+                                                                                   (string/split  #\"\\?\")\n+                                                                                   first)}})\n                                     [:data :uploadDiscreteTree])\n \n         _ (is :TREE_AND_LOCATIONS_UPLOADED (keyword status))\n@@ -75,9 +77,9 @@\n                                                   status\n                                                 }\n                                               }\"\n-                                             :variables {:id id\n+                                             :variables {:id                id\n                                                          :locationAttribute \"states\"\n-                                                         :mrsd \"2019/02/12\"}})\n+                                                         :mrsd              \"2019/02/12\"}})\n                                  [:data :updateDiscreteTree])\n \n         _ (is :PARSER_ARGUMENTS_SET (keyword status))\n@@ -95,25 +97,29 @@\n \n         _ (block-on-status id :SUCCEEDED)\n \n-        {:keys [id status outputFileUrl]} (get-in (run-query {:query\n-                                                              \"query GetTree($id: ID!) {\n+        {:keys [id status progress outputFileUrl]} (get-in (run-query {:query\n+                                                                       \"query GetTree($id: ID!) {\n                                                                        getDiscreteTree(id: $id) {\n                                                                          id\n                                                                          status\n+                                                                         progress\n                                                                          outputFileUrl\n                                                                        }\n                                                                      }\"\n-                                                              :variables {:id id}})\n-                                                  [:data :getDiscreteTree])]\n+                                                                       :variables {:id id}})\n+                                                           [:data :getDiscreteTree])]\n \n-    (log/debug \"response\" {:id id\n-                           :status status\n-                           :tree/url tree-url\n+    (log/debug \"response\" {:id            id\n+                           :status        status\n+                           :progress      progress\n+                           :tree/url      tree-url\n                            :locations/url locations-url})\n \n     (is #{\"height\" \"height_95%_HPD\" \"height_median\" \"height_range\" \"length\" \"length_95%_HPD\"\n           \"length_median\" \"length_range\" \"posterior\" \"rate\" \"rate_95%_HPD\" \"rate_median\"\n           \"rate_range\" \"states\" \"states.prob\" \"states.set\" \"states.set.prob\"}\n         (set attributeNames))\n \n+    (is (= 1.0 progress))\n+\n     (is outputFileUrl)))"
  },
  {
    "sha": "b35786ff247504bc4875fa2ebdd9f72da204ecb7",
    "filename": "src/clj/tests/integration/time_slicer_test.clj",
    "status": "modified",
    "additions": 17,
    "deletions": 15,
    "changes": 32,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/src/clj/tests/integration/time_slicer_test.clj",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/src/clj/tests/integration/time_slicer_test.clj",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/src/clj/tests/integration/time_slicer_test.clj?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -20,14 +20,16 @@\n     (loop [current-status (query-status id)]\n       (if (= status current-status)\n         current-status\n-        (recur (query-status id))))))\n+        (do\n+          (Thread/sleep 1000)\n+          (recur (query-status id)))))))\n \n (deftest time-slicer-test\n   (let [[url _] (get-in (run-query {:query\n                                     \"mutation GetUploadUrl($files: [File]) {\n                                        getUploadUrls(files: $files)\n                                      }\"\n-                                    :variables {:files [{:name \"WNV_small\"\n+                                    :variables {:files [{:name      \"WNV_small\"\n                                                          :extension \"trees\"}]}})\n                         [:data :getUploadUrls])\n \n@@ -80,14 +82,14 @@\n                                                      status\n                                                    }\n                                               }\"\n-                                             :variables {:id id\n-                                                         :traitAttributeName \"location\"\n+                                             :variables {:id                   id\n+                                                         :traitAttributeName   \"location\"\n                                                          :rrwRateAttributeName \"rate\"\n-                                                         :contouringGridSize 100\n-                                                         :burnIn 1\n-                                                         :numberOfIntervals 10\n-                                                         :hpd 0.8\n-                                                         :mrsd \"2021/01/12\"}})\n+                                                         :contouringGridSize   100\n+                                                         :burnIn               1\n+                                                         :numberOfIntervals    10\n+                                                         :hpd                  0.8\n+                                                         :mrsd                 \"2021/01/12\"}})\n                                  [:data :updateContinuousTree])\n \n         _ (is :PARSER_ARGUMENTS_SET (keyword status))\n@@ -105,19 +107,19 @@\n \n         _ (block-on-status id :SUCCEEDED)\n \n-        {:keys [outputFileUrl]} (get-in (run-query {:query\n-                                                    \"query GetTree($id: ID!) {\n+        {:keys [outputFileUrl progress]} (get-in (run-query {:query\n+                                                             \"query GetTree($id: ID!) {\n                                                                             getTimeSlicer(id: $id) {\n                                                                               id\n                                                                               status\n+                                                                              progress\n                                                                               outputFileUrl\n                                                                             }\n                                                                           }\"\n-                                                    :variables {:id id}})\n-                                        [:data :getTimeSlicer])]\n+                                                             :variables {:id id}})\n+                                                 [:data :getTimeSlicer])]\n \n     (is #{\"rate\" \"location\"} (set attributeNames))\n-\n     (is (= 10 treesCount))\n-\n+    (is (= 1.0 progress))\n     (is outputFileUrl)))"
  },
  {
    "sha": "d567898935577b51f76dd4c2725e413e4dfc1740",
    "filename": "src/clj/worker/listener.clj",
    "status": "modified",
    "additions": 207,
    "deletions": 163,
    "changes": 370,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/src/clj/worker/listener.clj",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/src/clj/worker/listener.clj",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/src/clj/worker/listener.clj?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -10,14 +10,20 @@\n             [clojure.core.match :refer [match]]\n             [clojure.data.json :as json]\n             [mount.core :as mount :refer [defstate]]\n-            [shared.utils :refer [file-exists?]]\n+            [shared.utils :refer [file-exists? round]]\n             [taoensso.timbre :as log])\n-  (:import [com.spread.parsers BayesFactorParser ContinuousTreeParser DiscreteTreeParser TimeSlicerParser]))\n+  (:import [com.spread.parsers BayesFactorParser ContinuousTreeParser DiscreteTreeParser TimeSlicerParser]\n+           [com.spread.progress IProgressObserver IProgressReporter]))\n \n (declare listener)\n \n (defonce tmp-dir \"/tmp\")\n \n+(defn new-progress-handler [callback]\n+  (proxy [IProgressObserver] []\n+    (handleProgress [progress]\n+      (callback progress))))\n+\n (defmulti handler\n   (fn [{:message/keys [type]} _]\n     type))\n@@ -32,170 +38,189 @@\n   (try\n     (let [;; TODO: parse extension\n           tree-object-key (str user-id \"/\" id \".tree\")\n-          tree-file-path (str tmp-dir \"/\" tree-object-key)\n-          _ (aws-s3/download-file s3 {:bucket bucket-name\n-                                      :key tree-object-key\n-                                      :dest-path tree-file-path})\n-          parser (doto (new DiscreteTreeParser)\n-                   (.setTreeFilePath tree-file-path))\n-          attributes (json/read-str (.parseAttributes parser))]\n-      (log/info \"Parsed attributes\" {:id id\n+          tree-file-path  (str tmp-dir \"/\" tree-object-key)\n+          _               (aws-s3/download-file s3 {:bucket    bucket-name\n+                                                    :key       tree-object-key\n+                                                    :dest-path tree-file-path})\n+          parser          (doto (new DiscreteTreeParser)\n+                            (.setTreeFilePath tree-file-path))\n+          attributes      (json/read-str (.parseAttributes parser))]\n+      (log/info \"Parsed attributes\" {:id         id\n                                      :attributes attributes})\n       (discrete-tree-model/insert-attributes! db id attributes)\n-      (discrete-tree-model/update! db {:id id\n-                                       :status :ATTRIBUTES_PARSED}))\n+      (discrete-tree-model/upsert-status! db {:tree-id id\n+                                              :status  :ATTRIBUTES_PARSED}))\n     (catch Exception e\n       (log/error \"Exception when handling discrete-tree-upload\" {:error e})\n-      (discrete-tree-model/update! db {:id id\n-                                       :status :ERROR}))))\n+      (discrete-tree-model/upsert-status! db {:tree-id id\n+                                              :status  :ERROR}))))\n \n (defmethod handler :parse-discrete-tree\n   [{:keys [id] :as args} {:keys [db s3 bucket-name aws-config]}]\n   (log/info \"handling parse-discrete-tree\" args)\n   (try\n-    (let [_ (discrete-tree-model/update! db {:id id\n-                                             :status :RUNNING})\n-          {:keys [user-id location-attribute-name\n+    (let [{:keys [user-id location-attribute-name\n                   timescale-multiplier most-recent-sampling-date\n                   locations-file-url]}\n           (discrete-tree-model/get-tree db {:id id})\n           ;; TODO: parse extension\n-          tree-object-key (str user-id \"/\" id \".tree\")\n-          tree-file-path (str tmp-dir \"/\" tree-object-key)\n+          tree-object-key      (str user-id \"/\" id \".tree\")\n+          tree-file-path       (str tmp-dir \"/\" tree-object-key)\n           ;; is it cached on disk?\n-          _ (when-not (file-exists? tree-file-path)\n-              (aws-s3/download-file s3 {:bucket bucket-name\n-                                        :key tree-object-key\n-                                        :dest-path tree-file-path}))\n-          locations-file-id (s3-url->id locations-file-url user-id)\n+          _                    (when-not (file-exists? tree-file-path)\n+                                 (aws-s3/download-file s3 {:bucket    bucket-name\n+                                                           :key       tree-object-key\n+                                                           :dest-path tree-file-path}))\n+          locations-file-id    (s3-url->id locations-file-url user-id)\n           ;; TODO: parse extension\n           locations-object-key (str user-id \"/\" locations-file-id \".txt\")\n-          locations-file-path (str tmp-dir \"/\" locations-object-key)\n+          locations-file-path  (str tmp-dir \"/\" locations-object-key)\n           ;; is it cached on disk?\n-          _ (when-not (file-exists? locations-file-path)\n-              (aws-s3/download-file s3 {:bucket bucket-name\n-                                        :key locations-object-key\n-                                        :dest-path locations-file-path}))\n-          parser (doto (new DiscreteTreeParser)\n-                   (.setTreeFilePath tree-file-path)\n-                   (.setLocationsFilePath locations-file-path)\n-                   (.setLocationTraitAttributeName location-attribute-name)\n-                   (.setTimescaleMultiplier timescale-multiplier)\n-                   (.setMostRecentSamplingDate most-recent-sampling-date))\n-          output-object-key (str user-id \"/\" id \".json\")\n-          output-object-path (str tmp-dir \"/\" output-object-key)\n-          _ (spit output-object-path (.parse parser) :append false)\n-          _ (aws-s3/upload-file s3 {:bucket bucket-name\n-                                    :key output-object-key\n-                                    :file-path output-object-path})\n-          url (aws-s3/build-url aws-config bucket-name output-object-key)]\n-      (discrete-tree-model/update! db {:id id\n-                                       :output-file-url url\n-                                       :status :SUCCEEDED}))\n+          _                    (when-not (file-exists? locations-file-path)\n+                                 (aws-s3/download-file s3 {:bucket    bucket-name\n+                                                           :key       locations-object-key\n+                                                           :dest-path locations-file-path}))\n+          progress-handler     (new-progress-handler (fn [progress]\n+                                                       (let [progress (round 2 progress)]\n+                                                         (when (= (mod progress 0.1) 0.0)\n+                                                           (log/debug \"discrete tree progress\" {:id       id\n+                                                                                                :progress progress})\n+                                                           (discrete-tree-model/upsert-status! db {:tree-id  id\n+                                                                                                   :status   :RUNNING\n+                                                                                                   :progress progress})))))\n+          parser               (doto (new DiscreteTreeParser)\n+                                 (.setTreeFilePath tree-file-path)\n+                                 (.setLocationsFilePath locations-file-path)\n+                                 (.setLocationTraitAttributeName location-attribute-name)\n+                                 (.setTimescaleMultiplier timescale-multiplier)\n+                                 (.setMostRecentSamplingDate most-recent-sampling-date)\n+                                 (.registerProgressObserver progress-handler))\n+          output-object-key    (str user-id \"/\" id \".json\")\n+          output-object-path   (str tmp-dir \"/\" output-object-key)\n+          _                    (spit output-object-path (.parse parser) :append false)\n+          _                    (aws-s3/upload-file s3 {:bucket    bucket-name\n+                                                       :key       output-object-key\n+                                                       :file-path output-object-path})\n+          url                  (aws-s3/build-url aws-config bucket-name output-object-key)]\n+      ;; TODO : in a transaction\n+      (discrete-tree-model/update! db {:id              id\n+                                       :output-file-url url})\n+      (discrete-tree-model/upsert-status! db {:tree-id id\n+                                              :status  :SUCCEEDED}))\n     (catch Exception e\n       (log/error \"Exception when handling parse-discrete-tree\" {:error e})\n-      (discrete-tree-model/update! db {:id id\n-                                       :status :ERROR}))))\n+      (discrete-tree-model/upsert-status! db {:tree-id id\n+                                              :status  :ERROR}))))\n \n (defmethod handler :continuous-tree-upload\n   [{:keys [id user-id] :as args} {:keys [db s3 bucket-name]}]\n   (log/info \"handling continuous-tree-upload\" args)\n   (try\n     (let [;; TODO: parse extension\n-          tree-object-key (str user-id \"/\" id \".tree\")\n-          tree-file-path (str tmp-dir \"/\" tree-object-key)\n-          _ (aws-s3/download-file s3 {:bucket bucket-name\n-                                      :key tree-object-key\n-                                      :dest-path tree-file-path})\n-          parser (doto (new ContinuousTreeParser)\n-                   (.setTreeFilePath tree-file-path))\n+          tree-object-key         (str user-id \"/\" id \".tree\")\n+          tree-file-path          (str tmp-dir \"/\" tree-object-key)\n+          _                       (aws-s3/download-file s3 {:bucket    bucket-name\n+                                                            :key       tree-object-key\n+                                                            :dest-path tree-file-path})\n+          parser                  (doto (new ContinuousTreeParser)\n+                                    (.setTreeFilePath tree-file-path))\n           [attributes hpd-levels] (json/read-str (.parseAttributesAndHpdLevels parser))]\n-      (log/info \"Parsed attributes and hpd-levels\" {:id id\n+      (log/info \"Parsed attributes and hpd-levels\" {:id         id\n                                                     :attributes attributes\n                                                     :hpd-levels hpd-levels})\n+      ;; TODO : in a transaction\n       (continuous-tree-model/insert-attributes! db id attributes)\n       (continuous-tree-model/insert-hpd-levels! db id hpd-levels)\n-      (continuous-tree-model/update! db {:id id\n-                                         :status :ATTRIBUTES_AND_HPD_LEVELS_PARSED}))\n+      (continuous-tree-model/upsert-status! db {:tree-id id\n+                                                :status  :ATTRIBUTES_AND_HPD_LEVELS_PARSED}))\n     (catch Exception e\n       (log/error \"Exception when handling continuous-tree-upload\" {:error e})\n-      (continuous-tree-model/update! db {:id id\n-                                         :status :ERROR}))))\n+      (continuous-tree-model/upsert-status! db {:tree-id id\n+                                                :status  :ERROR}))))\n \n (defmethod handler :parse-continuous-tree\n   [{:keys [id] :as args} {:keys [db s3 bucket-name aws-config]}]\n   (log/info \"handling parse-continuous-tree\" args)\n   (try\n-    (let [_ (continuous-tree-model/update! db {:id id\n-                                               :status :RUNNING})\n-          {:keys [user-id x-coordinate-attribute-name y-coordinate-attribute-name\n+    (let [{:keys [user-id x-coordinate-attribute-name y-coordinate-attribute-name\n                   hpd-level has-external-annotations timescale-multiplier\n                   most-recent-sampling-date]}\n           (continuous-tree-model/get-tree db {:id id})\n           ;; TODO: parse extension\n-          tree-object-key (str user-id \"/\" id \".tree\")\n-          tree-file-path (str tmp-dir \"/\" tree-object-key)\n+          tree-object-key    (str user-id \"/\" id \".tree\")\n+          tree-file-path     (str tmp-dir \"/\" tree-object-key)\n           ;; is it cached on disk?\n-          _ (when-not (file-exists? tree-file-path)\n-              (aws-s3/download-file s3 {:bucket bucket-name\n-                                        :key tree-object-key\n-                                        :dest-path tree-file-path}))\n+          _                  (when-not (file-exists? tree-file-path)\n+                               (aws-s3/download-file s3 {:bucket    bucket-name\n+                                                         :key       tree-object-key\n+                                                         :dest-path tree-file-path}))\n+          progress-handler   (new-progress-handler (fn [progress]\n+                                                     (let [progress (round 2 progress)]\n+                                                       (when (= (mod progress 0.1) 0.0)\n+                                                         (log/debug \"continuous tree progress\" {:id       id\n+                                                                                                :progress progress})\n+                                                         (continuous-tree-model/upsert-status! db {:tree-id  id\n+                                                                                                   :status   :RUNNING\n+                                                                                                   :progress progress})))))\n           ;; call all setters\n-          parser (doto (new ContinuousTreeParser)\n-                   (.setTreeFilePath tree-file-path)\n-                   (.setXCoordinateAttributeName x-coordinate-attribute-name)\n-                   (.setYCoordinateAttributeName y-coordinate-attribute-name)\n-                   (.setHpdLevel hpd-level)\n-                   (.hasExternalAnnotations has-external-annotations)\n-                   (.setTimescaleMultiplier timescale-multiplier)\n-                   (.setMostRecentSamplingDate most-recent-sampling-date))\n-          output-object-key (str user-id \"/\" id \".json\")\n+          parser             (doto (new ContinuousTreeParser)\n+                               (.setTreeFilePath tree-file-path)\n+                               (.setXCoordinateAttributeName x-coordinate-attribute-name)\n+                               (.setYCoordinateAttributeName y-coordinate-attribute-name)\n+                               (.setHpdLevel hpd-level)\n+                               (.hasExternalAnnotations has-external-annotations)\n+                               (.setTimescaleMultiplier timescale-multiplier)\n+                               (.setMostRecentSamplingDate most-recent-sampling-date)\n+                               (.registerProgressObserver progress-handler))\n+          output-object-key  (str user-id \"/\" id \".json\")\n           output-object-path (str tmp-dir \"/\" output-object-key)\n-          _ (spit output-object-path (.parse parser) :append false)\n-          _ (aws-s3/upload-file s3 {:bucket bucket-name\n-                                    :key output-object-key\n-                                    :file-path output-object-path})\n-          url (aws-s3/build-url aws-config bucket-name output-object-key)]\n-      (continuous-tree-model/update! db {:id id\n-                                         :output-file-url url\n-                                         :status :SUCCEEDED}))\n+          _                  (spit output-object-path (.parse parser) :append false)\n+          _                  (aws-s3/upload-file s3 {:bucket    bucket-name\n+                                                     :key       output-object-key\n+                                                     :file-path output-object-path})\n+          url                (aws-s3/build-url aws-config bucket-name output-object-key)]\n+      ;; TODO : in a transaction\n+      (continuous-tree-model/update! db {:id              id\n+                                         :output-file-url url})\n+      (continuous-tree-model/upsert-status! db {:tree-id id\n+                                                :status  :SUCCEEDED}))\n     (catch Exception e\n       (log/error \"Exception when handling parse-continuous-tree\" {:error e})\n-      (continuous-tree-model/update! db {:id id\n-                                         :status :ERROR}))))\n+      (continuous-tree-model/upsert-status! db {:tree-id id\n+                                                :status  :ERROR}))))\n \n (defmethod handler :time-slicer-upload\n   [{:keys [id user-id] :as args} {:keys [db s3 bucket-name]}]\n   (log/info \"handling timeslicer-upload\" args)\n   (try\n     (let [;; TODO: parse extension\n-          trees-object-key (str user-id \"/\" id \".trees\")\n-          trees-file-path (str tmp-dir \"/\" trees-object-key)\n-          _ (aws-s3/download-file s3 {:bucket bucket-name\n-                                      :key trees-object-key\n-                                      :dest-path trees-file-path})\n-          parser (doto (new TimeSlicerParser)\n-                   (.setTreesFilePath trees-file-path))\n+          trees-object-key         (str user-id \"/\" id \".trees\")\n+          trees-file-path          (str tmp-dir \"/\" trees-object-key)\n+          _                        (aws-s3/download-file s3 {:bucket    bucket-name\n+                                                             :key       trees-object-key\n+                                                             :dest-path trees-file-path})\n+          parser                   (doto (new TimeSlicerParser)\n+                                     (.setTreesFilePath trees-file-path))\n           [attributes trees-count] (json/read-str (.parseAttributesAndTreesCount parser))]\n-      (log/info \"Parsed attributes and hpd-levels\" {:id id\n+      (log/info \"Parsed attributes and hpd-levels\" {:id         id\n                                                     :attributes attributes\n-                                                    :count trees-count})\n+                                                    :count      trees-count})\n+      ;; TODO : in a transaction\n       (time-slicer-model/insert-attributes! db id attributes)\n-      (time-slicer-model/update! db {:id id\n-                                     :trees-count trees-count\n-                                     :status :ATTRIBUTES_AND_TREES_COUNT_PARSED}))\n+      (time-slicer-model/update! db {:id          id\n+                                     :trees-count trees-count})\n+      (time-slicer-model/upsert-status! db {:time-slicer-id id\n+                                            :status         :ATTRIBUTES_AND_TREES_COUNT_PARSED}))\n     (catch Exception e\n       (log/error \"Exception when handling time-slicer-upload\" {:error e})\n-      (time-slicer-model/update! db {:id id\n-                                     :status :ERROR}))))\n+      (time-slicer-model/upsert-status! db {:time-slicer-id id\n+                                            :status         :ERROR}))))\n \n (defmethod handler :parse-time-slicer\n   [{:keys [id] :as args} {:keys [db s3 bucket-name aws-config]}]\n   (log/info \"handling parse-time-slicer\" args)\n   (try\n-    (let [_ (time-slicer-model/update! db {:id id\n-                                           :status :RUNNING})\n-          {:keys [user-id\n+    (let [{:keys [user-id\n                   burn-in\n                   relaxed-random-walk-rate-attribute-name\n                   trait-attribute-name\n@@ -206,67 +231,86 @@\n                   most-recent-sampling-date]}\n           (time-slicer-model/get-time-slicer db {:id id})\n           ;; TODO: parse extension\n-          trees-object-key (str user-id \"/\" id \".trees\")\n-          trees-file-path (str tmp-dir \"/\" trees-object-key)\n+          trees-object-key   (str user-id \"/\" id \".trees\")\n+          trees-file-path    (str tmp-dir \"/\" trees-object-key)\n           ;; is it cached on disk?\n-          _ (when-not (file-exists? trees-file-path)\n-              (aws-s3/download-file s3 {:bucket bucket-name\n-                                        :key trees-object-key\n-                                        :dest-path trees-file-path}))\n+          _                  (when-not (file-exists? trees-file-path)\n+                               (aws-s3/download-file s3 {:bucket    bucket-name\n+                                                         :key       trees-object-key\n+                                                         :dest-path trees-file-path}))\n+          progress-handler   (new-progress-handler (fn [progress]\n+                                                     (let [progress (round 2 progress)]\n+                                                       (when (= (mod progress 0.1) 0.0)\n+                                                         (log/debug \"time-slicer progress\" {:id       id\n+                                                                                            :progress progress})\n+                                                         (time-slicer-model/upsert-status! db {:time-slicer-id id\n+                                                                                               :status         :RUNNING\n+                                                                                               :progress       progress})))))\n           ;; call all setters\n-          parser (doto (new TimeSlicerParser)\n-                   (.setTreesFilePath trees-file-path)\n-                   (.setBurnIn burn-in)\n-                   (.setRrwRateName relaxed-random-walk-rate-attribute-name)\n-                   (.setTraitName trait-attribute-name)\n-                   (.setNumberOfIntervals number-of-intervals)\n-                   (.setHpdLevel hpd-level)\n-                   (.setGridSize contouring-grid-size)\n-                   (.setTimescaleMultiplier timescale-multiplier)\n-                   (.setMostRecentSamplingDate most-recent-sampling-date))\n-          output-object-key (str user-id \"/\" id \".json\")\n+          parser             (doto (new TimeSlicerParser)\n+                               (.setTreesFilePath trees-file-path)\n+                               (.setBurnIn burn-in)\n+                               (.setRrwRateName relaxed-random-walk-rate-attribute-name)\n+                               (.setTraitName trait-attribute-name)\n+                               (.setNumberOfIntervals number-of-intervals)\n+                               (.setHpdLevel hpd-level)\n+                               (.setGridSize contouring-grid-size)\n+                               (.setTimescaleMultiplier timescale-multiplier)\n+                               (.setMostRecentSamplingDate most-recent-sampling-date)\n+                               (.registerProgressObserver progress-handler))\n+          output-object-key  (str user-id \"/\" id \".json\")\n           output-object-path (str tmp-dir \"/\" output-object-key)\n-          _ (spit output-object-path (.parse parser) :append false)\n-          _ (aws-s3/upload-file s3 {:bucket bucket-name\n-                                    :key output-object-key\n-                                    :file-path output-object-path})\n-          url (aws-s3/build-url aws-config bucket-name output-object-key)]\n-      (time-slicer-model/update! db {:id id\n-                                     :output-file-url url\n-                                     :status :SUCCEEDED}))\n+          _                  (spit output-object-path (.parse parser) :append false)\n+          _                  (aws-s3/upload-file s3 {:bucket    bucket-name\n+                                                     :key       output-object-key\n+                                                     :file-path output-object-path})\n+          url                (aws-s3/build-url aws-config bucket-name output-object-key)]\n+      ;; TODO : in a transaction\n+      (time-slicer-model/update! db {:id              id\n+                                     :output-file-url url})\n+      (time-slicer-model/upsert-status! db {:time-slicer-id id\n+                                            :status         :SUCCEEDED}))\n     (catch Exception e\n       (log/error \"Exception when handling parse-time-slicer\" {:error e})\n-      (time-slicer-model/update! db {:id id\n-                                     :status :ERROR}))))\n+      (time-slicer-model/upsert-status! db {:time-slicer-id id\n+                                            :status         :ERROR}))))\n \n (defmethod handler :parse-bayes-factors\n   [{:keys [id] :as args} {:keys [db s3 bucket-name aws-config]}]\n   (log/info \"handling parse-bayes-factors\" args)\n   (try\n-    (let [_              (bayes-factor-model/update! db {:id     id\n-                                                         :status :RUNNING})\n+    (let [_                (bayes-factor-model/update! db {:id     id\n+                                                           :status :RUNNING})\n           {:keys [user-id\n-                  ;; log-file-url\n                   locations-file-url\n                   number-of-locations\n                   burn-in]}\n           (bayes-factor-model/get-bayes-factor-analysis db {:id id})\n           ;; TODO: parse extension\n-          log-object-key (str user-id \"/\" id \".log\")\n-          log-file-path  (str tmp-dir \"/\" log-object-key)\n+          log-object-key   (str user-id \"/\" id \".log\")\n+          log-file-path    (str tmp-dir \"/\" log-object-key)\n           ;; is it cached on disk?\n-          _              (when-not (file-exists? log-file-path)\n-                           (aws-s3/download-file s3 {:bucket    bucket-name\n-                                                     :key       log-object-key\n-                                                     :dest-path log-file-path}))\n+          _                (when-not (file-exists? log-file-path)\n+                             (aws-s3/download-file s3 {:bucket    bucket-name\n+                                                       :key       log-object-key\n+                                                       :dest-path log-file-path}))\n+          progress-handler (new-progress-handler (fn [progress]\n+                                                   (let [progress (round 2 progress)]\n+                                                     (when (= (mod progress 0.1) 0.0)\n+                                                       (log/debug \"bayes factor progress\" {:id       id\n+                                                                                           :progress progress})\n+                                                       (bayes-factor-model/upsert-status! db {:bayes-factor-analysis-id id\n+                                                                                              :status                   :RUNNING\n+                                                                                              :progress                 progress})))))\n           parser\n           (match [(nil? locations-file-url) (nil? number-of-locations)]\n \n                  [true false]\n                  (doto (new BayesFactorParser)\n                    (.setLogFilePath log-file-path)\n                    (.setNumberOfGeneratedLocations number-of-locations)\n-                   (.setBurnIn (double burn-in)))\n+                   (.setBurnIn (double burn-in))\n+                   (.registerProgressObserver progress-handler))\n \n                  [false true]\n                  (let [locations-file-id    (s3-url->id locations-file-url user-id)\n@@ -281,7 +325,8 @@\n                    (doto (new BayesFactorParser)\n                      (.setLogFilePath log-file-path)\n                      (.setLocationsFilePath locations-file-path)\n-                     (.setBurnIn (double burn-in))))\n+                     (.setBurnIn (double burn-in))\n+                     (.registerProgressObserver progress-handler)))\n \n                  [true true]\n                  (throw (ex-info \"Bad input settings\"\n@@ -292,36 +337,35 @@\n                                  {:why?   \"You need to specify one of `log-file-path` and `number-of-locations`\"\n                                   :where? ::parse-bayes-factors}))\n                  :else (throw (Exception. \"Unexpected error\")))\n-\n           {:keys [bayesFactors spreadData]} (-> (.parse parser) (json/read-str :key-fn keyword))\n-\n-          output-object-key  (str user-id \"/\" id \".json\")\n-          output-object-path (str tmp-dir \"/\" output-object-key)\n-          _                  (spit output-object-path (json/write-str spreadData) :append false)\n-          _                  (aws-s3/upload-file s3 {:bucket    bucket-name\n-                                                     :key       output-object-key\n-                                                     :file-path output-object-path})\n-          url                (aws-s3/build-url aws-config bucket-name output-object-key)]\n-      ;; TODO : this should be in a a single transaction\n+          output-object-key                 (str user-id \"/\" id \".json\")\n+          output-object-path                (str tmp-dir \"/\" output-object-key)\n+          _                                 (spit output-object-path (json/write-str spreadData) :append false)\n+          _                                 (aws-s3/upload-file s3 {:bucket    bucket-name\n+                                                                    :key       output-object-key\n+                                                                    :file-path output-object-path})\n+          url                               (aws-s3/build-url aws-config bucket-name output-object-key)]\n+      ;; TODO : in a transaction\n       (bayes-factor-model/insert-bayes-factors db {:bayes-factor-analysis-id id\n                                                    :bayes-factors            (json/write-str bayesFactors)})\n       (bayes-factor-model/update! db {:id              id\n-                                      :output-file-url url\n-                                      :status          :SUCCEEDED}))\n+                                      :output-file-url url})\n+      (bayes-factor-model/upsert-status! db {:bayes-factor-analysis-id id\n+                                             :status                   :SUCCEEDED}))\n     (catch Exception e\n       (log/error \"Exception when handling parse-bayes-factors\" {:error e})\n-      (bayes-factor-model/update! db {:id     id\n-                                      :status :ERROR}))))\n+      (bayes-factor-model/upsert-status! db {:bayes-factor-analysis-id id\n+                                             :status                   :ERROR}))))\n \n (defn start [{:keys [aws db] :as config}]\n   (let [{:keys [workers-queue-url bucket-name]} aws\n-        sqs (aws-sqs/create-client aws)\n-        s3 (aws-s3/create-client aws)\n-        db (db/init db)\n-        context {:s3 s3\n-                 :db db\n-                 :bucket-name bucket-name\n-                 :aws-config aws}]\n+        sqs                                     (aws-sqs/create-client aws)\n+        s3                                      (aws-s3/create-client aws)\n+        db                                      (db/init db)\n+        context                                 {:s3          s3\n+                                                 :db          db\n+                                                 :bucket-name bucket-name\n+                                                 :aws-config  aws}]\n     (log/info \"Starting worker listener\" config)\n     (loop []\n       (try"
  },
  {
    "sha": "bc6b17b4b45d1534499a5fdf514849f12a6433aa",
    "filename": "src/main/java/com/spread/parsers/BayesFactorParser.java",
    "status": "modified",
    "additions": 48,
    "deletions": 27,
    "changes": 75,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/src/main/java/com/spread/parsers/BayesFactorParser.java",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/src/main/java/com/spread/parsers/BayesFactorParser.java",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/src/main/java/com/spread/parsers/BayesFactorParser.java?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -17,14 +17,16 @@\n import com.spread.data.attributable.Point;\n import com.spread.data.primitive.Coordinate;\n import com.spread.exceptions.SpreadException;\n+import com.spread.progress.IProgressObserver;\n+import com.spread.progress.IProgressReporter;\n import com.spread.utils.ParsersUtils;\n \n import lombok.Setter;\n import lombok.Getter;\n import lombok.ToString;\n import lombok.EqualsAndHashCode;\n \n-public class BayesFactorParser {\n+public class BayesFactorParser implements IProgressReporter {\n \n     public static final String BAYES_FACTOR = \"bayesFactor\";\n     public static final String POSTERIOR_PROBABILITY = \"posteriorProbability\";\n@@ -38,8 +40,7 @@\n     @Setter\n     private Integer numberOfLocations;\n \n-    private final Double poissonPriorMean = Math.log(2);;\n-    private Integer poissonPriorOffset;\n+    private IProgressObserver progressObserver;\n \n     public class BayesFactorParserOutput {\n         public LinkedList<BayesFactor> bayesFactors;\n@@ -50,7 +51,6 @@\n             this.bayesFactors = bayesFactors;\n             this.spreadData = spreadData;\n         }\n-\n     }\n \n     @EqualsAndHashCode\n@@ -98,11 +98,13 @@ public BayesFactorParser(String logFilePath,\n \n     public String parse() throws IOException, SpreadException {\n \n-        Double[][] indicators = new LogParser(this.logFilePath, this.burnIn).parseIndicators();\n-\n-        System.out.println(\"Imported log file\");\n+        double progress = 0;\n+        double progressStepSize = 0;\n+        this.updateProgress(progress);\n \n+        Double[][] indicators = new LogParser(this.logFilePath, this.burnIn).parseIndicators();\n         LinkedList<Location> locationsList = null;\n+\n         if (this.locationsFilePath != null) {\n             locationsList = new DiscreteLocationsParser(this.locationsFilePath, false)\n                 .parseLocations();\n@@ -113,7 +115,8 @@ public String parse() throws IOException, SpreadException {\n         }\n \n         int numberOfLocations = locationsList.size();\n-        this.poissonPriorOffset = numberOfLocations - 1;\n+        int poissonPriorOffset = numberOfLocations - 1;\n+        double poissonPriorMean = Math.log(2);\n         int nrow = indicators.length;\n         int ncol = indicators[0].length;\n \n@@ -139,24 +142,27 @@ public String parse() throws IOException, SpreadException {\n \n         double priorOdds = qk / (1 - qk);\n         double[] pk = getColumnMeans(indicators);\n+        this.updateProgress(0.1);\n \n         LinkedList<Double> bayesFactors = new LinkedList<Double>();\n         LinkedList<Double> posteriorProbabilities = new LinkedList<Double>();\n+\n+        progressStepSize = 0.2 / (double) pk.length;\n         for (int row = 0; row < pk.length; row++) {\n \n-            double bf = (pk[row] / (1 - pk[row])) / priorOdds;\n+            progress += progressStepSize;\n+            this.updateProgress(progress);\n \n+            double bf = (pk[row] / (1 - pk[row])) / priorOdds;\n+            // correcting for infinite bf\n             if (bf == Double.POSITIVE_INFINITY) {\n                 bf = ((pk[row] - (double) (1.0 / nrow)) / (1 - (pk[row] - (double) (1.0 / nrow)))) / priorOdds;\n-                System.out.println(\"Correcting for infinite bf: \" + bf);\n             }\n \n             bayesFactors.add(bf);\n             posteriorProbabilities.add(pk[row]);\n         }\n \n-        System.out.println(\"Calculated Bayes Factors\");\n-\n         LinkedList<String> from = new LinkedList<String>();\n         LinkedList<String> to = new LinkedList<String>();\n \n@@ -181,13 +187,14 @@ public String parse() throws IOException, SpreadException {\n         }\n \n         HashMap<Location, Point> pointsMap = new HashMap<Location, Point>();\n-\n-        // Location dummy;\n         LinkedList<Line> linesList = new LinkedList<Line>();\n+        progressStepSize = 0.2 / (double) bayesFactors.size();\n         for (int i = 0; i < bayesFactors.size(); i++) {\n \n-            // from is parsed first\n+            progress += progressStepSize;\n+            this.updateProgress(progress);\n \n+            // from is parsed first\n             Location dummy = new Location(from.get(i));\n             int fromLocationIndex = Integer.MAX_VALUE;\n             if (locationsList.contains(dummy)) {\n@@ -205,7 +212,6 @@ public String parse() throws IOException, SpreadException {\n             }\n \n             // to is parsed second\n-\n             dummy = new Location(to.get(i));\n             int toLocationIndex = Integer.MAX_VALUE;\n             if (locationsList.contains(dummy)) {\n@@ -254,13 +260,13 @@ public String parse() throws IOException, SpreadException {\n \n         LinkedList<Point> pointsList = new LinkedList<Point>(pointsMap.values());\n \n-        System.out.println(\"Parsed points and lines\");\n-\n         // collect attributes from lines\n         HashMap<String, Attribute> branchAttributesMap = new HashMap<String, Attribute>();\n-\n+        progressStepSize = 0.2 / (double) linesList.size();\n         for (Line line : linesList) {\n \n+            progress += progressStepSize;\n+            this.updateProgress(progress);\n             for (Entry<String, Object> entry : line.getAttributes().entrySet()) {\n \n                 String attributeId = entry.getKey();\n@@ -306,12 +312,11 @@ public String parse() throws IOException, SpreadException {\n                     } // END: isNumeric check\n \n                     branchAttributesMap.put(attributeId, attribute);\n-                } // END: key check\n-            } // END: attributes loop\n-\n-        } // END: lines loop\n+                }\n+            }\n+        }\n \n-        LinkedList<Attribute> uniqueLineAttributes = new LinkedList<Attribute> (branchAttributesMap.values());\n+        LinkedList<Attribute> uniqueLineAttributes = new LinkedList<Attribute>(branchAttributesMap.values());\n \n         LinkedList<Attribute> rangeAttributes = getCoordinateRangeAttributes(locationsList);\n         Attribute xCoordinate = rangeAttributes.get(ParsersUtils.X_INDEX);\n@@ -340,17 +345,21 @@ public String parse() throws IOException, SpreadException {\n                                                layersList);\n \n         LinkedList<BayesFactor> bayesFactorsData = new LinkedList<BayesFactor>();\n+        progressStepSize = 0.2 / (double) bayesFactors.size();\n         for (int i = 0; i < bayesFactors.size(); i++) {\n+            progress += progressStepSize;\n+            this.updateProgress(progress);\n             bayesFactorsData.add(new BayesFactor(from.get(i),\n                                                  to.get(i),\n                                                  bayesFactors.get(i),\n                                                  posteriorProbabilities.get(i)));\n         }\n \n-        BayesFactorParserOutput out = new BayesFactorParserOutput (bayesFactorsData, spreadData);\n-        String json = new GsonBuilder().create().toJson(out);\n+        this.updateProgress(1.0);\n \n-        return json;\n+        return new GsonBuilder()\n+            .create()\n+            .toJson(new BayesFactorParserOutput (bayesFactorsData, spreadData));\n     }\n \n     private LinkedList<Attribute> getCoordinateRangeAttributes(LinkedList<Location> locationsList) throws SpreadException {\n@@ -464,4 +473,16 @@ private double getColumnMean(Double a[][], int col) {\n         return locationsList;\n     }\n \n+    @Override\n+    public void registerProgressObserver(IProgressObserver observer) {\n+        this.progressObserver = observer;\n+    }\n+\n+    @Override\n+    public void updateProgress(double progress) {\n+        if (this.progressObserver != null) {\n+            this.progressObserver.handleProgress(progress);\n+        }\n+    }\n+\n }"
  },
  {
    "sha": "d3460ad1e3e5bd6a35e559edbeb94c62b577cdc7",
    "filename": "src/main/java/com/spread/parsers/ContinuousTreeParser.java",
    "status": "modified",
    "additions": 69,
    "deletions": 65,
    "changes": 134,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/src/main/java/com/spread/parsers/ContinuousTreeParser.java",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/src/main/java/com/spread/parsers/ContinuousTreeParser.java",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/src/main/java/com/spread/parsers/ContinuousTreeParser.java?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -24,6 +24,8 @@\n import com.spread.data.primitive.Coordinate;\n import com.spread.data.primitive.Polygon;\n import com.spread.exceptions.SpreadException;\n+import com.spread.progress.IProgressObserver;\n+import com.spread.progress.IProgressReporter;\n import com.spread.utils.ParsersUtils;\n \n import jebl.evolution.graphs.Node;\n@@ -33,7 +35,7 @@\n import lombok.Setter;\n import lombok.experimental.Accessors;\n \n-public class ContinuousTreeParser {\n+public class ContinuousTreeParser implements IProgressReporter {\n \n     @Getter @Setter\n     private String treeFilePath;\n@@ -50,6 +52,7 @@\n     private double timescaleMultiplier;\n     @Getter @Setter\n     private String mostRecentSamplingDate;\n+    private IProgressObserver progressObserver;\n \n     public ContinuousTreeParser() {\n     }\n@@ -73,6 +76,10 @@ public ContinuousTreeParser(String treeFilePath,\n \n     public String parse() throws IOException, ImportException, SpreadException {\n \n+        double progress = 0;\n+        double progressStepSize = 0;\n+        this.updateProgress(progress);\n+\n         RootedTree rootedTree = ParsersUtils.importRootedTree(treeFilePath);\n         TimeParser timeParser = new TimeParser(this.getMostRecentSamplingDate());\n         TimeLine timeLine = timeParser.getTimeLine(rootedTree.getHeight(rootedTree.getRootNode()));\n@@ -91,18 +98,15 @@ public String parse() throws IOException, ImportException, SpreadException {\n \n         // remove digits to get name\n         String prefix = xCoordinateAttributeName.replaceAll(\"\\\\d*$\", \"\");\n+        String modalityAttributeName = prefix.concat(\"_\").concat(hpd).concat(\"%\").concat(\"HPD_modality\");\n \n-        String modalityAttributeName = \"\";\n-\n-        try {\n+        // progress = 0;\n+        progressStepSize = 0.25 / (double) rootedTree.getNodes().size();\n+        for (Node node : rootedTree.getNodes()) {\n \n-            modalityAttributeName = prefix.concat(\"_\").concat(hpd).concat(\"%\").concat(\"HPD_modality\");\n+            progress += progressStepSize;\n+            this.updateProgress(progress);\n \n-        } catch (Exception e) {\n-            throw new SpreadException(\"Trouble creating HPD modality attribute name. I suspect this is not a continuously annotated tree.\");\n-        }\n-\n-        for (Node node : rootedTree.getNodes()) {\n             if (!rootedTree.isRoot(node)) {\n \n                 // node parsed first\n@@ -126,7 +130,7 @@ public String parse() throws IOException, ImportException, SpreadException {\n                         + \" child node. Resulting visualisation may be incomplete!\";\n                     System.out.println (message);\n                     continue;\n-                } // END: try-catch\n+                }\n \n                 nodeCoordinate = new Coordinate(nodeCoordinateY, // latitude\n                                                 nodeCoordinateX // longitude\n@@ -138,14 +142,12 @@ public String parse() throws IOException, ImportException, SpreadException {\n                 if (nodePoint == null) {\n                     nodePoint = createPoint(node, nodeCoordinate, rootedTree, timeParser);\n                     pointsMap.put(node, nodePoint);\n-                } // END: null check\n+                }\n \n                 // parent node parsed second\n \n-                // this spills to the root node, resulting in exception\n-                // if not anotated\n-                // root node will be annotated with locations but not with e.g.\n-                // rate (facepalm)\n+                // this spills to the root node, resulting in exception if not anotated\n+                // root node will be annotated with locations but not with e.g. rate (facepalm)\n                 Node parentNode = rootedTree.getParent(node);\n \n                 Double parentCoordinateX = null;\n@@ -166,7 +168,7 @@ public String parse() throws IOException, ImportException, SpreadException {\n                         + \" parent node. Resulting visualisation may be incomplete!\";\n                     System.out.println (message);\n                     continue;\n-                } // END: try-catch\n+                }\n \n                 Coordinate parentCoordinate = new Coordinate(parentCoordinateY, // lat\n                                                              parentCoordinateX // long\n@@ -175,16 +177,15 @@ public String parse() throws IOException, ImportException, SpreadException {\n                 if (parentPoint == null) {\n                     parentPoint = createPoint(parentNode, parentCoordinate, rootedTree, timeParser);\n                     pointsMap.put(parentNode, parentPoint);\n-                } // END: null check\n+                }\n \n                 // ---LINES PARSED SECOND DO NOT CHANGE ORDER---//\n \n                 Line line = new Line(parentPoint.getId(), //\n                                      nodePoint.getId(), //\n                                      parentPoint.getStartTime(), //\n                                      nodePoint.getStartTime(), //\n-                                     nodePoint.getAttributes() //\n-                                     );\n+                                     nodePoint.getAttributes());\n \n                 linesList.add(line);\n \n@@ -199,7 +200,7 @@ public String parse() throws IOException, ImportException, SpreadException {\n \n                 } else {\n                     parseNode = true;\n-                } // END: parse logic\n+                }\n \n                 if (parseNode) {\n \n@@ -210,10 +211,10 @@ public String parse() throws IOException, ImportException, SpreadException {\n                         modality = (Integer) ParsersUtils.getObjectNodeAttribute(node, modalityAttributeName);\n \n                     } catch (SpreadException e) {\n-                        String nodeType = (rootedTree.isExternal(node) ? \"external\" : \"internal\");\n-                        String message = modalityAttributeName + \" attribute could not be found on the \" + nodeType\n-                            + \" node. Resulting visualisation may be incomplete!\";\n-                        System.out.println (message);\n+                        // String nodeType = (rootedTree.isExternal(node) ? \"external\" : \"internal\");\n+                        // String message = modalityAttributeName + \" attribute could not be found on the \" + nodeType\n+                        //     + \" node. Resulting visualisation may be incomplete!\";\n+                        // System.out.println (message);\n                         continue;\n                     }\n \n@@ -245,23 +246,18 @@ public String parse() throws IOException, ImportException, SpreadException {\n                                 + \" attribute could not be found on the child node. Resulting visualisation may be incomplete!\";\n                             System.out.println (message);\n                             continue;\n-                        } // END: try-catch\n+                        }\n \n                         List<Coordinate> coordinateList = new ArrayList<Coordinate>();\n                         for (int c = 0; c < xCoordinateHPD.length; c++) {\n-\n                             Double xCoordinate = (Double) xCoordinateHPD[c];\n                             Double yCoordinate = (Double) yCoordinateHPD[c];\n \n-                            Coordinate coordinate = new Coordinate(\n-                                                                   // xCoordinate,\n-                                                                   // yCoordinate\n-                                                                   yCoordinate, // lat\n+                            Coordinate coordinate = new Coordinate(yCoordinate, // lat\n                                                                    xCoordinate // long\n                                                                    );\n                             coordinateList.add(coordinate);\n-\n-                        } // END: c loop\n+                        }\n \n                         Polygon polygon = new Polygon(coordinateList);\n \n@@ -272,9 +268,9 @@ public String parse() throws IOException, ImportException, SpreadException {\n                         Area area = new Area(polygon, nodePoint.getStartTime(), areaAttributesMap);\n                         areasList.add(area);\n \n-                    } // END: modality loop\n+                    }\n \n-                } // parse check\n+                }\n \n             } else {\n \n@@ -296,7 +292,7 @@ public String parse() throws IOException, ImportException, SpreadException {\n                         + \"Resulting visualisation may be incomplete!\";\n                     System.out.println (message);\n                     continue;\n-                } // END: try-catch\n+                }\n \n                 Coordinate rootCoordinate = new Coordinate(rootCoordinateY, // lat\n                                                            rootCoordinateX // long\n@@ -305,17 +301,21 @@ public String parse() throws IOException, ImportException, SpreadException {\n                 Point rootPoint = createPoint(node, rootCoordinate, rootedTree, timeParser);\n                 pointsMap.put(node, rootPoint);\n \n-            } // END: root check\n-        } // END: nodes loop\n+            }\n+        }\n \n         pointsList.addAll(pointsMap.values());\n \n         // ---collect attributes from lines---//\n \n         Map<String, Attribute> branchAttributesMap = new HashMap<String, Attribute>();\n \n+        progressStepSize = 0.25 / (double) linesList.size();\n         for (Line line : linesList) {\n \n+            progress += progressStepSize;\n+            this.updateProgress(progress);\n+\n             for (Entry<String, Object> entry : line.getAttributes().entrySet()) {\n \n                 String attributeId = entry.getKey();\n@@ -368,7 +368,6 @@ public String parse() throws IOException, ImportException, SpreadException {\n                 } // END: key check\n \n             } // END: attributes loop\n-\n         } // END: lines loop\n \n         uniqueBranchAttributes.addAll(branchAttributesMap.values());\n@@ -377,8 +376,12 @@ public String parse() throws IOException, ImportException, SpreadException {\n \n         Map<String, Attribute> nodeAttributesMap = new HashMap<String, Attribute>();\n \n+        progressStepSize = 0.25 / (double) pointsList.size();\n         for (Point point : pointsList) {\n \n+            progress += progressStepSize;\n+            this.updateProgress(progress);\n+\n             for (Entry<String, Object> entry : point.getAttributes().entrySet()) {\n \n                 String attributeId = entry.getKey();\n@@ -440,8 +443,12 @@ public String parse() throws IOException, ImportException, SpreadException {\n \n         Map<String, Attribute> areasAttributesMap = new HashMap<String, Attribute>();\n \n+        progressStepSize = 0.24 / (double) areasList.size();\n         for (Area area : areasList) {\n \n+            progress += progressStepSize;\n+            this.updateProgress(progress);\n+\n             for (Entry<String, Object> entry : area.getAttributes().entrySet()) {\n \n                 String attributeId = entry.getKey();\n@@ -495,7 +502,7 @@ public String parse() throws IOException, ImportException, SpreadException {\n \n             } // END: attributes loop\n \n-        } // END: points loop\n+        } // END: areas loop\n \n         uniqueAreaAttributes.addAll(areasAttributesMap.values());\n \n@@ -506,13 +513,6 @@ public String parse() throws IOException, ImportException, SpreadException {\n \n         // --- DATA LAYER (TREE LINES & POINTS, AREAS) --- //\n \n-        // String treeLayerId = ParsersUtils.splitString(this.treeFilePath, \"/\");\n-        // Layer treeLayer = new Layer(treeLayerId, //\n-        //                             \"Tree layer\", //\n-        //                             pointsList, //\n-        //                             linesList, //\n-        //                             areasList);\n-\n         Layer treeLayer = new Layer.Builder ()\n             .withPoints (pointsList)\n             .withLines (linesList)\n@@ -529,6 +529,7 @@ public String parse() throws IOException, ImportException, SpreadException {\n                                                null, // locations\n                                                layersList);\n \n+        this.updateProgress(1.0);\n         return new GsonBuilder().create().toJson(spreadData);\n     }\n \n@@ -538,22 +539,20 @@ public String parseAttributesAndHpdLevels() throws IOException, ImportException\n         RootedTree tree = ParsersUtils.importRootedTree(this.treeFilePath);\n \n         Set<String> uniqueAttributes = tree.getNodes().stream().filter(node -> !tree.isRoot(node))\n-            .flatMap(node -> node.getAttributeNames().stream()).map(name -> {\n-                    return name;\n-                }).collect(Collectors.toSet());\n+            .flatMap(node -> node.getAttributeNames().stream())\n+            .map(name -> name)\n+            .collect(Collectors.toSet());\n \n         Set<String> hpdLevels = uniqueAttributes.stream().filter(attributeName -> attributeName.contains(\"HPD_modality\"))\n-            .map(hpdString -> {\n-                    return hpdString.replaceAll(\"\\\\D+\", \"\");\n-                })\n+            .map(hpdString -> hpdString.replaceAll(\"\\\\D+\", \"\"))\n             .collect(Collectors.toSet());\n \n         Object pair = new Object[] {uniqueAttributes, hpdLevels};\n-\n         return new GsonBuilder().create().toJson(pair);\n     }\n \n-    private Point createPoint(Node node, Coordinate coordinate, RootedTree rootedTree, TimeParser timeParser) throws SpreadException {\n+    private Point createPoint(Node node, Coordinate coordinate, RootedTree rootedTree, TimeParser timeParser)\n+        throws SpreadException {\n \n         Double height = ParsersUtils.getNodeHeight(rootedTree, node) * this.timescaleMultiplier;\n         String startTime = timeParser.getNodeDate(height);\n@@ -562,13 +561,9 @@ private Point createPoint(Node node, Coordinate coordinate, RootedTree rootedTre\n         for (String attributeName : node.getAttributeNames()) {\n \n             Object nodeAttribute = node.getAttribute(attributeName);\n-\n             if (!(nodeAttribute instanceof Object[])) {\n-\n                 // remove invalid characters\n-                attributeName = attributeName.replaceAll(\"%\", \"\");\n-                attributeName = attributeName.replaceAll(\"!\", \"\");\n-\n+                attributeName = attributeName.replaceAll(\"%\", \"\").replaceAll(\"!\", \"\");\n                 attributes.put(attributeName, nodeAttribute);\n             } // END: multivariate check\n \n@@ -584,17 +579,26 @@ private Point createPoint(Node node, Coordinate coordinate, RootedTree rootedTre\n             value = \"internal\";\n         }\n \n-        String attributeName = \"nodeName\";\n-        attributes.put(attributeName, value);\n+        attributes.put(\"nodeName\", value);\n \n         // external nodes have no posterior annotated, need to fix that\n         if (rootedTree.isExternal(node)) {\n             attributes.put(ParsersUtils.POSTERIOR, 1.0);\n         }\n \n-        Point point = new Point(coordinate, startTime, attributes);\n+        return new Point(coordinate, startTime, attributes);\n+    }\n+    \n+    @Override\n+    public void registerProgressObserver(IProgressObserver observer) {\n+        this.progressObserver = observer;\n+    }\n \n-        return point;\n-    }// END: createPoint\n+    @Override\n+    public void updateProgress(double progress) {\n+        if (this.progressObserver != null) {\n+            this.progressObserver.handleProgress(progress);\n+        }\n+    }\n \n }"
  },
  {
    "sha": "b9bd7fa835c93278cec5d9a82c4b0b90ce767b40",
    "filename": "src/main/java/com/spread/parsers/DiscreteTreeParser.java",
    "status": "modified",
    "additions": 52,
    "deletions": 45,
    "changes": 97,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/src/main/java/com/spread/parsers/DiscreteTreeParser.java",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/src/main/java/com/spread/parsers/DiscreteTreeParser.java",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/src/main/java/com/spread/parsers/DiscreteTreeParser.java?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -21,18 +21,16 @@\n import com.spread.data.attributable.Point;\n import com.spread.data.primitive.Coordinate;\n import com.spread.exceptions.SpreadException;\n+import com.spread.progress.IProgressObserver;\n+import com.spread.progress.IProgressReporter;\n import com.spread.utils.ParsersUtils;\n \n import jebl.evolution.graphs.Node;\n import jebl.evolution.io.ImportException;\n import jebl.evolution.trees.RootedTree;\n-import lombok.Getter;\n import lombok.Setter;\n \n-public class DiscreteTreeParser {\n-\n-    // private static final String X_COORDINATE = \"xCoordinate\";\n-    // private static final String Y_COORDINATE = \"yCoordinate\";\n+public class DiscreteTreeParser implements IProgressReporter {\n \n     public static final String COUNT = \"count\";\n     private static final Integer UNRESOLVED_INDEX = Integer.MAX_VALUE;\n@@ -47,6 +45,7 @@\n     private double timescaleMultiplier;\n     @Setter\n     private String mostRecentSamplingDate;\n+    private IProgressObserver progressObserver;\n \n     public DiscreteTreeParser() {\n     }\n@@ -55,17 +54,20 @@ public DiscreteTreeParser(String treeFilePath,\n                               String locationsFilePath,\n                               String locationTraitAttributeName,\n                               double timescaleMultiplier,\n-                              String mostRecentSamplingDate\n-                              ) {\n+                              String mostRecentSamplingDate) {\n         this.treeFilePath = treeFilePath;\n         this.locationsFilePath = locationsFilePath;\n         this.locationTraitAttributeName = locationTraitAttributeName;\n         this.timescaleMultiplier = timescaleMultiplier;\n         this.mostRecentSamplingDate = mostRecentSamplingDate;\n-    }// END: Constructor\n+    }\n \n     public String parse() throws IOException, ImportException, SpreadException {\n \n+        double progress = 0;\n+        double progressStepSize = 0;\n+        this.updateProgress(progress);\n+\n         RootedTree rootedTree = ParsersUtils.importRootedTree(this.treeFilePath);\n         TimeParser timeParser = new TimeParser(this.mostRecentSamplingDate);\n         TimeLine timeLine = timeParser.getTimeLine(rootedTree.getHeight(rootedTree.getRootNode()));\n@@ -86,9 +88,13 @@ public String parse() throws IOException, ImportException, SpreadException {\n         int[][] locationCounts = new int[sliceHeights.length][locationsList.size()];\n \n         Location dummy;\n+        progressStepSize = 0.25 / (double) rootedTree.getNodes().size();\n         for (Node node : rootedTree.getNodes()) {\n             if (!rootedTree.isRoot(node)) {\n \n+                progress += progressStepSize;\n+                this.updateProgress(progress);\n+\n                 // node parsed first\n                 String nodeState = getNodeState(node, this.locationTraitAttributeName);\n \n@@ -162,24 +168,18 @@ public String parse() throws IOException, ImportException, SpreadException {\n                                 && (rootedTree.getHeight(parentNode) > sliceHeight)) {\n \n                                 if (nodeLocation.equals(parentLocation) && parentLocation.equals(location)) {\n-\n                                     int j = locationsList.lastIndexOf(location);\n                                     locationCounts[i][j]++;\n+                                }\n \n-                                } // END: location check\n-\n-                            } // END:\n-                        } // END: locations loop\n-                    } // END: sliceHeights lop\n-\n-                } // END: state check\n+                            }\n+                        }\n+                    }\n+                }\n \n             } else {\n \n-                System.out.println(\"At the root node\");\n-\n                 String rootState = getNodeState(node, this.locationTraitAttributeName);\n-\n                 dummy = new Location(rootState);\n                 int locationIndex = UNRESOLVED_INDEX;\n                 if (locationsList.contains(dummy)) {\n@@ -196,20 +196,22 @@ public String parse() throws IOException, ImportException, SpreadException {\n                 Location location = locationsList.get(locationIndex);\n                 Point rootPoint = createPoint(node, location, rootedTree, timeParser);\n                 pointsMap.put(node, rootPoint);\n-\n-            } // END: root check\n-        } // END: node loop\n+            }\n+        }\n \n         pointsList.addAll(pointsMap.values());\n \n         // create Points list with count attributes\n-\n         Double[] countRange = new Double[2];\n         countRange[Attribute.MIN_INDEX] = Double.MAX_VALUE;\n         countRange[Attribute.MAX_INDEX] = Double.MIN_VALUE;\n \n+        progressStepSize = 0.25 / (double) locationCounts.length;\n         for (int sliceIndex = 0; sliceIndex < locationCounts.length; sliceIndex++) {\n \n+            progress += progressStepSize;\n+            this.updateProgress(progress);\n+\n             double height = sliceHeights[sliceIndex];\n             double nextHeight = sliceIndex < locationCounts.length - 1 ? sliceHeights[sliceIndex + 1] : 0.0;\n \n@@ -245,9 +247,11 @@ public String parse() throws IOException, ImportException, SpreadException {\n \n         // collect attributes from lines\n         Map<String, Attribute> branchAttributesMap = new HashMap<String, Attribute>();\n-\n+        progressStepSize = 0.25 / (double) linesList.size();\n         for (Line line : linesList) {\n \n+            progress += progressStepSize;\n+            this.updateProgress(progress);\n             for (Entry<String, Object> entry : line.getAttributes().entrySet()) {\n \n                 String attributeId = entry.getKey();\n@@ -279,37 +283,30 @@ public String parse() throws IOException, ImportException, SpreadException {\n \n                     Attribute attribute;\n                     if (attributeValue instanceof Double) {\n-\n                         Double[] range = new Double[2];\n                         range[Attribute.MIN_INDEX] = (Double) attributeValue;\n                         range[Attribute.MAX_INDEX] = (Double) attributeValue;\n-\n                         attribute = new Attribute(attributeId, range);\n-\n                     } else {\n-\n                         HashSet<Object> domain = new HashSet<Object>();\n                         domain.add(attributeValue);\n-\n                         attribute = new Attribute(attributeId, domain);\n-\n                     } // END: isNumeric check\n \n                     branchAttributesMap.put(attributeId, attribute);\n-\n-                } // END: key check\n-\n-            } // END: attributes loop\n-\n-        } // END: lines loop\n+                }\n+            }\n+        }\n \n         uniqueBranchAttributes.addAll(branchAttributesMap.values());\n \n         // collect attributes from nodes\n         Map<String, Attribute> nodeAttributesMap = new HashMap<String, Attribute>();\n-\n+        progressStepSize = 0.24 / (double) pointsList.size();\n         for (Point point : pointsList) {\n \n+            progress += progressStepSize;\n+            this.updateProgress(progress);\n             for (Entry<String, Object> entry : point.getAttributes().entrySet()) {\n \n                 String attributeId = entry.getKey();\n@@ -359,11 +356,9 @@ public String parse() throws IOException, ImportException, SpreadException {\n \n                     nodeAttributesMap.put(attributeId, attribute);\n \n-                } // END: key check\n-\n-            } // END: attributes loop\n-\n-        } // END: points loop\n+                }\n+            }\n+        }\n \n         uniqueNodeAttributes.addAll(branchAttributesMap.values());\n         // we dump it here with node attributes\n@@ -399,9 +394,9 @@ public String parse() throws IOException, ImportException, SpreadException {\n                                               uniqueNodeAttributes, //\n                                               null, // areaAttributes\n                                               locationsList, //\n-                                              layersList //\n-                                              );\n+                                              layersList);\n \n+        this.updateProgress(1.0);\n         return new GsonBuilder().create().toJson(spreadData);\n     }// END: parse\n \n@@ -540,4 +535,16 @@ public String parseAttributes() throws IOException, ImportException  {\n         return new GsonBuilder().create().toJson(uniqueAttributes);\n     }\n \n-}// END: class\n+    @Override\n+    public void registerProgressObserver(IProgressObserver observer) {\n+        this.progressObserver = observer;\n+    }\n+\n+    @Override\n+    public void updateProgress(double progress) {\n+        if (this.progressObserver != null) {\n+            this.progressObserver.handleProgress(progress);\n+        }\n+    }\n+\n+}"
  },
  {
    "sha": "e03152b16ee190584fa6b48b0add24c80a456af9",
    "filename": "src/main/java/com/spread/parsers/TimeSlicerParser.java",
    "status": "modified",
    "additions": 47,
    "deletions": 107,
    "changes": 154,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/src/main/java/com/spread/parsers/TimeSlicerParser.java",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/src/main/java/com/spread/parsers/TimeSlicerParser.java",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/src/main/java/com/spread/parsers/TimeSlicerParser.java?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -2,7 +2,6 @@\n \n import java.io.BufferedInputStream;\n import java.io.FileInputStream;\n-import java.io.FileNotFoundException;\n import java.io.FileReader;\n import java.io.IOException;\n import java.io.InputStream;\n@@ -29,46 +28,38 @@\n import com.spread.data.primitive.Coordinate;\n import com.spread.data.primitive.Polygon;\n import com.spread.exceptions.SpreadException;\n+import com.spread.progress.IProgressObserver;\n+import com.spread.progress.IProgressReporter;\n import com.spread.utils.ParsersUtils;\n-import com.spread.utils.PrintUtils;\n-import com.spread.utils.ProgressBar;\n \n import jebl.evolution.io.ImportException;\n import jebl.evolution.io.NexusImporter;\n import jebl.evolution.trees.RootedTree;\n import lombok.Setter;\n \n-public class TimeSlicerParser {\n+public class TimeSlicerParser implements IProgressReporter {\n \n     @Setter\n     private String treesFilePath;\n-\n     @Setter\n     private String sliceHeightsFilePath;\n-\n     @Setter\n     private int numberOfIntervals;\n-\n     @Setter\n     private int burnIn;\n-\n     @Setter\n     private String traitName;\n-\n     @Setter\n     private String rrwRateName;\n-\n     @Setter\n     private double hpdLevel;\n-\n     @Setter\n     private int gridSize;\n-\n     @Setter\n     private String mostRecentSamplingDate;\n-\n     @Setter\n     private double timescaleMultiplier;\n+    private IProgressObserver progressObserver;\n \n     public TimeSlicerParser() {\n     }\n@@ -81,8 +72,7 @@ public TimeSlicerParser(String treesFilePath, // path to the trees file\n                             double hpdLevel,\n                             int gridSize,\n                             String mostRecentSamplingDate,\n-                            double timescaleMultiplier\n-                            ) {\n+                            double timescaleMultiplier) {\n         this.treesFilePath = treesFilePath;\n         this.sliceHeightsFilePath = sliceHeightsFilePath;\n         this.burnIn = burnIn;\n@@ -102,8 +92,7 @@ public TimeSlicerParser(String treesFilePath,\n                             double hpdLevel,\n                             int gridSize,\n                             String mostRecentSamplingDate,\n-                            double timescaleMultiplier\n-                            ) {\n+                            double timescaleMultiplier) {\n         this.treesFilePath = treesFilePath;\n         this.burnIn = burnIn;\n         this.numberOfIntervals = numberOfIntervals;\n@@ -117,11 +106,11 @@ public TimeSlicerParser(String treesFilePath,\n \n     public String parse() throws IOException, ImportException, SpreadException {\n \n-        // ---parse trees---//\n+        double progress = 0;\n+        double progressStepSize = 0;\n+        this.updateProgress(progress);\n \n-        int barLength = 100;\n-        int assumedTrees = getAssumedTrees(this.treesFilePath);\n-        double stepSize = (double) barLength / (double) assumedTrees;\n+        // ---parse trees---//\n \n         Double sliceHeights[] = null;\n         if (this.sliceHeightsFilePath == null) {\n@@ -130,83 +119,47 @@ public String parse() throws IOException, ImportException, SpreadException {\n             SliceHeightsParser sliceHeightsParser = new SliceHeightsParser(this.sliceHeightsFilePath);\n             sliceHeights = sliceHeightsParser.parseSliceHeights();\n         }\n-\n         // sort them in ascending order\n         Arrays.sort(sliceHeights);\n \n-        System.out.println(\"Using as slice heights: \");\n-        PrintUtils.printArray(sliceHeights);\n-\n-        System.out.println(\"Reading trees (bar assumes \" + assumedTrees + \" trees)\");\n-\n-        ProgressBar progressBar = new ProgressBar(barLength);\n-        progressBar.start();\n-\n-        System.out.println(\"0                        25                       50                       75                       100%\");\n-        System.out.println(\"|------------------------|------------------------|------------------------|------------------------|\");\n-\n         NexusImporter treesImporter = new NexusImporter(new FileReader(this.treesFilePath));\n         HashMap<Double, List<double[]>> slicesMap = new HashMap<Double, List<double[]>>(sliceHeights.length);\n-\n         RootedTree currentTree;\n-        int treesRead = 0;\n-        int counter = 0;\n+        int treesReadCounter = 0;\n+        progressStepSize = 0.33 / (double) getTreesCount(this.treesFilePath);\n \n         while (treesImporter.hasTree()) {\n-\n             currentTree = (RootedTree) treesImporter.importNextTree();\n-            if (counter >= burnIn) {\n+            if (treesReadCounter >= burnIn) {\n                 new TimeSliceTree(slicesMap, //\n                                   currentTree, //\n                                   sliceHeights, //\n                                   this.traitName, //\n-                                  this.rrwRateName //\n-                                  ).call();\n-\n-                treesRead++;\n+                                  this.rrwRateName).call();\n             } // END: burnin check\n-\n-            counter++;\n-            double progress = (stepSize * counter) / barLength;\n-            progressBar.setProgressPercentage(progress);\n+            treesReadCounter++;\n+            progress += progressStepSize;\n+            this.updateProgress(progress);\n         }\n \n-        progressBar.showCompleted();\n-        progressBar.setShowProgress(false);\n-\n-        System.out.print(\"\\n\");\n-        System.out.println(\"Analyzed \" + treesRead + \" trees with burn-in of \" + burnIn + \" for the total of \" + counter + \" trees\");\n-\n         // --- make contours ---//\n \n-        System.out.println(\"Creating contours for \" + traitName + \" trait at \" + hpdLevel + \" HPD level\");\n-        System.out.println(\"0                        25                       50                       75                       100%\");\n-        System.out.println(\"|------------------------|------------------------|------------------------|------------------------|\");\n-\n-        counter = 0;\n-        stepSize = (double) barLength / (double) slicesMap.size();\n-\n-        progressBar = new ProgressBar(barLength);\n-        progressBar.start();\n-\n         TimeParser timeParser = new TimeParser(this.mostRecentSamplingDate);\n         LinkedList<Area> areasList = new LinkedList<Area>();\n \n-        // TODO : multithreaded execution\n+        progressStepSize = 0.33 / (double) slicesMap.size();\n         for (Double sliceHeight : slicesMap.keySet()) {\n \n+            progress += progressStepSize;\n+            this.updateProgress(progress);\n+\n             List<double[]> coords = slicesMap.get(sliceHeight);\n             int n = coords.size();\n \n             double[] x = new double[n];\n             double[] y = new double[n];\n \n             for (int i = 0; i < n; i++) {\n-\n-                // if (coords.get(i) == null) {\n-                //     System.err.println(\"null found\");\n-                // }\n-\n                 x[i] = coords.get(i)[ParsersUtils.LONGITUDE_INDEX];\n                 y[i] = coords.get(i)[ParsersUtils.LATITUDE_INDEX];\n             } // END: i loop\n@@ -234,24 +187,18 @@ public String parse() throws IOException, ImportException, SpreadException {\n \n                 Area area = new Area(polygon, startTime, areaAttributesMap);\n                 areasList.add(area);\n-\n-            } // END: paths loop\n-\n-            counter++;\n-            double progress = (stepSize * counter) / barLength;\n-            progressBar.setProgressPercentage(progress);\n-        } // END: iterate\n-\n-        progressBar.showCompleted();\n-        progressBar.setShowProgress(false);\n-        System.out.print(\"\\n\");\n+            }\n+        }\n \n         // ---collect attributes from areas---//\n \n         Map<String, Attribute> areasAttributesMap = new HashMap<String, Attribute>();\n \n+        progressStepSize = 0.33 / (double) areasList.size();\n         for (Area area : areasList) {\n \n+            progress += progressStepSize;\n+            this.updateProgress(progress);\n             for (Entry<String, Object> entry : area.getAttributes().entrySet()) {\n \n                 String attributeId = entry.getKey();\n@@ -262,9 +209,7 @@ public String parse() throws IOException, ImportException, SpreadException {\n                     Attribute attribute = areasAttributesMap.get(attributeId);\n \n                     if (attribute.getScale().equals(Attribute.ORDINAL)) {\n-\n                         attribute.getDomain().add(attributeValue);\n-\n                     } else {\n \n                         double value = ParsersUtils\n@@ -294,16 +239,13 @@ public String parse() throws IOException, ImportException, SpreadException {\n \n                         HashSet<Object> domain = new HashSet<Object>();\n                         domain.add(attributeValue);\n-\n                         attribute = new Attribute(attributeId, domain);\n                     } // END: isNumeric check\n \n                     areasAttributesMap.put(attributeId, attribute);\n \n                 } // END: key check\n-\n             } // END: attributes loop\n-\n         } // END: points loop\n \n         LinkedList<Attribute> uniqueAreaAttributes = new LinkedList<Attribute>();\n@@ -312,15 +254,7 @@ public String parse() throws IOException, ImportException, SpreadException {\n         TimeLine timeLine = timeParser.getTimeLine(sliceHeights[sliceHeights.length - 1]);\n         LinkedList<Layer> layersList = new LinkedList<Layer>();\n \n-        // String contoursLayerId = ParsersUtils.splitString(this.treesFilePath, \"/\");\n         Layer contoursLayer = new Layer.Builder ().withAreas (areasList).build ();\n-\n-        // Layer contoursLayer = new Layer(contoursLayerId, //\n-        //                                 \"Density contour layer\", //\n-        //                                 // null, //\n-        //                                 // null, //\n-        //                                 areasList);\n-\n         layersList.add(contoursLayer);\n \n         SpreadData spreadData = new SpreadData(timeLine, //\n@@ -329,13 +263,12 @@ public String parse() throws IOException, ImportException, SpreadException {\n                                                null, // pointAttributes\n                                                uniqueAreaAttributes , // areaAttributes\n                                                null, // locationsList\n-                                               layersList //\n-                                               );\n-\n+                                               layersList);\n+        this.updateProgress(1.0);\n         return new GsonBuilder().create().toJson(spreadData);\n     }\n \n-    private int getAssumedTrees(String file) throws IOException {\n+    private int getTreesCount(String file) throws IOException {\n         InputStream is = new BufferedInputStream(new FileInputStream(file));\n         try {\n \n@@ -369,22 +302,20 @@ private int getAssumedTrees(String file) throws IOException {\n         }\n     }\n \n-    private Double[] generateSliceHeights(String treesFilePath, int numberOfIntervals) throws IOException, ImportException {\n-        Double[] timeSlices = new Double[numberOfIntervals];\n+    private Double[] generateSliceHeights(String treesFilePath, int numberOfIntervals)\n+            throws IOException, ImportException {\n \n+        Double[] timeSlices = new Double[numberOfIntervals];\n         NexusImporter treesImporter = new NexusImporter(new FileReader(this.treesFilePath));\n-\n         double maxRootHeight = 0;\n         RootedTree currentTree = null;\n+\n         while (treesImporter.hasTree()) {\n             currentTree = (RootedTree) treesImporter.importNextTree();\n-\n             double rootHeight = currentTree.getHeight(currentTree.getRootNode());\n-\n             if (rootHeight > maxRootHeight) {\n                 maxRootHeight = rootHeight;\n             }\n-\n         }\n \n         for (int i = 0; i < numberOfIntervals; i++) {\n@@ -404,15 +335,24 @@ public String parseAttributesAndTreesCount() throws IOException, ImportException\n         RootedTree tree = (RootedTree) treesImporter.importNextTree();\n \n         Set<String> uniqueAttributes = tree.getNodes().stream().filter(node -> !tree.isRoot(node))\n-            .flatMap(node -> node.getAttributeNames().stream()).map(name -> {\n-                    return name;\n-                }).collect(Collectors.toSet());\n-\n-        int treesCount = getAssumedTrees(this.treesFilePath);\n+            .flatMap(node -> node.getAttributeNames().stream()).map(name -> name).collect(Collectors.toSet());\n \n+        int treesCount = getTreesCount(this.treesFilePath);\n         Object tuple = new Object[] {uniqueAttributes, treesCount};\n \n         return new GsonBuilder().create().toJson(tuple);\n     }\n \n+    @Override\n+    public void registerProgressObserver(IProgressObserver observer) {\n+        this.progressObserver = observer;\n+    }\n+\n+    @Override\n+    public void updateProgress(double progress) {\n+        if (this.progressObserver != null) {\n+            this.progressObserver.handleProgress(progress);\n+        }\n+    }\n+\n }"
  },
  {
    "sha": "78712a8f6b891b136ca7d638a71d2f07215aa602",
    "filename": "src/main/java/com/spread/progress/IProgressObserver.java",
    "status": "added",
    "additions": 5,
    "deletions": 0,
    "changes": 5,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/src/main/java/com/spread/progress/IProgressObserver.java",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/src/main/java/com/spread/progress/IProgressObserver.java",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/src/main/java/com/spread/progress/IProgressObserver.java?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -0,0 +1,5 @@\n+package com.spread.progress;\n+\n+public interface IProgressObserver {\n+    void handleProgress (double progress);\n+}"
  },
  {
    "sha": "2ad0cee76aec7b824dd71af8925dd7366ee7e7a0",
    "filename": "src/main/java/com/spread/progress/IProgressReporter.java",
    "status": "added",
    "additions": 6,
    "deletions": 0,
    "changes": 6,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/src/main/java/com/spread/progress/IProgressReporter.java",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/src/main/java/com/spread/progress/IProgressReporter.java",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/src/main/java/com/spread/progress/IProgressReporter.java?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -0,0 +1,6 @@\n+package com.spread.progress;\n+\n+public interface IProgressReporter {\n+   void registerProgressObserver(IProgressObserver observer);\n+   void updateProgress(double progress);\n+}"
  },
  {
    "sha": "adbf12a27a9c1f3c7882f95e8bf9ff0838fc72ae",
    "filename": "src/main/java/com/spread/utils/ProgressBar.java",
    "status": "removed",
    "additions": 0,
    "deletions": 69,
    "changes": 69,
    "blob_url": "https://github.com/fbielejec/spread/blob/52fbc375e7fbc597d1e00905728bec4e94e0d0bb/src/main/java/com/spread/utils/ProgressBar.java",
    "raw_url": "https://github.com/fbielejec/spread/raw/52fbc375e7fbc597d1e00905728bec4e94e0d0bb/src/main/java/com/spread/utils/ProgressBar.java",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/src/main/java/com/spread/utils/ProgressBar.java?ref=52fbc375e7fbc597d1e00905728bec4e94e0d0bb",
    "patch": "@@ -1,69 +0,0 @@\n-package com.spread.utils;\n-\n-public class ProgressBar extends Thread {\n-\n-\tprivate static final String anim = \"|/-\\\\\";\n-\n-\tprivate boolean showProgress;\n-\tprivate double progressPercentage;\n-\tprivate final int barLength;\n-\n-\tpublic ProgressBar(int barLength) {\n-\t\tthis.barLength = barLength;\n-\t\tthis.showProgress = true;\n-\t\tthis.progressPercentage = 0;\n-\t}// END: Constructor\n-\n-\tpublic void run() {\n-\n-\t\tint i = 0;\n-\n-\t\twhile (showProgress) {\n-\n-\t\t\tint column = (int) (progressPercentage * (barLength));\n-\t\t\tint j = 0;\n-\t\t\t\n-\t\t\tString progress = \"\\r[\";\n-\t\t\tfor (; j < column - 1; j++) {\n-\t\t\t\tprogress += (\"*\");\n-\t\t\t}\n-\n-\t\t\tString whitespace = \"\";\n-\t\t\tfor (; j < barLength - 2; j++) {\n-\t\t\t\twhitespace += (\" \");\n-\t\t\t}\n-\t\t\twhitespace += (\"]\");\n-\n-\t\t\tSystem.out.print(progress + anim.charAt(i++ % anim.length())\n-\t\t\t\t\t+ whitespace);\n-\n-\t\t\ttry {\n-\n-\t\t\t\tThread.sleep(100);\n-\n-\t\t\t} catch (Exception e) {\n-\t\t\t\t// do nothing\n-\t\t\t}// END: try-catch\n-\n-\t\t}// END: while\n-\n-\t}// END: run\n-\n-\tpublic void showCompleted() {\n-\t\tString progress = \"\\r[\";\n-\t\tfor (int i = 0; i < barLength - 1; i++) {\n-\t\t\tprogress += (\"*\");\n-\t\t}\n-\t\tprogress += (\"]\");\n-\t\tSystem.out.print(progress);\n-\t}// END: showCompleted\n-\n-\tpublic void setShowProgress(boolean showProgress) {\n-\t\tthis.showProgress = showProgress;\n-\t}// END: setShowProgress\n-\n-\tpublic void setProgressPercentage(double progressPercentage) {\n-\t\tthis.progressPercentage = progressPercentage;\n-\t}// END: setProgressPercentage\n-\n-}// END: class"
  },
  {
    "sha": "dc788697b88627338a4457b3b98b87bc69a29780",
    "filename": "src/test/java/com/spread/BayesFactorParserTest.java",
    "status": "modified",
    "additions": 4,
    "deletions": 0,
    "changes": 4,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/src/test/java/com/spread/BayesFactorParserTest.java",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/src/test/java/com/spread/BayesFactorParserTest.java",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/src/test/java/com/spread/BayesFactorParserTest.java?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -37,6 +37,10 @@ public void runTest() throws IOException, SpreadException {\n                                                          0.1,\n                                                          locationsFile.getAbsolutePath());\n \n+        ConsoleProgressObserver progressObserver = new ConsoleProgressObserver();\n+        parser.registerProgressObserver(progressObserver);\n+        progressObserver.start ();\n+\n         LinkedList<BayesFactor> expected =\n             new LinkedList<BayesFactor>(Arrays.asList(new BayesFactor (\"Fujian\", \"Guangdong\", 19.014687619229807, 0.8989450305385897),\n                                                       new BayesFactor (\"Fujian\", \"Guangxi\", 2.914568793008483, 0.5769017212659634),"
  },
  {
    "sha": "4c5e8907600a321fe8b026882f8cd9159dee59ad",
    "filename": "src/test/java/com/spread/ConsoleProgressObserver.java",
    "status": "added",
    "additions": 75,
    "deletions": 0,
    "changes": 75,
    "blob_url": "https://github.com/fbielejec/spread/blob/beecd22fdf75d9766d78537083e99daa88bdd309/src/test/java/com/spread/ConsoleProgressObserver.java",
    "raw_url": "https://github.com/fbielejec/spread/raw/beecd22fdf75d9766d78537083e99daa88bdd309/src/test/java/com/spread/ConsoleProgressObserver.java",
    "contents_url": "https://api.github.com/repos/fbielejec/spread/contents/src/test/java/com/spread/ConsoleProgressObserver.java?ref=beecd22fdf75d9766d78537083e99daa88bdd309",
    "patch": "@@ -0,0 +1,75 @@\n+package com.spread;\n+\n+import com.spread.progress.IProgressObserver;\n+\n+public class ConsoleProgressObserver extends Thread implements IProgressObserver {\n+\n+    private static final String anim = \"|/-\\\\\";\n+\n+    private boolean showProgress;\n+    private double progress;\n+    private final int barLength;\n+\n+    public ConsoleProgressObserver() {\n+        this.barLength = 100;\n+        this.showProgress = true;\n+        this.progress = 0.0;\n+    }\n+\n+    @Override\n+    public void handleProgress(double progress) {\n+        assert ((progress >= 0.0) && (progress <= 1.0)) : \"Got progress value \" + progress + \". Make sure that 0 ≤ progress ≤ 1\";\n+        this.progress = progress;\n+    }\n+\n+    public void start() {\n+        System.out.println(\"0                        25                       50                       75                       100%\");\n+        System.out.println(\"|------------------------|------------------------|------------------------|------------------------|\");\n+        super.start ();\n+    }\n+\n+    public void run() {\n+        int i = 0;\n+        while (showProgress) {\n+\n+            int column = (int) (progress * barLength);\n+            int j = 0;\n+\n+            String progressIndicator = \"\\r[\";\n+            for (; j < column - 1; j++) {\n+                progressIndicator += (\"*\");\n+            }\n+\n+            String whitespace = \"\";\n+            for (; j < barLength - 2; j++) {\n+                whitespace += (\" \");\n+            }\n+            whitespace += (\"]\");\n+\n+            System.out.print(progressIndicator + anim.charAt(i++ % anim.length()) + whitespace);\n+\n+            if (progress >= 1) {\n+                showCompleted();\n+                this.showProgress = false;\n+            } else {\n+                try {\n+                    Thread.sleep(100);\n+                } catch (InterruptedException e) {\n+                    // do nothing\n+                }\n+            }\n+\n+        }\n+    }\n+\n+    private void showCompleted() {\n+        String progress = \"\\r[\";\n+        for (int i = 0; i < barLength - 1; i++) {\n+            progress += (\"*\");\n+        }\n+        progress += (\"]\");\n+        System.out.print(progress);\n+        // System.out.print(\"\\n\");\n+    }\n+\n+}"
  }
]
