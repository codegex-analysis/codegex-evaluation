[
  {
    "sha": "75ab693d132e2ce10edd77128d6f309f9e7c6d5e",
    "filename": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/HoodieBaseBloomIndex.java",
    "status": "added",
    "additions": 261,
    "deletions": 0,
    "changes": 261,
    "blob_url": "https://github.com/apache/hudi/blob/bdfdf2b76f68053e404b33a7d034631b7152da85/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/HoodieBaseBloomIndex.java",
    "raw_url": "https://github.com/apache/hudi/raw/bdfdf2b76f68053e404b33a7d034631b7152da85/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/HoodieBaseBloomIndex.java",
    "contents_url": "https://api.github.com/repos/apache/hudi/contents/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/HoodieBaseBloomIndex.java?ref=bdfdf2b76f68053e404b33a7d034631b7152da85",
    "patch": "@@ -0,0 +1,261 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import com.beust.jcommander.internal.Lists;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.engine.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.MetadataNotFoundException;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.index.HoodieIndexUtils;\n+import org.apache.hudi.io.HoodieKeyLookupHandle;\n+import org.apache.hudi.io.HoodieRangeInfoHandle;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.mapping;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hudi.index.HoodieIndexUtils.getLatestBaseFilesForAllPartitions;\n+\n+@SuppressWarnings(\"checkstyle:LineLength\")\n+public class HoodieBaseBloomIndex<T extends HoodieRecordPayload> extends HoodieIndex<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieBaseBloomIndex.class);\n+\n+  public HoodieBaseBloomIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public List<HoodieRecord<T>> tagLocation(List<HoodieRecord<T>> records, HoodieEngineContext context,\n+                                           HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> hoodieTable) {\n+    // Step 1: Extract out thinner Map of (partitionPath, recordKey)\n+    Map<String, List<String>> partitionRecordKeyMap = new HashMap<>();\n+    records.forEach(record -> {\n+      if (partitionRecordKeyMap.containsKey(record.getPartitionPath())) {\n+        partitionRecordKeyMap.get(record.getPartitionPath()).add(record.getRecordKey());\n+      } else {\n+        List<String> recordKeys = Lists.newArrayList();\n+        recordKeys.add(record.getRecordKey());\n+        partitionRecordKeyMap.put(record.getPartitionPath(), recordKeys);\n+      }\n+    });\n+\n+    // Step 2: Lookup indexes for all the partition/recordkey pair\n+    Map<HoodieKey, HoodieRecordLocation> keyFilenamePairMap =\n+        lookupIndex(partitionRecordKeyMap, context, hoodieTable);\n+\n+    if (LOG.isDebugEnabled()) {\n+      long totalTaggedRecords = keyFilenamePairMap.values().size();\n+      LOG.debug(\"Number of update records (ones tagged with a fileID): \" + totalTaggedRecords);\n+    }\n+\n+    // Step 3: Tag the incoming records, as inserts or updates, by joining with existing record keys\n+    List<HoodieRecord<T>> taggedRecords = tagLocationBacktoRecords(keyFilenamePairMap, records);\n+\n+    return taggedRecords;\n+  }\n+\n+  /**\n+   * Lookup the location for each record key and return the pair<record_key,location> for all record keys already\n+   * present and drop the record keys if not present.\n+   */\n+  private Map<HoodieKey, HoodieRecordLocation> lookupIndex(\n+      Map<String, List<String>> partitionRecordKeyMap, final HoodieEngineContext context,\n+      final HoodieTable hoodieTable) {\n+    // Obtain records per partition, in the incoming records\n+    Map<String, Long> recordsPerPartition = new HashMap<>();\n+    partitionRecordKeyMap.keySet().forEach(k -> recordsPerPartition.put(k, Long.valueOf(partitionRecordKeyMap.get(k).size())));\n+    List<String> affectedPartitionPathList = new ArrayList<>(recordsPerPartition.keySet());\n+\n+    // Step 2: Load all involved files as <Partition, filename> pairs\n+    List<Pair<String, BloomIndexFileInfo>> fileInfoList =\n+        loadInvolvedFiles(affectedPartitionPathList, context, hoodieTable);\n+    final Map<String, List<BloomIndexFileInfo>> partitionToFileInfo =\n+        fileInfoList.stream().collect(groupingBy(Pair::getLeft, mapping(Pair::getRight, toList())));\n+\n+    // Step 3: Obtain a List, for each incoming record, that already exists, with the file id,\n+    // that contains it.\n+    List<Pair<String, HoodieKey>> fileComparisons =\n+        explodeRecordsWithFileComparisons(partitionToFileInfo, partitionRecordKeyMap);\n+    return findMatchingFilesForRecordKeys(fileComparisons, hoodieTable);\n+  }\n+\n+  /**\n+   * Load all involved files as <Partition, filename> pair List.\n+   */\n+  //TODO duplicate code with spark, we can optimize this method later\n+  List<Pair<String, BloomIndexFileInfo>> loadInvolvedFiles(List<String> partitions, final HoodieEngineContext context,\n+                                                           final HoodieTable hoodieTable) {\n+    // Obtain the latest data files from all the partitions.\n+    List<Pair<String, String>> partitionPathFileIDList = getLatestBaseFilesForAllPartitions(partitions, context, hoodieTable).stream()\n+        .map(pair -> Pair.of(pair.getKey(), pair.getValue().getFileId()))\n+        .collect(toList());\n+\n+    if (config.getBloomIndexPruneByRanges()) {\n+      // also obtain file ranges, if range pruning is enabled\n+      context.setJobStatus(this.getClass().getName(), \"Obtain key ranges for file slices (range pruning=on)\");\n+      return context.map(partitionPathFileIDList, pf -> {\n+        try {\n+          HoodieRangeInfoHandle rangeInfoHandle = new HoodieRangeInfoHandle(config, hoodieTable, pf);\n+          String[] minMaxKeys = rangeInfoHandle.getMinMaxKeys();\n+          return Pair.of(pf.getKey(), new BloomIndexFileInfo(pf.getValue(), minMaxKeys[0], minMaxKeys[1]));\n+        } catch (MetadataNotFoundException me) {\n+          LOG.warn(\"Unable to find range metadata in file :\" + pf);\n+          return Pair.of(pf.getKey(), new BloomIndexFileInfo(pf.getValue()));\n+        }\n+      }, Math.max(partitionPathFileIDList.size(), 1));\n+    } else {\n+      return partitionPathFileIDList.stream()\n+          .map(pf -> Pair.of(pf.getKey(), new BloomIndexFileInfo(pf.getValue()))).collect(toList());\n+    }\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String instantTime) {\n+    // Nope, don't need to do anything.\n+    return true;\n+  }\n+\n+  /**\n+   * This is not global, since we depend on the partitionPath to do the lookup.\n+   */\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  /**\n+   * No indexes into log files yet.\n+   */\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  /**\n+   * Bloom filters are stored, into the same data files.\n+   */\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  /**\n+   * For each incoming record, produce N output records, 1 each for each file against which the record's key needs to be\n+   * checked. For tables, where the keys have a definite insert order (e.g: timestamp as prefix), the number of files\n+   * to be compared gets cut down a lot from range pruning.\n+   * <p>\n+   * Sub-partition to ensure the records can be looked up against files & also prune file<=>record comparisons based on\n+   * recordKey ranges in the index info.\n+   */\n+  List<Pair<String, HoodieKey>> explodeRecordsWithFileComparisons(\n+      final Map<String, List<BloomIndexFileInfo>> partitionToFileIndexInfo,\n+      Map<String, List<String>> partitionRecordKeyMap) {\n+    IndexFileFilter indexFileFilter =\n+        config.useBloomIndexTreebasedFilter() ? new IntervalTreeBasedIndexFileFilter(partitionToFileIndexInfo)\n+            : new ListBasedIndexFileFilter(partitionToFileIndexInfo);\n+\n+    List<Pair<String, HoodieKey>> fileRecordPairs = new ArrayList<>();\n+    partitionRecordKeyMap.keySet().forEach(partitionPath ->  {\n+      List<String> hoodieRecordKeys = partitionRecordKeyMap.get(partitionPath);\n+      hoodieRecordKeys.forEach(hoodieRecordKey -> {\n+        indexFileFilter.getMatchingFilesAndPartition(partitionPath, hoodieRecordKey).forEach(partitionFileIdPair -> {\n+          fileRecordPairs.add(Pair.of(partitionFileIdPair.getRight(),\n+              new HoodieKey(hoodieRecordKey, partitionPath)));\n+        });\n+      });\n+    });\n+    return fileRecordPairs;\n+  }\n+\n+  /**\n+   * Find out <RowKey, filename> pair.\n+   */\n+  Map<HoodieKey, HoodieRecordLocation> findMatchingFilesForRecordKeys(\n+      List<Pair<String, HoodieKey>> fileComparisons,\n+      HoodieTable hoodieTable) {\n+\n+    fileComparisons = fileComparisons.stream().sorted((o1, o2) -> o1.getLeft().compareTo(o2.getLeft())).collect(toList());\n+\n+    List<HoodieKeyLookupHandle.KeyLookupResult> keyLookupResults = new ArrayList<>();\n+\n+    Iterator<List<HoodieKeyLookupHandle.KeyLookupResult>> iterator = new HoodieBaseBloomIndexCheckFunction(hoodieTable, config).apply(fileComparisons.iterator());\n+    while (iterator.hasNext()) {\n+      keyLookupResults.addAll(iterator.next());\n+    }\n+\n+    Map<HoodieKey, HoodieRecordLocation> hoodieRecordLocationMap = new HashMap<>();\n+\n+    keyLookupResults = keyLookupResults.stream().filter(lr -> lr.getMatchingRecordKeys().size() > 0).collect(toList());\n+    keyLookupResults.forEach(lookupResult -> {\n+      lookupResult.getMatchingRecordKeys().forEach(r -> {\n+        hoodieRecordLocationMap.put(new HoodieKey(r, lookupResult.getPartitionPath()), new HoodieRecordLocation(lookupResult.getBaseInstantTime(), lookupResult.getFileId()));\n+      });\n+    });\n+\n+    return hoodieRecordLocationMap;\n+  }\n+\n+\n+  /**\n+   * Tag the <rowKey, filename> back to the original HoodieRecord List.\n+   */\n+  protected List<HoodieRecord<T>> tagLocationBacktoRecords(\n+      Map<HoodieKey, HoodieRecordLocation> keyFilenamePair, List<HoodieRecord<T>> records) {\n+    Map<HoodieKey, HoodieRecord<T>> keyRecordPairMap = new HashMap<>();\n+    records.forEach(r -> keyRecordPairMap.put(r.getKey(), r));\n+    // Here as the record might have more data than rowKey (some rowKeys' fileId is null),\n+    // so we do left outer join.\n+    List<Pair<HoodieRecord<T>, HoodieRecordLocation>> newList = new ArrayList<>();\n+    keyRecordPairMap.keySet().forEach(k -> {\n+      if (keyFilenamePair.containsKey(k)) {\n+        newList.add(Pair.of(keyRecordPairMap.get(k), keyFilenamePair.get(k)));\n+      } else {\n+        newList.add(Pair.of(keyRecordPairMap.get(k), null));\n+      }\n+    });\n+    List<HoodieRecord<T>> res = Lists.newArrayList();\n+    for (Pair<HoodieRecord<T>, HoodieRecordLocation> v : newList) {\n+      res.add(HoodieIndexUtils.getTaggedRecord(v.getLeft(), Option.ofNullable(v.getRight())));\n+    }\n+    return res;\n+  }\n+\n+  @Override\n+  public List<WriteStatus> updateLocation(List<WriteStatus> writeStatusList, HoodieEngineContext context,\n+                                          HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> hoodieTable) {\n+    return writeStatusList;\n+  }\n+}"
  },
  {
    "sha": "95a2255470acf4d90f1ee9eea6f55401f103095d",
    "filename": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/HoodieBaseBloomIndexCheckFunction.java",
    "status": "renamed",
    "additions": 2,
    "deletions": 2,
    "changes": 4,
    "blob_url": "https://github.com/apache/hudi/blob/bdfdf2b76f68053e404b33a7d034631b7152da85/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/HoodieBaseBloomIndexCheckFunction.java",
    "raw_url": "https://github.com/apache/hudi/raw/bdfdf2b76f68053e404b33a7d034631b7152da85/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/HoodieBaseBloomIndexCheckFunction.java",
    "contents_url": "https://api.github.com/repos/apache/hudi/contents/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/HoodieBaseBloomIndexCheckFunction.java?ref=bdfdf2b76f68053e404b33a7d034631b7152da85",
    "patch": "@@ -37,14 +37,14 @@\n  * Function performing actual checking of list containing (fileId, hoodieKeys) against the actual files.\n  */\n //TODO we can move this class into the hudi-client-common and reuse it for spark client\n-public class HoodieFlinkBloomIndexCheckFunction\n+public class HoodieBaseBloomIndexCheckFunction \n         implements Function<Iterator<Pair<String, HoodieKey>>, Iterator<List<KeyLookupResult>>> {\n \n   private final HoodieTable hoodieTable;\n \n   private final HoodieWriteConfig config;\n \n-  public HoodieFlinkBloomIndexCheckFunction(HoodieTable hoodieTable, HoodieWriteConfig config) {\n+  public HoodieBaseBloomIndexCheckFunction(HoodieTable hoodieTable, HoodieWriteConfig config) {\n     this.hoodieTable = hoodieTable;\n     this.config = config;\n   }",
    "previous_filename": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/bloom/HoodieFlinkBloomIndexCheckFunction.java"
  },
  {
    "sha": "272da8c6c881611f3fc43ef2fe4df7c59db887f4",
    "filename": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/FlinkHoodieIndex.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/apache/hudi/blob/bdfdf2b76f68053e404b33a7d034631b7152da85/hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/FlinkHoodieIndex.java",
    "raw_url": "https://github.com/apache/hudi/raw/bdfdf2b76f68053e404b33a7d034631b7152da85/hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/FlinkHoodieIndex.java",
    "contents_url": "https://api.github.com/repos/apache/hudi/contents/hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/FlinkHoodieIndex.java?ref=bdfdf2b76f68053e404b33a7d034631b7152da85",
    "patch": "@@ -46,7 +46,7 @@ protected FlinkHoodieIndex(HoodieWriteConfig config) {\n     super(config);\n   }\n \n-  public static FlinkHoodieIndex createIndex(HoodieFlinkEngineContext context, HoodieWriteConfig config) {\n+  public static HoodieIndex createIndex(HoodieFlinkEngineContext context, HoodieWriteConfig config) {\n     // first use index class config to create index.\n     if (!StringUtils.isNullOrEmpty(config.getIndexClass())) {\n       Object instance = ReflectionUtils.loadClass(config.getIndexClass(), config);"
  },
  {
    "sha": "355dced71d8ad0dca51782a097cbbb1ac1cd5fd0",
    "filename": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/bloom/FlinkHoodieBloomIndex.java",
    "status": "modified",
    "additions": 1,
    "deletions": 234,
    "changes": 235,
    "blob_url": "https://github.com/apache/hudi/blob/bdfdf2b76f68053e404b33a7d034631b7152da85/hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/bloom/FlinkHoodieBloomIndex.java",
    "raw_url": "https://github.com/apache/hudi/raw/bdfdf2b76f68053e404b33a7d034631b7152da85/hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/bloom/FlinkHoodieBloomIndex.java",
    "contents_url": "https://api.github.com/repos/apache/hudi/contents/hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/bloom/FlinkHoodieBloomIndex.java?ref=bdfdf2b76f68053e404b33a7d034631b7152da85",
    "patch": "@@ -18,248 +18,15 @@\n \n package org.apache.hudi.index.bloom;\n \n-import org.apache.hudi.client.WriteStatus;\n-import org.apache.hudi.common.engine.HoodieEngineContext;\n-import org.apache.hudi.common.model.HoodieKey;\n-import org.apache.hudi.common.model.HoodieRecord;\n-import org.apache.hudi.common.model.HoodieRecordLocation;\n import org.apache.hudi.common.model.HoodieRecordPayload;\n-import org.apache.hudi.common.util.Option;\n-import org.apache.hudi.common.util.collection.Pair;\n import org.apache.hudi.config.HoodieWriteConfig;\n-import org.apache.hudi.exception.MetadataNotFoundException;\n-import org.apache.hudi.index.FlinkHoodieIndex;\n-import org.apache.hudi.index.HoodieIndexUtils;\n-import org.apache.hudi.io.HoodieKeyLookupHandle;\n-import org.apache.hudi.io.HoodieRangeInfoHandle;\n-import org.apache.hudi.table.HoodieTable;\n-\n-import org.apache.log4j.LogManager;\n-import org.apache.log4j.Logger;\n-import com.beust.jcommander.internal.Lists;\n-\n-import java.util.ArrayList;\n-import java.util.HashMap;\n-import java.util.Iterator;\n-import java.util.List;\n-import java.util.Map;\n-\n-import static java.util.stream.Collectors.mapping;\n-import static java.util.stream.Collectors.groupingBy;\n-import static java.util.stream.Collectors.toList;\n-import static org.apache.hudi.index.HoodieIndexUtils.getLatestBaseFilesForAllPartitions;\n \n /**\n  * Indexing mechanism based on bloom filter. Each parquet file includes its row_key bloom filter in its metadata.\n  */\n @SuppressWarnings(\"checkstyle:LineLength\")\n-public class FlinkHoodieBloomIndex<T extends HoodieRecordPayload> extends FlinkHoodieIndex<T> {\n-\n-  private static final Logger LOG = LogManager.getLogger(FlinkHoodieBloomIndex.class);\n-\n+public class FlinkHoodieBloomIndex<T extends HoodieRecordPayload> extends HoodieBaseBloomIndex<T> {\n   public FlinkHoodieBloomIndex(HoodieWriteConfig config) {\n     super(config);\n   }\n-\n-  @Override\n-  public List<HoodieRecord<T>> tagLocation(List<HoodieRecord<T>> records, HoodieEngineContext context,\n-                                           HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> hoodieTable) {\n-    // Step 1: Extract out thinner Map of (partitionPath, recordKey)\n-    Map<String, List<String>> partitionRecordKeyMap = new HashMap<>();\n-    records.forEach(record -> {\n-      if (partitionRecordKeyMap.containsKey(record.getPartitionPath())) {\n-        partitionRecordKeyMap.get(record.getPartitionPath()).add(record.getRecordKey());\n-      } else {\n-        List<String> recordKeys = Lists.newArrayList();\n-        recordKeys.add(record.getRecordKey());\n-        partitionRecordKeyMap.put(record.getPartitionPath(), recordKeys);\n-      }\n-    });\n-\n-    // Step 2: Lookup indexes for all the partition/recordkey pair\n-    Map<HoodieKey, HoodieRecordLocation> keyFilenamePairMap =\n-        lookupIndex(partitionRecordKeyMap, context, hoodieTable);\n-\n-    if (LOG.isDebugEnabled()) {\n-      long totalTaggedRecords = keyFilenamePairMap.values().size();\n-      LOG.debug(\"Number of update records (ones tagged with a fileID): \" + totalTaggedRecords);\n-    }\n-\n-    // Step 3: Tag the incoming records, as inserts or updates, by joining with existing record keys\n-    List<HoodieRecord<T>> taggedRecords = tagLocationBacktoRecords(keyFilenamePairMap, records);\n-\n-    return taggedRecords;\n-  }\n-\n-  /**\n-   * Lookup the location for each record key and return the pair<record_key,location> for all record keys already\n-   * present and drop the record keys if not present.\n-   */\n-  private Map<HoodieKey, HoodieRecordLocation> lookupIndex(\n-      Map<String, List<String>> partitionRecordKeyMap, final HoodieEngineContext context,\n-      final HoodieTable hoodieTable) {\n-    // Obtain records per partition, in the incoming records\n-    Map<String, Long> recordsPerPartition = new HashMap<>();\n-    partitionRecordKeyMap.keySet().forEach(k -> recordsPerPartition.put(k, Long.valueOf(partitionRecordKeyMap.get(k).size())));\n-    List<String> affectedPartitionPathList = new ArrayList<>(recordsPerPartition.keySet());\n-\n-    // Step 2: Load all involved files as <Partition, filename> pairs\n-    List<Pair<String, BloomIndexFileInfo>> fileInfoList =\n-        loadInvolvedFiles(affectedPartitionPathList, context, hoodieTable);\n-    final Map<String, List<BloomIndexFileInfo>> partitionToFileInfo =\n-        fileInfoList.stream().collect(groupingBy(Pair::getLeft, mapping(Pair::getRight, toList())));\n-\n-    // Step 3: Obtain a List, for each incoming record, that already exists, with the file id,\n-    // that contains it.\n-    List<Pair<String, HoodieKey>> fileComparisons =\n-            explodeRecordsWithFileComparisons(partitionToFileInfo, partitionRecordKeyMap);\n-    return findMatchingFilesForRecordKeys(fileComparisons, hoodieTable);\n-  }\n-\n-  /**\n-   * Load all involved files as <Partition, filename> pair List.\n-   */\n-  //TODO duplicate code with spark, we can optimize this method later\n-  List<Pair<String, BloomIndexFileInfo>> loadInvolvedFiles(List<String> partitions, final HoodieEngineContext context,\n-                                                             final HoodieTable hoodieTable) {\n-    // Obtain the latest data files from all the partitions.\n-    List<Pair<String, String>> partitionPathFileIDList = getLatestBaseFilesForAllPartitions(partitions, context, hoodieTable).stream()\n-        .map(pair -> Pair.of(pair.getKey(), pair.getValue().getFileId()))\n-        .collect(toList());\n-\n-    if (config.getBloomIndexPruneByRanges()) {\n-      // also obtain file ranges, if range pruning is enabled\n-      context.setJobStatus(this.getClass().getName(), \"Obtain key ranges for file slices (range pruning=on)\");\n-      return context.map(partitionPathFileIDList, pf -> {\n-        try {\n-          HoodieRangeInfoHandle rangeInfoHandle = new HoodieRangeInfoHandle(config, hoodieTable, pf);\n-          String[] minMaxKeys = rangeInfoHandle.getMinMaxKeys();\n-          return Pair.of(pf.getKey(), new BloomIndexFileInfo(pf.getValue(), minMaxKeys[0], minMaxKeys[1]));\n-        } catch (MetadataNotFoundException me) {\n-          LOG.warn(\"Unable to find range metadata in file :\" + pf);\n-          return Pair.of(pf.getKey(), new BloomIndexFileInfo(pf.getValue()));\n-        }\n-      }, Math.max(partitionPathFileIDList.size(), 1));\n-    } else {\n-      return partitionPathFileIDList.stream()\n-          .map(pf -> Pair.of(pf.getKey(), new BloomIndexFileInfo(pf.getValue()))).collect(toList());\n-    }\n-  }\n-\n-  @Override\n-  public boolean rollbackCommit(String instantTime) {\n-    // Nope, don't need to do anything.\n-    return true;\n-  }\n-\n-  /**\n-   * This is not global, since we depend on the partitionPath to do the lookup.\n-   */\n-  @Override\n-  public boolean isGlobal() {\n-    return false;\n-  }\n-\n-  /**\n-   * No indexes into log files yet.\n-   */\n-  @Override\n-  public boolean canIndexLogFiles() {\n-    return false;\n-  }\n-\n-  /**\n-   * Bloom filters are stored, into the same data files.\n-   */\n-  @Override\n-  public boolean isImplicitWithStorage() {\n-    return true;\n-  }\n-\n-  /**\n-   * For each incoming record, produce N output records, 1 each for each file against which the record's key needs to be\n-   * checked. For tables, where the keys have a definite insert order (e.g: timestamp as prefix), the number of files\n-   * to be compared gets cut down a lot from range pruning.\n-   * <p>\n-   * Sub-partition to ensure the records can be looked up against files & also prune file<=>record comparisons based on\n-   * recordKey ranges in the index info.\n-   */\n-  List<Pair<String, HoodieKey>> explodeRecordsWithFileComparisons(\n-      final Map<String, List<BloomIndexFileInfo>> partitionToFileIndexInfo,\n-      Map<String, List<String>> partitionRecordKeyMap) {\n-    IndexFileFilter indexFileFilter =\n-        config.useBloomIndexTreebasedFilter() ? new IntervalTreeBasedIndexFileFilter(partitionToFileIndexInfo)\n-            : new ListBasedIndexFileFilter(partitionToFileIndexInfo);\n-\n-    List<Pair<String, HoodieKey>> fileRecordPairs = new ArrayList<>();\n-    partitionRecordKeyMap.keySet().forEach(partitionPath ->  {\n-      List<String> hoodieRecordKeys = partitionRecordKeyMap.get(partitionPath);\n-      hoodieRecordKeys.forEach(hoodieRecordKey -> {\n-        indexFileFilter.getMatchingFilesAndPartition(partitionPath, hoodieRecordKey).forEach(partitionFileIdPair -> {\n-          fileRecordPairs.add(Pair.of(partitionFileIdPair.getRight(),\n-                  new HoodieKey(hoodieRecordKey, partitionPath)));\n-        });\n-      });\n-    });\n-    return fileRecordPairs;\n-  }\n-\n-  /**\n-   * Find out <RowKey, filename> pair.\n-   */\n-  Map<HoodieKey, HoodieRecordLocation> findMatchingFilesForRecordKeys(\n-      List<Pair<String, HoodieKey>> fileComparisons,\n-      HoodieTable hoodieTable) {\n-\n-    fileComparisons = fileComparisons.stream().sorted((o1, o2) -> o1.getLeft().compareTo(o2.getLeft())).collect(toList());\n-\n-    List<HoodieKeyLookupHandle.KeyLookupResult> keyLookupResults = new ArrayList<>();\n-\n-    Iterator<List<HoodieKeyLookupHandle.KeyLookupResult>> iterator = new HoodieFlinkBloomIndexCheckFunction(hoodieTable, config).apply(fileComparisons.iterator());\n-    while (iterator.hasNext()) {\n-      keyLookupResults.addAll(iterator.next());\n-    }\n-\n-    Map<HoodieKey, HoodieRecordLocation> hoodieRecordLocationMap = new HashMap<>();\n-\n-    keyLookupResults = keyLookupResults.stream().filter(lr -> lr.getMatchingRecordKeys().size() > 0).collect(toList());\n-    keyLookupResults.forEach(lookupResult -> {\n-      lookupResult.getMatchingRecordKeys().forEach(r -> {\n-        hoodieRecordLocationMap.put(new HoodieKey(r, lookupResult.getPartitionPath()), new HoodieRecordLocation(lookupResult.getBaseInstantTime(), lookupResult.getFileId()));\n-      });\n-    });\n-\n-    return hoodieRecordLocationMap;\n-  }\n-\n-\n-  /**\n-   * Tag the <rowKey, filename> back to the original HoodieRecord List.\n-   */\n-  protected List<HoodieRecord<T>> tagLocationBacktoRecords(\n-          Map<HoodieKey, HoodieRecordLocation> keyFilenamePair, List<HoodieRecord<T>> records) {\n-    Map<HoodieKey, HoodieRecord<T>> keyRecordPairMap = new HashMap<>();\n-    records.forEach(r -> keyRecordPairMap.put(r.getKey(), r));\n-    // Here as the record might have more data than rowKey (some rowKeys' fileId is null),\n-    // so we do left outer join.\n-    List<Pair<HoodieRecord<T>, HoodieRecordLocation>> newList = new ArrayList<>();\n-    keyRecordPairMap.keySet().forEach(k -> {\n-      if (keyFilenamePair.containsKey(k)) {\n-        newList.add(Pair.of(keyRecordPairMap.get(k), keyFilenamePair.get(k)));\n-      } else {\n-        newList.add(Pair.of(keyRecordPairMap.get(k), null));\n-      }\n-    });\n-    List<HoodieRecord<T>> res = Lists.newArrayList();\n-    for (Pair<HoodieRecord<T>, HoodieRecordLocation> v : newList) {\n-      res.add(HoodieIndexUtils.getTaggedRecord(v.getLeft(), Option.ofNullable(v.getRight())));\n-    }\n-    return res;\n-  }\n-\n-  @Override\n-  public List<WriteStatus> updateLocation(List<WriteStatus> writeStatusList, HoodieEngineContext context,\n-                                          HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> hoodieTable) {\n-    return writeStatusList;\n-  }\n }"
  },
  {
    "sha": "47d47c8478932d5096cfb630b16295f9bd8e9963",
    "filename": "hudi-client/hudi-java-client/src/main/java/org/apache/hudi/index/JavaHoodieBloomIndex.java",
    "status": "added",
    "additions": 32,
    "deletions": 0,
    "changes": 32,
    "blob_url": "https://github.com/apache/hudi/blob/bdfdf2b76f68053e404b33a7d034631b7152da85/hudi-client/hudi-java-client/src/main/java/org/apache/hudi/index/JavaHoodieBloomIndex.java",
    "raw_url": "https://github.com/apache/hudi/raw/bdfdf2b76f68053e404b33a7d034631b7152da85/hudi-client/hudi-java-client/src/main/java/org/apache/hudi/index/JavaHoodieBloomIndex.java",
    "contents_url": "https://api.github.com/repos/apache/hudi/contents/hudi-client/hudi-java-client/src/main/java/org/apache/hudi/index/JavaHoodieBloomIndex.java?ref=bdfdf2b76f68053e404b33a7d034631b7152da85",
    "patch": "@@ -0,0 +1,32 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.bloom.HoodieBaseBloomIndex;\n+\n+/**\n+ * Indexing mechanism based on bloom filter. Each parquet file includes its row_key bloom filter in its metadata.\n+ */\n+public class JavaHoodieBloomIndex<T extends HoodieRecordPayload> extends HoodieBaseBloomIndex<T> {\n+  public JavaHoodieBloomIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+}"
  },
  {
    "sha": "fc7a451dcf34e73c0167d4f52715c5c36df05772",
    "filename": "hudi-client/hudi-java-client/src/main/java/org/apache/hudi/index/JavaHoodieIndex.java",
    "status": "modified",
    "additions": 3,
    "deletions": 1,
    "changes": 4,
    "blob_url": "https://github.com/apache/hudi/blob/bdfdf2b76f68053e404b33a7d034631b7152da85/hudi-client/hudi-java-client/src/main/java/org/apache/hudi/index/JavaHoodieIndex.java",
    "raw_url": "https://github.com/apache/hudi/raw/bdfdf2b76f68053e404b33a7d034631b7152da85/hudi-client/hudi-java-client/src/main/java/org/apache/hudi/index/JavaHoodieIndex.java",
    "contents_url": "https://api.github.com/repos/apache/hudi/contents/hudi-client/hudi-java-client/src/main/java/org/apache/hudi/index/JavaHoodieIndex.java?ref=bdfdf2b76f68053e404b33a7d034631b7152da85",
    "patch": "@@ -38,7 +38,7 @@ protected JavaHoodieIndex(HoodieWriteConfig config) {\n     super(config);\n   }\n \n-  public static JavaHoodieIndex createIndex(HoodieWriteConfig config) {\n+  public static HoodieIndex createIndex(HoodieWriteConfig config) {\n     // first use index class config to create index.\n     if (!StringUtils.isNullOrEmpty(config.getIndexClass())) {\n       Object instance = ReflectionUtils.loadClass(config.getIndexClass(), config);\n@@ -52,6 +52,8 @@ public static JavaHoodieIndex createIndex(HoodieWriteConfig config) {\n     switch (config.getIndexType()) {\n       case INMEMORY:\n         return new JavaInMemoryHashIndex(config);\n+      case BLOOM:\n+        return new JavaHoodieBloomIndex(config);\n       default:\n         throw new HoodieIndexException(\"Unsupported index type \" + config.getIndexType());\n     }"
  }
]
