[
  {
    "sha": "89fa0b043e172a25280403df713283e0ae43caf1",
    "filename": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/io/storage/TestHoodieFileWriterFactory.java",
    "status": "modified",
    "additions": 261,
    "deletions": 3,
    "changes": 264,
    "blob_url": "https://github.com/apache/hudi/blob/241e56f6d7d55811119e1a398fc8c80059a81b54/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/io/storage/TestHoodieFileWriterFactory.java",
    "raw_url": "https://github.com/apache/hudi/raw/241e56f6d7d55811119e1a398fc8c80059a81b54/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/io/storage/TestHoodieFileWriterFactory.java",
    "contents_url": "https://api.github.com/repos/apache/hudi/contents/hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/io/storage/TestHoodieFileWriterFactory.java?ref=241e56f6d7d55811119e1a398fc8c80059a81b54",
    "patch": "@@ -18,19 +18,35 @@\n \n package org.apache.hudi.io.storage;\n \n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hudi.avro.model.HoodieFileInfo;\n+import org.apache.hudi.avro.model.HoodieMetadataRecord;\n+import org.apache.hudi.avro.model.HoodieRangeIndexInfo;\n import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n import org.apache.hudi.config.HoodieWriteConfig;\n import org.apache.hudi.table.HoodieSparkTable;\n import org.apache.hudi.table.HoodieTable;\n import org.apache.hudi.testutils.HoodieClientTestBase;\n-\n-import org.apache.avro.generic.IndexedRecord;\n-import org.apache.hadoop.fs.Path;\n import org.junit.jupiter.api.Test;\n \n import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.UUID;\n \n+import static org.junit.jupiter.api.Assertions.assertEquals;\n import static org.junit.jupiter.api.Assertions.assertThrows;\n import static org.junit.jupiter.api.Assertions.assertTrue;\n \n@@ -64,4 +80,246 @@ public void testGetFileWriter() throws IOException {\n     }, \"should fail since log storage writer is not supported yet.\");\n     assertTrue(thrown.getMessage().contains(\"format not supported yet.\"));\n   }\n+  \n+  // key: full file path (/tmp/.../partition0000/file-000.parquet, value: column range \n+  @Test\n+  public void testPerformanceRangeKeyPartitionFile() throws IOException {\n+    final String instantTime = \"100\";\n+    final HoodieWriteConfig cfg = getConfig();\n+    final Path hfilePath = new Path(basePath + \"/hfile_partition/f1_1-0-1_000.hfile\");\n+    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n+    HoodieHFileWriter<HoodieRecordPayload, IndexedRecord> hfileWriter = (HoodieHFileWriter<HoodieRecordPayload, IndexedRecord>) HoodieFileWriterFactory.getFileWriter(instantTime,\n+        hfilePath, table, cfg, HoodieMetadataRecord.SCHEMA$, supplier);\n+    Random random = new Random();\n+    \n+    int numPartitions = 1000;\n+    int avgFilesPerPartition = 10;\n+\n+    long startTime = System.currentTimeMillis();\n+    List<String> partitions = new ArrayList<>();\n+    for (int i = 0; i < numPartitions; i++) {\n+      String partitionPath = \"partition-\" + String.format(\"%010d\", i);\n+      partitions.add(partitionPath);\n+      for (int j = 0; j < avgFilesPerPartition; j++) {\n+        String filePath = \"file-\" + String.format(\"%010d\", j) + \"_1-0-1_000.parquet\";\n+        int max = random.nextInt();\n+        if (max < 0) {\n+          max = -max;\n+        }\n+        int min = random.nextInt(max);\n+\n+        HoodieKey key = new HoodieKey(partitionPath + filePath, partitionPath);\n+        GenericRecord rec = new GenericData.Record(HoodieMetadataRecord.SCHEMA$);\n+        rec.put(\"key\", key.getRecordKey());\n+        rec.put(\"type\", 2);\n+        rec.put(\"rangeIndexMetadata\", HoodieRangeIndexInfo.newBuilder().setMax(\"\" + max).setMin(\"\" + min).setIsDeleted(false).build());\n+        hfileWriter.writeAvro(key.getRecordKey(), rec);\n+      }\n+    }\n+    \n+    hfileWriter.close();\n+    long durationInMs = System.currentTimeMillis() - startTime;\n+    System.out.println(\"Time taken to generate & write: \" + durationInMs + \" ms. file path: \" + hfilePath\n+        + \" File size: \" + FSUtils.getFileSize(metaClient.getFs(), hfilePath));\n+    \n+    CacheConfig cacheConfig = new CacheConfig(hadoopConf);\n+    cacheConfig.setCacheDataInL1(false);\n+    HoodieHFileReader reader = new HoodieHFileReader(hadoopConf, hfilePath, cacheConfig);\n+    long duration  = 0;\n+    int numRuns = 1000;\n+    long numRecordsInRange = 0;\n+    for (int i = 0; i < numRuns; i++) {\n+      int partitionPicked = Math.max(0, partitions.size() - 30);\n+      long start = System.currentTimeMillis();\n+      Map<String, GenericRecord> records = reader.getRecordsInRange(partitions.get(partitionPicked), partitions.get(partitions.size() - 1));\n+      duration += (System.currentTimeMillis() - start);\n+      numRecordsInRange += records.size();\n+    }\n+    double avgDuration = duration / (double) numRuns;\n+    double avgRecordsFetched = numRecordsInRange / (double) numRuns;\n+    System.out.println(\"Average time taken to lookup a range: \" + avgDuration + \"ms. Avg number records: \" + avgRecordsFetched);\n+  }\n+\n+  // key: partition (partition0000), value: map (filePath -> column range)\n+  @Test\n+  public void testPerformanceRangeKeyPartition() throws IOException {\n+    final String instantTime = \"100\";\n+    final HoodieWriteConfig cfg = getConfig();\n+    final Path hfilePath = new Path(basePath + \"/hfile_partition/f1_1-0-1_000.hfile\");\n+    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n+    HoodieHFileWriter<HoodieRecordPayload, IndexedRecord> hfileWriter = (HoodieHFileWriter<HoodieRecordPayload, IndexedRecord>) HoodieFileWriterFactory.getFileWriter(instantTime,\n+        hfilePath, table, cfg, HoodieMetadataRecord.SCHEMA$, supplier);\n+    Random random = new Random();\n+\n+    int numPartitions = 1;\n+    int avgFilesPerPartition = 10;\n+\n+    long startTime = System.currentTimeMillis();\n+    List<String> partitions = new ArrayList<>();\n+    for (int i = 0; i < numPartitions; i++) {\n+      String partitionPath = \"partition-\" + String.format(\"%010d\", i);\n+      partitions.add(partitionPath);\n+      Map<String, HoodieRangeIndexInfo> fileToRangeInfo = new HashMap<>();\n+      for (int j = 0; j < avgFilesPerPartition; j++) {\n+        String filePath = \"file-\" + String.format(\"%010d\", j) + \"_1-0-1_000.parquet\";\n+        int max = random.nextInt();\n+        if (max < 0) {\n+          max = -max;\n+        }\n+        int min = random.nextInt(max);\n+        fileToRangeInfo.put(filePath, HoodieRangeIndexInfo.newBuilder().setMax(\"\" + max).setMin(\"\" + min).setIsDeleted(false).build());\n+      }\n+\n+      HoodieKey key = new HoodieKey(partitionPath, partitionPath);\n+      GenericRecord rec = new GenericData.Record(HoodieMetadataRecord.SCHEMA$);\n+      rec.put(\"key\", key.getRecordKey());\n+      rec.put(\"type\", 2);\n+      rec.put(\"partitionRangeIndexMetadata\", fileToRangeInfo);\n+      hfileWriter.writeAvro(key.getRecordKey(), rec);\n+    }\n+\n+    hfileWriter.close();\n+    long durationInMs = System.currentTimeMillis() - startTime;\n+    System.out.println(\"Time taken to generate & write: \" + durationInMs + \" ms. file path: \" + hfilePath\n+        + \" File size: \" + FSUtils.getFileSize(metaClient.getFs(), hfilePath));\n+    \n+    CacheConfig cacheConfig = new CacheConfig(hadoopConf);\n+    cacheConfig.setCacheDataInL1(false);\n+    HoodieHFileReader reader = new HoodieHFileReader(hadoopConf, hfilePath, cacheConfig);\n+    GenericRecord record = (GenericRecord) reader.getRecordByKey(partitions.get(0), reader.getSchema()).get();\n+    assertEquals(avgFilesPerPartition, ((Map<String, HoodieRangeIndexInfo> )record.get(\"partitionRangeIndexMetadata\")).size());\n+    long duration  = 0;\n+    int numRuns = 1000;\n+    long numRecordsInRange = 0;\n+    for (int i = 0; i < numRuns; i++) {\n+      int partitionPicked = Math.max(0, partitions.size() - 30);\n+      long start = System.currentTimeMillis();\n+      Map<String, GenericRecord> records = (Map<String, GenericRecord>) \n+          reader.getRecordsInRange(partitions.get(partitionPicked), partitions.get(partitions.size() - 1));\n+      numRecordsInRange += records.size();\n+      duration += (System.currentTimeMillis() - start);\n+    }\n+    double avgDuration = duration / (double) numRuns;\n+    double avgRecordsFetched = numRecordsInRange / (double) numRuns;\n+    System.out.println(\"Average time taken to lookup a range: \" + avgDuration + \"ms. Avg number records: \" + avgRecordsFetched);\n+  }\n+\n+  // key: column range (400), value: list (filePaths containing that range)\n+  @Test\n+  public void testPerformanceRangeKeyColumnRange() throws IOException {\n+    final String instantTime = \"100\";\n+    final HoodieWriteConfig cfg = getConfig();\n+    final Path hfilePath = new Path(basePath + \"/hfile_partition/f1_1-0-1_000.hfile\");\n+    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n+    HoodieHFileWriter<HoodieRecordPayload, IndexedRecord> hfileWriter = (HoodieHFileWriter<HoodieRecordPayload, IndexedRecord>) HoodieFileWriterFactory.getFileWriter(instantTime,\n+        hfilePath, table, cfg, HoodieMetadataRecord.SCHEMA$, supplier);\n+    Random random = new Random();\n+\n+    int numKeys = 300;\n+    int minFilesPerRange = 10;\n+    int maxFilesPerRange = 30;\n+\n+    int currentStart = 0;\n+    long startTime = System.currentTimeMillis();\n+    List<String> keys = new ArrayList<>();\n+    for (int i = 0; i < numKeys; i++) {\n+      String key = \"ID-\" + String.format(\"%010d\", currentStart);\n+      int numFiles = minFilesPerRange + random.nextInt(maxFilesPerRange - minFilesPerRange);\n+      List<HoodieFileInfo> files = new ArrayList<>();\n+      for (int j = 0; j < numFiles; j++) {\n+        files.add(HoodieFileInfo.newBuilder().setFilePath(UUID.randomUUID().toString()).setIsDeleted(false).build());\n+      }\n+      GenericRecord rec = new GenericData.Record(HoodieMetadataRecord.SCHEMA$);\n+      rec.put(\"key\", key);\n+      rec.put(\"type\", 4);\n+      rec.put(\"rangeToFileInfo\", files);\n+      hfileWriter.writeAvro(key, rec);\n+      if (i % 10 == 0) {\n+        keys.add(key);\n+      }\n+      currentStart += 1000;\n+    }\n+\n+    hfileWriter.close();\n+    long durationInMs = System.currentTimeMillis() - startTime;\n+    System.out.println(\"Time taken to generate & write: \" + durationInMs + \" ms. file path: \" + hfilePath \n+        + \" File size: \" + FSUtils.getFileSize(metaClient.getFs(), hfilePath));\n+\n+    CacheConfig cacheConfig = new CacheConfig(hadoopConf);\n+    cacheConfig.setCacheDataInL1(false);\n+    HoodieHFileReader reader = new HoodieHFileReader(hadoopConf, hfilePath, cacheConfig);\n+    long duration  = 0;\n+    int numRuns = 1000;\n+    long numRecords = 0;\n+    for (int i = 0; i < numRuns; i++) {\n+      int minKeyPicked = random.nextInt(keys.size() / 2);\n+      int maxKeyPicked = minKeyPicked + random.nextInt(keys.size() / 2 - 1);\n+      long start = System.currentTimeMillis();\n+      Map<String, GenericRecord> records = (Map<String, GenericRecord>)\n+          reader.getRecordsInRange(keys.get(minKeyPicked), keys.get(maxKeyPicked));\n+      numRecords += records.size();\n+      duration += (System.currentTimeMillis() - start);\n+    }\n+    double avgDuration = duration / (double) numRuns;\n+    double avgRecordsFetched = numRecords / (double) numRuns;\n+    System.out.println(\"Average time taken to lookup a range: \" + avgDuration + \"ms. Avg number records: \" + avgRecordsFetched);\n+  }\n+\n+  // measure time for single key lookup\n+  @Test\n+  public void testPerformanceSingleKey() throws IOException {\n+    final String instantTime = \"100\";\n+    final HoodieWriteConfig cfg = getConfig();\n+    final Path hfilePath = new Path(basePath + \"/hfile_partition/f1_1-0-1_000.hfile\");\n+    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n+    HoodieHFileWriter<HoodieRecordPayload, IndexedRecord> hfileWriter = (HoodieHFileWriter<HoodieRecordPayload, IndexedRecord>) HoodieFileWriterFactory.getFileWriter(instantTime,\n+        hfilePath, table, cfg, HoodieMetadataRecord.SCHEMA$, supplier);\n+    Random random = new Random();\n+\n+    int numPartitions = 10000;\n+    int avgFilesPerPartition = 10;\n+\n+    long startTime = System.currentTimeMillis();\n+    List<String> allKeys = new ArrayList<>();\n+    for (int i = 0; i < numPartitions; i++) {\n+      String partitionPath = \"partition-\" + String.format(\"%010d\", i);\n+      for (int j = 0; j < avgFilesPerPartition; j++) {\n+        String filePath = \"file-\" + String.format(\"%010d\", j) + \"_1-0-1_000.parquet\";\n+        int max = random.nextInt() + 1;\n+        if (max < 0) {\n+          max = -max;\n+        }\n+        int min = random.nextInt(max);\n+\n+        HoodieKey key = new HoodieKey(partitionPath + filePath, partitionPath);\n+        GenericRecord rec = new GenericData.Record(HoodieMetadataRecord.SCHEMA$);\n+        if (j == 0) {\n+          allKeys.add(key.getRecordKey());\n+        }\n+        rec.put(\"key\", key.getRecordKey());\n+        rec.put(\"type\", 2);\n+        rec.put(\"rangeIndexMetadata\", HoodieRangeIndexInfo.newBuilder().setMax(\"\" + max).setMin(\"\" + min).setIsDeleted(false).build());\n+        hfileWriter.writeAvro(key.getRecordKey(), rec);\n+      }\n+    }\n+\n+    hfileWriter.close();\n+    long durationInMs = System.currentTimeMillis() - startTime;\n+    System.out.println(\"Time taken to write: \" + durationInMs + \" ms\");\n+\n+    CacheConfig cacheConfig = new CacheConfig(hadoopConf);\n+    cacheConfig.setCacheDataInL1(false);\n+    HoodieHFileReader reader = new HoodieHFileReader(hadoopConf, hfilePath, cacheConfig);\n+    long duration  = 0;\n+    int numRuns = 1000;\n+    Schema schema = reader.getSchema();\n+    for (int i = 0; i < numRuns; i++) {\n+      String keyPicked = allKeys.get(random.nextInt(allKeys.size() - 1));\n+      long start = System.currentTimeMillis();\n+      GenericRecord record = (GenericRecord) reader.getRecordByKey(keyPicked, schema).get();\n+      duration += (System.currentTimeMillis() - start);\n+    }\n+    double avgDuration = duration / (double) numRuns;\n+    System.out.println(\"Average time taken to lookup a key: \" + avgDuration + \"ms.\");\n+  }\n }"
  },
  {
    "sha": "d4423af44257e54c7872dc518acdff3995dcbe6e",
    "filename": "hudi-common/src/main/avro/HoodieMetadata.avsc",
    "status": "modified",
    "additions": 72,
    "deletions": 1,
    "changes": 73,
    "blob_url": "https://github.com/apache/hudi/blob/241e56f6d7d55811119e1a398fc8c80059a81b54/hudi-common/src/main/avro/HoodieMetadata.avsc",
    "raw_url": "https://github.com/apache/hudi/raw/241e56f6d7d55811119e1a398fc8c80059a81b54/hudi-common/src/main/avro/HoodieMetadata.avsc",
    "contents_url": "https://api.github.com/repos/apache/hudi/contents/hudi-common/src/main/avro/HoodieMetadata.avsc?ref=241e56f6d7d55811119e1a398fc8c80059a81b54",
    "patch": "@@ -51,6 +51,77 @@\n                     ]\n                 }\n             }]\n-        }\n+        },\n+         {   \"name\": \"rangeIndexMetadata\",\n+             \"doc\": \"Key is (partition+Full_File_Name). Value is range for a column in that group.\",\n+             \"type\": [\"null\", {\n+                     \"type\": \"record\",\n+                     \"name\": \"HoodieRangeIndexInfo\",\n+                     \"fields\": [\n+                         {\n+                             \"name\": \"min\",\n+                             \"type\": \"string\",\n+                             \"doc\": \"Min value in file\"\n+                         },\n+                         {\n+                             \"name\": \"max\",\n+                             \"type\": \"string\",\n+                             \"doc\": \"Max value in file\"\n+                         },\n+                         {\n+                             \"name\": \"isDeleted\",\n+                             \"type\": \"boolean\",\n+                             \"doc\": \"True if this file has been deleted\"\n+                         }\n+                     ]\n+             }]\n+         },\n+           {   \"name\": \"partitionRangeIndexMetadata\",\n+               \"doc\": \"Key is partition. Value is map<Full_File_Name -> range>\",\n+               \"type\": [\"null\", {\n+                   \"type\": \"map\",\n+                   \"values\": {\n+                       \"type\": \"record\",\n+                       \"name\": \"HoodiePartitionRangeIndexInfo\",\n+                       \"fields\": [\n+                           {\n+                               \"name\": \"min\",\n+                               \"type\": \"string\",\n+                               \"doc\": \"Min value in file\"\n+                           },\n+                           {\n+                               \"name\": \"max\",\n+                               \"type\": \"string\",\n+                               \"doc\": \"Max value in file\"\n+                           },\n+                           {\n+                               \"name\": \"isDeleted\",\n+                               \"type\": \"boolean\",\n+                               \"doc\": \"True if this file has been deleted\"\n+                           }\n+                       ]\n+               }}]\n+           },\n+           {   \"name\": \"rangeToFileInfo\",\n+               \"doc\": \"Key represents range, value is list of file paths\",\n+               \"type\": [\"null\", {\n+                   \"type\": \"array\",\n+                   \"items\": {\n+                       \"type\": \"record\",\n+                       \"name\": \"HoodieFileInfo\",\n+                       \"fields\": [\n+                           {\n+                               \"name\": \"filePath\",\n+                               \"type\": \"string\",\n+                               \"doc\": \"full file path\"\n+                           },\n+                           {\n+                               \"name\": \"isDeleted\",\n+                               \"type\": \"boolean\",\n+                               \"doc\": \"True if this file has been deleted\"\n+                           }\n+                       ]\n+               }}]\n+           }\n     ]\n }"
  },
  {
    "sha": "cf47330bace2add3af3dce89d5cae1f9e02d9924",
    "filename": "hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieHFileReader.java",
    "status": "modified",
    "additions": 27,
    "deletions": 0,
    "changes": 27,
    "blob_url": "https://github.com/apache/hudi/blob/241e56f6d7d55811119e1a398fc8c80059a81b54/hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieHFileReader.java",
    "raw_url": "https://github.com/apache/hudi/raw/241e56f6d7d55811119e1a398fc8c80059a81b54/hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieHFileReader.java",
    "contents_url": "https://api.github.com/repos/apache/hudi/contents/hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieHFileReader.java?ref=241e56f6d7d55811119e1a398fc8c80059a81b54",
    "patch": "@@ -22,6 +22,8 @@\n import java.io.IOException;\n import java.nio.ByteBuffer;\n import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n import java.util.HashSet;\n import java.util.Iterator;\n import java.util.LinkedList;\n@@ -220,6 +222,31 @@ public Option getRecordByKey(String key, Schema readerSchema) throws IOException\n \n     return Option.empty();\n   }\n+  \n+  public Map<String, R> getRecordsInRange(String startKey, String endKey) throws IOException {\n+    HFileScanner scanner = reader.getScanner(false, true);\n+    KeyValue kv = new KeyValue(startKey.getBytes(), null, null, null);\n+    if (scanner.seekBefore(kv)) {\n+      scanner.next();\n+    } else if (!scanner.seekTo()) {\n+      return Collections.emptyMap(); \n+    } \n+    \n+    Map<String, R> keyToValueMap = new HashMap<>();\n+    while (true) {\n+      R record = getRecordFromCell(scanner.getKeyValue(), getSchema(), getSchema());\n+      String recordKey = record.get(0).toString();\n+      if (recordKey.compareTo(endKey) > 0) {\n+        return keyToValueMap;\n+      }\n+      keyToValueMap.put(recordKey, record);\n+      if (!scanner.next()) {\n+        break;\n+      }\n+    }\n+    \n+    return keyToValueMap;\n+  }\n \n   private R getRecordFromCell(Cell c, Schema writerSchema, Schema readerSchema) throws IOException {\n     byte[] value = Arrays.copyOfRange(c.getValueArray(), c.getValueOffset(), c.getValueOffset() + c.getValueLength());"
  },
  {
    "sha": "8c37df3fffae803f3c5eee2984fd92e36b528956",
    "filename": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java",
    "status": "modified",
    "additions": 8,
    "deletions": 8,
    "changes": 16,
    "blob_url": "https://github.com/apache/hudi/blob/241e56f6d7d55811119e1a398fc8c80059a81b54/hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java",
    "raw_url": "https://github.com/apache/hudi/raw/241e56f6d7d55811119e1a398fc8c80059a81b54/hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java",
    "contents_url": "https://api.github.com/repos/apache/hudi/contents/hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataPayload.java?ref=241e56f6d7d55811119e1a398fc8c80059a81b54",
    "patch": "@@ -18,6 +18,13 @@\n \n package org.apache.hudi.metadata;\n \n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n import org.apache.hudi.avro.model.HoodieMetadataFileInfo;\n import org.apache.hudi.avro.model.HoodieMetadataRecord;\n import org.apache.hudi.common.model.HoodieKey;\n@@ -26,13 +33,6 @@\n import org.apache.hudi.common.util.Option;\n import org.apache.hudi.common.util.ValidationUtils;\n import org.apache.hudi.exception.HoodieMetadataException;\n-import org.apache.avro.Schema;\n-import org.apache.avro.generic.GenericRecord;\n-import org.apache.avro.generic.IndexedRecord;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.FileStatus;\n-import org.apache.hadoop.fs.FileSystem;\n-import org.apache.hadoop.fs.Path;\n \n import java.io.IOException;\n import java.util.Arrays;\n@@ -158,7 +158,7 @@ public HoodieMetadataPayload preCombine(HoodieMetadataPayload previousRecord) {\n       return Option.empty();\n     }\n \n-    HoodieMetadataRecord record = new HoodieMetadataRecord(key, type, filesystemMetadata);\n+    HoodieMetadataRecord record = new HoodieMetadataRecord(key, type, filesystemMetadata, null, null, null);\n     return Option.of(record);\n   }\n "
  },
  {
    "sha": "6975bbd8c5bfb4989465f70beb885a35ee3a11c3",
    "filename": "hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieCommonTestHarness.java",
    "status": "modified",
    "additions": 4,
    "deletions": 5,
    "changes": 9,
    "blob_url": "https://github.com/apache/hudi/blob/241e56f6d7d55811119e1a398fc8c80059a81b54/hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieCommonTestHarness.java",
    "raw_url": "https://github.com/apache/hudi/raw/241e56f6d7d55811119e1a398fc8c80059a81b54/hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieCommonTestHarness.java",
    "contents_url": "https://api.github.com/repos/apache/hudi/contents/hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieCommonTestHarness.java?ref=241e56f6d7d55811119e1a398fc8c80059a81b54",
    "patch": "@@ -25,9 +25,9 @@\n import org.apache.hudi.common.table.view.SyncableFileSystemView;\n import org.apache.hudi.exception.HoodieIOException;\n \n-import org.junit.jupiter.api.io.TempDir;\n-\n import java.io.IOException;\n+import java.nio.file.Paths;\n+import java.util.UUID;\n \n /**\n  * The common hoodie test harness to provide the basic infrastructure.\n@@ -37,15 +37,14 @@\n   protected String basePath = null;\n   protected transient HoodieTestDataGenerator dataGen = null;\n   protected transient HoodieTableMetaClient metaClient;\n-  @TempDir\n-  public java.nio.file.Path tempDir;\n+  public java.nio.file.Path tempDir = Paths.get(\"/tmp\");\n \n   /**\n    * Initializes basePath.\n    */\n   protected void initPath() {\n     try {\n-      java.nio.file.Path basePath = tempDir.resolve(\"dataset\");\n+      java.nio.file.Path basePath = tempDir.resolve(\"dataset\" + UUID.randomUUID().toString());\n       java.nio.file.Files.createDirectories(basePath);\n       this.basePath = basePath.toString();\n     } catch (IOException ioe) {"
  }
]
