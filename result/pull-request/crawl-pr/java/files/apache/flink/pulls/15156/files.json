[
  {
    "sha": "fa1e4582394e0334478de19d8d38d11ec9146df5",
    "filename": "flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/AvroRowSerializationSchema.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/apache/flink/blob/18a8fb0c34cfff0c5f7974c5d1571db09b50a1fb/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/AvroRowSerializationSchema.java",
    "raw_url": "https://github.com/apache/flink/raw/18a8fb0c34cfff0c5f7974c5d1571db09b50a1fb/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/AvroRowSerializationSchema.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/AvroRowSerializationSchema.java?ref=18a8fb0c34cfff0c5f7974c5d1571db09b50a1fb",
    "patch": "@@ -157,7 +157,7 @@ public int hashCode() {\n \n     // --------------------------------------------------------------------------------------------\n \n-    private GenericRecord convertRowToAvroRecord(Schema schema, Row row) {\n+    public GenericRecord convertRowToAvroRecord(Schema schema, Row row) {\n         final List<Schema.Field> fields = schema.getFields();\n         final int length = fields.size();\n         final GenericRecord record = new GenericData.Record(schema);"
  },
  {
    "sha": "2daea18fb363a431f263aa9ee3fd4b72b0f695fe",
    "filename": "flink-formats/flink-parquet/pom.xml",
    "status": "modified",
    "additions": 6,
    "deletions": 7,
    "changes": 13,
    "blob_url": "https://github.com/apache/flink/blob/18a8fb0c34cfff0c5f7974c5d1571db09b50a1fb/flink-formats/flink-parquet/pom.xml",
    "raw_url": "https://github.com/apache/flink/raw/18a8fb0c34cfff0c5f7974c5d1571db09b50a1fb/flink-formats/flink-parquet/pom.xml",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-formats/flink-parquet/pom.xml?ref=18a8fb0c34cfff0c5f7974c5d1571db09b50a1fb",
    "patch": "@@ -51,6 +51,12 @@ under the License.\n \t\t\t<scope>provided</scope>\n \t\t</dependency>\n \n+\t\t<dependency>\n+\t\t\t<groupId>org.apache.flink</groupId>\n+\t\t\t<artifactId>flink-avro</artifactId>\n+\t\t\t<version>${project.version}</version>\n+\t\t</dependency>\n+\n \t\t<!-- Table ecosystem -->\n \t\t<!-- Projects depending on this project won't depend on flink-table-*. -->\n \t\t<dependency>\n@@ -172,13 +178,6 @@ under the License.\n \t\t\t<scope>test</scope>\n \t\t</dependency>\n \n-\t\t<dependency>\n-\t\t\t<groupId>org.apache.flink</groupId>\n-\t\t\t<artifactId>flink-avro</artifactId>\n-\t\t\t<version>${project.version}</version>\n-\t\t\t<scope>test</scope>\n-\t\t</dependency>\n-\n \t\t<dependency>\n \t\t\t<groupId>org.apache.flink</groupId>\n \t\t\t<artifactId>flink-table-planner-blink_${scala.binary.version}</artifactId>"
  },
  {
    "sha": "441a704a22ae4c77234d0b3b4412f9bc3b39afe8",
    "filename": "flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ParquetAvroInputFormat.java",
    "status": "added",
    "additions": 120,
    "deletions": 0,
    "changes": 120,
    "blob_url": "https://github.com/apache/flink/blob/18a8fb0c34cfff0c5f7974c5d1571db09b50a1fb/flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ParquetAvroInputFormat.java",
    "raw_url": "https://github.com/apache/flink/raw/18a8fb0c34cfff0c5f7974c5d1571db09b50a1fb/flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ParquetAvroInputFormat.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-formats/flink-parquet/src/main/java/org/apache/flink/formats/parquet/ParquetAvroInputFormat.java?ref=18a8fb0c34cfff0c5f7974c5d1571db09b50a1fb",
    "patch": "@@ -0,0 +1,120 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.parquet;\n+\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.formats.avro.AvroRowSerializationSchema;\n+import org.apache.flink.types.Row;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.parquet.avro.AvroSchemaConverter;\n+import org.apache.parquet.schema.MessageType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * An implementation of {@link ParquetInputFormat} to read records from Parquet files and convert\n+ * them to Avro GenericRecord. Usage:\n+ *\n+ * <pre>{@code\n+ * final ParquetAvroInputFormat inputFormat = new ParquetAvroInputFormat(new Path(filePath), parquetSchema);\n+ * DataSource<GenericRecord> source = env.createInput(inputFormat, new GenericRecordAvroTypeInfo(inputFormat.getAvroSchema()));\n+ *\n+ * }</pre>\n+ */\n+public class ParquetAvroInputFormat extends ParquetInputFormat<GenericRecord> {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(ParquetAvroInputFormat.class);\n+\n+    private transient Schema avroSchema;\n+    private String avroSchemaString;\n+    private AvroRowSerializationSchema avroRowSerializationSchema;\n+\n+    public ParquetAvroInputFormat(Path filePath, MessageType messageType) {\n+        super(filePath, messageType);\n+        avroSchema = new AvroSchemaConverter().convert(messageType);\n+        avroSchemaString = avroSchema.toString();\n+        avroRowSerializationSchema = new AvroRowSerializationSchema(avroSchemaString);\n+    }\n+\n+    @Override\n+    public void selectFields(String[] fieldNames) {\n+        avroSchema = getProjectedSchema(fieldNames, avroSchema);\n+        avroSchemaString = avroSchema.toString();\n+        avroRowSerializationSchema = new AvroRowSerializationSchema(avroSchemaString);\n+        super.selectFields(fieldNames);\n+    }\n+\n+    @Override\n+    protected GenericRecord convert(Row row) {\n+        // after deserialization\n+        if (avroSchema == null) {\n+            avroSchema = new Schema.Parser().parse(avroSchemaString);\n+        }\n+        return avroRowSerializationSchema.convertRowToAvroRecord(avroSchema, row);\n+    }\n+\n+    private Schema getProjectedSchema(String[] projectedFieldNames, Schema sourceAvroSchema) {\n+        // Avro fields need to be in the same order than row field for compatibility with flink 1.12\n+        // (row\n+        // fields not accessible by name). Row field order now is the order of\n+        // ParquetInputFormat.selectFields()\n+        // for flink 1.13+ where row fields are accessible by name, we will match the fields names\n+        // between avro schema and row schema.\n+\n+        List<Schema.Field> projectedFields = new ArrayList<>();\n+        for (String fieldName : projectedFieldNames) {\n+            projectedFields.add(deepCopyField(sourceAvroSchema.getField(fieldName)));\n+        }\n+        return Schema.createRecord(\n+                sourceAvroSchema.getName() + \"_projected\",\n+                sourceAvroSchema.getDoc(),\n+                sourceAvroSchema.getNamespace(),\n+                sourceAvroSchema.isError(),\n+                projectedFields);\n+    }\n+\n+    private Schema.Field deepCopyField(Schema.Field field) {\n+        Schema.Field newField =\n+                new Schema.Field(\n+                        field.name(),\n+                        field.schema(),\n+                        field.doc(),\n+                        field.defaultVal(),\n+                        field.order());\n+        for (Map.Entry<String, Object> kv : field.getObjectProps().entrySet()) {\n+            newField.addProp(kv.getKey(), kv.getValue());\n+        }\n+        if (field.aliases() != null) {\n+            for (String alias : field.aliases()) {\n+                newField.addAlias(alias);\n+            }\n+        }\n+        return newField;\n+    }\n+\n+    public Schema getAvroSchema() {\n+        return avroSchema;\n+    }\n+}"
  },
  {
    "sha": "3c9f1e320898c41cf14c253f17d3634fa3a07189",
    "filename": "flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/ParquetAvroInputFormatTest.java",
    "status": "added",
    "additions": 111,
    "deletions": 0,
    "changes": 111,
    "blob_url": "https://github.com/apache/flink/blob/18a8fb0c34cfff0c5f7974c5d1571db09b50a1fb/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/ParquetAvroInputFormatTest.java",
    "raw_url": "https://github.com/apache/flink/raw/18a8fb0c34cfff0c5f7974c5d1571db09b50a1fb/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/ParquetAvroInputFormatTest.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-formats/flink-parquet/src/test/java/org/apache/flink/formats/parquet/ParquetAvroInputFormatTest.java?ref=18a8fb0c34cfff0c5f7974c5d1571db09b50a1fb",
    "patch": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.formats.parquet;\n+\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.core.fs.FileInputSplit;\n+import org.apache.flink.core.fs.Path;\n+import org.apache.flink.formats.parquet.utils.TestUtil;\n+import org.apache.flink.types.Row;\n+\n+import org.apache.avro.AvroRuntimeException;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.specific.SpecificRecord;\n+import org.apache.parquet.schema.MessageType;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+/** Test cases for reading Parquet files and convert parquet records to Avro GenericRecords. */\n+@RunWith(Parameterized.class)\n+public class ParquetAvroInputFormatTest extends TestUtil {\n+\n+    @ClassRule public static TemporaryFolder tempRoot = new TemporaryFolder();\n+\n+    public ParquetAvroInputFormatTest(boolean useLegacyMode) {\n+        // AvroRowSerializationSchema does not work with parquet.avro.write-old-list-structure set\n+        // to false\n+        super(true);\n+    }\n+\n+    @Test\n+    public void testReadFromSimpleRecord() throws IOException {\n+        Tuple3<Class<? extends SpecificRecord>, SpecificRecord, Row> testData =\n+                getSimpleRecordTestData();\n+        MessageType messageType = getSchemaConverter().convert(SIMPLE_SCHEMA);\n+        Path path =\n+                createTempParquetFile(\n+                        tempRoot.getRoot(),\n+                        SIMPLE_SCHEMA,\n+                        Collections.singletonList(testData.f1),\n+                        getConfiguration());\n+\n+        ParquetAvroInputFormat inputFormat = new ParquetAvroInputFormat(path, messageType);\n+        inputFormat.setRuntimeContext(TestUtil.getMockRuntimeContext());\n+\n+        FileInputSplit[] splits = inputFormat.createInputSplits(1);\n+        assertEquals(1, splits.length);\n+        inputFormat.open(splits[0]);\n+\n+        final GenericRecord genericRecord = inputFormat.nextRecord(null);\n+        assertEquals(testData.f2.getField(0), genericRecord.get(\"foo\"));\n+        assertEquals(testData.f2.getField(1), genericRecord.get(\"bar\").toString());\n+        assertArrayEquals(\n+                (Long[]) testData.f2.getField(2),\n+                ((List<Long>) genericRecord.get(\"arr\")).toArray());\n+    }\n+\n+    @Test(expected = AvroRuntimeException.class)\n+    public void testProjectedReadFromSimpleRecord()\n+            throws IOException, NoSuchFieldError, AvroRuntimeException {\n+        Tuple3<Class<? extends SpecificRecord>, SpecificRecord, Row> testData =\n+                TestUtil.getSimpleRecordTestData();\n+        MessageType messageType = getSchemaConverter().convert(SIMPLE_SCHEMA);\n+        Path path =\n+                createTempParquetFile(\n+                        tempRoot.getRoot(),\n+                        SIMPLE_SCHEMA,\n+                        Collections.singletonList(testData.f1),\n+                        getConfiguration());\n+\n+        ParquetAvroInputFormat inputFormat = new ParquetAvroInputFormat(path, messageType);\n+        inputFormat.setRuntimeContext(getMockRuntimeContext());\n+\n+        FileInputSplit[] splits = inputFormat.createInputSplits(1);\n+        assertEquals(1, splits.length);\n+\n+        inputFormat.selectFields(new String[] {\"foo\", \"bar\"});\n+        inputFormat.open(splits[0]);\n+\n+        final GenericRecord genericRecord = inputFormat.nextRecord(null);\n+        assertEquals(testData.f2.getField(0), genericRecord.get(\"foo\"));\n+        assertEquals(testData.f2.getField(1), genericRecord.get(\"bar\").toString());\n+        // should throw AvroRuntimeException(\"Not a valid schema field: arr\")\n+        genericRecord.get(\"arr\");\n+    }\n+}"
  }
]
