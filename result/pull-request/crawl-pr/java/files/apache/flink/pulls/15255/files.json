[
  {
    "sha": "17ee4f11bc27a1d898c2ef3e60310f9ff6bd72c2",
    "filename": "docs/content.zh/docs/dev/table/sql/gettingStarted.md",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/apache/flink/blob/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/docs/content.zh/docs/dev/table/sql/gettingStarted.md",
    "raw_url": "https://github.com/apache/flink/raw/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/docs/content.zh/docs/dev/table/sql/gettingStarted.md",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/docs/content.zh/docs/dev/table/sql/gettingStarted.md?ref=7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22",
    "patch": "@@ -50,7 +50,7 @@ Flink SQL 使得使用标准 SQL 开发流应用程序变的简单。如果你\n 在安装文件夹中运行 `sql-client` 脚本来启动 SQL 客户端。\n \n  ```bash\n-./bin/sql-client.sh embedded\n+./bin/sql-client.sh\n  ``` \n \n ### Hello World"
  },
  {
    "sha": "87583401ebb3d465d11a5cf47c0b01643de3cda9",
    "filename": "docs/content.zh/docs/dev/table/sqlClient.md",
    "status": "modified",
    "additions": 10,
    "deletions": 4,
    "changes": 14,
    "blob_url": "https://github.com/apache/flink/blob/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/docs/content.zh/docs/dev/table/sqlClient.md",
    "raw_url": "https://github.com/apache/flink/raw/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/docs/content.zh/docs/dev/table/sqlClient.md",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/docs/content.zh/docs/dev/table/sqlClient.md?ref=7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22",
    "patch": "@@ -47,7 +47,13 @@ SQL 客户端捆绑在常规 Flink 发行版中，因此可以直接运行。它\n \n ### 启动 SQL 客户端命令行界面\n \n-SQL Client 脚本也位于 Flink 的 bin 目录中。[将来](sqlClient.html#limitations--future)，用户可以通过启动嵌入式 standalone 进程或通过连接到远程 SQL 客户端网关来启动 SQL 客户端命令行界面。目前仅支持 `embedded` 模式。可以通过以下方式启动 CLI：\n+SQL Client 脚本也位于 Flink 的 bin 目录中。[将来](sqlClient.html#limitations--future)，用户可以通过启动嵌入式 standalone 进程或通过连接到远程 SQL 客户端网关来启动 SQL 客户端命令行界面。目前仅支持 `embedded`，模式默认值`embedded`。可以通过以下方式启动 CLI：\n+\n+```bash\n+./bin/sql-client.sh\n+```\n+\n+或者显式使用 `embedded` 模式:\n \n ```bash\n ./bin/sql-client.sh embedded\n@@ -158,11 +164,11 @@ Received a total of 5 rows\n SQL 客户端启动时可以添加 CLI 选项，具体如下。\n \n ```text\n-./bin/sql-client.sh embedded --help\n+./bin/sql-client.sh --help\n \n-Mode \"embedded\" submits Flink jobs from the local machine.\n+Mode \"embedded\" (default) submits Flink jobs from the local machine.\n \n-  Syntax: embedded [OPTIONS]\n+  Syntax: [embedded] [OPTIONS]\n   \"embedded\" mode options:\n      -d,--defaults <environment file>      The environment properties with which\n                                            every new session is initialized."
  },
  {
    "sha": "2c9559c58b36f679bb749d174abda4faf8f428c0",
    "filename": "docs/content/docs/dev/table/sql/gettingStarted.md",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/apache/flink/blob/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/docs/content/docs/dev/table/sql/gettingStarted.md",
    "raw_url": "https://github.com/apache/flink/raw/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/docs/content/docs/dev/table/sql/gettingStarted.md",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/docs/content/docs/dev/table/sql/gettingStarted.md?ref=7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22",
    "patch": "@@ -50,7 +50,7 @@ The [SQL Client]({{< ref \"docs/dev/table/sqlClient\" >}}) is an interactive clien\n To start the SQL client, run the `sql-client` script from the installation folder.\n \n  ```bash\n-./bin/sql-client.sh embedded\n+./bin/sql-client.sh\n  ``` \n \n ### Hello World"
  },
  {
    "sha": "bfe2fc71d113b08e2cbc22bc6e2a8a4ed33aaa56",
    "filename": "docs/content/docs/dev/table/sqlClient.md",
    "status": "modified",
    "additions": 10,
    "deletions": 4,
    "changes": 14,
    "blob_url": "https://github.com/apache/flink/blob/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/docs/content/docs/dev/table/sqlClient.md",
    "raw_url": "https://github.com/apache/flink/raw/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/docs/content/docs/dev/table/sqlClient.md",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/docs/content/docs/dev/table/sqlClient.md?ref=7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22",
    "patch": "@@ -48,7 +48,13 @@ The SQL Client is bundled in the regular Flink distribution and thus runnable ou\n \n ### Starting the SQL Client CLI\n \n-The SQL Client scripts are also located in the binary directory of Flink. [In the future](sqlClient.html#limitations--future), a user will have two possibilities of starting the SQL Client CLI either by starting an embedded standalone process or by connecting to a remote SQL Client Gateway. At the moment only the `embedded` mode is supported. You can start the CLI by calling:\n+The SQL Client scripts are also located in the binary directory of Flink. [In the future](sqlClient.html#limitations--future), a user will have two possibilities of starting the SQL Client CLI either by starting an embedded standalone process or by connecting to a remote SQL Client Gateway. At the moment only the `embedded` mode is supported, and default mode is `embedded`. You can start the CLI by calling:\n+\n+```bash\n+./bin/sql-client.sh\n+```\n+\n+or explicitly use `embedded` mode:\n \n ```bash\n ./bin/sql-client.sh embedded\n@@ -157,11 +163,11 @@ Configuration\n The SQL Client can be started with the following optional CLI commands. They are discussed in detail in the subsequent paragraphs.\n \n ```text\n-./bin/sql-client.sh embedded --help\n+./bin/sql-client.sh --help\n \n-Mode \"embedded\" submits Flink jobs from the local machine.\n+Mode \"embedded\" (default) submits Flink jobs from the local machine.\n \n-  Syntax: embedded [OPTIONS]\n+  Syntax: [embedded] [OPTIONS]\n   \"embedded\" mode options:\n      -d,--defaults <environment file>      The environment properties with which\n                                            every new session is initialized."
  },
  {
    "sha": "ad167c64bc01377414a338aa6d3e639917976522",
    "filename": "flink-end-to-end-tests/flink-end-to-end-tests-common/src/main/java/org/apache/flink/tests/util/flink/FlinkContainer.java",
    "status": "modified",
    "additions": 0,
    "deletions": 1,
    "changes": 1,
    "blob_url": "https://github.com/apache/flink/blob/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/flink-end-to-end-tests/flink-end-to-end-tests-common/src/main/java/org/apache/flink/tests/util/flink/FlinkContainer.java",
    "raw_url": "https://github.com/apache/flink/raw/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/flink-end-to-end-tests/flink-end-to-end-tests-common/src/main/java/org/apache/flink/tests/util/flink/FlinkContainer.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-end-to-end-tests/flink-end-to-end-tests-common/src/main/java/org/apache/flink/tests/util/flink/FlinkContainer.java?ref=7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22",
    "patch": "@@ -178,7 +178,6 @@ public void submitSQLJob(SQLJobSubmission job) throws IOException, InterruptedEx\n         copyFileToContainer(MountableFile.forHostPath(script), \"/tmp/script.sql\");\n         commands.add(\"cat /tmp/script.sql | \");\n         commands.add(FLINK_BIN + \"/sql-client.sh\");\n-        commands.add(\"embedded\");\n         job.getDefaultEnvFile()\n                 .ifPresent(\n                         defaultEnvFile -> {"
  },
  {
    "sha": "0b1d398996468ba48ab17afea4311ba537ac6730",
    "filename": "flink-end-to-end-tests/flink-end-to-end-tests-common/src/main/java/org/apache/flink/tests/util/flink/FlinkDistribution.java",
    "status": "modified",
    "additions": 0,
    "deletions": 1,
    "changes": 1,
    "blob_url": "https://github.com/apache/flink/blob/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/flink-end-to-end-tests/flink-end-to-end-tests-common/src/main/java/org/apache/flink/tests/util/flink/FlinkDistribution.java",
    "raw_url": "https://github.com/apache/flink/raw/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/flink-end-to-end-tests/flink-end-to-end-tests-common/src/main/java/org/apache/flink/tests/util/flink/FlinkDistribution.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-end-to-end-tests/flink-end-to-end-tests-common/src/main/java/org/apache/flink/tests/util/flink/FlinkDistribution.java?ref=7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22",
    "patch": "@@ -209,7 +209,6 @@ public JobID submitJob(final JobSubmission jobSubmission, Duration timeout) thro\n     public void submitSQLJob(SQLJobSubmission job, Duration timeout) throws IOException {\n         final List<String> commands = new ArrayList<>();\n         commands.add(bin.resolve(\"sql-client.sh\").toAbsolutePath().toString());\n-        commands.add(\"embedded\");\n         job.getDefaultEnvFile()\n                 .ifPresent(\n                         defaultEnvFile -> {"
  },
  {
    "sha": "20c4e7493d36c869367f81382c4c5e68690cfeda",
    "filename": "flink-end-to-end-tests/test-scripts/test_pyflink.sh",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/apache/flink/blob/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/flink-end-to-end-tests/test-scripts/test_pyflink.sh",
    "raw_url": "https://github.com/apache/flink/raw/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/flink-end-to-end-tests/test-scripts/test_pyflink.sh",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-end-to-end-tests/test-scripts/test_pyflink.sh?ref=7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22",
    "patch": "@@ -198,7 +198,7 @@ EOF\n \n SQL_STATEMENT=\"insert into sink select add_one(a) from (VALUES (1), (2), (3)) as source (a)\"\n \n-JOB_ID=$($FLINK_DIR/bin/sql-client.sh embedded \\\n+JOB_ID=$($FLINK_DIR/bin/sql-client.sh \\\n   --environment $SQL_CONF \\\n   -pyfs \"${FLINK_PYTHON_TEST_DIR}/python/add_one.py\" \\\n   -pyreq \"${REQUIREMENTS_PATH}\" \\"
  },
  {
    "sha": "0075b363b76e07a2ed9d87228e21e585ec9c24fc",
    "filename": "flink-end-to-end-tests/test-scripts/test_sql_client.sh",
    "status": "modified",
    "additions": 3,
    "deletions": 3,
    "changes": 6,
    "blob_url": "https://github.com/apache/flink/blob/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/flink-end-to-end-tests/test-scripts/test_sql_client.sh",
    "raw_url": "https://github.com/apache/flink/raw/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/flink-end-to-end-tests/test-scripts/test_sql_client.sh",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-end-to-end-tests/test-scripts/test_sql_client.sh?ref=7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22",
    "patch": "@@ -231,7 +231,7 @@ INSERT INTO ElasticsearchUpsertSinkTable\n EOF\n )\n \n-JOB_ID=$($FLINK_DIR/bin/sql-client.sh embedded \\\n+JOB_ID=$($FLINK_DIR/bin/sql-client.sh \\\n   --jar $KAFKA_SQL_JAR \\\n   --jar $ELASTICSEARCH_SQL_JAR \\\n   --jar $SQL_TOOLBOX_JAR \\\n@@ -259,7 +259,7 @@ INSERT INTO ElasticsearchAppendSinkTable\n EOF\n )\n \n-JOB_ID=$($FLINK_DIR/bin/sql-client.sh embedded \\\n+JOB_ID=$($FLINK_DIR/bin/sql-client.sh \\\n   --jar $KAFKA_SQL_JAR \\\n   --jar $ELASTICSEARCH_SQL_JAR \\\n   --jar $SQL_TOOLBOX_JAR \\\n@@ -291,7 +291,7 @@ INSERT INTO ElasticsearchAppendSinkTable\n EOF\n )\n \n-JOB_ID=$($FLINK_DIR/bin/sql-client.sh embedded \\\n+JOB_ID=$($FLINK_DIR/bin/sql-client.sh \\\n   --jar $KAFKA_SQL_JAR \\\n   --jar $ELASTICSEARCH_SQL_JAR \\\n   --jar $SQL_TOOLBOX_JAR \\"
  },
  {
    "sha": "f2416c1151ed742573c2c5e49def15a1a2931b74",
    "filename": "flink-end-to-end-tests/test-scripts/test_tpch.sh",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/apache/flink/blob/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/flink-end-to-end-tests/test-scripts/test_tpch.sh",
    "raw_url": "https://github.com/apache/flink/raw/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/flink-end-to-end-tests/test-scripts/test_tpch.sh",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-end-to-end-tests/test-scripts/test_tpch.sh?ref=7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22",
    "patch": "@@ -83,7 +83,7 @@ EOF\n         SQL_STATEMENT=\"INSERT INTO q$i $(cat \"$ORIGIN_QUERY_DIR/q$i.sql\")\"\n     fi\n \n-    JOB_ID=$(\"$FLINK_DIR/bin/sql-client.sh\" embedded \\\n+    JOB_ID=$(\"$FLINK_DIR/bin/sql-client.sh\" \\\n         --environment \"$SQL_CONF\" \\\n         --update \"$SQL_STATEMENT\" | grep \"Job ID:\" | sed 's/.* //g')\n "
  },
  {
    "sha": "8e2bcbc48e7fa308a51abfe4fd5045e3916cf0f9",
    "filename": "flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/SqlClient.java",
    "status": "modified",
    "additions": 14,
    "deletions": 8,
    "changes": 22,
    "blob_url": "https://github.com/apache/flink/blob/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/SqlClient.java",
    "raw_url": "https://github.com/apache/flink/raw/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/SqlClient.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/SqlClient.java?ref=7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22",
    "patch": "@@ -44,8 +44,8 @@\n  * <p>- In future versions: In gateway mode, the SQL CLI client connects to the REST API of the\n  * gateway and allows for managing queries via console.\n  *\n- * <p>For debugging in an IDE you can execute the main method of this class using: \"embedded\n- * --defaults /path/to/sql-client-defaults.yaml --jar /path/to/target/flink-sql-client-*.jar\"\n+ * <p>For debugging in an IDE you can execute the main method of this class using: \"--defaults\n+ * /path/to/sql-client-defaults.yaml --jar /path/to/target/flink-sql-client-*.jar\"\n  *\n  * <p>Make sure that the FLINK_CONF_DIR environment variable is set.\n  */\n@@ -125,15 +125,21 @@ private void openCli(String sessionId, Executor executor) {\n     // --------------------------------------------------------------------------------------------\n \n     public static void main(String[] args) {\n-        if (args.length < 1) {\n-            CliOptionsParser.printHelpClient();\n-            return;\n+        final String mode;\n+        final String[] modeArgs;\n+        if (args.length < 1 || args[0].startsWith(\"-\")) {\n+            // mode is not specified, use the default `embedded` mode\n+            mode = MODE_EMBEDDED;\n+            modeArgs = args;\n+        } else {\n+            // mode is specified, extract the mode value and reaming args\n+            mode = args[0];\n+            // remove mode\n+            modeArgs = Arrays.copyOfRange(args, 1, args.length);\n         }\n \n-        switch (args[0]) {\n+        switch (mode) {\n             case MODE_EMBEDDED:\n-                // remove mode\n-                final String[] modeArgs = Arrays.copyOfRange(args, 1, args.length);\n                 final CliOptions options = CliOptionsParser.parseEmbeddedModeClient(modeArgs);\n                 if (options.isPrintHelp()) {\n                     CliOptionsParser.printHelpEmbeddedModeClient();"
  },
  {
    "sha": "79a57976631c19ea13714c233db0d1ef0d6eb5fa",
    "filename": "flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliClient.java",
    "status": "modified",
    "additions": 16,
    "deletions": 1,
    "changes": 17,
    "blob_url": "https://github.com/apache/flink/blob/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliClient.java",
    "raw_url": "https://github.com/apache/flink/raw/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliClient.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliClient.java?ref=7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22",
    "patch": "@@ -20,6 +20,7 @@\n \n import org.apache.flink.annotation.VisibleForTesting;\n import org.apache.flink.table.api.TableResult;\n+import org.apache.flink.table.client.SqlClient;\n import org.apache.flink.table.client.SqlClientException;\n import org.apache.flink.table.client.cli.SqlCommandParser.SqlCommandCall;\n import org.apache.flink.table.client.config.YamlConfigUtils;\n@@ -684,9 +685,23 @@ private void printInfo(String message) {\n \n     // --------------------------------------------------------------------------------------------\n \n+    /**\n+     * Internal flag to use {@link System#in} and {@link System#out} stream to construct {@link\n+     * Terminal} for tests. This allows tests can easily mock input stream when startup {@link\n+     * SqlClient}.\n+     */\n+    protected static boolean useSystemInOutStream = false;\n+\n     private static Terminal createDefaultTerminal() {\n         try {\n-            return TerminalBuilder.builder().name(CliStrings.CLI_NAME).build();\n+            if (useSystemInOutStream) {\n+                return TerminalBuilder.builder()\n+                        .name(CliStrings.CLI_NAME)\n+                        .streams(System.in, System.out)\n+                        .build();\n+            } else {\n+                return TerminalBuilder.builder().name(CliStrings.CLI_NAME).build();\n+            }\n         } catch (IOException e) {\n             throw new SqlClientException(\"Error opening command line interface.\", e);\n         }"
  },
  {
    "sha": "880cbd589d692f43cd0f3ce022d43e4d0ef0080d",
    "filename": "flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliOptionsParser.java",
    "status": "modified",
    "additions": 3,
    "deletions": 2,
    "changes": 5,
    "blob_url": "https://github.com/apache/flink/blob/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliOptionsParser.java",
    "raw_url": "https://github.com/apache/flink/raw/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliOptionsParser.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliOptionsParser.java?ref=7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22",
    "patch": "@@ -216,8 +216,9 @@ public static void printHelpEmbeddedModeClient() {\n         formatter.setLeftPadding(5);\n         formatter.setWidth(80);\n \n-        System.out.println(\"\\nMode \\\"embedded\\\" submits Flink jobs from the local machine.\");\n-        System.out.println(\"\\n  Syntax: embedded [OPTIONS]\");\n+        System.out.println(\n+                \"\\nMode \\\"embedded\\\" (default) submits Flink jobs from the local machine.\");\n+        System.out.println(\"\\n  Syntax: [embedded] [OPTIONS]\");\n         formatter.setSyntaxPrefix(\"  \\\"embedded\\\" mode options:\");\n         formatter.printHelp(\" \", EMBEDDED_MODE_CLIENT_OPTIONS);\n "
  },
  {
    "sha": "128e12676c6ce2c149cbc7271bb57dfc7ce2f07d",
    "filename": "flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/SqlClientTest.java",
    "status": "added",
    "additions": 149,
    "deletions": 0,
    "changes": 149,
    "blob_url": "https://github.com/apache/flink/blob/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/SqlClientTest.java",
    "raw_url": "https://github.com/apache/flink/raw/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/SqlClientTest.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/SqlClientTest.java?ref=7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22",
    "patch": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.client;\n+\n+import org.apache.flink.core.testutils.CommonTestUtils;\n+import org.apache.flink.table.client.cli.TerminalStreamsResource;\n+import org.apache.flink.util.FileUtils;\n+\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.rules.TemporaryFolder;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.PrintStream;\n+import java.net.URL;\n+import java.nio.charset.StandardCharsets;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+import static org.apache.flink.configuration.ConfigConstants.ENV_FLINK_CONF_DIR;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertThat;\n+\n+/** Tests for {@link SqlClient}. */\n+public class SqlClientTest {\n+\n+    @Rule public ExpectedException thrown = ExpectedException.none();\n+\n+    @Rule public TemporaryFolder tempFolder = new TemporaryFolder();\n+\n+    @Rule public final TerminalStreamsResource useSystemStream = TerminalStreamsResource.INSTANCE;\n+\n+    private PrintStream originalPrintStream;\n+\n+    private InputStream originalInputStream;\n+\n+    private ByteArrayOutputStream testOutputStream;\n+\n+    private Map<String, String> originalEnv;\n+\n+    private String historyPath;\n+\n+    @Before\n+    public void before() throws IOException {\n+        originalEnv = System.getenv();\n+        originalPrintStream = System.out;\n+        originalInputStream = System.in;\n+        testOutputStream = new ByteArrayOutputStream();\n+        System.setOut(new PrintStream(testOutputStream, true));\n+        // send \"QUIT;\" command to gracefully shutdown the terminal\n+        System.setIn(new ByteArrayInputStream(\"QUIT;\".getBytes(StandardCharsets.UTF_8)));\n+\n+        // prepare conf dir\n+        File confFolder = tempFolder.newFolder(\"conf\");\n+        File confYaml = new File(confFolder, \"flink-conf.yaml\");\n+        if (!confYaml.createNewFile()) {\n+            throw new IOException(\"Can't create testing flink-conf.yaml file.\");\n+        }\n+\n+        // adjust the test environment for the purposes of this test\n+        Map<String, String> map = new HashMap<>(System.getenv());\n+        map.put(ENV_FLINK_CONF_DIR, confFolder.getAbsolutePath());\n+        CommonTestUtils.setEnv(map);\n+\n+        historyPath = tempFolder.newFile(\"history\").toString();\n+    }\n+\n+    @After\n+    public void after() throws InterruptedException {\n+        System.setOut(originalPrintStream);\n+        System.setIn(originalInputStream);\n+        CommonTestUtils.setEnv(originalEnv);\n+    }\n+\n+    private String getStdoutString() {\n+        return testOutputStream.toString();\n+    }\n+\n+    @Test(timeout = 20000)\n+    public void testEmbeddedWithOptions() throws InterruptedException {\n+        String[] args = new String[] {\"embedded\", \"-hist\", historyPath};\n+        SqlClient.main(args);\n+        assertThat(getStdoutString(), containsString(\"Command history file path: \" + historyPath));\n+    }\n+\n+    @Test(timeout = 20000)\n+    public void testEmbeddedWithLongOptions() throws InterruptedException {\n+        String[] args = new String[] {\"embedded\", \"--history\", historyPath};\n+        SqlClient.main(args);\n+        assertThat(getStdoutString(), containsString(\"Command history file path: \" + historyPath));\n+    }\n+\n+    @Test(timeout = 20000)\n+    public void testEmbeddedWithoutOptions() throws InterruptedException {\n+        String[] args = new String[] {\"embedded\"};\n+        SqlClient.main(args);\n+        assertThat(getStdoutString(), containsString(\"Command history file path\"));\n+    }\n+\n+    @Test(timeout = 20000)\n+    public void testEmptyOptions() throws InterruptedException {\n+        String[] args = new String[] {};\n+        SqlClient.main(args);\n+        assertThat(getStdoutString(), containsString(\"Command history file path\"));\n+    }\n+\n+    @Test(timeout = 20000)\n+    public void testUnsupportedGatewayMode() throws Exception {\n+        String[] args = new String[] {\"gateway\"};\n+        thrown.expect(SqlClientException.class);\n+        thrown.expectMessage(\"Gateway mode is not supported yet.\");\n+        SqlClient.main(args);\n+    }\n+\n+    @Test(timeout = 20000)\n+    public void testPrintHelpForUnknownMode() throws IOException {\n+        String[] args = new String[] {\"unknown\"};\n+        SqlClient.main(args);\n+        final URL url = getClass().getClassLoader().getResource(\"sql-client-help.out\");\n+        Objects.requireNonNull(url);\n+        final String help = FileUtils.readFileUtf8(new File(url.getFile()));\n+        assertEquals(help, getStdoutString());\n+    }\n+}"
  },
  {
    "sha": "85a58492691923277549119fea87e5f52332aaae",
    "filename": "flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/cli/TerminalStreamsResource.java",
    "status": "added",
    "additions": 45,
    "deletions": 0,
    "changes": 45,
    "blob_url": "https://github.com/apache/flink/blob/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/cli/TerminalStreamsResource.java",
    "raw_url": "https://github.com/apache/flink/raw/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/cli/TerminalStreamsResource.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/cli/TerminalStreamsResource.java?ref=7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22",
    "patch": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.client.cli;\n+\n+import org.junit.rules.ExternalResource;\n+\n+/**\n+ * Enables {@link org.apache.flink.table.client.SqlClient} to create a default terminal using {@link\n+ * System#in} and {@link System#out} as the input and output stream. This can allows tests to easily\n+ * mock input stream of the SqlClient by hijacking the standard stream.\n+ */\n+public class TerminalStreamsResource extends ExternalResource {\n+\n+    public static final TerminalStreamsResource INSTANCE = new TerminalStreamsResource();\n+\n+    private TerminalStreamsResource() {\n+        // singleton\n+    }\n+\n+    @Override\n+    protected void before() throws Throwable {\n+        CliClient.useSystemInOutStream = true;\n+    }\n+\n+    @Override\n+    protected void after() {\n+        CliClient.useSystemInOutStream = false;\n+    }\n+}"
  },
  {
    "sha": "fe9173f0740182a21519b39b6f439511e14695f3",
    "filename": "flink-table/flink-sql-client/src/test/resources/sql-client-help.out",
    "status": "added",
    "additions": 116,
    "deletions": 0,
    "changes": 116,
    "blob_url": "https://github.com/apache/flink/blob/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/flink-table/flink-sql-client/src/test/resources/sql-client-help.out",
    "raw_url": "https://github.com/apache/flink/raw/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/flink-table/flink-sql-client/src/test/resources/sql-client-help.out",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-table/flink-sql-client/src/test/resources/sql-client-help.out?ref=7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22",
    "patch": "@@ -0,0 +1,116 @@\n+./sql-client [MODE] [OPTIONS]\n+\n+The following options are available:\n+\n+Mode \"embedded\" (default) submits Flink jobs from the local machine.\n+\n+  Syntax: [embedded] [OPTIONS]\n+  \"embedded\" mode options:\n+     -d,--defaults <environment file>      The environment properties with which\n+                                           every new session is initialized.\n+                                           Properties might be overwritten by\n+                                           session properties.\n+     -e,--environment <environment file>   The environment properties to be\n+                                           imported into the session. It might\n+                                           overwrite default environment\n+                                           properties.\n+     -h,--help                             Show the help message with\n+                                           descriptions of all options.\n+     -hist,--history <History file path>   The file which you want to save the\n+                                           command history into. If not\n+                                           specified, we will auto-generate one\n+                                           under your user's home directory.\n+     -j,--jar <JAR file>                   A JAR file to be imported into the\n+                                           session. The file might contain\n+                                           user-defined classes needed for the\n+                                           execution of statements such as\n+                                           functions, table sources, or sinks.\n+                                           Can be used multiple times.\n+     -l,--library <JAR directory>          A JAR file directory with which every\n+                                           new session is initialized. The files\n+                                           might contain user-defined classes\n+                                           needed for the execution of\n+                                           statements such as functions, table\n+                                           sources, or sinks. Can be used\n+                                           multiple times.\n+     -pyarch,--pyArchives <arg>            Add python archive files for job. The\n+                                           archive files will be extracted to\n+                                           the working directory of python UDF\n+                                           worker. Currently only zip-format is\n+                                           supported. For each archive file, a\n+                                           target directory be specified. If the\n+                                           target directory name is specified,\n+                                           the archive file will be extracted to\n+                                           a name can directory with the\n+                                           specified name. Otherwise, the\n+                                           archive file will be extracted to a\n+                                           directory with the same name of the\n+                                           archive file. The files uploaded via\n+                                           this option are accessible via\n+                                           relative path. '#' could be used as\n+                                           the separator of the archive file\n+                                           path and the target directory name.\n+                                           Comma (',') could be used as the\n+                                           separator to specify multiple archive\n+                                           files. This option can be used to\n+                                           upload the virtual environment, the\n+                                           data files used in Python UDF (e.g.:\n+                                           --pyArchives\n+                                           file:///tmp/py37.zip,file:///tmp/data\n+                                           .zip#data --pyExecutable\n+                                           py37.zip/py37/bin/python). The data\n+                                           files could be accessed in Python\n+                                           UDF, e.g.: f = open('data/data.txt',\n+                                           'r').\n+     -pyexec,--pyExecutable <arg>          Specify the path of the python\n+                                           interpreter used to execute the\n+                                           python UDF worker (e.g.:\n+                                           --pyExecutable\n+                                           /usr/local/bin/python3). The python\n+                                           UDF worker depends on Python 3.6+,\n+                                           Apache Beam (version == 2.27.0), Pip\n+                                           (version >= 7.1.0) and SetupTools\n+                                           (version >= 37.0.0). Please ensure\n+                                           that the specified environment meets\n+                                           the above requirements.\n+     -pyfs,--pyFiles <pythonFiles>         Attach custom python files for job.\n+                                           The standard python resource file\n+                                           suffixes such as .py/.egg/.zip or\n+                                           directory are all supported. These\n+                                           files will be added to the PYTHONPATH\n+                                           of both the local client and the\n+                                           remote python UDF worker. Files\n+                                           suffixed with .zip will be extracted\n+                                           and added to PYTHONPATH. Comma (',')\n+                                           could be used as the separator to\n+                                           specify multiple files (e.g.:\n+                                           --pyFiles\n+                                           file:///tmp/myresource.zip,hdfs:///$n\n+                                           amenode_address/myresource2.zip).\n+     -pyreq,--pyRequirements <arg>         Specify a requirements.txt file which\n+                                           defines the third-party dependencies.\n+                                           These dependencies will be installed\n+                                           and added to the PYTHONPATH of the\n+                                           python UDF worker. A directory which\n+                                           contains the installation packages of\n+                                           these dependencies could be specified\n+                                           optionally. Use '#' as the separator\n+                                           if the optional parameter exists\n+                                           (e.g.: --pyRequirements\n+                                           file:///tmp/requirements.txt#file:///\n+                                           tmp/cached_dir).\n+     -s,--session <session identifier>     The identifier for a session.\n+                                           'default' is the default identifier.\n+     -u,--update <SQL update statement>    Experimental (for testing only!):\n+                                           Instructs the SQL Client to\n+                                           immediately execute the given update\n+                                           statement after starting up. The\n+                                           process is shut down after the\n+                                           statement has been submitted to the\n+                                           cluster and returns an appropriate\n+                                           return code. Currently, this feature\n+                                           is only supported for INSERT INTO\n+                                           statements that declare the target\n+                                           sink table.\n+\n+"
  },
  {
    "sha": "b1d5bc8f048fc7b3fbb9216c59200e9641ba5aac",
    "filename": "pom.xml",
    "status": "modified",
    "additions": 1,
    "deletions": 0,
    "changes": 1,
    "blob_url": "https://github.com/apache/flink/blob/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/pom.xml",
    "raw_url": "https://github.com/apache/flink/raw/7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22/pom.xml",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/pom.xml?ref=7bdb57fdd9c7db5906ee1f2d28c1b542b875ae22",
    "patch": "@@ -1480,6 +1480,7 @@ under the License.\n \t\t\t\t\t\t<exclude>flink-core/src/test/resources/kryo-serializer-config-snapshot-v1</exclude>\n \t\t\t\t\t\t<exclude>flink-formats/flink-avro/src/test/resources/avro/*.avsc</exclude>\n \t\t\t\t\t\t<exclude>out/test/flink-avro/avro/user.avsc</exclude>\n+\t\t\t\t\t\t<exclude>flink-table/flink-sql-client/src/test/resources/*.out</exclude>\n \t\t\t\t\t\t<exclude>flink-table/flink-table-planner/src/test/scala/resources/*.out</exclude>\n \t\t\t\t\t\t<exclude>flink-table/flink-table-planner-blink/src/test/resources/**/*.out</exclude>\n \t\t\t\t\t\t<exclude>flink-yarn/src/test/resources/krb5.keytab</exclude>"
  }
]
