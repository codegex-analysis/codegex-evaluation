[
  {
    "sha": "931b06df0e8040ebba3f877d468f17ac2be79595",
    "filename": "flink-connectors/flink-connector-hbase-unified/README.md",
    "status": "added",
    "additions": 122,
    "deletions": 0,
    "changes": 122,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/README.md",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/README.md",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/README.md?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,122 @@\n+# Flink HBase Connector\n+\n+This module provides connectors that allow Flink to access [HBase](https://hbase.apache.org/) using [CDC](https://en.wikipedia.org/wiki/Change_data_capture). \n+It supports the new Source and Sink API specified in [FLIP-27](https://cwiki.apache.org/confluence/display/FLINK/FLIP-27%3A+Refactor+Source+Interface) and [FLIP-143](https://cwiki.apache.org/confluence/display/FLINK/FLIP-143%3A+Unified+Sink+API).\n+\n+## Installing HBase\n+\n+Follow the instructions from the [HBase Quick Start Guide](http://hbase.apache.org/book.html#quickstart) to install HBase.\n+\n+*Version Compatibility*: This module is compatible with Apache HBase *2.0.0+*.\n+\n+## HBase Configuration\n+\n+Connecting to HBase always requires a `Configuration` instance.\n+If there is an HBase gateway on the same host as the Flink gateway where the application is started, this can be obtained by invoking `HBaseConfiguration.create()` as in the examples below.\n+If that's not the case a configuration should be provided where the proper core-site, hdfs-site, and hbase-site are added as resources.\n+\n+The application needs the following permissions for HBase: \n+- Create/delete ZooKeeper paths\n+- Register/unregister replication peers\n+\n+## DataStream API\n+\n+### Reading data from HBase\n+\n+To receive data from HBase, the connector makes use of the internal replication mechanism of HBase. \n+The connector registers at the HBase cluster as a *Replication Peer* and will receive all change events from HBase.\n+\n+For the replication to work, the HBase config needs to have replication enabled in the `hbase-site.xml` file.\n+This needs be done only once per cluster:\n+```xml\n+<configuration>\n+  <property>\n+    <name>hbase.replication</name>\n+    <value>true</value>\n+  </property>\n+  ...\n+</configuration>\n+```\n+All incoming events to Flink will be processed as an `HBaseSourceEvent`. \n+You will need to specify a Deserializer which will transform each event from an `HBaseSourceEvent` to the desired DataStream type.\n+\n+```java\n+StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\n+Configuration hbaseConfig = HBaseConfiguration.create();\n+String tableName = \"TestTable\";\n+\n+HBaseSource<String> hbaseSource =\n+    HBaseSource.builder()\n+        .setTableName(tableName)\n+        .setSourceDeserializer(new HBaseStringDeserializer())\n+        .setHBaseConfiguration(hbaseConfig)\n+        .build();\n+\n+DataStream<String> stream = env.fromSource(\n+        hbaseSource,\n+        WatermarkStrategy.noWatermarks(),\n+        \"HBaseSource\");\n+// ...\n+```\n+\n+The Deserializer is created as follows:\n+\n+```java\n+static class HBaseStringDeserializer implements HBaseSourceDeserializer<String> {\n+    @Override\n+    public String deserialize(HBaseSourceEvent event) {\n+        return new String(event.getPayload(), HBaseEvent.DEFAULT_CHARSET);\n+    }\n+}\n+```\n+\n+### Writing data to HBase\n+To write data from Flink into HBase, you can use the `HBaseSink`.\n+Similar to the `HBaseSource` you need to specify a Serializer which knows how to write your DataStream element into HBase.\n+\n+```java\n+StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\n+Configuration hbaseConfig = HBaseConfiguration.create();\n+String tableName = \"TestTable\";\n+\n+DataStream<Long> longStream = env.fromSequence(0, 100);\n+\n+HBaseSink<Long> hbaseSink =\n+    HBaseSink.builder()\n+        .setTableName(tableName)\n+        .setSinkSerializer(new HBaseLongSerializer())\n+        .setHBaseConfiguration(hbaseConfig)\n+        .build();\n+\n+longStream.sinkTo(hbaseSink);\n+// ...\n+```\n+An example Serializer is given below. You need to implement the following five methods, so the connector \n+knows how to save the data to HBase.\n+```java\n+static class HBaseLongSerializer implements HBaseSinkSerializer<Long> {\n+    @Override\n+    public HBaseEvent serialize(Long event) {\n+        return HBaseEvent.putWith(                  // or deleteWith()                 \n+                event.toString(),                   // rowId\n+                \"exampleColumnFamily\",              // column family\n+                \"exampleQualifier\",                 // qualifier\n+                Bytes.toBytes(event.toString()));   // payload\n+    }\n+}\n+```\n+\n+## Building the connector\n+\n+Note that the streaming connectors are not part of the binary distribution of Flink.\n+You need to link them into your job jar for cluster execution.\n+See how to link with them for cluster execution [here](https://ci.apache.org/projects/flink/flink-docs-stable/dev/project-configuration.html#adding-connector-and-library-dependencies).\n+\n+The connector can be built by using maven:\n+\n+```\n+cd flink-connectors/flink-connector-hbase\n+mvn clean install\n+```"
  },
  {
    "sha": "b31968799d1f5d25b6ca81ef3e33fb5076ebaebb",
    "filename": "flink-connectors/flink-connector-hbase-unified/pom.xml",
    "status": "added",
    "additions": 169,
    "deletions": 0,
    "changes": 169,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/pom.xml",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/pom.xml",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/pom.xml?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,169 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+  http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing,\n+software distributed under the License is distributed on an\n+\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+KIND, either express or implied.  See the License for the\n+specific language governing permissions and limitations\n+under the License.\n+-->\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n+\t\t xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n+\t\t xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+\t<parent>\n+\t\t<artifactId>flink-connectors</artifactId>\n+\t\t<groupId>org.apache.flink</groupId>\n+\t\t<version>1.13-SNAPSHOT</version>\n+\t\t<relativePath>..</relativePath>\n+\t</parent>\n+\t<modelVersion>4.0.0</modelVersion>\n+\n+\t<artifactId>flink-connector-hbase-unified</artifactId>\n+\t<name>Flink : Connectors : HBase : Unified</name>\n+\n+\t<packaging>jar</packaging>\n+\n+\t<properties>\n+\t\t<hbase.version>2.3.4</hbase.version>\n+\t</properties>\n+\n+\t<dependencies>\n+\n+\n+\t\t<!-- Add HBase 2.x as a dependency -->\n+\t\t<dependency>\n+\t\t\t<groupId>org.apache.hbase</groupId>\n+\t\t\t<artifactId>hbase-testing-util</artifactId>\n+\t\t\t<version>${hbase.version}</version>\n+\t\t\t<scope>test</scope>\n+\t\t\t<!-- Exclude forbidden dependencies -->\n+\t\t\t<exclusions>\n+\t\t\t\t<exclusion>\n+\t\t\t\t\t<groupId>log4j</groupId>\n+\t\t\t\t\t<artifactId>log4j</artifactId>\n+\t\t\t\t</exclusion>\n+\t\t\t\t<exclusion>\n+\t\t\t\t\t<groupId>org.slf4j</groupId>\n+\t\t\t\t\t<artifactId>slf4j-log4j12</artifactId>\n+\t\t\t\t</exclusion>\n+\t\t\t</exclusions>\n+\t\t</dependency>\n+\n+\t\t<dependency>\n+\t\t\t<groupId>org.apache.hbase</groupId>\n+\t\t\t<artifactId>hbase-common</artifactId>\n+\t\t\t<version>${hbase.version}</version>\n+\t\t</dependency>\n+\n+\t\t<dependency>\n+\t\t\t<groupId>org.apache.hbase</groupId>\n+\t\t\t<artifactId>hbase-client</artifactId>\n+\t\t\t<version>${hbase.version}</version>\n+\t\t</dependency>\n+\n+\t\t<dependency>\n+\t\t\t<groupId>org.apache.hbase</groupId>\n+\t\t\t<artifactId>hbase-server</artifactId>\n+\t\t\t<version>${hbase.version}</version>\n+\n+\t\t\t<exclusions>\n+\t\t\t\t<exclusion>\n+\t\t\t\t\t<groupId>*</groupId>\n+\t\t\t\t\t<artifactId>*</artifactId>\n+\t\t\t\t</exclusion>\n+\t\t\t</exclusions>\n+\t\t</dependency>\n+\n+\t\t<dependency>\n+\t\t\t<groupId>org.apache.hbase</groupId>\n+\t\t\t<artifactId>hbase-zookeeper</artifactId>\n+\t\t\t<version>${hbase.version}</version>\n+\t\t</dependency>\n+\n+\t\t<dependency>\n+\t\t\t<groupId>org.apache.hbase.thirdparty</groupId>\n+\t\t\t<artifactId>hbase-shaded-protobuf</artifactId>\n+\t\t\t<version>3.4.1</version>\n+\t\t</dependency>\n+\n+\n+\n+\t\t<!-- streaming-java dependencies -->\n+\t\t<dependency>\n+\t\t\t<groupId>org.apache.flink</groupId>\n+\t\t\t<artifactId>flink-streaming-java_${scala.binary.version}</artifactId>\n+\t\t\t<version>${project.version}</version>\n+\t\t\t<scope>provided</scope>\n+\t\t</dependency>\n+\n+\t\t<!-- For creating other sources for testing etc. -->\n+\t\t<dependency>\n+\t\t\t<groupId>org.apache.flink</groupId>\n+\t\t\t<artifactId>flink-clients_${scala.binary.version}</artifactId>\n+\t\t\t<version>${project.version}</version>\n+\t\t\t<scope>provided</scope>\n+\t\t</dependency>\n+\n+\t\t<!-- Flink connector base -->\n+\t\t<dependency>\n+\t\t\t<groupId>org.apache.flink</groupId>\n+\t\t\t<artifactId>flink-connector-base</artifactId>\n+\t\t\t<version>${project.version}</version>\n+\t\t\t<scope>provided</scope>\n+\t\t</dependency>\n+\n+\t\t<!-- Hadoop dependencies bumped to 2.10 because compatibility with hbase (especially testcluster) -->\n+\t\t<dependency>\n+\t\t\t<groupId>org.apache.hadoop</groupId>\n+\t\t\t<artifactId>hadoop-common</artifactId>\n+\t\t\t<version>2.10.0</version>\n+\t\t</dependency>\n+\n+\t\t<dependency>\n+\t\t\t<groupId>org.apache.hadoop</groupId>\n+\t\t\t<artifactId>hadoop-hdfs</artifactId>\n+\t\t\t<version>2.10.0</version>\n+\t\t</dependency>\n+\n+\t\t<dependency>\n+\t\t\t<groupId>org.apache.zookeeper</groupId>\n+\t\t\t<artifactId>zookeeper</artifactId>\n+\t\t\t<version>3.5.8</version>\n+\t\t</dependency>\n+        <dependency>\n+            <groupId>org.apache.flink</groupId>\n+            <artifactId>flink-core</artifactId>\n+            <version>${project.version}</version>\n+            <scope>test</scope>\n+        </dependency>\n+\t\t<dependency>\n+\t\t\t<groupId>org.apache.flink</groupId>\n+\t\t\t<artifactId>flink-core</artifactId>\n+\t\t\t<version>${project.version}</version>\n+\t\t</dependency>\n+\t\t<dependency>\n+\t\t\t<groupId>org.apache.flink</groupId>\n+\t\t\t<artifactId>flink-connector-hbase-base_${scala.binary.version}</artifactId>\n+\t\t\t<version>${project.version}</version>\n+\t\t\t<scope>compile</scope>\n+\t\t</dependency>\n+        <dependency>\n+            <groupId>org.apache.flink</groupId>\n+            <artifactId>flink-test-utils_${scala.binary.version}</artifactId>\n+            <version>${project.version}</version>\n+            <scope>test</scope>\n+        </dependency>\n+\n+    </dependencies>\n+\n+</project>"
  },
  {
    "sha": "e2bed8cdfbf5b056832341ece1019114b1a4cf2c",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/HBaseEvent.java",
    "status": "added",
    "additions": 132,
    "deletions": 0,
    "changes": 132,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/HBaseEvent.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/HBaseEvent.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/HBaseEvent.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase;\n+\n+import org.apache.flink.connector.hbase.sink.HBaseSinkSerializer;\n+import org.apache.flink.connector.hbase.source.HBaseSource;\n+import org.apache.flink.connector.hbase.source.reader.HBaseSourceEvent;\n+\n+import org.apache.hadoop.hbase.Cell;\n+\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.Objects;\n+\n+/**\n+ * The base HBaseEvent which needs to be created in the {@link HBaseSinkSerializer} to write data to\n+ * HBase. The subclass {@link HBaseSourceEvent} contains additional information and is used by the\n+ * {@link HBaseSource} to represent an incoming event from HBase.\n+ */\n+public class HBaseEvent {\n+\n+    public static final Charset DEFAULT_CHARSET = StandardCharsets.UTF_8;\n+\n+    private final String rowId;\n+    private final String columnFamily;\n+    private final String qualifier;\n+    private final byte[] payload;\n+    private final Cell.Type type;\n+\n+    protected HBaseEvent(\n+            Cell.Type type, String rowId, String columnFamily, String qualifier, byte[] payload) {\n+        this.rowId = rowId;\n+        this.columnFamily = columnFamily;\n+        this.qualifier = qualifier;\n+        this.payload = payload;\n+        this.type = type;\n+    }\n+\n+    public static HBaseEvent deleteWith(String rowId, String columnFamily, String qualifier) {\n+        return new HBaseEvent(Cell.Type.Delete, rowId, columnFamily, qualifier, null);\n+    }\n+\n+    public static HBaseEvent putWith(\n+            String rowId, String columnFamily, String qualifier, byte[] payload) {\n+        return new HBaseEvent(Cell.Type.Put, rowId, columnFamily, qualifier, payload);\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return type.name()\n+                + \" \"\n+                + rowId\n+                + \" \"\n+                + columnFamily\n+                + \" \"\n+                + qualifier\n+                + \" \"\n+                + new String(payload);\n+    }\n+\n+    public Cell.Type getType() {\n+        return type;\n+    }\n+\n+    public byte[] getPayload() {\n+        return payload;\n+    }\n+\n+    public String getRowId() {\n+        return rowId;\n+    }\n+\n+    public String getColumnFamily() {\n+        return columnFamily;\n+    }\n+\n+    public String getQualifier() {\n+        return qualifier;\n+    }\n+\n+    public byte[] getRowIdBytes() {\n+        return rowId.getBytes(DEFAULT_CHARSET);\n+    }\n+\n+    public byte[] getColumnFamilyBytes() {\n+        return columnFamily.getBytes(DEFAULT_CHARSET);\n+    }\n+\n+    public byte[] getQualifierBytes() {\n+        return qualifier.getBytes(DEFAULT_CHARSET);\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+        if (this == o) {\n+            return true;\n+        }\n+        if (o == null || getClass() != o.getClass()) {\n+            return false;\n+        }\n+        HBaseEvent that = (HBaseEvent) o;\n+        return rowId.equals(that.rowId)\n+                && columnFamily.equals(that.columnFamily)\n+                && qualifier.equals(that.qualifier)\n+                && Arrays.equals(payload, that.payload)\n+                && type == that.type;\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+        int result = Objects.hash(rowId, columnFamily, qualifier, type);\n+        result = 31 * result + Arrays.hashCode(payload);\n+        return result;\n+    }\n+}"
  },
  {
    "sha": "2627bfd71d464cc52380f7f6308abe6ae805c74a",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/sink/HBaseSink.java",
    "status": "added",
    "additions": 124,
    "deletions": 0,
    "changes": 124,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/sink/HBaseSink.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/sink/HBaseSink.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/sink/HBaseSink.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.sink;\n+\n+import org.apache.flink.api.connector.sink.Committer;\n+import org.apache.flink.api.connector.sink.GlobalCommitter;\n+import org.apache.flink.api.connector.sink.Sink;\n+import org.apache.flink.api.connector.sink.SinkWriter;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.hbase.sink.writer.HBaseWriter;\n+import org.apache.flink.connector.hbase.util.HBaseConfigurationUtil;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+\n+import org.apache.hadoop.hbase.client.Mutation;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.List;\n+import java.util.Optional;\n+\n+/**\n+ * A Sink Connector for HBase. Please use an {@link HBaseSinkBuilder} to construct a {@link\n+ * HBaseSink}. As HBase does not support transactions, this sink does not guarantee exactly-once,\n+ * but at-least-once.\n+ *\n+ * <p>The following example shows how to create an HBaseSink that writes Long values to HBase.\n+ *\n+ * <pre>{@code\n+ * HBaseSink<Long> hbaseSink =\n+ *      HBaseSink.builder()\n+ *          .setTableName(tableName)\n+ *          .setSinkSerializer(new HBaseLongSerializer())\n+ *          .setHBaseConfiguration(hbaseConfig)\n+ *          .build();\n+ * }</pre>\n+ *\n+ * <p>Here is an example for the Serializer:\n+ *\n+ * <pre>{@code\n+ * static class HBaseLongSerializer implements HBaseSinkSerializer<Long> {\n+ *     @Override\n+ *     public HBaseEvent serialize(Long event) {\n+ *         return HBaseEvent.putWith(                  // or deleteWith()\n+ *                 event.toString(),                   // rowId\n+ *                 \"exampleColumnFamily\",              // column family\n+ *                 \"exampleQualifier\",                 // qualifier\n+ *                 Bytes.toBytes(event.toString()));   // payload\n+ *     }\n+ * }\n+ * }</pre>\n+ *\n+ * @see HBaseSinkBuilder HBaseSinkBuilder for more details on creation\n+ * @see HBaseSinkSerializer HBaseSinkSerializer for more details on serialization\n+ */\n+public class HBaseSink<IN> implements Sink<IN, Void, Mutation, Void> {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(HBaseSink.class);\n+\n+    private final HBaseSinkSerializer<IN> sinkSerializer;\n+    private final byte[] serializedHBaseConfig;\n+    private final Configuration sinkConfiguration;\n+\n+    HBaseSink(\n+            HBaseSinkSerializer<IN> sinkSerializer,\n+            org.apache.hadoop.conf.Configuration hbaseConfiguration,\n+            Configuration sinkConfiguration) {\n+        this.sinkSerializer = sinkSerializer;\n+        this.serializedHBaseConfig =\n+                HBaseConfigurationUtil.serializeConfiguration(hbaseConfiguration);\n+        this.sinkConfiguration = sinkConfiguration;\n+        LOG.debug(\"constructed sink\");\n+    }\n+\n+    public static <IN> HBaseSinkBuilder<IN> builder() {\n+        return new HBaseSinkBuilder<>();\n+    }\n+\n+    @Override\n+    public SinkWriter<IN, Void, Mutation> createWriter(InitContext context, List<Mutation> states) {\n+        return new HBaseWriter<>(\n+                context, states, sinkSerializer, serializedHBaseConfig, sinkConfiguration);\n+    }\n+\n+    @Override\n+    public Optional<Committer<Void>> createCommitter() {\n+        return Optional.empty();\n+    }\n+\n+    @Override\n+    public Optional<GlobalCommitter<Void, Void>> createGlobalCommitter() {\n+        return Optional.empty();\n+    }\n+\n+    @Override\n+    public Optional<SimpleVersionedSerializer<Void>> getCommittableSerializer() {\n+        return Optional.empty();\n+    }\n+\n+    @Override\n+    public Optional<SimpleVersionedSerializer<Void>> getGlobalCommittableSerializer() {\n+        return Optional.empty();\n+    }\n+\n+    @Override\n+    public Optional<SimpleVersionedSerializer<Mutation>> getWriterStateSerializer() {\n+        return Optional.of(new HBaseSinkMutationSerializer());\n+    }\n+}"
  },
  {
    "sha": "368dccda666258b2ba693771468ae6633400f0b5",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/sink/HBaseSinkBuilder.java",
    "status": "added",
    "additions": 117,
    "deletions": 0,
    "changes": 117,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/sink/HBaseSinkBuilder.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/sink/HBaseSinkBuilder.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/sink/HBaseSinkBuilder.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.sink;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.configuration.Configuration;\n+\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * The builder class to create an {@link HBaseSink}. The following example shows how to create an\n+ * HBaseSink that writes Long values to HBase.\n+ *\n+ * <pre>{@code\n+ * HBaseSink<Long> hbaseSink = HBaseSink.builder()\n+ *          .setTableName(tableName)\n+ *          .setSinkSerializer(new HBaseLongSerializer())\n+ *          .setHBaseConfiguration(hbaseConfig)\n+ *          .build();\n+ * }</pre>\n+ *\n+ * <p>Here is an example for the Serializer:\n+ *\n+ * <pre>{@code\n+ * static class HBaseLongSerializer implements HBaseSinkSerializer<Long> {\n+ *     @Override\n+ *     public HBaseEvent serialize(Long event) {\n+ *         return HBaseEvent.putWith(                  // or deleteWith()\n+ *                 event.toString(),                   // rowId\n+ *                 \"exampleColumnFamily\",              // column family\n+ *                 \"exampleQualifier\",                 // qualifier\n+ *                 Bytes.toBytes(event.toString()));   // payload\n+ *     }\n+ * }\n+ * }</pre>\n+ *\n+ * <p>A SerializationSchema is always required, as well as a table name to write to and an\n+ * HBaseConfiguration.\n+ *\n+ * <p>By default each HBaseWriter has a queue limit of 1000 entries used for batching. This can be\n+ * changed with {@link #setQueueLimit(int)}. The maximum allowed latency of an event can be set by\n+ * {@link #setMaxLatencyMs(int)}. After the specified elements will be sent to HBase no matter how\n+ * many elements are currently in the batching queue.\n+ */\n+public class HBaseSinkBuilder<IN> {\n+\n+    private static final ConfigOption<?>[] REQUIRED_CONFIGS = {HBaseSinkOptions.TABLE_NAME};\n+    private final Configuration sinkConfiguration;\n+    private org.apache.hadoop.conf.Configuration hbaseConfiguration;\n+    private HBaseSinkSerializer<IN> sinkSerializer;\n+\n+    protected HBaseSinkBuilder() {\n+        this.sinkSerializer = null;\n+        this.hbaseConfiguration = null;\n+        this.sinkConfiguration = new Configuration();\n+    }\n+\n+    public HBaseSinkBuilder<IN> setTableName(String tableName) {\n+        sinkConfiguration.setString(HBaseSinkOptions.TABLE_NAME, tableName);\n+        return this;\n+    }\n+\n+    public <T extends IN> HBaseSinkBuilder<T> setSinkSerializer(\n+            HBaseSinkSerializer<T> sinkSerializer) {\n+        final HBaseSinkBuilder<T> sinkBuilder = (HBaseSinkBuilder<T>) this;\n+        sinkBuilder.sinkSerializer = sinkSerializer;\n+        return sinkBuilder;\n+    }\n+\n+    public HBaseSinkBuilder<IN> setHBaseConfiguration(\n+            org.apache.hadoop.conf.Configuration hbaseConfiguration) {\n+        this.hbaseConfiguration = hbaseConfiguration;\n+        return this;\n+    }\n+\n+    public HBaseSinkBuilder<IN> setQueueLimit(int queueLimit) {\n+        sinkConfiguration.setInteger(HBaseSinkOptions.QUEUE_LIMIT, queueLimit);\n+        return this;\n+    }\n+\n+    public HBaseSinkBuilder<IN> setMaxLatencyMs(int maxLatencyMs) {\n+        sinkConfiguration.setInteger(HBaseSinkOptions.MAX_LATENCY, maxLatencyMs);\n+        return this;\n+    }\n+\n+    public HBaseSink<IN> build() {\n+        sanityCheck();\n+        return new HBaseSink<>(sinkSerializer, hbaseConfiguration, sinkConfiguration);\n+    }\n+\n+    private void sanityCheck() {\n+        for (ConfigOption<?> requiredConfig : REQUIRED_CONFIGS) {\n+            checkNotNull(\n+                    sinkConfiguration.get(requiredConfig),\n+                    String.format(\"Config option %s is required but not provided\", requiredConfig));\n+        }\n+\n+        checkNotNull(sinkSerializer, \"No sink serializer was specified.\");\n+        checkNotNull(hbaseConfiguration, \"No hbase configuration was specified.\");\n+    }\n+}"
  },
  {
    "sha": "4928e9569c5e30362582d22caf3cd4245e45e8f9",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/sink/HBaseSinkMutationSerializer.java",
    "status": "added",
    "additions": 84,
    "deletions": 0,
    "changes": 84,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/sink/HBaseSinkMutationSerializer.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/sink/HBaseSinkMutationSerializer.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/sink/HBaseSinkMutationSerializer.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,84 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.sink;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.connector.hbase.sink.writer.HBaseWriter;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+\n+import org.apache.hadoop.hbase.client.Delete;\n+import org.apache.hadoop.hbase.client.Mutation;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.protobuf.ProtobufUtil;\n+import org.apache.hadoop.hbase.protobuf.generated.ClientProtos;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+\n+/**\n+ * This class serializes {@link Mutation} and is used to serialize the state of the {@link\n+ * HBaseWriter}.\n+ */\n+@Internal\n+class HBaseSinkMutationSerializer implements SimpleVersionedSerializer<Mutation> {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(HBaseSinkMutationSerializer.class);\n+\n+    private static final int VERSION = 1;\n+\n+    @Override\n+    public int getVersion() {\n+        return VERSION;\n+    }\n+\n+    @Override\n+    public byte[] serialize(Mutation mutation) throws IOException {\n+        LOG.debug(\"serializing mutation {}\", mutation);\n+        ClientProtos.MutationProto.MutationType type;\n+\n+        if (mutation instanceof Put) {\n+            type = ClientProtos.MutationProto.MutationType.PUT;\n+        } else if (mutation instanceof Delete) {\n+            type = ClientProtos.MutationProto.MutationType.DELETE;\n+        } else {\n+            throw new IllegalArgumentException(\"Only Put and Delete are supported\");\n+        }\n+\n+        try (ByteArrayOutputStream baos = new ByteArrayOutputStream();\n+                DataOutputStream out = new DataOutputStream(baos)) {\n+            ProtobufUtil.toMutation(type, mutation).writeDelimitedTo(out);\n+            return baos.toByteArray();\n+        }\n+    }\n+\n+    @Override\n+    public Mutation deserialize(int version, byte[] serialized) throws IOException {\n+        LOG.debug(\"deserializing mutation\");\n+        try (ByteArrayInputStream bais = new ByteArrayInputStream(serialized);\n+                DataInputStream in = new DataInputStream(bais)) {\n+            ClientProtos.MutationProto proto = ClientProtos.MutationProto.parseDelimitedFrom(in);\n+            return ProtobufUtil.toMutation(proto);\n+        }\n+    }\n+}"
  },
  {
    "sha": "06295ed812e524ce156dc497fcb94315e4dca3e3",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/sink/HBaseSinkOptions.java",
    "status": "added",
    "additions": 45,
    "deletions": 0,
    "changes": 45,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/sink/HBaseSinkOptions.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/sink/HBaseSinkOptions.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/sink/HBaseSinkOptions.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.sink;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.configuration.ConfigOptions;\n+\n+/** HBaseSinkOptions contains configuration options used for building an {@link HBaseSink}. */\n+public class HBaseSinkOptions {\n+\n+    public static final ConfigOption<String> TABLE_NAME =\n+            ConfigOptions.key(\"table_name\")\n+                    .stringType()\n+                    .noDefaultValue()\n+                    .withDescription(\"The table in which the sink will store data.\");\n+\n+    public static final ConfigOption<Integer> QUEUE_LIMIT =\n+            ConfigOptions.key(\"queue.limit\")\n+                    .intType()\n+                    .defaultValue(1000)\n+                    .withDescription(\"The maximum buffer size before sending data to HBase\");\n+\n+    public static final ConfigOption<Integer> MAX_LATENCY =\n+            ConfigOptions.key(\"buffer.timeout.ms\")\n+                    .intType()\n+                    .defaultValue(1000)\n+                    .withDescription(\n+                            \"The maximum time an element stays in the queue before being flushed.\");\n+}"
  },
  {
    "sha": "530b17e35ed58df8cd51e0dfcec94c4c237c3709",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/sink/HBaseSinkSerializer.java",
    "status": "added",
    "additions": 46,
    "deletions": 0,
    "changes": 46,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/sink/HBaseSinkSerializer.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/sink/HBaseSinkSerializer.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/sink/HBaseSinkSerializer.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.sink;\n+\n+import org.apache.flink.connector.hbase.HBaseEvent;\n+\n+import java.io.Serializable;\n+\n+/**\n+ * The serialization interface that needs to be implemented for constructing an {@link HBaseSink}.\n+ *\n+ * <p>A minimal implementation can be seen in the following example.\n+ *\n+ * <pre>{@code\n+ * static class HBaseLongSerializer implements HBaseSinkSerializer<Long> {\n+ *     @Override\n+ *     public HBaseEvent serialize(Long event) {\n+ *         return HBaseEvent.putWith(                  // or deleteWith()\n+ *                 event.toString(),                   // rowId\n+ *                 \"exampleColumnFamily\",              // column family\n+ *                 \"exampleQualifier\",                 // qualifier\n+ *                 Bytes.toBytes(event.toString()));   // payload\n+ *     }\n+ * }\n+ * }</pre>\n+ */\n+@FunctionalInterface\n+public interface HBaseSinkSerializer<T> extends Serializable {\n+    HBaseEvent serialize(T event);\n+}"
  },
  {
    "sha": "a88fa7fcf8e7b2db45734416dfcbb26aec02258b",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/sink/writer/HBaseWriter.java",
    "status": "added",
    "additions": 173,
    "deletions": 0,
    "changes": 173,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/sink/writer/HBaseWriter.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/sink/writer/HBaseWriter.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/sink/writer/HBaseWriter.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.sink.writer;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.connector.sink.Sink;\n+import org.apache.flink.api.connector.sink.SinkWriter;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.hbase.HBaseEvent;\n+import org.apache.flink.connector.hbase.sink.HBaseSinkOptions;\n+import org.apache.flink.connector.hbase.sink.HBaseSinkSerializer;\n+import org.apache.flink.connector.hbase.util.HBaseConfigurationUtil;\n+\n+import org.apache.flink.shaded.curator4.org.apache.curator.shaded.com.google.common.io.Closer;\n+\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Connection;\n+import org.apache.hadoop.hbase.client.ConnectionFactory;\n+import org.apache.hadoop.hbase.client.Delete;\n+import org.apache.hadoop.hbase.client.Mutation;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Timer;\n+import java.util.TimerTask;\n+import java.util.concurrent.ArrayBlockingQueue;\n+\n+/**\n+ * The HBaseWriter is responsible for writing incoming events to HBase.\n+ *\n+ * <p>Stored events will be flushed to HBase eiter if the {@link #queueLimit} is reached or if the\n+ * {@link #maxLatencyMs} has elapsed.\n+ */\n+@Internal\n+public class HBaseWriter<IN> implements SinkWriter<IN, Void, Mutation> {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(HBaseWriter.class);\n+\n+    private final int queueLimit;\n+    private final int maxLatencyMs;\n+    private final HBaseSinkSerializer<IN> sinkSerializer;\n+    private final ArrayBlockingQueue<Mutation> pendingMutations;\n+    private final Connection connection;\n+    private final Table table;\n+    private volatile long lastFlushTimeStamp = 0;\n+    private TimerTask batchSendTimer;\n+\n+    public HBaseWriter(\n+            Sink.InitContext context,\n+            List<Mutation> states,\n+            HBaseSinkSerializer<IN> sinkSerializer,\n+            byte[] serializedHBaseConfig,\n+            Configuration sinkConfiguration) {\n+        this.sinkSerializer = sinkSerializer;\n+        this.queueLimit = sinkConfiguration.get(HBaseSinkOptions.QUEUE_LIMIT);\n+        this.maxLatencyMs = sinkConfiguration.get(HBaseSinkOptions.MAX_LATENCY);\n+        String tableName = sinkConfiguration.get(HBaseSinkOptions.TABLE_NAME);\n+\n+        // Queue limit is multiplied by 2, to reduce the probability of blocking while committing\n+        this.pendingMutations = new ArrayBlockingQueue<>(2 * queueLimit);\n+        pendingMutations.addAll(states);\n+\n+        org.apache.hadoop.conf.Configuration hbaseConfiguration =\n+                HBaseConfigurationUtil.deserializeConfiguration(serializedHBaseConfig, null);\n+        try {\n+            connection = ConnectionFactory.createConnection(hbaseConfiguration);\n+            table = connection.getTable(TableName.valueOf(tableName));\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"Connection to HBase couldn't be established\", e);\n+        }\n+\n+        startBatchSendTimer();\n+        LOG.debug(\"started sink writer for table {}\", tableName);\n+    }\n+\n+    private void startBatchSendTimer() {\n+        batchSendTimer =\n+                new TimerTask() {\n+                    @Override\n+                    public void run() {\n+                        long diff = System.currentTimeMillis() - lastFlushTimeStamp;\n+                        if (diff > maxLatencyMs) {\n+                            LOG.debug(\"Time based flushing of mutations\");\n+                            flushBuffer();\n+                        }\n+                    }\n+                };\n+        new Timer().scheduleAtFixedRate(batchSendTimer, 0, maxLatencyMs / 2);\n+    }\n+\n+    private void flushBuffer() {\n+        lastFlushTimeStamp = System.currentTimeMillis();\n+        if (pendingMutations.size() == 0) {\n+            return;\n+        }\n+        try {\n+            ArrayList<Mutation> batch = new ArrayList<>();\n+            pendingMutations.drainTo(batch);\n+            table.batch(batch, null);\n+            pendingMutations.clear();\n+        } catch (IOException | InterruptedException e) {\n+            throw new RuntimeException(\"Failed storing batch data in HBase\", e);\n+        }\n+    }\n+\n+    @Override\n+    public void write(IN element, Context context) {\n+        HBaseEvent event = sinkSerializer.serialize(element);\n+        if (event.getType() == Cell.Type.Put) {\n+            Put put = new Put(event.getRowIdBytes());\n+            put.addColumn(\n+                    event.getColumnFamilyBytes(), event.getQualifierBytes(), event.getPayload());\n+            pendingMutations.add(put);\n+        } else if (event.getType() == Cell.Type.Delete) {\n+            Delete delete = new Delete(event.getRowIdBytes());\n+            delete.addColumn(event.getColumnFamilyBytes(), event.getQualifierBytes());\n+            pendingMutations.add(delete);\n+        } else {\n+            throw new UnsupportedOperationException(\"event type not supported\");\n+        }\n+\n+        if (pendingMutations.size() >= queueLimit) {\n+            LOG.debug(\"Capacity based flushing of mutations\");\n+            flushBuffer();\n+        }\n+    }\n+\n+    @Override\n+    public List<Void> prepareCommit(boolean flush) {\n+        return Collections.emptyList();\n+    }\n+\n+    @Override\n+    public List<Mutation> snapshotState() {\n+        ArrayList<Mutation> snapshot = new ArrayList<>();\n+        pendingMutations.drainTo(snapshot);\n+        LOG.debug(\"Snapshotting state with {} elements\", snapshot.size());\n+        return snapshot;\n+    }\n+\n+    @Override\n+    public void close() throws Exception {\n+        try (Closer closer = Closer.create()) {\n+            closer.register(connection);\n+            closer.register(table);\n+            closer.register(this::flushBuffer);\n+            closer.register(batchSendTimer::cancel);\n+        }\n+    }\n+}"
  },
  {
    "sha": "e4209f86bfa8aa058840b23d606e532fcc2c228d",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/HBaseSource.java",
    "status": "added",
    "additions": 132,
    "deletions": 0,
    "changes": 132,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/HBaseSource.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/HBaseSource.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/HBaseSource.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.source;\n+\n+import org.apache.flink.api.connector.source.Boundedness;\n+import org.apache.flink.api.connector.source.Source;\n+import org.apache.flink.api.connector.source.SourceReader;\n+import org.apache.flink.api.connector.source.SourceReaderContext;\n+import org.apache.flink.api.connector.source.SplitEnumerator;\n+import org.apache.flink.api.connector.source.SplitEnumeratorContext;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.hbase.source.enumerator.HBaseSourceEnumeratorCheckpointSerializer;\n+import org.apache.flink.connector.hbase.source.enumerator.HBaseSplitEnumerator;\n+import org.apache.flink.connector.hbase.source.reader.HBaseSourceDeserializer;\n+import org.apache.flink.connector.hbase.source.reader.HBaseSourceReader;\n+import org.apache.flink.connector.hbase.source.split.HBaseSourceSplit;\n+import org.apache.flink.connector.hbase.source.split.HBaseSourceSplitSerializer;\n+import org.apache.flink.connector.hbase.util.HBaseConfigurationUtil;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Collection;\n+\n+/**\n+ * A source connector for HBase. Please use an {@link HBaseSourceBuilder} to construct an {@link\n+ * HBaseSource}. The following example shows how to create a HBaseSource that reads String values\n+ * from each cell.\n+ *\n+ * <pre>{@code\n+ * HBaseSource<String> source =\n+ *     HBaseSource.builder()\n+ *         .setSourceDeserializer(new HBaseStringDeserializer())\n+ *         .setTableName(\"test-table\")\n+ *         .setHBaseConfiguration(new HBaseTestClusterUtil().getConfig())\n+ *         .build();\n+ *\n+ * static class HBaseStringDeserializer implements HBaseSourceDeserializer<String> {\n+ *     @Override\n+ *     public String deserialize(HBaseSourceEvent event) {\n+ *         return new String(event.getPayload(), HBaseEvent.DEFAULT_CHARSET);\n+ *     }\n+ * }\n+ * }</pre>\n+ *\n+ * @see HBaseSourceBuilder HBaseSourceBuilder for more details.\n+ */\n+public class HBaseSource<T> implements Source<T, HBaseSourceSplit, Collection<HBaseSourceSplit>> {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(HBaseSource.class);\n+\n+    private static final long serialVersionUID = 1L;\n+\n+    private final HBaseSourceDeserializer<T> sourceDeserializer;\n+    private final byte[] serializedHBaseConfig;\n+    private final Configuration sourceConfiguration;\n+\n+    HBaseSource(\n+            HBaseSourceDeserializer<T> sourceDeserializer,\n+            org.apache.hadoop.conf.Configuration hbaseConfiguration,\n+            Configuration sourceConfiguration) {\n+        this.serializedHBaseConfig =\n+                HBaseConfigurationUtil.serializeConfiguration(hbaseConfiguration);\n+        this.sourceDeserializer = sourceDeserializer;\n+        this.sourceConfiguration = sourceConfiguration;\n+    }\n+\n+    public static <IN> HBaseSourceBuilder<IN> builder() {\n+        return new HBaseSourceBuilder<>();\n+    }\n+\n+    @Override\n+    public Boundedness getBoundedness() {\n+        return Boundedness.CONTINUOUS_UNBOUNDED;\n+    }\n+\n+    @Override\n+    public SourceReader<T, HBaseSourceSplit> createReader(SourceReaderContext readerContext) {\n+        LOG.debug(\"creating reader\");\n+        return new HBaseSourceReader<>(\n+                serializedHBaseConfig, sourceDeserializer, sourceConfiguration, readerContext);\n+    }\n+\n+    @Override\n+    public SplitEnumerator<HBaseSourceSplit, Collection<HBaseSourceSplit>> restoreEnumerator(\n+            SplitEnumeratorContext<HBaseSourceSplit> enumContext,\n+            Collection<HBaseSourceSplit> checkpoint) {\n+        LOG.debug(\"restoring enumerator with {} splits\", checkpoint.size());\n+\n+        HBaseSplitEnumerator enumerator =\n+                new HBaseSplitEnumerator(enumContext, serializedHBaseConfig, sourceConfiguration);\n+        enumerator.addSplits(checkpoint);\n+        return enumerator;\n+    }\n+\n+    @Override\n+    public SplitEnumerator<HBaseSourceSplit, Collection<HBaseSourceSplit>> createEnumerator(\n+            SplitEnumeratorContext<HBaseSourceSplit> enumContext) {\n+        LOG.debug(\"creating enumerator\");\n+        return new HBaseSplitEnumerator(enumContext, serializedHBaseConfig, sourceConfiguration);\n+    }\n+\n+    @Override\n+    public SimpleVersionedSerializer<HBaseSourceSplit> getSplitSerializer() {\n+        LOG.debug(\"getSplitSerializer\");\n+        return new HBaseSourceSplitSerializer();\n+    }\n+\n+    @Override\n+    public SimpleVersionedSerializer<Collection<HBaseSourceSplit>>\n+            getEnumeratorCheckpointSerializer() {\n+        LOG.debug(\"getEnumeratorCheckpointSerializer\");\n+        return new HBaseSourceEnumeratorCheckpointSerializer();\n+    }\n+}"
  },
  {
    "sha": "2b2a54d8309c8e731859fea6138118ca0e727404",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/HBaseSourceBuilder.java",
    "status": "added",
    "additions": 150,
    "deletions": 0,
    "changes": 150,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/HBaseSourceBuilder.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/HBaseSourceBuilder.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/HBaseSourceBuilder.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.source;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.hbase.source.hbaseendpoint.HBaseEndpoint;\n+import org.apache.flink.connector.hbase.source.reader.HBaseSourceDeserializer;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.flink.util.Preconditions.checkNotNull;\n+\n+/**\n+ * The builder class to create an {@link HBaseSource}.\n+ *\n+ * <p>The following example shows a minimum setup to create an {@link HBaseSource} that reads String\n+ * values from each cell.\n+ *\n+ * <pre>{@code\n+ * HBaseSource<String> source = HBaseSource.builder()\n+ *     .setSourceDeserializer(new HBaseStringDeserializer())\n+ *     .setTableName(\"test-table\")\n+ *     .setHBaseConfiguration(new HBaseTestClusterUtil().getConfig())\n+ *     .build();\n+ *\n+ * static class HBaseStringDeserializer implements HBaseSourceDeserializer<String> {\n+ *     @Override\n+ *     public String deserialize(HBaseSourceEvent event) {\n+ *         return new String(event.getPayload(), HBaseEvent.DEFAULT_CHARSET);\n+ *     }\n+ * }\n+ * }</pre>\n+ *\n+ * <p>A {@link HBaseSourceDeserializer} is always required to be set, as well as a table name to\n+ * read from and a {@link org.apache.hadoop.conf.Configuration} for HBase.\n+ *\n+ * <p>By default each {@link HBaseEndpoint} has a queue capacity of 1000 entries for WALedits. This\n+ * can be changed with {@link #setQueueCapacity(int queueSize)}. The hostname of the created RPC\n+ * Servers can be changed with {@link #setHostName(String hostname)}.\n+ *\n+ * @see HBaseSourceOptions HBaseSourceOptions\n+ */\n+public class HBaseSourceBuilder<IN> {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(HBaseSourceBuilder.class);\n+\n+    private static final ConfigOption<?>[] REQUIRED_CONFIGS = {HBaseSourceOptions.TABLE_NAME};\n+    private final Configuration sourceConfiguration;\n+    private org.apache.hadoop.conf.Configuration hbaseConfiguration;\n+    private HBaseSourceDeserializer<IN> sourceDeserializer;\n+\n+    protected HBaseSourceBuilder() {\n+        this.sourceDeserializer = null;\n+        this.hbaseConfiguration = null;\n+        this.sourceConfiguration = new Configuration();\n+    }\n+\n+    /**\n+     * Sets the table name from which changes will be processed.\n+     *\n+     * @param tableName the HBase table name.\n+     * @return this HBaseSourceBuilder.\n+     */\n+    public HBaseSourceBuilder<IN> setTableName(String tableName) {\n+        sourceConfiguration.setString(HBaseSourceOptions.TABLE_NAME, tableName);\n+        return this;\n+    }\n+\n+    /**\n+     * Sets the Deserializer, that will be used to deserialize cell entries in HBase.\n+     *\n+     * @param sourceDeserializer the HBase Source Deserializer.\n+     * @return this HBaseSourceBuilder.\n+     */\n+    public <T> HBaseSourceBuilder<T> setSourceDeserializer(\n+            HBaseSourceDeserializer<T> sourceDeserializer) {\n+        final HBaseSourceBuilder<T> sourceBuilder = (HBaseSourceBuilder<T>) this;\n+        sourceBuilder.sourceDeserializer = sourceDeserializer;\n+        return sourceBuilder;\n+    }\n+\n+    /**\n+     * Sets the HBaseConfiguration.\n+     *\n+     * @param hbaseConfiguration the HBaseConfiguration.\n+     * @return this HBaseSourceBuilder.\n+     */\n+    public HBaseSourceBuilder<IN> setHBaseConfiguration(\n+            org.apache.hadoop.conf.Configuration hbaseConfiguration) {\n+        this.hbaseConfiguration = hbaseConfiguration;\n+        return this;\n+    }\n+\n+    /**\n+     * Sets the queue capacity for incoming WALedits in the HBaseEndpoint.\n+     *\n+     * @param queueCapacity integer value of the queue capacity.\n+     * @return this KafkaSourceBuilder.\n+     */\n+    public HBaseSourceBuilder<IN> setQueueCapacity(int queueCapacity) {\n+        sourceConfiguration.setInteger(HBaseSourceOptions.ENDPOINT_QUEUE_CAPACITY, queueCapacity);\n+        return this;\n+    }\n+\n+    /**\n+     * Sets the hostname of the RPC Server in the HBaseEndpoint.\n+     *\n+     * @param hostName the hostname String.\n+     * @return this KafkaSourceBuilder.\n+     */\n+    public HBaseSourceBuilder<IN> setHostName(String hostName) {\n+        sourceConfiguration.setString(HBaseSourceOptions.HOST_NAME, hostName);\n+        return this;\n+    }\n+\n+    public HBaseSource<IN> build() {\n+        sanityCheck();\n+        LOG.debug(\"constructing source with config: {}\", sourceConfiguration.toString());\n+        return new HBaseSource<>(sourceDeserializer, hbaseConfiguration, sourceConfiguration);\n+    }\n+\n+    private void sanityCheck() {\n+        for (ConfigOption<?> requiredConfig : REQUIRED_CONFIGS) {\n+            checkNotNull(\n+                    sourceConfiguration.get(requiredConfig),\n+                    String.format(\"Config option %s is required but not provided\", requiredConfig));\n+        }\n+\n+        checkNotNull(sourceDeserializer, \"No source deserializer was specified.\");\n+        checkNotNull(hbaseConfiguration, \"No hbase configuration was specified.\");\n+    }\n+}"
  },
  {
    "sha": "415d3166802a9309e1104e300112d3737f01f1b0",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/HBaseSourceOptions.java",
    "status": "added",
    "additions": 45,
    "deletions": 0,
    "changes": 45,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/HBaseSourceOptions.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/HBaseSourceOptions.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/HBaseSourceOptions.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.source;\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.configuration.ConfigOptions;\n+\n+/** Configuration options for the {@link HBaseSourceBuilder}. */\n+public class HBaseSourceOptions {\n+\n+    public static final ConfigOption<String> TABLE_NAME =\n+            ConfigOptions.key(\"table_name\")\n+                    .stringType()\n+                    .noDefaultValue()\n+                    .withDescription(\"The table from which changes will be received\");\n+\n+    public static final ConfigOption<Integer> ENDPOINT_QUEUE_CAPACITY =\n+            ConfigOptions.key(\"endpoint.queue.capacity\")\n+                    .intType()\n+                    .defaultValue(1000)\n+                    .withDescription(\"The maximum queue size in the HBase endpoint\");\n+\n+    public static final ConfigOption<String> HOST_NAME =\n+            ConfigOptions.key(\"endpoint.hostname\")\n+                    .stringType()\n+                    .defaultValue(\"localhost\")\n+                    .withDescription(\n+                            \"The hostname of the RPC server in the HBaseEndpoint receiving events from HBase\");\n+}"
  },
  {
    "sha": "db32c7f2fb89ba7a18cd9d7ed8f97e26fbded8f2",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/enumerator/HBaseSourceEnumeratorCheckpointSerializer.java",
    "status": "added",
    "additions": 89,
    "deletions": 0,
    "changes": 89,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/enumerator/HBaseSourceEnumeratorCheckpointSerializer.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/enumerator/HBaseSourceEnumeratorCheckpointSerializer.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/enumerator/HBaseSourceEnumeratorCheckpointSerializer.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.source.enumerator;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.connector.hbase.source.HBaseSource;\n+import org.apache.flink.connector.hbase.source.split.HBaseSourceSplit;\n+import org.apache.flink.connector.hbase.source.split.HBaseSourceSplitSerializer;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+\n+/** Checkpoint serializer for {@link HBaseSource}. (De-)Serializes the collection of splits. */\n+@Internal\n+public class HBaseSourceEnumeratorCheckpointSerializer\n+        implements SimpleVersionedSerializer<Collection<HBaseSourceSplit>> {\n+\n+    private static final Logger LOG =\n+            LoggerFactory.getLogger(HBaseSourceEnumeratorCheckpointSerializer.class);\n+\n+    private final HBaseSourceSplitSerializer splitSerializer = new HBaseSourceSplitSerializer();\n+\n+    @Override\n+    public int getVersion() {\n+        return 1;\n+    }\n+\n+    @Override\n+    public byte[] serialize(Collection<HBaseSourceSplit> checkpointState) throws IOException {\n+        LOG.debug(\"serializing checkpoint state with {} splits\", checkpointState.size());\n+        try (ByteArrayOutputStream baos = new ByteArrayOutputStream();\n+                DataOutputStream out = new DataOutputStream(baos)) {\n+            out.writeInt(checkpointState.size());\n+            for (HBaseSourceSplit split : checkpointState) {\n+                byte[] serializedSplit = splitSerializer.serialize(split);\n+                out.write(serializedSplit.length);\n+                out.write(serializedSplit);\n+            }\n+            out.flush();\n+            return baos.toByteArray();\n+        }\n+    }\n+\n+    @Override\n+    public Collection<HBaseSourceSplit> deserialize(int version, byte[] serialized)\n+            throws IOException {\n+        LOG.debug(\"deserializing checkpoint state\");\n+        List<HBaseSourceSplit> checkPointState = new ArrayList<>();\n+        try (ByteArrayInputStream bais = new ByteArrayInputStream(serialized);\n+                DataInputStream in = new DataInputStream(bais)) {\n+            int numSplits = in.readInt();\n+            for (int i = 0; i < numSplits; i++) {\n+                int splitSize = in.readInt();\n+                byte[] serializedSplit = new byte[splitSize];\n+                in.read(serializedSplit);\n+                HBaseSourceSplit split =\n+                        splitSerializer.deserialize(splitSerializer.getVersion(), serializedSplit);\n+                checkPointState.add(split);\n+            }\n+        }\n+        return checkPointState;\n+    }\n+}"
  },
  {
    "sha": "a02a4206bbe756969fb7b8485061aef7fd6a91a2",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/enumerator/HBaseSplitEnumerator.java",
    "status": "added",
    "additions": 151,
    "deletions": 0,
    "changes": 151,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/enumerator/HBaseSplitEnumerator.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/enumerator/HBaseSplitEnumerator.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/enumerator/HBaseSplitEnumerator.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.source.enumerator;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.connector.source.SplitEnumerator;\n+import org.apache.flink.api.connector.source.SplitEnumeratorContext;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.hbase.source.HBaseSourceOptions;\n+import org.apache.flink.connector.hbase.source.split.HBaseSourceSplit;\n+import org.apache.flink.connector.hbase.util.HBaseConfigurationUtil;\n+\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Admin;\n+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptor;\n+import org.apache.hadoop.hbase.client.Connection;\n+import org.apache.hadoop.hbase.client.ConnectionFactory;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Queue;\n+\n+/**\n+ * The SourceEnumerator is responsible for work discovery and assignment of splits. The\n+ * HBaseSourceEnumerator does the following:\n+ *\n+ * <ol>\n+ *   <li>Connect to HBase to discover all column families of the table that is being watched.\n+ *   <li>Equally distribute column families to HBaseSourceSplits depending on the parallelism. One\n+ *       {@link HBaseSourceSplit} will be created per parallel reader. Each {@link HBaseSourceSplit}\n+ *       can cover multiple column families.\n+ * </ol>\n+ */\n+@Internal\n+public class HBaseSplitEnumerator\n+        implements SplitEnumerator<HBaseSourceSplit, Collection<HBaseSourceSplit>> {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(HBaseSplitEnumerator.class);\n+\n+    private final SplitEnumeratorContext<HBaseSourceSplit> context;\n+    private final Queue<HBaseSourceSplit> remainingSplits;\n+    private final String table;\n+    private final byte[] serializedHBaseConfig;\n+    private final String host;\n+\n+    public HBaseSplitEnumerator(\n+            SplitEnumeratorContext<HBaseSourceSplit> context,\n+            byte[] serializedHBaseConfig,\n+            Configuration sourceConfiguration) {\n+        this.context = context;\n+        this.remainingSplits = new ArrayDeque<>();\n+        this.host = sourceConfiguration.get(HBaseSourceOptions.HOST_NAME);\n+        this.table = sourceConfiguration.get(HBaseSourceOptions.TABLE_NAME);\n+        this.serializedHBaseConfig = serializedHBaseConfig;\n+        LOG.debug(\"Constructed with {} remaining splits\", remainingSplits.size());\n+    }\n+\n+    @Override\n+    public void start() {\n+        org.apache.hadoop.conf.Configuration hbaseConfiguration =\n+                HBaseConfigurationUtil.deserializeConfiguration(this.serializedHBaseConfig, null);\n+        try (Connection connection = ConnectionFactory.createConnection(hbaseConfiguration);\n+                Admin admin = connection.getAdmin()) {\n+            ColumnFamilyDescriptor[] colFamDes =\n+                    admin.getDescriptor(TableName.valueOf(this.table)).getColumnFamilies();\n+            List<HBaseSourceSplit> splits = new ArrayList<>();\n+            List<ArrayList<String>> parallelInstances =\n+                    new ArrayList<>(context.currentParallelism());\n+\n+            for (int i = 0; i < context.currentParallelism(); i++) {\n+                parallelInstances.add(new ArrayList<>());\n+            }\n+            int index = 0;\n+            for (ColumnFamilyDescriptor colFamDe : colFamDes) {\n+                parallelInstances.get(index).add(colFamDe.getNameAsString());\n+                index = (index + 1) % parallelInstances.size();\n+            }\n+\n+            parallelInstances.forEach(\n+                    (list) -> {\n+                        if (!list.isEmpty()) {\n+                            splits.add(new HBaseSourceSplit(list.get(0), host, list));\n+                        }\n+                    });\n+            addSplits(splits);\n+        } catch (IOException e) {\n+            throw new RuntimeException(\"could not start HBaseSplitEnumerator\", e);\n+        }\n+    }\n+\n+    @Override\n+    public void close() {}\n+\n+    @Override\n+    public void handleSplitRequest(int subtaskId, @Nullable String requesterHostname) {\n+        final HBaseSourceSplit nextSplit = remainingSplits.poll();\n+        if (nextSplit != null) {\n+            context.assignSplit(nextSplit, subtaskId);\n+        } else {\n+            context.signalNoMoreSplits(subtaskId);\n+        }\n+    }\n+\n+    @Override\n+    public void addSplitsBack(List<HBaseSourceSplit> splits, int subtaskId) {\n+        remainingSplits.addAll(splits);\n+    }\n+\n+    @Override\n+    public void addReader(int subtaskId) {\n+        LOG.debug(\"adding reader with id {}\", subtaskId);\n+        HBaseSourceSplit nextSplit = remainingSplits.poll();\n+        if (nextSplit != null) {\n+            context.assignSplit(nextSplit, subtaskId);\n+        } else {\n+            context.signalNoMoreSplits(subtaskId);\n+        }\n+    }\n+\n+    @Override\n+    public Collection<HBaseSourceSplit> snapshotState() {\n+        return remainingSplits;\n+    }\n+\n+    public void addSplits(Collection<HBaseSourceSplit> splits) {\n+        remainingSplits.addAll(splits);\n+    }\n+}"
  },
  {
    "sha": "5c1ba91059aafa9d139850e514f0c4efd433cde6",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/hbaseendpoint/EmptyHBaseServer.java",
    "status": "added",
    "additions": 122,
    "deletions": 0,
    "changes": 122,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/hbaseendpoint/EmptyHBaseServer.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/hbaseendpoint/EmptyHBaseServer.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/hbaseendpoint/EmptyHBaseServer.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.source.hbaseendpoint;\n+\n+import org.apache.flink.annotation.Internal;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.ChoreService;\n+import org.apache.hadoop.hbase.CoordinatedStateManager;\n+import org.apache.hadoop.hbase.Server;\n+import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.client.ClusterConnection;\n+import org.apache.hadoop.hbase.client.Connection;\n+import org.apache.hadoop.hbase.ipc.RpcScheduler;\n+import org.apache.hadoop.hbase.ipc.RpcServer;\n+import org.apache.hadoop.hbase.ipc.RpcServerFactory;\n+import org.apache.hadoop.hbase.zookeeper.ZKWatcher;\n+\n+import java.net.InetSocketAddress;\n+import java.util.List;\n+\n+/**\n+ * Minimal implementation of {@link Server} to enable the creation of {@link RpcServer} via {@link\n+ * RpcServerFactory#createRpcServer(Server, String, List, InetSocketAddress, Configuration,\n+ * RpcScheduler)}.\n+ */\n+@Internal\n+public class EmptyHBaseServer implements Server {\n+\n+    private final Configuration configuration;\n+\n+    public EmptyHBaseServer(Configuration configuration) {\n+        this.configuration = configuration;\n+    }\n+\n+    @Override\n+    public Configuration getConfiguration() {\n+        return configuration;\n+    }\n+\n+    @Override\n+    public void abort(String why, Throwable e) {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"abort\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    public boolean isAborted() {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"isAborted\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    public void stop(String why) {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"stop\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    public boolean isStopped() {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"isStopped\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    public ZKWatcher getZooKeeper() {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"getZooKeeper\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    public Connection getConnection() {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"getConnection\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    public Connection createConnection(Configuration conf) {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"createConnection\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    public ClusterConnection getClusterConnection() {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"getClusterConnection\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    public ServerName getServerName() {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"getServerName\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    public CoordinatedStateManager getCoordinatedStateManager() {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"getCoordinatedStateManager\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    public ChoreService getChoreService() {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"getChoreService\\\" not implemented in class \" + getClass());\n+    }\n+}"
  },
  {
    "sha": "689d941d700ea04cf8dc4b9b27621808b572b71e",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/hbaseendpoint/HBaseEndpoint.java",
    "status": "added",
    "additions": 315,
    "deletions": 0,
    "changes": 315,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/hbaseendpoint/HBaseEndpoint.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/hbaseendpoint/HBaseEndpoint.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/hbaseendpoint/HBaseEndpoint.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,315 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.source.hbaseendpoint;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.base.source.reader.synchronization.FutureCompletingBlockingQueue;\n+import org.apache.flink.connector.hbase.source.HBaseSourceOptions;\n+import org.apache.flink.connector.hbase.source.reader.HBaseSourceEvent;\n+\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.CellScanner;\n+import org.apache.hadoop.hbase.Server;\n+import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Admin;\n+import org.apache.hadoop.hbase.client.Connection;\n+import org.apache.hadoop.hbase.client.ConnectionFactory;\n+import org.apache.hadoop.hbase.ipc.FifoRpcScheduler;\n+import org.apache.hadoop.hbase.ipc.HBaseRpcController;\n+import org.apache.hadoop.hbase.ipc.RpcServer;\n+import org.apache.hadoop.hbase.ipc.RpcServerFactory;\n+import org.apache.hadoop.hbase.replication.ReplicationPeerConfig;\n+import org.apache.hadoop.hbase.replication.ReplicationPeerDescription;\n+import org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos;\n+import org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos.AdminService.BlockingInterface;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper;\n+import org.apache.hadoop.hbase.zookeeper.ZKUtil;\n+import org.apache.hbase.thirdparty.com.google.protobuf.RpcController;\n+import org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n+import org.apache.zookeeper.CreateMode;\n+import org.apache.zookeeper.KeeperException;\n+import org.apache.zookeeper.ZooDefs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.net.InetSocketAddress;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.concurrent.ExecutionException;\n+\n+/**\n+ * Endpoint for HBase CDC via HBase replication. Runs an rpc server that is registered as\n+ * replication peer at HBase. A table is specified in the constructor, and the column families to\n+ * replicate are specified when the replication is started with {@link #startReplication}. The\n+ * incoming WAL edits are internally buffered and can be retrieved with {@link #getAll}.\n+ */\n+@Internal\n+public class HBaseEndpoint implements ReplicationTargetInterface {\n+\n+    private static final AdminProtos.ReplicateWALEntryResponse REPLICATE_WAL_ENTRY_RESPONSE =\n+            AdminProtos.ReplicateWALEntryResponse.newBuilder().build();\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(HBaseEndpoint.class);\n+    private final String clusterKey;\n+    /**\n+     * The id under which the replication target is made known to the source cluster. Using the same\n+     * id as a previous consumer - that has not been unregistered - prevents that already\n+     * acknowledged events are sent again.\n+     */\n+    private final String replicationPeerId;\n+\n+    private final org.apache.hadoop.conf.Configuration hbaseConf;\n+    private final RecoverableZooKeeper zooKeeper;\n+    private final RpcServer rpcServer;\n+    private final FutureCompletingBlockingQueue<HBaseSourceEvent> walEdits;\n+    private final String hostName;\n+    private final String tableName;\n+    private boolean isRunning = false;\n+\n+    public HBaseEndpoint(\n+            org.apache.hadoop.conf.Configuration hbaseConf, Configuration sourceConfiguration)\n+            throws IOException, InterruptedException {\n+        this.hbaseConf = hbaseConf;\n+        this.replicationPeerId = UUID.randomUUID().toString().substring(0, 5);\n+        this.clusterKey = replicationPeerId + \"_clusterKey\";\n+        this.hostName = sourceConfiguration.get(HBaseSourceOptions.HOST_NAME);\n+        this.tableName = sourceConfiguration.get(HBaseSourceOptions.TABLE_NAME);\n+        int queueCapacity = sourceConfiguration.get(HBaseSourceOptions.ENDPOINT_QUEUE_CAPACITY);\n+        this.walEdits = new FutureCompletingBlockingQueue<>(queueCapacity);\n+\n+        // Setup\n+        rpcServer = createServer();\n+        zooKeeper = connectToZooKeeper();\n+        registerAtZooKeeper();\n+    }\n+\n+    private RecoverableZooKeeper connectToZooKeeper() throws IOException {\n+        // Using private ZKUtil and RecoverableZooKeeper here, because the stability is so much\n+        // improved\n+        RecoverableZooKeeper zooKeeper =\n+                ZKUtil.connect(hbaseConf, getZookeeperClientAddress(), null);\n+        LOG.debug(\"Connected to Zookeeper\");\n+        return zooKeeper;\n+    }\n+\n+    private RpcServer createServer() throws IOException {\n+        Server server = new EmptyHBaseServer(hbaseConf);\n+        InetSocketAddress initialIsa = new InetSocketAddress(hostName, 0);\n+        String name = \"regionserver/\" + initialIsa.toString();\n+\n+        RpcServer.BlockingServiceAndInterface bsai =\n+                new RpcServer.BlockingServiceAndInterface(\n+                        AdminProtos.AdminService.newReflectiveBlockingService(this),\n+                        BlockingInterface.class);\n+        RpcServer rpcServer =\n+                RpcServerFactory.createRpcServer(\n+                        server,\n+                        name,\n+                        Arrays.asList(bsai),\n+                        initialIsa,\n+                        hbaseConf,\n+                        new FifoRpcScheduler(\n+                                hbaseConf,\n+                                hbaseConf.getInt(\"hbase.regionserver.handler.count\", 10)));\n+\n+        rpcServer.start();\n+        LOG.debug(\"Started rpc server at {}\", initialIsa);\n+        return rpcServer;\n+    }\n+\n+    private void registerAtZooKeeper() throws InterruptedException {\n+        createZKPath(getZookeeperRootNode() + \"/\" + clusterKey, null, CreateMode.PERSISTENT);\n+        createZKPath(\n+                getZookeeperRootNode() + \"/\" + clusterKey + \"/rs\", null, CreateMode.PERSISTENT);\n+\n+        UUID uuid = UUID.nameUUIDFromBytes(Bytes.toBytes(clusterKey));\n+        createZKPath(\n+                getZookeeperRootNode() + \"/\" + clusterKey + \"/hbaseid\",\n+                Bytes.toBytes(uuid.toString()),\n+                CreateMode.PERSISTENT);\n+\n+        ServerName serverName =\n+                ServerName.valueOf(\n+                        hostName,\n+                        rpcServer.getListenerAddress().getPort(),\n+                        System.currentTimeMillis());\n+        createZKPath(\n+                getZookeeperRootNode() + \"/\" + clusterKey + \"/rs/\" + serverName.getServerName(),\n+                null,\n+                CreateMode.EPHEMERAL);\n+\n+        LOG.debug(\"Registered rpc server node at zookeeper\");\n+    }\n+\n+    /**\n+     * Blocks as long as the queue is empty. If the queue isn't empty it will try and get as many\n+     * elements as possible from the queue. It is not guaranteed that at least one element is in the\n+     * returned list.\n+     *\n+     * @return a list of {@link HBaseSourceEvent}.\n+     */\n+    public List<HBaseSourceEvent> getAll() {\n+        if (!isRunning) {\n+            // Protects from infinite waiting\n+            throw new RuntimeException(\"Consumer is not running\");\n+        }\n+        List<HBaseSourceEvent> elements = new ArrayList<>();\n+        HBaseSourceEvent event;\n+\n+        try {\n+            walEdits.getAvailabilityFuture().get();\n+            while ((event = walEdits.poll()) != null) {\n+                elements.add(event);\n+            }\n+            return elements;\n+        } catch (InterruptedException | ExecutionException e) {\n+            throw new RuntimeException(\"Can't retrieve elements from queue\", e);\n+        }\n+    }\n+\n+    public void wakeup() {\n+        walEdits.notifyAvailable();\n+    }\n+\n+    public void close() throws InterruptedException {\n+        isRunning = false;\n+\n+        try (Connection connection = ConnectionFactory.createConnection(hbaseConf);\n+                Admin admin = connection.getAdmin()) {\n+            admin.removeReplicationPeer(replicationPeerId);\n+        } catch (IOException e) {\n+            LOG.error(\n+                    \"Error unregistering HBase endpoint replication peer. Will proceed with shutdown, but source cluster might be dirty.\",\n+                    e);\n+        }\n+\n+        try {\n+            org.apache.zookeeper.ZKUtil.deleteRecursive(\n+                    zooKeeper.getZooKeeper(), getZookeeperRootNode() + \"/\" + clusterKey);\n+        } catch (KeeperException e) {\n+            LOG.error(\n+                    \"Error unregistering up HBase endpoint from zookeeper. Will proceed with shutdown, but source cluster might be dirty.\",\n+                    e);\n+        }\n+\n+        try {\n+            zooKeeper.close();\n+            LOG.debug(\"Closed connection to ZooKeeper\");\n+        } finally {\n+            rpcServer.stop();\n+            LOG.debug(\"Closed HBase replication target rpc server\");\n+        }\n+    }\n+\n+    public void startReplication(List<String> columnFamilies) throws IOException {\n+        if (isRunning) {\n+            throw new RuntimeException(\"HBase replication endpoint is already running\");\n+        }\n+        try (Connection connection = ConnectionFactory.createConnection(hbaseConf);\n+                Admin admin = connection.getAdmin()) {\n+\n+            ReplicationPeerConfig peerConfig = createPeerConfig(this.tableName, columnFamilies);\n+            if (admin.listReplicationPeers().stream()\n+                    .map(ReplicationPeerDescription::getPeerId)\n+                    .anyMatch(replicationPeerId::equals)) {\n+                admin.updateReplicationPeerConfig(replicationPeerId, peerConfig);\n+            } else {\n+                admin.addReplicationPeer(replicationPeerId, peerConfig);\n+            }\n+            isRunning = true;\n+        }\n+    }\n+\n+    @Override\n+    public AdminProtos.ReplicateWALEntryResponse replicateWALEntry(\n+            RpcController controller, AdminProtos.ReplicateWALEntryRequest request)\n+            throws ServiceException {\n+        List<AdminProtos.WALEntry> entries = request.getEntryList();\n+        CellScanner cellScanner = ((HBaseRpcController) controller).cellScanner();\n+\n+        try {\n+            for (final AdminProtos.WALEntry entry : entries) {\n+                final String table =\n+                        TableName.valueOf(entry.getKey().getTableName().toByteArray()).toString();\n+                final int count = entry.getAssociatedCellCount();\n+                for (int i = 0; i < count; i++) {\n+                    if (!cellScanner.advance()) {\n+                        throw new ArrayIndexOutOfBoundsException(\n+                                \"Expected WAL entry to have \"\n+                                        + count\n+                                        + \"elements, but cell scanner did not have cell for index\"\n+                                        + i);\n+                    }\n+\n+                    Cell cell = cellScanner.current();\n+                    HBaseSourceEvent event = HBaseSourceEvent.fromCell(table, cell, i);\n+                    walEdits.put(0, event);\n+                }\n+            }\n+        } catch (Exception e) {\n+            throw new ServiceException(\"Could not replicate WAL entry in HBase endpoint\", e);\n+        }\n+\n+        return REPLICATE_WAL_ENTRY_RESPONSE;\n+    }\n+\n+    private ReplicationPeerConfig createPeerConfig(String table, List<String> columnFamilies) {\n+        Map<TableName, List<String>> tableColumnFamiliesMap = new HashMap<>();\n+        tableColumnFamiliesMap.put(TableName.valueOf(table), columnFamilies);\n+        return ReplicationPeerConfig.newBuilder()\n+                .setClusterKey(\n+                        getZookeeperClientAddress()\n+                                + \":\"\n+                                + getZookeeperRootNode()\n+                                + \"/\"\n+                                + clusterKey)\n+                .setReplicateAllUserTables(false)\n+                .setTableCFsMap(tableColumnFamiliesMap)\n+                .build();\n+    }\n+\n+    private String getZookeeperClientAddress() {\n+        return hbaseConf.get(\"hbase.zookeeper.quorum\")\n+                + \":\"\n+                + hbaseConf.get(\"hbase.zookeeper.property.clientPort\");\n+    }\n+\n+    private String getZookeeperRootNode() {\n+        return hbaseConf.get(\"zookeeper.znode.parent\", \"/hbase\");\n+    }\n+\n+    private void createZKPath(final String path, byte[] data, CreateMode createMode)\n+            throws InterruptedException {\n+        try {\n+            if (zooKeeper.exists(path, false) == null) {\n+                zooKeeper.create(path, data, ZooDefs.Ids.OPEN_ACL_UNSAFE, createMode);\n+            }\n+        } catch (KeeperException e) {\n+            throw new RuntimeException(\"Error creating ZK path in HBase replication endpoint\", e);\n+        }\n+    }\n+}"
  },
  {
    "sha": "5b05e6beb478f85b9bd8355b0d1b5670a2c64be2",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/hbaseendpoint/ReplicationTargetInterface.java",
    "status": "added",
    "additions": 195,
    "deletions": 0,
    "changes": 195,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/hbaseendpoint/ReplicationTargetInterface.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/hbaseendpoint/ReplicationTargetInterface.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/hbaseendpoint/ReplicationTargetInterface.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.source.hbaseendpoint;\n+\n+import org.apache.flink.annotation.Internal;\n+\n+import org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos;\n+import org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos;\n+import org.apache.hbase.thirdparty.com.google.protobuf.RpcController;\n+\n+/**\n+ * Reduces {@link AdminProtos.AdminService.BlockingInterface} to interface necessary for WAL\n+ * replication ({@link AdminProtos.AdminService.BlockingInterface#replicateWALEntry(RpcController,\n+ * AdminProtos.ReplicateWALEntryRequest)}) to keep implementing class small.\n+ */\n+@Internal\n+public interface ReplicationTargetInterface extends AdminProtos.AdminService.BlockingInterface {\n+\n+    @Override\n+    default AdminProtos.ClearCompactionQueuesResponse clearCompactionQueues(\n+            RpcController arg0, AdminProtos.ClearCompactionQueuesRequest arg1) {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"clearCompactionQueues\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    default AdminProtos.GetRegionInfoResponse getRegionInfo(\n+            RpcController controller, AdminProtos.GetRegionInfoRequest request) {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"getRegionInfo\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    default AdminProtos.GetStoreFileResponse getStoreFile(\n+            RpcController controller, AdminProtos.GetStoreFileRequest request) {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"getStoreFile\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    default AdminProtos.GetOnlineRegionResponse getOnlineRegion(\n+            RpcController controller, AdminProtos.GetOnlineRegionRequest request) {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"getOnlineRegion\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    default AdminProtos.OpenRegionResponse openRegion(\n+            RpcController controller, AdminProtos.OpenRegionRequest request) {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"openRegion\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    default AdminProtos.WarmupRegionResponse warmupRegion(\n+            RpcController controller, AdminProtos.WarmupRegionRequest request) {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"warmupRegion\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    default AdminProtos.CloseRegionResponse closeRegion(\n+            RpcController controller, AdminProtos.CloseRegionRequest request) {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"closeRegion\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    default AdminProtos.FlushRegionResponse flushRegion(\n+            RpcController controller, AdminProtos.FlushRegionRequest request) {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"flushRegion\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    default AdminProtos.CompactionSwitchResponse compactionSwitch(\n+            RpcController controller, AdminProtos.CompactionSwitchRequest request) {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"compactionSwitch\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    default AdminProtos.CompactRegionResponse compactRegion(\n+            RpcController controller, AdminProtos.CompactRegionRequest request) {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"compactRegion\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    default AdminProtos.ReplicateWALEntryResponse replay(\n+            RpcController controller, AdminProtos.ReplicateWALEntryRequest request) {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"replay\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    default AdminProtos.RollWALWriterResponse rollWALWriter(\n+            RpcController controller, AdminProtos.RollWALWriterRequest request) {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"rollWALWriter\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    default AdminProtos.GetServerInfoResponse getServerInfo(\n+            RpcController controller, AdminProtos.GetServerInfoRequest request) {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"getServerInfo\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    default AdminProtos.StopServerResponse stopServer(\n+            RpcController controller, AdminProtos.StopServerRequest request) {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"stopServer\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    default AdminProtos.UpdateFavoredNodesResponse updateFavoredNodes(\n+            RpcController controller, AdminProtos.UpdateFavoredNodesRequest request) {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"updateFavoredNodes\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    default AdminProtos.UpdateConfigurationResponse updateConfiguration(\n+            RpcController controller, AdminProtos.UpdateConfigurationRequest request) {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"updateConfiguration\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    default AdminProtos.GetRegionLoadResponse getRegionLoad(\n+            RpcController controller, AdminProtos.GetRegionLoadRequest request) {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"getRegionLoad\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    default AdminProtos.ClearRegionBlockCacheResponse clearRegionBlockCache(\n+            RpcController controller, AdminProtos.ClearRegionBlockCacheRequest request) {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"clearRegionBlockCache\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    default AdminProtos.ExecuteProceduresResponse executeProcedures(\n+            RpcController controller, AdminProtos.ExecuteProceduresRequest request) {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"executeProcedures\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    default QuotaProtos.GetSpaceQuotaSnapshotsResponse getSpaceQuotaSnapshots(\n+            RpcController arg0, QuotaProtos.GetSpaceQuotaSnapshotsRequest arg1) {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"getSpaceQuotaSnapshots\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    default AdminProtos.SlowLogResponses getSlowLogResponses(\n+            RpcController controller, AdminProtos.SlowLogResponseRequest request) {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"getSlowLogResponses\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    default AdminProtos.SlowLogResponses getLargeLogResponses(\n+            RpcController controller, AdminProtos.SlowLogResponseRequest request) {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"getLargeLogResponses\\\" not implemented in class \" + getClass());\n+    }\n+\n+    @Override\n+    default AdminProtos.ClearSlowLogResponses clearSlowLogsResponses(\n+            RpcController controller, AdminProtos.ClearSlowLogResponseRequest request) {\n+        throw new UnsupportedOperationException(\n+                \"Operation \\\"clearSlowLogsResponses\\\" not implemented in class \" + getClass());\n+    }\n+}"
  },
  {
    "sha": "c0672c6ce803005da479218ce5df236079e696c8",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/reader/HBaseRecordEmitter.java",
    "status": "added",
    "additions": 52,
    "deletions": 0,
    "changes": 52,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/reader/HBaseRecordEmitter.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/reader/HBaseRecordEmitter.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/reader/HBaseRecordEmitter.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.source.reader;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.connector.source.SourceOutput;\n+import org.apache.flink.connector.base.source.reader.RecordEmitter;\n+import org.apache.flink.connector.hbase.source.split.HBaseSourceSplitState;\n+\n+/**\n+ * The {@link RecordEmitter} implementation for {@link HBaseSourceReader}. It updates the {@link\n+ * HBaseSourceSplitState} and deserializes the record before emitting it.\n+ */\n+@Internal\n+public class HBaseRecordEmitter<T>\n+        implements RecordEmitter<HBaseSourceEvent, T, HBaseSourceSplitState> {\n+\n+    private final HBaseSourceDeserializer<T> sourceDeserializer;\n+\n+    public HBaseRecordEmitter(HBaseSourceDeserializer<T> sourceDeserializer) {\n+        this.sourceDeserializer = sourceDeserializer;\n+    }\n+\n+    @Override\n+    public void emitRecord(\n+            HBaseSourceEvent event, SourceOutput<T> output, HBaseSourceSplitState splitState)\n+            throws Exception {\n+        if (!splitState.isAlreadyProcessedEvent(event)) {\n+            splitState.notifyEmittedEvent(event);\n+            T deserializedPayload = sourceDeserializer.deserialize(event);\n+            output.collect(deserializedPayload, event.getTimestamp());\n+        } else {\n+            // Ignore event, was already processed\n+        }\n+    }\n+}"
  },
  {
    "sha": "fc3f6c2b3bb4d514866d068b8706702e9e82ec93",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/reader/HBaseSourceDeserializer.java",
    "status": "added",
    "additions": 67,
    "deletions": 0,
    "changes": 67,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/reader/HBaseSourceDeserializer.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/reader/HBaseSourceDeserializer.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/reader/HBaseSourceDeserializer.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.source.reader;\n+\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.ResultTypeQueryable;\n+import org.apache.flink.api.java.typeutils.TypeExtractor;\n+import org.apache.flink.connector.hbase.source.HBaseSource;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+\n+/**\n+ * The deserialization interface that needs to be implemented for constructing an {@link\n+ * HBaseSource}.\n+ *\n+ * <p>A minimal implementation can be seen in the following example.\n+ *\n+ * <pre>{@code\n+ * static class HBaseStringDeserializer implements HBaseSourceDeserializer<String> {\n+ *     @Override\n+ *     public String deserialize(HBaseSourceEvent event) {\n+ *         return new String(event.getPayload(), HBaseEvent.DEFAULT_CHARSET);\n+ *     }\n+ * }\n+ * }</pre>\n+ *\n+ * {@link #getProducedType()} should be overridden if the type information cannot be extracted this\n+ * way.\n+ */\n+@FunctionalInterface\n+public interface HBaseSourceDeserializer<T> extends Serializable, ResultTypeQueryable<T> {\n+\n+    /**\n+     * This method wraps a {@link DeserializationSchema} in an HBaseSourceDeserializer. It will only\n+     * have access to the payload of the {@link HBaseSourceEvent}.\n+     */\n+    static <T> HBaseSourceDeserializer<T> valueOnly(\n+            DeserializationSchema<T> deserializationSchema) {\n+        return new HBaseSourceDeserializerWrapper<>(deserializationSchema);\n+    }\n+\n+    T deserialize(HBaseSourceEvent event) throws IOException;\n+\n+    @Override\n+    default TypeInformation<T> getProducedType() {\n+        return TypeExtractor.createTypeInfo(\n+                HBaseSourceDeserializer.class, getClass(), 0, null, null);\n+    }\n+}"
  },
  {
    "sha": "d369bd34b35524bc9db9ebc37141322add63c303",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/reader/HBaseSourceDeserializerWrapper.java",
    "status": "added",
    "additions": 51,
    "deletions": 0,
    "changes": 51,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/reader/HBaseSourceDeserializerWrapper.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/reader/HBaseSourceDeserializerWrapper.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/reader/HBaseSourceDeserializerWrapper.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.source.reader;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.common.serialization.DeserializationSchema;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.connector.hbase.source.HBaseSource;\n+\n+import java.io.IOException;\n+\n+/**\n+ * This class wraps a {@link DeserializationSchema} so it can be used in an {@link HBaseSource} as a\n+ * {@link HBaseSourceDeserializer}.\n+ *\n+ * @see HBaseSourceDeserializer#valueOnly(DeserializationSchema) valueOnly(DeserializationSchema).\n+ */\n+@Internal\n+class HBaseSourceDeserializerWrapper<T> implements HBaseSourceDeserializer<T> {\n+    private final DeserializationSchema<T> deserializationSchema;\n+\n+    HBaseSourceDeserializerWrapper(DeserializationSchema<T> deserializationSchema) {\n+        this.deserializationSchema = deserializationSchema;\n+    }\n+\n+    @Override\n+    public T deserialize(HBaseSourceEvent event) throws IOException {\n+        return deserializationSchema.deserialize(event.getPayload());\n+    }\n+\n+    @Override\n+    public TypeInformation<T> getProducedType() {\n+        return deserializationSchema.getProducedType();\n+    }\n+}"
  },
  {
    "sha": "3901d5c7b143cb7706f73e98f3648341c09bc907",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/reader/HBaseSourceEvent.java",
    "status": "added",
    "additions": 103,
    "deletions": 0,
    "changes": 103,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/reader/HBaseSourceEvent.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/reader/HBaseSourceEvent.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/reader/HBaseSourceEvent.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.source.reader;\n+\n+import org.apache.flink.connector.hbase.HBaseEvent;\n+\n+import org.apache.hadoop.hbase.Cell;\n+\n+import java.util.Arrays;\n+import java.util.Objects;\n+\n+/** The HBaseSourceEvent is used to represent incoming events from HBase. */\n+public class HBaseSourceEvent extends HBaseEvent {\n+\n+    private final String table;\n+    private final long timestamp;\n+    /** Index of operation inside one wal entry. */\n+    private final int index;\n+\n+    public HBaseSourceEvent(\n+            Cell.Type type,\n+            String rowId,\n+            String table,\n+            String columnFamily,\n+            String qualifier,\n+            byte[] payload,\n+            long timestamp,\n+            int index) {\n+        super(type, rowId, columnFamily, qualifier, payload);\n+        this.table = table;\n+        this.timestamp = timestamp;\n+        this.index = index;\n+    }\n+\n+    public static HBaseSourceEvent fromCell(String table, Cell cell, int index) {\n+        final String row = new String(cell.getRowArray(), DEFAULT_CHARSET);\n+        final String columnFamily = new String(cell.getFamilyArray(), DEFAULT_CHARSET);\n+        final String qualifier = new String(cell.getQualifierArray(), DEFAULT_CHARSET);\n+        final byte[] payload = Arrays.copyOf(cell.getValueArray(), cell.getValueLength());\n+        final long timestamp = cell.getTimestamp();\n+        final Cell.Type type = cell.getType();\n+        return new HBaseSourceEvent(\n+                type, row, table, columnFamily, qualifier, payload, timestamp, index);\n+    }\n+\n+    public long getTimestamp() {\n+        return timestamp;\n+    }\n+\n+    public int getIndex() {\n+        return index;\n+    }\n+\n+    public String getTable() {\n+        return table;\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return table + \" \" + super.toString() + \" \" + timestamp + \" \" + index;\n+    }\n+\n+    public boolean isLaterThan(long timestamp, int index) {\n+        return timestamp < this.getTimestamp()\n+                || (timestamp == this.getTimestamp() && index < this.getIndex());\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+        if (this == o) {\n+            return true;\n+        }\n+        if (o == null || getClass() != o.getClass()) {\n+            return false;\n+        }\n+        if (!super.equals(o)) {\n+            return false;\n+        }\n+        HBaseSourceEvent that = (HBaseSourceEvent) o;\n+        return timestamp == that.timestamp && index == that.index && table.equals(that.table);\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+        return Objects.hash(super.hashCode(), table, timestamp, index);\n+    }\n+}"
  },
  {
    "sha": "810cf772d7755a503eec68f2ba5671458f905c67",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/reader/HBaseSourceReader.java",
    "status": "added",
    "additions": 70,
    "deletions": 0,
    "changes": 70,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/reader/HBaseSourceReader.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/reader/HBaseSourceReader.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/reader/HBaseSourceReader.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.source.reader;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.connector.source.SourceReaderContext;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.base.source.reader.SingleThreadMultiplexSourceReaderBase;\n+import org.apache.flink.connector.hbase.source.split.HBaseSourceSplit;\n+import org.apache.flink.connector.hbase.source.split.HBaseSourceSplitState;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Map;\n+\n+/** The source reader for Hbase. */\n+@Internal\n+public class HBaseSourceReader<T>\n+        extends SingleThreadMultiplexSourceReaderBase<\n+                HBaseSourceEvent, T, HBaseSourceSplit, HBaseSourceSplitState> {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(HBaseSourceReader.class);\n+\n+    public HBaseSourceReader(\n+            byte[] serializedHBaseConfig,\n+            HBaseSourceDeserializer<T> sourceDeserializer,\n+            Configuration sourceConfiguration,\n+            SourceReaderContext context) {\n+        super(\n+                () -> new HBaseSourceSplitReader(serializedHBaseConfig, sourceConfiguration),\n+                new HBaseRecordEmitter<>(sourceDeserializer),\n+                sourceConfiguration,\n+                context);\n+        LOG.debug(\"constructing Source Reader with config: {}\", sourceConfiguration.toString());\n+    }\n+\n+    @Override\n+    protected void onSplitFinished(Map<String, HBaseSourceSplitState> finishedSplitIds) {\n+        LOG.debug(\"finished {} split(s)\", finishedSplitIds.entrySet().size());\n+        context.sendSplitRequest();\n+    }\n+\n+    @Override\n+    protected HBaseSourceSplitState initializedState(HBaseSourceSplit split) {\n+        LOG.debug(\"initialized state for split {}\", split.splitId());\n+        return new HBaseSourceSplitState(split);\n+    }\n+\n+    @Override\n+    protected HBaseSourceSplit toSplitType(String splitId, HBaseSourceSplitState splitState) {\n+        return splitState.toSplit();\n+    }\n+}"
  },
  {
    "sha": "1f5b3c498f9baa5b220140c37a2bbb2b3e19762a",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/reader/HBaseSourceSplitReader.java",
    "status": "added",
    "additions": 148,
    "deletions": 0,
    "changes": 148,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/reader/HBaseSourceSplitReader.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/reader/HBaseSourceSplitReader.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/reader/HBaseSourceSplitReader.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.source.reader;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.connector.base.source.reader.RecordsWithSplitIds;\n+import org.apache.flink.connector.base.source.reader.splitreader.SplitReader;\n+import org.apache.flink.connector.base.source.reader.splitreader.SplitsAddition;\n+import org.apache.flink.connector.base.source.reader.splitreader.SplitsChange;\n+import org.apache.flink.connector.hbase.source.hbaseendpoint.HBaseEndpoint;\n+import org.apache.flink.connector.hbase.source.split.HBaseSourceSplit;\n+import org.apache.flink.connector.hbase.util.HBaseConfigurationUtil;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.annotation.Nullable;\n+\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.Set;\n+\n+/** A {@link SplitReader} implementation for HBase. */\n+@Internal\n+public class HBaseSourceSplitReader implements SplitReader<HBaseSourceEvent, HBaseSourceSplit> {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(HBaseSourceSplitReader.class);\n+\n+    private final Queue<HBaseSourceSplit> splits;\n+    private final HBaseEndpoint hbaseEndpoint;\n+\n+    @Nullable private String currentSplitId;\n+\n+    public HBaseSourceSplitReader(byte[] serializedConfig, Configuration sourceConfiguration) {\n+        LOG.debug(\"constructing Split Reader\");\n+        try {\n+            this.hbaseEndpoint =\n+                    new HBaseEndpoint(\n+                            HBaseConfigurationUtil.deserializeConfiguration(serializedConfig, null),\n+                            sourceConfiguration);\n+        } catch (Exception e) {\n+            throw new RuntimeException(\"failed HBase consumer\", e);\n+        }\n+        this.splits = new ArrayDeque<>();\n+    }\n+\n+    @Override\n+    public RecordsWithSplitIds<HBaseSourceEvent> fetch() throws IOException {\n+        if (currentSplitId == null) {\n+            HBaseSourceSplit nextSplit = splits.poll();\n+            if (nextSplit != null) {\n+                currentSplitId = nextSplit.splitId();\n+            } else {\n+                throw new IOException(\"No split remaining\");\n+            }\n+        }\n+        List<HBaseSourceEvent> records = hbaseEndpoint.getAll();\n+        LOG.debug(\"{} records in the queue\", records.size());\n+        return new HBaseSplitRecords<>(currentSplitId, records.iterator());\n+    }\n+\n+    @Override\n+    public void handleSplitsChanges(SplitsChange<HBaseSourceSplit> splitsChanges) {\n+        LOG.debug(\"handle splits change {}\", splitsChanges);\n+        if (splitsChanges instanceof SplitsAddition) {\n+            HBaseSourceSplit split = splitsChanges.splits().get(0);\n+            try {\n+                this.hbaseEndpoint.startReplication(split.getColumnFamilies());\n+            } catch (Exception e) {\n+                throw new RuntimeException(\"failed HBase consumer\", e);\n+            }\n+            splits.addAll(splitsChanges.splits());\n+        } else {\n+            throw new UnsupportedOperationException(\n+                    \"Unsupported splits change type \"\n+                            + splitsChanges.getClass().getSimpleName()\n+                            + \" in \"\n+                            + this.getClass().getSimpleName());\n+        }\n+    }\n+\n+    @Override\n+    public void wakeUp() {\n+        LOG.debug(\"waking up HBaseEndpoint\");\n+        hbaseEndpoint.wakeup();\n+    }\n+\n+    @Override\n+    public void close() throws Exception {\n+        hbaseEndpoint.close();\n+    }\n+\n+    private static class HBaseSplitRecords<T> implements RecordsWithSplitIds<T> {\n+        private Iterator<T> recordsForSplit;\n+        private String splitId;\n+\n+        HBaseSplitRecords(String splitId, Iterator<T> recordsForSplit) {\n+            this.splitId = splitId;\n+            this.recordsForSplit = recordsForSplit;\n+        }\n+\n+        @Nullable\n+        @Override\n+        public String nextSplit() {\n+            final String nextSplit = this.splitId;\n+            this.splitId = null;\n+            this.recordsForSplit = nextSplit != null ? this.recordsForSplit : null;\n+\n+            return nextSplit;\n+        }\n+\n+        @Nullable\n+        @Override\n+        public T nextRecordFromSplit() {\n+            if (recordsForSplit != null && recordsForSplit.hasNext()) {\n+                return recordsForSplit.next();\n+            } else {\n+                return null;\n+            }\n+        }\n+\n+        @Override\n+        public Set<String> finishedSplits() {\n+            return Collections.emptySet();\n+        }\n+    }\n+}"
  },
  {
    "sha": "707974acce74cd9d8353b8867a41853c1085c518",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/split/HBaseSourceSplit.java",
    "status": "added",
    "additions": 86,
    "deletions": 0,
    "changes": 86,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/split/HBaseSourceSplit.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/split/HBaseSourceSplit.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/split/HBaseSourceSplit.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.source.split;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.connector.source.SourceSplit;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.connector.hbase.source.reader.HBaseSourceSplitReader;\n+\n+import java.io.Serializable;\n+import java.util.List;\n+\n+/**\n+ * An HBaseSourceSplit contains all necessary information a {@link HBaseSourceSplitReader} needs to\n+ * read from HBase.\n+ */\n+@Internal\n+public class HBaseSourceSplit implements SourceSplit, Serializable {\n+\n+    private static final long serialVersionUID = 1L;\n+\n+    private final String id;\n+\n+    private final String host;\n+\n+    private final List<String> columnFamilies;\n+\n+    private final Tuple2<Long, Integer> firstEventStamp;\n+\n+    public HBaseSourceSplit(String id, String host, List<String> columnFamilies) {\n+        this(id, host, columnFamilies, Tuple2.of(-1L, -1));\n+    }\n+\n+    public HBaseSourceSplit(\n+            String id,\n+            String host,\n+            List<String> columnFamilies,\n+            Tuple2<Long, Integer> firstEventStamp) {\n+        this.id = id;\n+        this.host = host;\n+        this.columnFamilies = columnFamilies;\n+        this.firstEventStamp = firstEventStamp;\n+    }\n+\n+    public String getHost() {\n+        return host;\n+    }\n+\n+    public List<String> getColumnFamilies() {\n+        return columnFamilies;\n+    }\n+\n+    @Override\n+    public String splitId() {\n+        return id;\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return String.format(\"HBaseSourceSplit: %s \", getHost());\n+    }\n+\n+    public Tuple2<Long, Integer> getFirstEventStamp() {\n+        return firstEventStamp;\n+    }\n+\n+    public HBaseSourceSplit withStamp(long lastTimeStamp, int lastIndex) {\n+        return new HBaseSourceSplit(id, host, columnFamilies, Tuple2.of(lastTimeStamp, lastIndex));\n+    }\n+}"
  },
  {
    "sha": "25856825c851649322361e5a6b262041615cd62c",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/split/HBaseSourceSplitSerializer.java",
    "status": "added",
    "additions": 85,
    "deletions": 0,
    "changes": 85,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/split/HBaseSourceSplitSerializer.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/split/HBaseSourceSplitSerializer.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/split/HBaseSourceSplitSerializer.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.source.split;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+\n+/** A serializer for {@link HBaseSourceSplit}. */\n+@Internal\n+public class HBaseSourceSplitSerializer implements SimpleVersionedSerializer<HBaseSourceSplit> {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(HBaseSourceSplitSerializer.class);\n+\n+    private static final int VERSION = 1;\n+\n+    @Override\n+    public int getVersion() {\n+        return VERSION;\n+    }\n+\n+    @Override\n+    public byte[] serialize(HBaseSourceSplit split) throws IOException {\n+        LOG.debug(\"serializing split {}\", split.splitId());\n+        try (ByteArrayOutputStream baos = new ByteArrayOutputStream();\n+                DataOutputStream out = new DataOutputStream(baos)) {\n+            out.writeUTF(split.splitId());\n+            out.writeUTF(split.getHost());\n+            out.writeInt(split.getColumnFamilies().size());\n+            for (String columnFamily : split.getColumnFamilies()) {\n+                out.writeUTF(columnFamily);\n+            }\n+            out.writeLong(split.getFirstEventStamp().f0);\n+            out.writeInt(split.getFirstEventStamp().f1);\n+            out.flush();\n+            return baos.toByteArray();\n+        }\n+    }\n+\n+    @Override\n+    public HBaseSourceSplit deserialize(int version, byte[] serialized) throws IOException {\n+        LOG.debug(\"deserializing split\");\n+        try (ByteArrayInputStream bais = new ByteArrayInputStream(serialized);\n+                DataInputStream in = new DataInputStream(bais)) {\n+            String id = in.readUTF();\n+            String host = in.readUTF();\n+            ArrayList<String> columnFamilies = new ArrayList<>();\n+            int numberOfColumnFamilies = in.readInt();\n+            for (int i = 0; i < numberOfColumnFamilies; i++) {\n+                columnFamilies.add(in.readUTF());\n+            }\n+\n+            long firstTimestamp = in.readLong();\n+            int firstIndex = in.readInt();\n+            return new HBaseSourceSplit(\n+                    id, host, columnFamilies, Tuple2.of(firstTimestamp, firstIndex));\n+        }\n+    }\n+}"
  },
  {
    "sha": "2c3c27344ff0139f247bd0f6490c1e5d3e439b9e",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/split/HBaseSourceSplitState.java",
    "status": "added",
    "additions": 60,
    "deletions": 0,
    "changes": 60,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/split/HBaseSourceSplitState.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/split/HBaseSourceSplitState.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/main/java/org/apache/flink/connector/hbase/source/split/HBaseSourceSplitState.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,60 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.source.split;\n+\n+import org.apache.flink.annotation.Internal;\n+import org.apache.flink.connector.hbase.HBaseEvent;\n+import org.apache.flink.connector.hbase.source.reader.HBaseSourceEvent;\n+\n+/**\n+ * Contains the state of an {@link HBaseSourceSplit}. It tracks the timestamp of the last emitted\n+ * event to ensure no duplicates appear on recovery.\n+ */\n+@Internal\n+public class HBaseSourceSplitState {\n+    private final HBaseSourceSplit split;\n+\n+    private long lastTimeStamp = -1;\n+    private int lastIndex = -1;\n+\n+    public HBaseSourceSplitState(HBaseSourceSplit split) {\n+        this.split = split;\n+        this.lastTimeStamp = this.split.getFirstEventStamp().f0;\n+        this.lastIndex = this.split.getFirstEventStamp().f1;\n+    }\n+\n+    public HBaseSourceSplit toSplit() {\n+        return split.withStamp(lastTimeStamp, lastIndex);\n+    }\n+\n+    /**\n+     * Update the state of which element was emitted last.\n+     *\n+     * @param event An {@link HBaseEvent} that has been emitted by the {@link\n+     *     org.apache.flink.connector.hbase.source.reader.HBaseRecordEmitter}\n+     */\n+    public void notifyEmittedEvent(HBaseSourceEvent event) {\n+        lastTimeStamp = event.getTimestamp();\n+        lastIndex = event.getIndex();\n+    }\n+\n+    public boolean isAlreadyProcessedEvent(HBaseSourceEvent event) {\n+        return !event.isLaterThan(lastTimeStamp, lastIndex);\n+    }\n+}"
  },
  {
    "sha": "b2fcf9da56cbed057fb096861ebfa5923d2e0800",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/test/java/org/apache/flink/connector/hbase/sink/HBaseSinkTests.java",
    "status": "added",
    "additions": 144,
    "deletions": 0,
    "changes": 144,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/test/java/org/apache/flink/connector/hbase/sink/HBaseSinkTests.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/test/java/org/apache/flink/connector/hbase/sink/HBaseSinkTests.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/test/java/org/apache/flink/connector/hbase/sink/HBaseSinkTests.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.sink;\n+\n+import org.apache.flink.connector.hbase.HBaseEvent;\n+import org.apache.flink.connector.hbase.testutil.HBaseTestCluster;\n+import org.apache.flink.connector.hbase.testutil.TestsWithTestHBaseCluster;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Connection;\n+import org.apache.hadoop.hbase.client.ConnectionFactory;\n+import org.apache.hadoop.hbase.client.Get;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.LongStream;\n+\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/** Test for {@link org.apache.flink.connector.hbase.sink.HBaseSink}. */\n+public class HBaseSinkTests extends TestsWithTestHBaseCluster {\n+\n+    @Test\n+    public void testSinkPut() throws Exception {\n+        cluster.makeTable(baseTableName);\n+\n+        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+        Configuration hbaseConfiguration = cluster.getConfig();\n+\n+        int start = 1;\n+        int end = 10;\n+\n+        DataStream<Long> numberStream = env.fromSequence(start, end);\n+\n+        final HBaseSink<Long> hbaseSink =\n+                HBaseSink.builder()\n+                        .setTableName(baseTableName)\n+                        .setSinkSerializer(new HBasePutLongSerializer())\n+                        .setHBaseConfiguration(hbaseConfiguration)\n+                        .build();\n+        numberStream.sinkTo(hbaseSink);\n+        env.execute();\n+\n+        long[] expected = LongStream.rangeClosed(start, end).toArray();\n+        long[] actual = new long[end - start + 1];\n+\n+        try (Connection connection = ConnectionFactory.createConnection(hbaseConfiguration)) {\n+            Table table = connection.getTable(TableName.valueOf(baseTableName));\n+            for (int i = start; i <= end; i++) {\n+                Get get = new Get(Bytes.toBytes(String.valueOf(i)));\n+                Result r = table.get(get);\n+                byte[] value =\n+                        r.getValue(\n+                                HBaseTestCluster.DEFAULT_COLUMN_FAMILY.getBytes(),\n+                                HBaseTestCluster.DEFAULT_QUALIFIER.getBytes());\n+                long l = Long.parseLong(new String(value));\n+                actual[i - start] = l;\n+            }\n+        }\n+\n+        assertArrayEquals(expected, actual);\n+    }\n+\n+    @Test\n+    public void testSinkDelete() throws Exception {\n+        cluster.makeTable(baseTableName);\n+        List<String> rows = new ArrayList<>();\n+        String rowId;\n+\n+        for (int i = 0; i < 10; i++) {\n+            rowId = cluster.put(baseTableName, String.valueOf(i));\n+            rows.add(rowId);\n+        }\n+\n+        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+        Configuration hbaseConfiguration = cluster.getConfig();\n+\n+        DataStream<String> numberStream = env.fromCollection(rows);\n+\n+        final HBaseSink<String> hbaseSink =\n+                HBaseSink.builder()\n+                        .setTableName(baseTableName)\n+                        .setSinkSerializer(new HBaseDeleteStringSerializer())\n+                        .setHBaseConfiguration(hbaseConfiguration)\n+                        .build();\n+\n+        numberStream.sinkTo(hbaseSink);\n+        env.execute();\n+\n+        try (Connection connection = ConnectionFactory.createConnection(hbaseConfiguration)) {\n+            Table table = connection.getTable(TableName.valueOf(baseTableName));\n+            for (String row : rows) {\n+                Get get = new Get(Bytes.toBytes(row));\n+                Result r = table.get(get);\n+                assertTrue(r.isEmpty());\n+            }\n+        }\n+    }\n+\n+    private static class HBasePutLongSerializer implements HBaseSinkSerializer<Long> {\n+        @Override\n+        public HBaseEvent serialize(Long event) {\n+            return HBaseEvent.putWith(\n+                    event.toString(),\n+                    HBaseTestCluster.DEFAULT_COLUMN_FAMILY,\n+                    HBaseTestCluster.DEFAULT_QUALIFIER,\n+                    Bytes.toBytes(event.toString()));\n+        }\n+    }\n+\n+    private static class HBaseDeleteStringSerializer implements HBaseSinkSerializer<String> {\n+        @Override\n+        public HBaseEvent serialize(String event) {\n+            return HBaseEvent.deleteWith(\n+                    event,\n+                    HBaseTestCluster.DEFAULT_COLUMN_FAMILY,\n+                    HBaseTestCluster.DEFAULT_QUALIFIER);\n+        }\n+    }\n+}"
  },
  {
    "sha": "eb82338935eeaedc1f759c590bf6f67787f92393",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/test/java/org/apache/flink/connector/hbase/source/HBaseSourceITCase.java",
    "status": "added",
    "additions": 317,
    "deletions": 0,
    "changes": 317,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/test/java/org/apache/flink/connector/hbase/source/HBaseSourceITCase.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/test/java/org/apache/flink/connector/hbase/source/HBaseSourceITCase.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/test/java/org/apache/flink/connector/hbase/source/HBaseSourceITCase.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,317 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.source;\n+\n+import org.apache.flink.api.common.eventtime.WatermarkStrategy;\n+import org.apache.flink.api.common.functions.RichFlatMapFunction;\n+import org.apache.flink.connector.hbase.HBaseEvent;\n+import org.apache.flink.connector.hbase.source.reader.HBaseSourceDeserializer;\n+import org.apache.flink.connector.hbase.source.reader.HBaseSourceEvent;\n+import org.apache.flink.connector.hbase.testutil.FailureSink;\n+import org.apache.flink.connector.hbase.testutil.FileSignal;\n+import org.apache.flink.connector.hbase.testutil.HBaseTestCluster;\n+import org.apache.flink.connector.hbase.testutil.TestsWithTestHBaseCluster;\n+import org.apache.flink.connector.hbase.testutil.Util;\n+import org.apache.flink.core.execution.JobClient;\n+import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration;\n+import org.apache.flink.streaming.api.CheckpointingMode;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.test.util.MiniClusterWithClientResource;\n+import org.apache.flink.util.Collector;\n+import org.apache.flink.util.function.RunnableWithException;\n+import org.apache.flink.util.function.ThrowingRunnable;\n+\n+import org.apache.hadoop.hbase.client.Put;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.internal.ArrayComparisonFailure;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+import static org.junit.Assert.assertArrayEquals;\n+\n+/** Tests the most basic use cases of the source with a mocked HBase system. */\n+public class HBaseSourceITCase extends TestsWithTestHBaseCluster {\n+\n+    @Rule\n+    public final MiniClusterWithClientResource miniClusterResource =\n+            new MiniClusterWithClientResource(\n+                    new MiniClusterResourceConfiguration.Builder()\n+                            .setNumberTaskManagers(1)\n+                            .setNumberSlotsPerTaskManager(8)\n+                            .build());\n+\n+    private DataStream<String> streamFromHBaseSource(\n+            StreamExecutionEnvironment environment, String tableName) {\n+        return streamFromHBaseSource(environment, tableName, 1);\n+    }\n+\n+    private DataStream<String> streamFromHBaseSource(\n+            StreamExecutionEnvironment environment, String tableName, int parallelism) {\n+        HBaseStringDeserializationScheme deserializationScheme =\n+                new HBaseStringDeserializationScheme();\n+        HBaseSource<String> source =\n+                HBaseSource.builder()\n+                        .setTableName(tableName)\n+                        .setSourceDeserializer(deserializationScheme)\n+                        .setHBaseConfiguration(cluster.getConfig())\n+                        .build();\n+        environment.setParallelism(parallelism);\n+        DataStream<String> stream =\n+                environment.fromSource(\n+                        source,\n+                        WatermarkStrategy.noWatermarks(),\n+                        \"hbaseSourceITCase\",\n+                        deserializationScheme.getProducedType());\n+        return stream;\n+    }\n+\n+    private static <T> void expectFirstValuesToBe(\n+            DataStream<T> stream, T[] expectedValues, String message) {\n+\n+        List<T> collectedValues = new ArrayList<>();\n+        stream.flatMap(\n+                new RichFlatMapFunction<T, Object>() {\n+\n+                    @Override\n+                    public void flatMap(T value, Collector<Object> out) {\n+                        LOG.info(\"Test collected: {}\", value);\n+                        collectedValues.add(value);\n+                        if (collectedValues.size() == expectedValues.length) {\n+                            assertArrayEquals(message, expectedValues, collectedValues.toArray());\n+                            throw new SuccessException();\n+                        }\n+                    }\n+                });\n+    }\n+\n+    private void doAndWaitForSuccess(\n+            StreamExecutionEnvironment env, RunnableWithException action, int timeout) {\n+        try {\n+            JobClient jobClient = env.executeAsync();\n+            Util.waitForClusterStart(miniClusterResource.getMiniCluster());\n+\n+            action.run();\n+            jobClient.getJobExecutionResult().get(timeout, TimeUnit.SECONDS);\n+            jobClient.cancel();\n+            throw new RuntimeException(\"Waiting for the correct data timed out\");\n+        } catch (Exception exception) {\n+            if (!causedBySuccess(exception)) {\n+                throw new RuntimeException(\"Test failed\", exception);\n+            } else {\n+                // Normal termination\n+            }\n+        }\n+    }\n+\n+    private void waitUntilNReplicationPeers(int n)\n+            throws InterruptedException, ExecutionException, TimeoutException {\n+        CompletableFuture.runAsync(\n+                        ThrowingRunnable.unchecked(\n+                                () -> {\n+                                    while (cluster.getReplicationPeers().size() != n) {\n+                                        sleep(1000);\n+                                    }\n+                                }))\n+                .get(90, TimeUnit.SECONDS);\n+    }\n+\n+    @Test\n+    public void testBasicPut() throws Exception {\n+        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+        DataStream<String> stream = streamFromHBaseSource(env, baseTableName);\n+        cluster.makeTable(baseTableName, DEFAULT_COLUMNFAMILY_COUNT);\n+        String[] expectedValues = uniqueValues(2 * DEFAULT_COLUMNFAMILY_COUNT);\n+\n+        expectFirstValuesToBe(\n+                stream,\n+                expectedValues,\n+                \"HBase source did not produce the right values after a basic put operation\");\n+        doAndWaitForSuccess(\n+                env,\n+                () -> cluster.put(baseTableName, DEFAULT_COLUMNFAMILY_COUNT, expectedValues),\n+                120);\n+    }\n+\n+    @Test\n+    public void testOnlyReplicateSpecifiedTable() throws Exception {\n+        String secondTable = baseTableName + \"-table2\";\n+        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+        DataStream<String> stream = streamFromHBaseSource(env, baseTableName);\n+        cluster.makeTable(baseTableName, DEFAULT_COLUMNFAMILY_COUNT);\n+        cluster.makeTable(secondTable, DEFAULT_COLUMNFAMILY_COUNT);\n+        String[] expectedValues = uniqueValues(DEFAULT_COLUMNFAMILY_COUNT);\n+\n+        expectFirstValuesToBe(\n+                stream,\n+                expectedValues,\n+                \"HBase source did not produce the values of the correct table\");\n+        doAndWaitForSuccess(\n+                env,\n+                () -> {\n+                    cluster.put(\n+                            secondTable,\n+                            DEFAULT_COLUMNFAMILY_COUNT,\n+                            uniqueValues(DEFAULT_COLUMNFAMILY_COUNT));\n+                    sleep(2000);\n+                    cluster.put(baseTableName, DEFAULT_COLUMNFAMILY_COUNT, expectedValues);\n+                },\n+                180);\n+    }\n+\n+    @Test\n+    public void testRecordsAreProducedExactlyOnceWithCheckpoints() throws Exception {\n+        final String collectedValueSignal = \"collectedValue\";\n+        final FileSignal testOracle = this.testOracle;\n+        String[] expectedValues = uniqueValues(20);\n+        cluster.makeTable(baseTableName);\n+\n+        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+        env.enableCheckpointing(2000);\n+        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);\n+        DataStream<String> stream = streamFromHBaseSource(env, baseTableName);\n+        stream.addSink(\n+                new FailureSink<String>(3500) {\n+\n+                    private void checkForSuccess() {\n+                        List<String> checkpointed = getCheckpointedValues();\n+                        LOG.info(\n+                                \"\\n\\tUncheckpointed: {}\\n\\tCheckpointed: {}\",\n+                                unCheckpointedValues,\n+                                checkpointed);\n+                        if (checkpointed.size() == expectedValues.length) {\n+                            try {\n+                                assertArrayEquals(\n+                                        \"Wrong values were produced.\",\n+                                        expectedValues,\n+                                        checkpointed.toArray());\n+                                testOracle.signalSuccess();\n+                            } catch (ArrayComparisonFailure e) {\n+                                LOG.error(\"Comparison failed\", e);\n+                                testOracle.signalFailure();\n+                                throw e;\n+                            }\n+                        }\n+                    }\n+\n+                    @Override\n+                    public void collectValue(String value) {\n+\n+                        if (getCheckpointedValues().contains(value)) {\n+                            LOG.error(\"Unique value {} was not seen only once\", value);\n+                            testOracle.signalFailure();\n+                            throw new RuntimeException(\n+                                    \"Unique value \" + value + \" was not seen only once\");\n+                        }\n+                        checkForSuccess();\n+                        testOracle.signal(collectedValueSignal + value);\n+                    }\n+\n+                    @Override\n+                    public void checkpoint() {\n+                        checkForSuccess();\n+                    }\n+                });\n+\n+        JobClient jobClient = env.executeAsync();\n+        Util.waitForClusterStart(miniClusterResource.getMiniCluster());\n+        try {\n+            Thread.sleep(8000);\n+            int putsPerPackage = 5;\n+            for (int i = 0; i < expectedValues.length; i += putsPerPackage) {\n+                LOG.info(\"Sending next package ...\");\n+                CompletableFuture<String>[] signalsToWait = new CompletableFuture[putsPerPackage];\n+                for (int j = i; j < expectedValues.length && j < i + putsPerPackage; j++) {\n+                    cluster.put(baseTableName, expectedValues[j]);\n+                    signalsToWait[j - i] =\n+                            testOracle.awaitSignal(collectedValueSignal + expectedValues[j]);\n+                }\n+\n+                // Assert that values have actually been sent over so there was an opportunity to\n+                // checkpoint them\n+                testOracle.awaitThrowOnFailure(\n+                        CompletableFuture.allOf(signalsToWait),\n+                        240,\n+                        TimeUnit.SECONDS,\n+                        \"Failure occurred while waiting for package to be collected\");\n+\n+                Thread.sleep(3000);\n+                LOG.info(\"Consuming collection signal\");\n+            }\n+            LOG.info(\"Finished sending packages, awaiting success ...\");\n+            testOracle.awaitSuccess(120, TimeUnit.SECONDS);\n+            LOG.info(\"Received success, ending test ...\");\n+        } finally {\n+            LOG.info(\"Cancelling job client\");\n+            jobClient.cancel();\n+        }\n+        LOG.info(\"End of test method reached\");\n+    }\n+\n+    @Test\n+    public void testMultipleSplitReadersAreCreated() throws Exception {\n+        int parallelism = 4;\n+        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+        streamFromHBaseSource(env, baseTableName, parallelism).print();\n+        cluster.makeTable(baseTableName, parallelism);\n+        JobClient jobClient = env.executeAsync();\n+        waitUntilNReplicationPeers(parallelism);\n+        jobClient.cancel();\n+    }\n+\n+    @Test\n+    public void testBasicPutWhenMoreColumnFamiliesThanThreads() throws Exception {\n+        int parallelism = 1;\n+        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+        DataStream<String> stream = streamFromHBaseSource(env, baseTableName, parallelism);\n+\n+        String[] expectedValues = new String[] {\"foo\", \"bar\", \"baz\", \"boo\"};\n+        assert expectedValues.length > parallelism;\n+        cluster.makeTable(baseTableName, expectedValues.length);\n+        Put put = new Put(\"rowkey\".getBytes());\n+        for (int i = 0; i < expectedValues.length; i++) {\n+            put.addColumn(\n+                    (HBaseTestCluster.COLUMN_FAMILY_BASE + i).getBytes(),\n+                    expectedValues[i].getBytes(),\n+                    expectedValues[i].getBytes());\n+        }\n+        cluster.commitPut(baseTableName, put);\n+\n+        expectFirstValuesToBe(\n+                stream,\n+                expectedValues,\n+                \"HBase source did not produce the right values after a multi-column-family put\");\n+        doAndWaitForSuccess(env, () -> {}, 120);\n+    }\n+\n+    /** Simple Deserialization Scheme to get event payloads as String. */\n+    public static class HBaseStringDeserializationScheme\n+            implements HBaseSourceDeserializer<String> {\n+        @Override\n+        public String deserialize(HBaseSourceEvent event) {\n+            return new String(event.getPayload(), HBaseEvent.DEFAULT_CHARSET);\n+        }\n+    }\n+}"
  },
  {
    "sha": "d315d9089f19276307de5adaafed1e63ffd871ce",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/test/java/org/apache/flink/connector/hbase/source/hbaseendpoint/HBaseEndpointTest.java",
    "status": "added",
    "additions": 122,
    "deletions": 0,
    "changes": 122,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/test/java/org/apache/flink/connector/hbase/source/hbaseendpoint/HBaseEndpointTest.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/test/java/org/apache/flink/connector/hbase/source/hbaseendpoint/HBaseEndpointTest.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/test/java/org/apache/flink/connector/hbase/source/hbaseendpoint/HBaseEndpointTest.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.source.hbaseendpoint;\n+\n+import org.apache.flink.connector.hbase.source.reader.HBaseSourceEvent;\n+import org.apache.flink.connector.hbase.testutil.HBaseTestCluster;\n+import org.apache.flink.connector.hbase.testutil.TestsWithTestHBaseCluster;\n+\n+import org.apache.hadoop.hbase.Cell;\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.TimeUnit;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+\n+/** Tests for {@link HBaseEndpoint}. */\n+public class HBaseEndpointTest extends TestsWithTestHBaseCluster {\n+\n+    @Test\n+    public void testSetup() throws Exception {\n+        new HBaseEndpoint(cluster.getConfig(), cluster.getConfigurationForTable(baseTableName))\n+                .startReplication(\n+                        Collections.singletonList(HBaseTestCluster.DEFAULT_COLUMN_FAMILY));\n+    }\n+\n+    @Test\n+    public void testPutCreatesEvent() throws Exception {\n+        cluster.makeTable(baseTableName);\n+        HBaseEndpoint consumer =\n+                new HBaseEndpoint(\n+                        cluster.getConfig(), cluster.getConfigurationForTable(baseTableName));\n+        consumer.startReplication(\n+                Collections.singletonList(HBaseTestCluster.DEFAULT_COLUMN_FAMILY));\n+        cluster.put(baseTableName, \"foobar\");\n+        List<HBaseSourceEvent> result =\n+                CompletableFuture.supplyAsync(consumer::getAll).get(30, TimeUnit.SECONDS);\n+        assertNotNull(result.get(0));\n+        assertEquals(Cell.Type.Put, result.get(0).getType());\n+    }\n+\n+    @Test\n+    public void testDeleteCreatesEvent() throws Exception {\n+        cluster.makeTable(baseTableName);\n+        HBaseEndpoint consumer =\n+                new HBaseEndpoint(\n+                        cluster.getConfig(), cluster.getConfigurationForTable(baseTableName));\n+        consumer.startReplication(\n+                Collections.singletonList(HBaseTestCluster.DEFAULT_COLUMN_FAMILY));\n+\n+        String rowKey = cluster.put(baseTableName, \"foobar\");\n+        cluster.delete(\n+                baseTableName,\n+                rowKey,\n+                HBaseTestCluster.DEFAULT_COLUMN_FAMILY,\n+                HBaseTestCluster.DEFAULT_QUALIFIER);\n+\n+        List<HBaseSourceEvent> results = new ArrayList<>();\n+        while (results.size() < 2) {\n+            results.addAll(\n+                    CompletableFuture.supplyAsync(consumer::getAll).get(30, TimeUnit.SECONDS));\n+        }\n+\n+        assertNotNull(results.get(1));\n+        assertEquals(Cell.Type.Delete, results.get(1).getType());\n+    }\n+\n+    @Test\n+    public void testTimestampsAndIndicesDefineStrictOrder() throws Exception {\n+        HBaseEndpoint consumer =\n+                new HBaseEndpoint(\n+                        cluster.getConfig(), cluster.getConfigurationForTable(baseTableName));\n+        consumer.startReplication(\n+                Collections.singletonList(HBaseTestCluster.DEFAULT_COLUMN_FAMILY));\n+        cluster.makeTable(baseTableName);\n+\n+        int numPuts = 3;\n+        int putSize = 2 * DEFAULT_COLUMNFAMILY_COUNT;\n+        for (int i = 0; i < numPuts; i++) {\n+            cluster.put(baseTableName, 1, uniqueValues(putSize));\n+        }\n+\n+        long lastTimeStamp = -1;\n+        int lastIndex = -1;\n+\n+        List<HBaseSourceEvent> events = new ArrayList<>();\n+        while (events.size() < numPuts * putSize) {\n+            events.addAll(\n+                    CompletableFuture.supplyAsync(consumer::getAll).get(30, TimeUnit.SECONDS));\n+        }\n+\n+        for (int i = 0; i < numPuts * putSize; i++) {\n+            HBaseSourceEvent nextEvent = events.get(i);\n+            assertTrue(\n+                    \"Events were not collected with strictly ordered, unique timestamp x index\",\n+                    nextEvent.isLaterThan(lastTimeStamp, lastIndex));\n+            lastTimeStamp = nextEvent.getTimestamp();\n+            lastIndex = nextEvent.getIndex();\n+        }\n+    }\n+}"
  },
  {
    "sha": "1f09f3150cc459f27ea038aa4be4b8979db64854",
    "filename": "flink-connectors/flink-connector-hbase-unified/src/test/java/org/apache/flink/connector/hbase/testutil/FailureSink.java",
    "status": "added",
    "additions": 142,
    "deletions": 0,
    "changes": 142,
    "blob_url": "https://github.com/apache/flink/blob/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/test/java/org/apache/flink/connector/hbase/testutil/FailureSink.java",
    "raw_url": "https://github.com/apache/flink/raw/27d2bcd81405305472be3eda8c87552e6f1872f1/flink-connectors/flink-connector-hbase-unified/src/test/java/org/apache/flink/connector/hbase/testutil/FailureSink.java",
    "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-hbase-unified/src/test/java/org/apache/flink/connector/hbase/testutil/FailureSink.java?ref=27d2bcd81405305472be3eda8c87552e6f1872f1",
    "patch": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.connector.hbase.testutil;\n+\n+import org.apache.flink.api.common.state.CheckpointListener;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.TypeExtractor;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Timer;\n+import java.util.TimerTask;\n+\n+/**\n+ * Utility sink for testing checkpointing. Throws exception after at least one value has been\n+ * processed and one checkpoint has been completed. Provides callbacks for value collection and\n+ * checkpointing.\n+ */\n+public abstract class FailureSink<T> extends RichSinkFunction<T>\n+        implements CheckpointedFunction, CheckpointListener {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(FailureSink.class);\n+\n+    private final long activateAfter;\n+    private boolean active = false;\n+    private boolean completedAtLeastOneCheckpoint = false;\n+    private boolean hasSeenAtLeastOneInput = false;\n+\n+    protected final List<T> unCheckpointedValues = new ArrayList<>();\n+    protected transient ListState<T> checkpointedValues;\n+\n+    public FailureSink(long activateAfter) {\n+        this.activateAfter = activateAfter;\n+    }\n+\n+    public void collectValue(T value) throws Exception {}\n+\n+    public void checkpoint() throws Exception {}\n+\n+    public List<T> getCheckpointedValues() {\n+        try {\n+            List<T> checkpointed = new ArrayList<>();\n+            checkpointedValues.get().forEach(checkpointed::add);\n+            return checkpointed;\n+        } catch (Exception e) {\n+            LOG.error(\"Could not retrieve checkpointed values\", e);\n+            return null;\n+        }\n+    }\n+\n+    @Override\n+    public void invoke(T value, Context context) throws Exception {\n+        LOG.info(\"FailureSink has been invoked with value {} is active={}\", value, active);\n+        unCheckpointedValues.add(value);\n+        collectValue(value);\n+        hasSeenAtLeastOneInput = true;\n+        throwFailureIfActive();\n+    }\n+\n+    @Override\n+    public void notifyCheckpointComplete(long checkpointId) throws Exception {\n+        LOG.info(\n+                \"FailureSink.notifyCheckpointComplete has been called with checkpointId={} is active{}\",\n+                checkpointId,\n+                active);\n+        completedAtLeastOneCheckpoint = true;\n+        throwFailureIfActive();\n+    }\n+\n+    @Override\n+    public void snapshotState(FunctionSnapshotContext context) throws Exception {\n+        LOG.info(\"FailureSink.snapshotState has been called\");\n+        checkpointedValues.addAll(unCheckpointedValues);\n+        unCheckpointedValues.clear();\n+        checkpoint();\n+    }\n+\n+    @Override\n+    public void initializeState(FunctionInitializationContext context) throws Exception {\n+        LOG.info(\"FailureSink.initializeState has been called\");\n+\n+        completedAtLeastOneCheckpoint = false;\n+        hasSeenAtLeastOneInput = false;\n+\n+        ListStateDescriptor<T> descriptor =\n+                new ListStateDescriptor<>(\"checkpointed\", getTypeInfo());\n+\n+        checkpointedValues = context.getOperatorStateStore().getListState(descriptor);\n+\n+        new Timer().schedule(activation(), activateAfter);\n+    }\n+\n+    private TimerTask activation() {\n+        return new TimerTask() {\n+            @Override\n+            public void run() {\n+                if (completedAtLeastOneCheckpoint && hasSeenAtLeastOneInput) {\n+                    active = true;\n+                    LOG.info(\"FailureSink activated\");\n+                } else {\n+                    new Timer().schedule(activation(), activateAfter / 2);\n+                }\n+            }\n+        };\n+    }\n+\n+    private void throwFailureIfActive() {\n+        if (active) {\n+            LOG.info(\"FailureSink triggered\");\n+            throw new RuntimeException(\"Failure Sink throws error\");\n+        }\n+    }\n+\n+    private TypeInformation<T> getTypeInfo() {\n+        return TypeExtractor.createTypeInfo(FailureSink.class, getClass(), 0, null, null);\n+    }\n+}"
  }
]
