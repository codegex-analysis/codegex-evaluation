[
  {
    "sha": "86faad455bf2826efb2b8ad10a786d452c3fecdc",
    "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java",
    "status": "modified",
    "additions": 8,
    "deletions": 0,
    "changes": 8,
    "blob_url": "https://github.com/apache/hadoop/blob/03b402270794b254cb9f040783f5bb6015c6c869/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java",
    "raw_url": "https://github.com/apache/hadoop/raw/03b402270794b254cb9f040783f5bb6015c6c869/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java",
    "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java?ref=03b402270794b254cb9f040783f5bb6015c6c869",
    "patch": "@@ -318,6 +318,14 @@\n   public static final boolean\n       DFS_NAMENODE_CORRUPT_BLOCK_DELETE_IMMEDIATELY_ENABLED_DEFAULT = true;\n \n+  public static final String DFS_NAMENODE_RECONSTRUCT_EC_BLOCK_GROUPS_LIMIT_ENABLED =\n+     \"dfs.namenode.reconstruct.ecblock-groups.limit.enable\";\n+  public static final boolean DFS_NAMENODE_RECONSTRUCT_EC_BLOCK_GROUPS_LIMIT_ENABLED_DEFAULT = false;\n+\n+  public static final String DFS_NAMENODE_RECONSTRUCT_EC_BLOCK_GROUPS_LIMIT =\n+      \"dfs.namenode.reconstruct.ecblock-groups.limit\";\n+  public static final long DFS_NAMENODE_RECONSTRUCT_EC_BLOCK_GROUPS_LIMIT_DEFAULT = 1000;\n+\n   @Deprecated\n   public static final String  DFS_WEBHDFS_USER_PATTERN_KEY =\n       HdfsClientConfigKeys.DFS_WEBHDFS_USER_PATTERN_KEY;"
  },
  {
    "sha": "ac0fa3a46b0468992c067f9e87cfd65855f4809d",
    "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
    "status": "modified",
    "additions": 93,
    "deletions": 0,
    "changes": 93,
    "blob_url": "https://github.com/apache/hadoop/blob/03b402270794b254cb9f040783f5bb6015c6c869/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
    "raw_url": "https://github.com/apache/hadoop/raw/03b402270794b254cb9f040783f5bb6015c6c869/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
    "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=03b402270794b254cb9f040783f5bb6015c6c869",
    "patch": "@@ -111,6 +111,7 @@\n import org.apache.hadoop.hdfs.server.protocol.StorageReport;\n import org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary;\n import org.apache.hadoop.hdfs.util.FoldedTreeSet;\n+import org.apache.hadoop.hdfs.util.LightWeightHashSet;\n import org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy;\n import org.apache.hadoop.hdfs.server.namenode.CacheManager;\n \n@@ -469,6 +470,12 @@ public long getTotalECBlockGroups() {\n   /** Storages accessible from multiple DNs. */\n   private final ProvidedStorageMap providedStorageMap;\n \n+  /** Whether to enable limit EC block reconstruct.*/\n+  private volatile boolean reconstructECBlockGroupsLimitEnabled;\n+  private volatile long reconstructECBlockGroupsLimit;\n+  private final LightWeightHashSet<BlockInfo> reconstructECBlockGroups =\n+      new LightWeightHashSet<>();\n+\n   public BlockManager(final Namesystem namesystem, boolean haEnabled,\n       final Configuration conf) throws IOException {\n     this.namesystem = namesystem;\n@@ -625,13 +632,22 @@ public BlockManager(final Namesystem namesystem, boolean haEnabled,\n         conf.getBoolean(DFS_NAMENODE_CORRUPT_BLOCK_DELETE_IMMEDIATELY_ENABLED,\n             DFS_NAMENODE_CORRUPT_BLOCK_DELETE_IMMEDIATELY_ENABLED_DEFAULT);\n \n+    this.reconstructECBlockGroupsLimitEnabled = conf.getBoolean(\n+        DFSConfigKeys.DFS_NAMENODE_RECONSTRUCT_EC_BLOCK_GROUPS_LIMIT_ENABLED,\n+        DFSConfigKeys.DFS_NAMENODE_RECONSTRUCT_EC_BLOCK_GROUPS_LIMIT_ENABLED_DEFAULT);\n+    this.reconstructECBlockGroupsLimit = conf.getLong(\n+        DFSConfigKeys.DFS_NAMENODE_RECONSTRUCT_EC_BLOCK_GROUPS_LIMIT,\n+        DFSConfigKeys.DFS_NAMENODE_RECONSTRUCT_EC_BLOCK_GROUPS_LIMIT_DEFAULT);\n+\n     LOG.info(\"defaultReplication         = {}\", defaultReplication);\n     LOG.info(\"maxReplication             = {}\", maxReplication);\n     LOG.info(\"minReplication             = {}\", minReplication);\n     LOG.info(\"maxReplicationStreams      = {}\", maxReplicationStreams);\n     LOG.info(\"redundancyRecheckInterval  = {}ms\", redundancyRecheckIntervalMs);\n     LOG.info(\"encryptDataTransfer        = {}\", encryptDataTransfer);\n     LOG.info(\"maxNumBlocksToLog          = {}\", maxNumBlocksToLog);\n+    LOG.info(\"reconstructECBlockGroupsLimit = {}\", reconstructECBlockGroupsLimit);\n+    LOG.info(\"reconstructECBlockGroupsLimitEnabled = {}\", reconstructECBlockGroupsLimitEnabled);\n   }\n \n   private static BlockTokenSecretManager createBlockTokenSecretManager(\n@@ -1295,6 +1311,8 @@ public LocatedBlock convertLastBlockToUnderConstruction(\n           new DatanodeStorageInfo[locations.size()];\n       locations.toArray(removedBlockTargets);\n       DatanodeStorageInfo.decrementBlocksScheduled(removedBlockTargets);\n+      //Remove block from reconstructECBlockGroups queue.\n+      removeReconstructECBlockGroups(lastBlock);\n     }\n \n     // remove this block from the list of pending blocks to be deleted. \n@@ -2089,12 +2107,21 @@ int computeReconstructionWorkForBlocks(\n         final DatanodeStorageInfo[] targets = rw.getTargets();\n         if (targets == null || targets.length == 0) {\n           rw.resetTargets();\n+          if (removeReconstructECBlockGroups(rw.getBlock())) {\n+            LOG.debug(\"Removing block {} from reconstructECBlockGroups, \" +\n+                \"now size {}\", rw.getBlock(), getReconstructECBlockGroupCount());\n+          }\n           continue;\n         }\n \n         synchronized (neededReconstruction) {\n           if (validateReconstructionWork(rw)) {\n             scheduledWork++;\n+          } else {\n+            if (removeReconstructECBlockGroups(rw.getBlock())) {\n+              LOG.debug(\"Removing block {} from reconstructECBlockGroups, \" +\n+                  \"now size {}\", rw.getBlock(), getReconstructECBlockGroupCount());\n+            }\n           }\n         }\n       }\n@@ -2200,6 +2227,26 @@ BlockReconstructionWork scheduleReconstruction(BlockInfo block,\n         additionalReplRequired = additionalReplRequired -\n             numReplicas.decommissioning() -\n             numReplicas.liveEnteringMaintenanceReplicas();\n+        if (reconstructECBlockGroupsLimitEnabled &&\n+            numReplicas.liveReplicas() < requiredRedundancy) {\n+          LOG.debug(\"Prepare creating an ErasureCodingWork to {} reconstruct, \" +\n+              \"currently there are {} EC BlockGroups to reconstruct and \" +\n+              \"the limit is {}\", block, getReconstructECBlockGroupCount(),\n+              reconstructECBlockGroupsLimit);\n+          if (checkReconstructECBlockGroups(block) ||\n+              getReconstructECBlockGroupCount() >= this.reconstructECBlockGroupsLimit) {\n+            LOG.warn(\"Currently there are {} EC BlockGroups to reconstruct and \" +\n+                \"the limit is {} block {} cannot create\",\n+                getReconstructECBlockGroupCount(),\n+                reconstructECBlockGroupsLimit, block);\n+            return null;\n+          }\n+          addReconstructECBlockGroups(block);\n+          LOG.debug(\"Complete creating an ErasureCodingWork to {} reconstruct, \" +\n+              \"currently there are {} EC BlockGroups to reconstruct and \" +\n+              \"the limit is {}\", block, getReconstructECBlockGroupCount(),\n+              reconstructECBlockGroupsLimit);\n+        }\n       }\n       final DatanodeDescriptor[] newSrcNodes =\n           new DatanodeDescriptor[srcNodes.length];\n@@ -4220,6 +4267,7 @@ public void addBlock(DatanodeStorageInfo storageInfo, Block block,\n     if (storedBlock != null &&\n         block.getGenerationStamp() == storedBlock.getGenerationStamp()) {\n       if (pendingReconstruction.decrement(storedBlock, storageInfo)) {\n+        removeReconstructECBlockGroups(storedBlock);\n         NameNode.getNameNodeMetrics().incSuccessfulReReplications();\n       }\n     }\n@@ -4649,6 +4697,7 @@ public void removeBlock(BlockInfo block) {\n     }\n     neededReconstruction.remove(block, LowRedundancyBlocks.LEVEL);\n     postponedMisreplicatedBlocks.remove(block);\n+    removeReconstructECBlockGroups(block);\n   }\n \n   public BlockInfo getStoredBlock(Block block) {\n@@ -5134,6 +5183,7 @@ public void clearQueues() {\n     invalidateBlocks.clear();\n     datanodeManager.clearPendingQueues();\n     postponedMisreplicatedBlocks.clear();\n+    clearReconstructECBlockGroups();\n   };\n \n   public static LocatedBlock newLocatedBlock(\n@@ -5498,4 +5548,47 @@ public void disableSPS() {\n   public StoragePolicySatisfyManager getSPSManager() {\n     return spsManager;\n   }\n+\n+  boolean removeReconstructECBlockGroups(BlockInfo block) {\n+    if (reconstructECBlockGroupsLimitEnabled && block.isStriped()) {\n+      synchronized (reconstructECBlockGroups) {\n+        if (reconstructECBlockGroups.remove(block)) {\n+          LOG.debug(\"Removing block {} from reconstructECBlockGroups, \" +\n+              \"now size {}\", block, getReconstructECBlockGroupCount());\n+          return true;\n+        }\n+      }\n+    }\n+    return false;\n+  }\n+\n+  boolean checkReconstructECBlockGroups(BlockInfo block) {\n+    synchronized (reconstructECBlockGroups) {\n+      if (reconstructECBlockGroups.contains(block) &&\n+          reconstructECBlockGroups.remove(block)) {\n+        LOG.warn(\"Check Removing block {} from reconstructECBlockGroups, \" +\n+            \"now size {}\", block, getReconstructECBlockGroupCount());\n+        return true;\n+      }\n+    }\n+    return false;\n+  }\n+\n+  boolean addReconstructECBlockGroups(BlockInfo block) {\n+    synchronized (reconstructECBlockGroups) {\n+      return reconstructECBlockGroups.add(block);\n+    }\n+  }\n+\n+  public long getReconstructECBlockGroupCount() {\n+    synchronized (reconstructECBlockGroups) {\n+      return reconstructECBlockGroups.size();\n+    }\n+  }\n+\n+  void clearReconstructECBlockGroups(){\n+    synchronized (reconstructECBlockGroups) {\n+      reconstructECBlockGroups.clear();\n+    }\n+  }\n }"
  },
  {
    "sha": "45fcf555b9c01a115963abeb4de63abf75b2dc8e",
    "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestReconstructStripedBlocks.java",
    "status": "modified",
    "additions": 92,
    "deletions": 0,
    "changes": 92,
    "blob_url": "https://github.com/apache/hadoop/blob/03b402270794b254cb9f040783f5bb6015c6c869/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestReconstructStripedBlocks.java",
    "raw_url": "https://github.com/apache/hadoop/raw/03b402270794b254cb9f040783f5bb6015c6c869/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestReconstructStripedBlocks.java",
    "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestReconstructStripedBlocks.java?ref=03b402270794b254cb9f040783f5bb6015c6c869",
    "patch": "@@ -43,6 +43,10 @@\n import org.apache.hadoop.hdfs.server.datanode.DataNode;\n import org.apache.hadoop.hdfs.server.datanode.DataNodeTestUtils;\n import org.apache.hadoop.hdfs.server.protocol.BlockECReconstructionCommand.BlockECReconstructionInfo;\n+import org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration;\n+import org.apache.hadoop.hdfs.server.protocol.DatanodeStorage;\n+import org.apache.hadoop.hdfs.server.protocol.ReceivedDeletedBlockInfo;\n+import org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks;\n \n import org.apache.hadoop.hdfs.util.StripedBlockUtil;\n import org.junit.Assert;\n@@ -430,4 +434,92 @@ public void testReconstructionWork() throws Exception {\n       dfsCluster.shutdown();\n     }\n   }\n+\n+  @Test\n+  public void testRecoveryTasksForBlockGroupsLimit() throws Exception {\n+    Configuration conf = new HdfsConfiguration();\n+    conf.setInt(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY, 1000);\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,\n+        1000);\n+    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, blockSize);\n+    conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_RECONSTRUCT_EC_BLOCK_GROUPS_LIMIT_ENABLED,\n+        true);\n+    long limit = 2;\n+    conf.setLong(DFSConfigKeys.DFS_NAMENODE_RECONSTRUCT_EC_BLOCK_GROUPS_LIMIT, limit);\n+    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(groupSize + 1).build();\n+    try {\n+      cluster.waitActive();\n+      cluster.getFileSystem().enableErasureCodingPolicy(\n+        StripedFileTestUtil.getDefaultECPolicy().getName());\n+      final int numBlocks = 6;\n+      DFSTestUtil.createStripedFile(cluster, filePath,\n+          dirPath, numBlocks, 1, true);\n+      // all blocks will be located at first GROUP_SIZE DNs, the last DN is\n+      // empty because of the util function createStripedFile\n+\n+      // make sure the file is complete in NN\n+      final INodeFile fileNode = cluster.getNamesystem().getFSDirectory()\n+          .getINode4Write(filePath.toString()).asFile();\n+      assertFalse(fileNode.isUnderConstruction());\n+      assertTrue(fileNode.isStriped());\n+      BlockInfo[] blocks = fileNode.getBlocks();\n+      assertEquals(numBlocks, blocks.length);\n+\n+      BlockManager bm = cluster.getNamesystem().getBlockManager();\n+\n+      BlockInfo firstBlock = fileNode.getBlocks()[0];\n+      DatanodeStorageInfo[] storageInfos = bm.getStorages(firstBlock);\n+\n+      // make numOfMissed internal blocks missed\n+      for (int i = 0; i < 1; i++) {\n+        DatanodeDescriptor missedNode = storageInfos[i].getDatanodeDescriptor();\n+        assertEquals(numBlocks, missedNode.numBlocks());\n+        bm.getDatanodeManager().removeDatanode(missedNode);\n+      }\n+      BlockManagerTestUtil.updateState(bm);\n+      DFSTestUtil.verifyClientStats(conf, cluster);\n+      assertEquals( numBlocks, bm.getLowRedundancyBlocksCount());\n+      // all the reconstruction work will be scheduled on the last DN\n+      DataNode lastDn = cluster.getDataNodes().get(groupSize);\n+      DatanodeDescriptor last = bm.getDatanodeManager().getDatanode\n+          (lastDn.getDatanodeId());\n+      BlockManagerTestUtil.getComputedDatanodeWork(bm);\n+      int count = 0;\n+      while (bm.getPendingReconstructionBlocksCount() > 0) {\n+        count++;\n+        assertEquals(\"Counting the number of outstanding EC tasks\", limit,\n+            last.getNumberOfBlocksToBeErasureCoded());\n+        assertEquals(limit, bm.getPendingReconstructionBlocksCount());\n+        assertEquals(limit, bm.getReconstructECBlockGroupCount());\n+        List<BlockECReconstructionInfo> reconstruction =\n+            last.getErasureCodeCommand((int) limit);\n+\n+        for (BlockECReconstructionInfo info : reconstruction) {\n+          String poolId = cluster.getNamesystem().getBlockPoolId();\n+          // let two datanodes (other than the one that already has the data) to\n+          // report to NN\n+          DatanodeRegistration dnR = lastDn.getDNRegistrationForBP(poolId);\n+          StorageReceivedDeletedBlocks[] report = {\n+              new StorageReceivedDeletedBlocks(\n+                  new DatanodeStorage(\"Fake-storage-ID-Ignored\"),\n+                  new ReceivedDeletedBlockInfo[]{new ReceivedDeletedBlockInfo(\n+                      info.getExtendedBlock().getLocalBlock(),\n+                      ReceivedDeletedBlockInfo.BlockStatus.RECEIVED_BLOCK, \"\")})\n+          };\n+          cluster.getNameNodeRpc().blockReceivedAndDeleted(dnR, poolId, report);\n+        }\n+        bm.flushBlockOps();\n+        BlockManagerTestUtil.updateState(bm);\n+        DFSTestUtil.verifyClientStats(conf, cluster);\n+        BlockManagerTestUtil.getComputedDatanodeWork(bm);\n+      }\n+      assertEquals(numBlocks/limit, count);\n+      BlockManagerTestUtil.updateState(bm);\n+      DFSTestUtil.verifyClientStats(conf, cluster);\n+      assertEquals(0, last.getNumberOfBlocksToBeErasureCoded());\n+      assertEquals(0, bm.getPendingReconstructionBlocksCount());\n+    } finally {\n+      cluster.shutdown();\n+    }\n+  }\n }"
  }
]
