[
  {
    "sha": "fb1fc11357f0629bfa3014822a9abaae82e528f9",
    "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java",
    "status": "modified",
    "additions": 20,
    "deletions": 11,
    "changes": 31,
    "blob_url": "https://github.com/apache/hadoop/blob/87edb72320b1db49a185a65528f771d83d7bb0c1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java",
    "raw_url": "https://github.com/apache/hadoop/raw/87edb72320b1db49a185a65528f771d83d7bb0c1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java",
    "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirAttrOp.java?ref=87edb72320b1db49a185a65528f771d83d7bb0c1",
    "patch": "@@ -82,15 +82,24 @@ static FileStatus setOwner(\n     fsd.writeLock();\n     try {\n       iip = fsd.resolvePath(pc, src, DirOp.WRITE);\n+      // Only the owner or super user can change the group or owner\n       fsd.checkOwner(pc, iip);\n-      if (!pc.isSuperUser()) {\n-        if (username != null && !pc.getUser().equals(username)) {\n-          throw new AccessControlException(\"User \" + pc.getUser()\n-              + \" is not a super user (non-super user cannot change owner).\");\n-        }\n-        if (group != null && !pc.isMemberOfGroup(group)) {\n-          throw new AccessControlException(\n-              \"User \" + pc.getUser() + \" does not belong to \" + group);\n+      // Only a super user can change ownership to a different user\n+      // or group to a different group that the user doesn't belong to\n+      if ((username != null && !pc.getUser().equals(username)) ||\n+          (group != null && !pc.isMemberOfGroup(group))) {\n+        try {\n+          // check if the user is superuser\n+          pc.checkSuperuserPrivilege(iip.getPath());\n+        } catch (AccessControlException e) {\n+          if (username != null && !pc.getUser().equals(username)) {\n+            throw new AccessControlException(\"User \" + pc.getUser()\n+                + \" is not a super user (non-super user cannot change owner).\");\n+          }\n+          if (group != null && !pc.isMemberOfGroup(group)) {\n+            throw new AccessControlException(\n+                \"User \" + pc.getUser() + \" does not belong to \" + group);\n+          }\n         }\n       }\n       changed = unprotectedSetOwner(fsd, iip, username, group);\n@@ -239,9 +248,9 @@ static void setQuota(FSDirectory fsd, FSPermissionChecker pc, String src,\n     try {\n       INodesInPath iip = fsd.resolvePath(pc, src, DirOp.WRITE);\n       if (fsd.isPermissionEnabled() && !pc.isSuperUser() && allowOwner) {\n-        INodeDirectory parentDir= iip.getLastINode().getParent();\n-        if (parentDir == null ||\n-            !parentDir.getUserName().equals(pc.getUser())) {\n+        try {\n+          fsd.checkOwner(pc, iip.getParentINodesInPath());\n+        } catch(AccessControlException ace) {\n           throw new AccessControlException(\n               \"Access denied for user \" + pc.getUser() +\n               \". Superuser or owner of parent folder privilege is required\");"
  },
  {
    "sha": "8529a3811e57d2b435177f35944e07bf757b8eb7",
    "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
    "status": "modified",
    "additions": 1,
    "deletions": 0,
    "changes": 1,
    "blob_url": "https://github.com/apache/hadoop/blob/87edb72320b1db49a185a65528f771d83d7bb0c1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
    "raw_url": "https://github.com/apache/hadoop/raw/87edb72320b1db49a185a65528f771d83d7bb0c1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
    "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java?ref=87edb72320b1db49a185a65528f771d83d7bb0c1",
    "patch": "@@ -105,6 +105,7 @@ static HdfsFileStatus getFileInfo(FSDirectory fsd, FSPermissionChecker pc,\n       // superuser to receive null instead.\n       try {\n         iip = fsd.resolvePath(pc, srcArg, dirOp);\n+        pc.checkSuperuserPrivilege(iip.getPath());\n       } catch (AccessControlException ace) {\n         return null;\n       }"
  },
  {
    "sha": "7373e54e8396295a088d596c8a6959b3ccbf660f",
    "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
    "status": "modified",
    "additions": 12,
    "deletions": 9,
    "changes": 21,
    "blob_url": "https://github.com/apache/hadoop/blob/87edb72320b1db49a185a65528f771d83d7bb0c1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
    "raw_url": "https://github.com/apache/hadoop/raw/87edb72320b1db49a185a65528f771d83d7bb0c1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
    "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java?ref=87edb72320b1db49a185a65528f771d83d7bb0c1",
    "patch": "@@ -717,18 +717,18 @@ public INodesInPath resolvePath(FSPermissionChecker pc, String src,\n \n     byte[][] components = INode.getPathComponents(src);\n     boolean isRaw = isReservedRawName(components);\n+    components = resolveComponents(components, this);\n+    INodesInPath iip = INodesInPath.resolve(rootDir, components, isRaw);\n     if (isPermissionEnabled && pc != null && isRaw) {\n       switch(dirOp) {\n-        case READ_LINK:\n-        case READ:\n-          break;\n-        default:\n-          pc.checkSuperuserPrivilege();\n-          break;\n+      case READ_LINK:\n+      case READ:\n+        break;\n+      default:\n+        pc.checkSuperuserPrivilege(iip.getPath());\n+        break;\n       }\n     }\n-    components = resolveComponents(components, this);\n-    INodesInPath iip = INodesInPath.resolve(rootDir, components, isRaw);\n     // verify all ancestors are dirs and traversable.  note that only\n     // methods that create new namespace items have the signature to throw\n     // PNDE\n@@ -1942,7 +1942,10 @@ void checkPermission(FSPermissionChecker pc, INodesInPath iip,\n       boolean doCheckOwner, FsAction ancestorAccess, FsAction parentAccess,\n       FsAction access, FsAction subAccess, boolean ignoreEmptyDir)\n       throws AccessControlException {\n-    if (!pc.isSuperUser()) {\n+    if (pc.isSuperUser()) {\n+      // call the enforcer for audit\n+      pc.checkSuperuserPrivilege(iip.getPath());\n+    } else {\n       readLock();\n       try {\n         pc.checkPermission(iip, doCheckOwner, ancestorAccess,"
  },
  {
    "sha": "ab736120d43000a0baa459e2bb0e12233c97bc65",
    "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
    "status": "modified",
    "additions": 36,
    "deletions": 31,
    "changes": 67,
    "blob_url": "https://github.com/apache/hadoop/blob/87edb72320b1db49a185a65528f771d83d7bb0c1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
    "raw_url": "https://github.com/apache/hadoop/raw/87edb72320b1db49a185a65528f771d83d7bb0c1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
    "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=87edb72320b1db49a185a65528f771d83d7bb0c1",
    "patch": "@@ -1954,7 +1954,7 @@ private void metaSave(PrintWriter out) {\n       EnumSet<OpenFilesType> openFilesTypes, String path) throws IOException {\n     INode.checkAbsolutePath(path);\n     final String operationName = \"listOpenFiles\";\n-    checkSuperuserPrivilege();\n+    checkSuperuserPrivilege(operationName, path);\n     checkOperation(OperationCategory.READ);\n     BatchedListEntries<OpenFileEntry> batchedListEntries;\n     String normalizedPath = new Path(path).toString(); // normalize path.\n@@ -2415,7 +2415,7 @@ private void checkStoragePolicyEnabled(final String operationNameReadable,\n     }\n     if (checkSuperUser && isStoragePolicySuperuserOnly) {\n       checkSuperuserPrivilege(\n-          CaseUtils.toCamelCase(operationNameReadable, false));\n+          CaseUtils.toCamelCase(operationNameReadable, false), null);\n     }\n   }\n \n@@ -3582,7 +3582,7 @@ void setQuota(String src, long nsQuota, long ssQuota, StorageType type)\n     FSPermissionChecker.setOperationType(operationName);\n     try {\n       if(!allowOwnerSetQuota) {\n-        checkSuperuserPrivilege(pc);\n+        checkSuperuserPrivilege(operationName, src);\n       }\n       writeLock();\n       try {\n@@ -5220,18 +5220,22 @@ PermissionStatus createFsOwnerPermissions(FsPermission permission) {\n     return new PermissionStatus(fsOwner.getShortUserName(), supergroup, permission);\n   }\n \n+  /**\n+   * This method is retained for backward compatibility.\n+   * Please use {@link #checkSuperuserPrivilege(String)} instead.\n+   *\n+   * @throws AccessControlException if user is not a super user.\n+   */\n   void checkSuperuserPrivilege() throws AccessControlException {\n     if (isPermissionEnabled) {\n       FSPermissionChecker pc = getPermissionChecker();\n-      pc.checkSuperuserPrivilege();\n+      pc.checkSuperuserPrivilege(null);\n     }\n   }\n \n-  void checkSuperuserPrivilege(FSPermissionChecker pc)\n-      throws AccessControlException {\n-    if (isPermissionEnabled) {\n-      pc.checkSuperuserPrivilege();\n-    }\n+  void checkSuperuserPrivilege(String operationName)\n+      throws IOException {\n+    checkSuperuserPrivilege(operationName, null);\n   }\n \n   /**\n@@ -6009,7 +6013,8 @@ public String toString() {\n    */\n   Collection<CorruptFileBlockInfo> listCorruptFileBlocks(String path,\n   String[] cookieTab) throws IOException {\n-    checkSuperuserPrivilege();\n+    final String operationName = \"listCorruptFileBlocks\";\n+    checkSuperuserPrivilege(operationName, path);\n     checkOperation(OperationCategory.READ);\n \n     int count = 0;\n@@ -6937,7 +6942,7 @@ public SnapshotManager getSnapshotManager() {\n   void allowSnapshot(String path) throws IOException {\n     checkOperation(OperationCategory.WRITE);\n     final String operationName = \"allowSnapshot\";\n-    checkSuperuserPrivilege(operationName);\n+    checkSuperuserPrivilege(operationName, path);\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n@@ -6954,7 +6959,7 @@ void allowSnapshot(String path) throws IOException {\n   void disallowSnapshot(String path) throws IOException {\n     checkOperation(OperationCategory.WRITE);\n     final String operationName = \"disallowSnapshot\";\n-    checkSuperuserPrivilege(operationName);\n+    checkSuperuserPrivilege(operationName, path);\n     writeLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n@@ -7665,7 +7670,7 @@ void addCachePool(CachePoolInfo req, boolean logRetryCache)\n     checkOperation(OperationCategory.WRITE);\n     String poolInfoStr = null;\n     try {\n-      checkSuperuserPrivilege();\n+      checkSuperuserPrivilege(operationName);\n       writeLock();\n       try {\n         checkOperation(OperationCategory.WRITE);\n@@ -7692,7 +7697,7 @@ void modifyCachePool(CachePoolInfo req, boolean logRetryCache)\n     String poolNameStr = \"{poolName: \" +\n         (req == null ? null : req.getPoolName()) + \"}\";\n     try {\n-      checkSuperuserPrivilege();\n+      checkSuperuserPrivilege(operationName);\n       writeLock();\n       try {\n         checkOperation(OperationCategory.WRITE);\n@@ -7719,7 +7724,7 @@ void removeCachePool(String cachePoolName, boolean logRetryCache)\n     checkOperation(OperationCategory.WRITE);\n     String poolNameStr = \"{poolName: \" + cachePoolName + \"}\";\n     try {\n-      checkSuperuserPrivilege();\n+      checkSuperuserPrivilege(operationName);\n       writeLock();\n       try {\n         checkOperation(OperationCategory.WRITE);\n@@ -7924,8 +7929,7 @@ void createEncryptionZone(final String src, final String keyName,\n       Metadata metadata = FSDirEncryptionZoneOp.ensureKeyIsInitialized(dir,\n           keyName, src);\n       final FSPermissionChecker pc = getPermissionChecker();\n-      FSPermissionChecker.setOperationType(operationName);\n-      checkSuperuserPrivilege(pc);\n+      checkSuperuserPrivilege(operationName, src);\n       checkOperation(OperationCategory.WRITE);\n       writeLock();\n       try {\n@@ -7986,9 +7990,7 @@ EncryptionZone getEZForPath(final String srcArg)\n     final String operationName = \"listEncryptionZones\";\n     boolean success = false;\n     checkOperation(OperationCategory.READ);\n-    final FSPermissionChecker pc = getPermissionChecker();\n-    FSPermissionChecker.setOperationType(operationName);\n-    checkSuperuserPrivilege(pc);\n+    checkSuperuserPrivilege(operationName);\n     readLock();\n     try {\n       checkOperation(OperationCategory.READ);\n@@ -8004,12 +8006,13 @@ EncryptionZone getEZForPath(final String srcArg)\n \n   void reencryptEncryptionZone(final String zone, final ReencryptAction action,\n       final boolean logRetryCache) throws IOException {\n+    final String operationName = \"reencryptEncryptionZone\";\n     boolean success = false;\n     try {\n       Preconditions.checkNotNull(zone, \"zone is null.\");\n       checkOperation(OperationCategory.WRITE);\n       final FSPermissionChecker pc = dir.getPermissionChecker();\n-      checkSuperuserPrivilege(pc);\n+      checkSuperuserPrivilege(operationName, zone);\n       checkNameNodeSafeMode(\"NameNode in safemode, cannot \" + action\n           + \" re-encryption on zone \" + zone);\n       reencryptEncryptionZoneInt(pc, zone, action, logRetryCache);\n@@ -8024,9 +8027,7 @@ void reencryptEncryptionZone(final String zone, final ReencryptAction action,\n     final String operationName = \"listReencryptionStatus\";\n     boolean success = false;\n     checkOperation(OperationCategory.READ);\n-    final FSPermissionChecker pc = getPermissionChecker();\n-    FSPermissionChecker.setOperationType(operationName);\n-    checkSuperuserPrivilege(pc);\n+    checkSuperuserPrivilege(operationName);\n     readLock();\n     try {\n       checkOperation(OperationCategory.READ);\n@@ -8869,15 +8870,19 @@ private ECTopologyVerifierResult getEcTopologyVerifierResultForEnabledPolicies()\n             Arrays.asList(enabledEcPolicies));\n   }\n \n-  // This method logs operatoinName without super user privilege.\n+  // This method logs operationName without super user privilege.\n   // It should be called without holding FSN lock.\n-  void checkSuperuserPrivilege(String operationName)\n+  void checkSuperuserPrivilege(String operationName, String path)\n       throws IOException {\n-    try {\n-      checkSuperuserPrivilege();\n-    } catch (AccessControlException ace) {\n-      logAuditEvent(false, operationName, null);\n-      throw ace;\n+    if (isPermissionEnabled) {\n+      try {\n+        FSPermissionChecker.setOperationType(operationName);\n+        FSPermissionChecker pc = getPermissionChecker();\n+        pc.checkSuperuserPrivilege(path);\n+      } catch(AccessControlException ace){\n+        logAuditEvent(false, operationName, null);\n+        throw ace;\n+      }\n     }\n   }\n "
  },
  {
    "sha": "927766d47ac296e9610814823d48d1b263e41ba5",
    "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSPermissionChecker.java",
    "status": "modified",
    "additions": 41,
    "deletions": 9,
    "changes": 50,
    "blob_url": "https://github.com/apache/hadoop/blob/87edb72320b1db49a185a65528f771d83d7bb0c1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSPermissionChecker.java",
    "raw_url": "https://github.com/apache/hadoop/raw/87edb72320b1db49a185a65528f771d83d7bb0c1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSPermissionChecker.java",
    "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSPermissionChecker.java?ref=87edb72320b1db49a185a65528f771d83d7bb0c1",
    "patch": "@@ -37,6 +37,7 @@\n import org.apache.hadoop.hdfs.DFSUtil;\n import org.apache.hadoop.hdfs.protocol.UnresolvedPathException;\n import org.apache.hadoop.hdfs.server.namenode.INodeAttributeProvider.AccessControlEnforcer;\n+import org.apache.hadoop.hdfs.server.namenode.INodeAttributeProvider.AuthorizationContext;\n import org.apache.hadoop.hdfs.util.ReadOnlyList;\n import org.apache.hadoop.security.AccessControlException;\n import org.apache.hadoop.security.UserGroupInformation;\n@@ -144,18 +145,50 @@ private AccessControlEnforcer getAccessControlEnforcer() {\n         ? attributeProvider.getExternalAccessControlEnforcer(this) : this;\n   }\n \n+  private AuthorizationContext getAuthorizationContextForSuperUser(\n+      String path) {\n+    String opType = operationType.get();\n+\n+    AuthorizationContext.Builder builder =\n+        new INodeAttributeProvider.AuthorizationContext.Builder();\n+    builder.fsOwner(fsOwner).\n+        supergroup(supergroup).\n+        callerUgi(callerUgi).\n+        operationName(opType).\n+        callerContext(CallerContext.getCurrent());\n+\n+    // Add path to the context builder only if it is not null.\n+    if (path != null && !path.isEmpty()) {\n+      builder.path(path);\n+    }\n+\n+    return builder.build();\n+  }\n+\n   /**\n-   * Verify if the caller has the required permission. This will result into \n-   * an exception if the caller is not allowed to access the resource.\n+   * This method is retained to maintain backward compatibility.\n+   * Please use the new method {@link #checkSuperuserPrivilege(String)} to make\n+   * sure that the external enforcers have the correct context to audit.\n+   *\n+   * @throws AccessControlException if the caller is not a super user.\n    */\n-  public void checkSuperuserPrivilege()\n+  public void checkSuperuserPrivilege() throws AccessControlException {\n+    checkSuperuserPrivilege(null);\n+  }\n+\n+  /**\n+   * Checks if the caller has super user privileges.\n+   * Throws {@link AccessControlException} for non super users.\n+   *\n+   * @param path The resource path for which permission is being requested.\n+   * @throws AccessControlException if the caller is not a super user.\n+   */\n+  public void checkSuperuserPrivilege(String path)\n       throws AccessControlException {\n-    if (!isSuperUser()) {\n-      throw new AccessControlException(\"Access denied for user \" \n-          + getUser() + \". Superuser privilege is required\");\n-    }\n+    getAccessControlEnforcer().checkSuperUserPermissionWithContext(\n+        getAuthorizationContextForSuperUser(path));\n   }\n-  \n+\n   /**\n    * Check whether current user have permissions to access the path.\n    * Traverse is always checked.\n@@ -549,7 +582,6 @@ private boolean hasPermission(INodeAttributes inode, FsAction access) {\n    * - Default entries may be present, but they are ignored during enforcement.\n    *\n    * @param inode INodeAttributes accessed inode\n-   * @param snapshotId int snapshot ID\n    * @param access FsAction requested permission\n    * @param mode FsPermission mode from inode\n    * @param aclFeature AclFeature of inode"
  },
  {
    "sha": "96b56ef3980feb5b3f50b808c9992d2782ba5d08",
    "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeAttributeProvider.java",
    "status": "modified",
    "additions": 26,
    "deletions": 2,
    "changes": 28,
    "blob_url": "https://github.com/apache/hadoop/blob/87edb72320b1db49a185a65528f771d83d7bb0c1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeAttributeProvider.java",
    "raw_url": "https://github.com/apache/hadoop/raw/87edb72320b1db49a185a65528f771d83d7bb0c1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeAttributeProvider.java",
    "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeAttributeProvider.java?ref=87edb72320b1db49a185a65528f771d83d7bb0c1",
    "patch": "@@ -362,7 +362,7 @@ public int hashCode() {\n      * Checks permission on a file system object. Has to throw an Exception\n      * if the filesystem object is not accessible by the calling Ugi.\n      * @param fsOwner Filesystem owner (The Namenode user)\n-     * @param supergroup super user geoup\n+     * @param supergroup super user group\n      * @param callerUgi UserGroupInformation of the caller\n      * @param inodeAttrs Array of INode attributes for each path element in the\n      *                   the path\n@@ -393,7 +393,7 @@ public abstract void checkPermission(String fsOwner, String supergroup,\n \n     /**\n      * Checks permission on a file system object. Has to throw an Exception\n-     * if the filesystem object is not accessessible by the calling Ugi.\n+     * if the filesystem object is not accessible by the calling Ugi.\n      * @param authzContext an {@link AuthorizationContext} object encapsulating\n      *                     the various parameters required to authorize an\n      *                     operation.\n@@ -405,6 +405,30 @@ default void checkPermissionWithContext(AuthorizationContext authzContext)\n           + \"implement the checkPermissionWithContext(AuthorizationContext) \"\n           + \"API.\");\n     }\n+\n+    /**\n+     * Checks if the user belongs to superuser group.\n+     * It throws an AccessControlException if user is not a superuser.\n+     *\n+     * @param authzContext an {@link AuthorizationContext} object encapsulating\n+     *                     the various parameters required to authorize an\n+     *                     operation.\n+     * @throws AccessControlException - if user is not a super user or part\n+     * of the super user group.\n+     */\n+    default void checkSuperUserPermissionWithContext(\n+        AuthorizationContext authzContext)\n+        throws AccessControlException {\n+      UserGroupInformation callerUgi = authzContext.getCallerUgi();\n+      boolean isSuperUser =\n+          callerUgi.getShortUserName().equals(authzContext.getFsOwner()) ||\n+          callerUgi.getGroupsSet().contains(authzContext.getSupergroup());\n+      if (!isSuperUser) {\n+        throw new AccessControlException(\"Access denied for user \" +\n+            callerUgi.getShortUserName() + \". Superuser privilege is \" +\n+            \"required for operation \" + authzContext.getOperationName());\n+      }\n+    }\n   }\n   /**\n    * Initialize the provider. This method is called at NameNode startup"
  },
  {
    "sha": "fba79b038c1e2870b9adfdbb590d52241b0c00bc",
    "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
    "status": "modified",
    "additions": 12,
    "deletions": 12,
    "changes": 24,
    "blob_url": "https://github.com/apache/hadoop/blob/87edb72320b1db49a185a65528f771d83d7bb0c1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
    "raw_url": "https://github.com/apache/hadoop/raw/87edb72320b1db49a185a65528f771d83d7bb0c1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
    "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java?ref=87edb72320b1db49a185a65528f771d83d7bb0c1",
    "patch": "@@ -1828,9 +1828,9 @@ public static void main(String argv[]) throws Exception {\n     }\n   }\n \n-  synchronized void monitorHealth() \n-      throws HealthCheckFailedException, AccessControlException {\n-    namesystem.checkSuperuserPrivilege();\n+  synchronized void monitorHealth() throws IOException {\n+    String operationName = \"monitorHealth\";\n+    namesystem.checkSuperuserPrivilege(operationName);\n     if (!haEnabled) {\n       return; // no-op, if HA is not enabled\n     }\n@@ -1852,9 +1852,9 @@ synchronized void monitorHealth()\n     }\n   }\n   \n-  synchronized void transitionToActive() \n-      throws ServiceFailedException, AccessControlException {\n-    namesystem.checkSuperuserPrivilege();\n+  synchronized void transitionToActive() throws IOException {\n+    String operationName = \"transitionToActive\";\n+    namesystem.checkSuperuserPrivilege(operationName);\n     if (!haEnabled) {\n       throw new ServiceFailedException(\"HA for namenode is not enabled\");\n     }\n@@ -1869,18 +1869,18 @@ synchronized void transitionToActive()\n     state.setState(haContext, ACTIVE_STATE);\n   }\n \n-  synchronized void transitionToStandby()\n-      throws ServiceFailedException, AccessControlException {\n-    namesystem.checkSuperuserPrivilege();\n+  synchronized void transitionToStandby() throws IOException {\n+    String operationName = \"transitionToStandby\";\n+    namesystem.checkSuperuserPrivilege(operationName);\n     if (!haEnabled) {\n       throw new ServiceFailedException(\"HA for namenode is not enabled\");\n     }\n     state.setState(haContext, STANDBY_STATE);\n   }\n \n-  synchronized void transitionToObserver()\n-      throws ServiceFailedException, AccessControlException {\n-    namesystem.checkSuperuserPrivilege();\n+  synchronized void transitionToObserver() throws IOException {\n+    String operationName = \"transitionToObserver\";\n+    namesystem.checkSuperuserPrivilege(operationName);\n     if (!haEnabled) {\n       throw new ServiceFailedException(\"HA for namenode is not enabled\");\n     }"
  },
  {
    "sha": "3df5ad956b5de13cc8798c58e515a398d12ee6b0",
    "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
    "status": "modified",
    "additions": 34,
    "deletions": 17,
    "changes": 51,
    "blob_url": "https://github.com/apache/hadoop/blob/87edb72320b1db49a185a65528f771d83d7bb0c1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
    "raw_url": "https://github.com/apache/hadoop/raw/87edb72320b1db49a185a65528f771d83d7bb0c1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
    "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java?ref=87edb72320b1db49a185a65528f771d83d7bb0c1",
    "patch": "@@ -642,6 +642,7 @@ private static UserGroupInformation getRemoteUser() throws IOException {\n   public BlocksWithLocations getBlocks(DatanodeInfo datanode, long size, long\n       minBlockSize, long timeInterval)\n       throws IOException {\n+    String operationName = \"getBlocks\";\n     if(size <= 0) {\n       throw new IllegalArgumentException(\n           \"Unexpected not positive size: \"+size);\n@@ -651,25 +652,27 @@ public BlocksWithLocations getBlocks(DatanodeInfo datanode, long size, long\n           \"Unexpected not positive size: \"+size);\n     }\n     checkNNStartup();\n-    namesystem.checkSuperuserPrivilege();\n+    namesystem.checkSuperuserPrivilege(operationName);\n     namesystem.checkNameNodeSafeMode(\"Cannot execute getBlocks\");\n     return namesystem.getBlocks(datanode, size, minBlockSize, timeInterval);\n   }\n \n   @Override // NamenodeProtocol\n   public ExportedBlockKeys getBlockKeys() throws IOException {\n+    String operationName = \"getBlockKeys\";\n     checkNNStartup();\n-    namesystem.checkSuperuserPrivilege();\n+    namesystem.checkSuperuserPrivilege(operationName);\n     return namesystem.getBlockManager().getBlockKeys();\n   }\n \n   @Override // NamenodeProtocol\n   public void errorReport(NamenodeRegistration registration,\n                           int errorCode, \n                           String msg) throws IOException {\n+    String operationName = \"errorReport\";\n     checkNNStartup();\n     namesystem.checkOperation(OperationCategory.UNCHECKED);\n-    namesystem.checkSuperuserPrivilege();\n+    namesystem.checkSuperuserPrivilege(operationName);\n     verifyRequest(registration);\n     LOG.info(\"Error report from \" + registration + \": \" + msg);\n     if (errorCode == FATAL) {\n@@ -680,8 +683,9 @@ public void errorReport(NamenodeRegistration registration,\n   @Override // NamenodeProtocol\n   public NamenodeRegistration registerSubordinateNamenode(\n       NamenodeRegistration registration) throws IOException {\n+    String operationName = \"registerSubordinateNamenode\";\n     checkNNStartup();\n-    namesystem.checkSuperuserPrivilege();\n+    namesystem.checkSuperuserPrivilege(operationName);\n     verifyLayoutVersion(registration.getVersion());\n     NamenodeRegistration myRegistration = nn.setRegistration();\n     namesystem.registerBackupNode(registration, myRegistration);\n@@ -691,8 +695,9 @@ public NamenodeRegistration registerSubordinateNamenode(\n   @Override // NamenodeProtocol\n   public NamenodeCommand startCheckpoint(NamenodeRegistration registration)\n       throws IOException {\n+    String operationName = \"startCheckpoint\";\n     checkNNStartup();\n-    namesystem.checkSuperuserPrivilege();\n+    namesystem.checkSuperuserPrivilege(operationName);\n     verifyRequest(registration);\n     if(!nn.isRole(NamenodeRole.NAMENODE))\n       throw new IOException(\"Only an ACTIVE node can invoke startCheckpoint.\");\n@@ -714,8 +719,9 @@ public NamenodeCommand startCheckpoint(NamenodeRegistration registration)\n   @Override // NamenodeProtocol\n   public void endCheckpoint(NamenodeRegistration registration,\n                             CheckpointSignature sig) throws IOException {\n+    String operationName = \"endCheckpoint\";\n     checkNNStartup();\n-    namesystem.checkSuperuserPrivilege();\n+    namesystem.checkSuperuserPrivilege(operationName);\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return; // Return previous response\n@@ -1316,17 +1322,19 @@ public void refreshNodes() throws IOException {\n \n   @Override // NamenodeProtocol\n   public long getTransactionID() throws IOException {\n+    String operationName = \"getTransactionID\";\n     checkNNStartup();\n     namesystem.checkOperation(OperationCategory.UNCHECKED);\n-    namesystem.checkSuperuserPrivilege();\n+    namesystem.checkSuperuserPrivilege(operationName);\n     return namesystem.getFSImage().getCorrectLastAppliedOrWrittenTxId();\n   }\n   \n   @Override // NamenodeProtocol\n   public long getMostRecentCheckpointTxId() throws IOException {\n+    String operationName = \"getMostRecentCheckpointTxId\";\n     checkNNStartup();\n     namesystem.checkOperation(OperationCategory.UNCHECKED);\n-    namesystem.checkSuperuserPrivilege();\n+    namesystem.checkSuperuserPrivilege(operationName);\n     return namesystem.getFSImage().getMostRecentCheckpointTxId();\n   }\n   \n@@ -1339,23 +1347,26 @@ public CheckpointSignature rollEditLog() throws IOException {\n   @Override // NamenodeProtocol\n   public RemoteEditLogManifest getEditLogManifest(long sinceTxId)\n       throws IOException {\n+    String operationName = \"getEditLogManifest\";\n     checkNNStartup();\n     namesystem.checkOperation(OperationCategory.READ);\n-    namesystem.checkSuperuserPrivilege();\n+    namesystem.checkSuperuserPrivilege(operationName);\n     return namesystem.getEditLog().getEditLogManifest(sinceTxId);\n   }\n \n   @Override // NamenodeProtocol\n   public boolean isUpgradeFinalized() throws IOException {\n+    String operationName = \"isUpgradeFinalized\";\n     checkNNStartup();\n-    namesystem.checkSuperuserPrivilege();\n+    namesystem.checkSuperuserPrivilege(operationName);\n     return namesystem.isUpgradeFinalized();\n   }\n \n   @Override // NamenodeProtocol\n   public boolean isRollingUpgrade() throws IOException {\n+    String operationName = \"isRollingUpgrade\";\n     checkNNStartup();\n-    namesystem.checkSuperuserPrivilege();\n+    namesystem.checkSuperuserPrivilege(operationName);\n     return namesystem.isRollingUpgrade();\n   }\n     \n@@ -2339,9 +2350,10 @@ public void checkAccess(String path, FsAction mode) throws IOException {\n \n   @Override // ClientProtocol\n   public long getCurrentEditLogTxid() throws IOException {\n+    String operationName = \"getCurrentEditLogTxid\";\n     checkNNStartup();\n     namesystem.checkOperation(OperationCategory.READ); // only active\n-    namesystem.checkSuperuserPrivilege();\n+    namesystem.checkSuperuserPrivilege(operationName);\n     // if it's not yet open for write, we may be in the process of transitioning\n     // from standby to active and may not yet know what the latest committed\n     // txid is\n@@ -2368,9 +2380,10 @@ private static FSEditLogOp readOp(EditLogInputStream elis)\n \n   @Override // ClientProtocol\n   public EventBatchList getEditsFromTxid(long txid) throws IOException {\n+    String operationName = \"getEditsFromTxid\";\n     checkNNStartup();\n     namesystem.checkOperation(OperationCategory.READ); // only active\n-    namesystem.checkSuperuserPrivilege();\n+    namesystem.checkSuperuserPrivilege(operationName);\n     int maxEventsPerRPC = nn.getConf().getInt(\n         DFSConfigKeys.DFS_NAMENODE_INOTIFY_MAX_EVENTS_PER_RPC_KEY,\n         DFSConfigKeys.DFS_NAMENODE_INOTIFY_MAX_EVENTS_PER_RPC_DEFAULT);\n@@ -2515,8 +2528,9 @@ public ECTopologyVerifierResult getECTopologyResultForPolicies(\n   @Override\n   public AddErasureCodingPolicyResponse[] addErasureCodingPolicies(\n       ErasureCodingPolicy[] policies) throws IOException {\n+    String operationName = \"addErasureCodingPolicies\";\n     checkNNStartup();\n-    namesystem.checkSuperuserPrivilege();\n+    namesystem.checkSuperuserPrivilege(operationName);\n     final CacheEntryWithPayload cacheEntry =\n         RetryCache.waitForCompletion(retryCache, null);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n@@ -2538,8 +2552,9 @@ public ECTopologyVerifierResult getECTopologyResultForPolicies(\n   @Override\n   public void removeErasureCodingPolicy(String ecPolicyName)\n       throws IOException {\n+    String operationName = \"removeErasureCodingPolicy\";\n     checkNNStartup();\n-    namesystem.checkSuperuserPrivilege();\n+    namesystem.checkSuperuserPrivilege(operationName);\n     final CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return;\n@@ -2556,8 +2571,9 @@ public void removeErasureCodingPolicy(String ecPolicyName)\n   @Override // ClientProtocol\n   public void enableErasureCodingPolicy(String ecPolicyName)\n       throws IOException {\n+    String operationName = \"enableErasureCodingPolicy\";\n     checkNNStartup();\n-    namesystem.checkSuperuserPrivilege();\n+    namesystem.checkSuperuserPrivilege(operationName);\n     final CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return;\n@@ -2574,8 +2590,9 @@ public void enableErasureCodingPolicy(String ecPolicyName)\n   @Override // ClientProtocol\n   public void disableErasureCodingPolicy(String ecPolicyName)\n       throws IOException {\n+    String operationName = \"disableErasureCodingPolicy\";\n     checkNNStartup();\n-    namesystem.checkSuperuserPrivilege();\n+    namesystem.checkSuperuserPrivilege(operationName);\n     final CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return;"
  },
  {
    "sha": "9c0b381885cb1253a833fc580f522b67b8ab2431",
    "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
    "status": "modified",
    "additions": 2,
    "deletions": 1,
    "changes": 3,
    "blob_url": "https://github.com/apache/hadoop/blob/87edb72320b1db49a185a65528f771d83d7bb0c1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
    "raw_url": "https://github.com/apache/hadoop/raw/87edb72320b1db49a185a65528f771d83d7bb0c1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
    "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java?ref=87edb72320b1db49a185a65528f771d83d7bb0c1",
    "patch": "@@ -377,9 +377,10 @@ private void printDatanodeReplicaStatus(Block block,\n    */\n   public void fsck() throws AccessControlException {\n     final long startTime = Time.monotonicNow();\n+    String operationName = \"fsck\";\n     try {\n       if(blockIds != null) {\n-        namenode.getNamesystem().checkSuperuserPrivilege();\n+        namenode.getNamesystem().checkSuperuserPrivilege(operationName);\n         StringBuilder sb = new StringBuilder();\n         sb.append(\"FSCK started by \" +\n             UserGroupInformation.getCurrentUser() + \" from \" +"
  }
]
