[
  {
    "sha": "20d1234f4f8e8c810a82a1ed6c81add13c2223a1",
    "filename": "build.gradle",
    "status": "modified",
    "additions": 76,
    "deletions": 0,
    "changes": 76,
    "blob_url": "https://github.com/apache/kafka/blob/94ac96df7bd244e009d1d150dc62a5b1843e89b8/build.gradle",
    "raw_url": "https://github.com/apache/kafka/raw/94ac96df7bd244e009d1d150dc62a5b1843e89b8/build.gradle",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/build.gradle?ref=94ac96df7bd244e009d1d150dc62a5b1843e89b8",
    "patch": "@@ -772,6 +772,7 @@ project(':core') {\n     api project(':clients')\n     api libs.scalaLibrary\n \n+    implementation project(':remote-storage')\n     implementation project(':metadata')\n     implementation project(':raft')\n \n@@ -784,6 +785,7 @@ project(':core') {\n     implementation libs.metrics\n     implementation libs.scalaCollectionCompat\n     implementation libs.scalaJava8Compat\n+\n     // only needed transitively, but set it explicitly to ensure it has the same version as scala-library\n     implementation libs.scalaReflect\n     implementation libs.scalaLogging\n@@ -1318,6 +1320,80 @@ project(':raft') {\n   }\n }\n \n+project(':remote-storage') {\n+  archivesBaseName = \"kafka-remote-storage\"\n+\n+  dependencies {\n+    implementation project(':clients')\n+    implementation libs.slf4jApi\n+    implementation libs.jacksonDatabind\n+\n+    testImplementation project(':clients')\n+    testImplementation project(':clients').sourceSets.test.output\n+    testImplementation libs.junitJupiter\n+    testImplementation libs.mockitoCore\n+\n+    testRuntime libs.slf4jlog4j\n+  }\n+\n+  task createVersionFile(dependsOn: determineCommitId) {\n+    ext.receiptFile = file(\"$buildDir/kafka/$buildVersionFileName\")\n+    outputs.file receiptFile\n+    outputs.upToDateWhen { false }\n+    doLast {\n+      def data = [\n+              commitId: commitId,\n+              version: version,\n+      ]\n+\n+      receiptFile.parentFile.mkdirs()\n+      def content = data.entrySet().collect { \"$it.key=$it.value\" }.sort().join(\"\\n\")\n+      receiptFile.setText(content, \"ISO-8859-1\")\n+    }\n+  }\n+\n+  task processMessages(type:JavaExec) {\n+    main = \"org.apache.kafka.message.MessageGenerator\"\n+    classpath = project(':generator').sourceSets.main.runtimeClasspath\n+    args = [ \"-p\", \" org.apache.kafka.server.log.remote.metadata.storage.generated\",\n+             \"-o\", \"src/generated/java/org/apache/kafka/server/log/remote/metadata/storage/generated\",\n+             \"-i\", \"src/main/resources/message\",\n+             \"-m\", \"MessageDataGenerator\", \"JsonConverterGenerator\"]\n+    inputs.dir(\"src/main/resources/message\")\n+    outputs.dir(\"src/generated/java/org/apache/kafka/server/log/remote/metadata/storage/generated\")\n+  }\n+\n+  sourceSets {\n+    main {\n+      java {\n+        srcDirs = [\"src/generated/java\", \"src/main/java\"]\n+      }\n+    }\n+    test {\n+      java {\n+        srcDirs = [\"src/generated/java\", \"src/test/java\"]\n+      }\n+    }\n+  }\n+\n+  compileJava.dependsOn 'processMessages'\n+\n+  jar {\n+    dependsOn createVersionFile\n+    from(\"$buildDir\") {\n+      include \"kafka/$buildVersionFileName\"\n+    }\n+  }\n+\n+  clean.doFirst {\n+    delete \"$buildDir/kafka/\"\n+  }\n+\n+  javadoc {\n+    enabled = false\n+  }\n+}\n+\n project(':tools') {\n   archivesBaseName = \"kafka-tools\"\n "
  },
  {
    "sha": "de50f7fcf12b5ba5a756c74240a21ba311049228",
    "filename": "checkstyle/import-control.xml",
    "status": "modified",
    "additions": 5,
    "deletions": 0,
    "changes": 5,
    "blob_url": "https://github.com/apache/kafka/blob/94ac96df7bd244e009d1d150dc62a5b1843e89b8/checkstyle/import-control.xml",
    "raw_url": "https://github.com/apache/kafka/raw/94ac96df7bd244e009d1d150dc62a5b1843e89b8/checkstyle/import-control.xml",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/checkstyle/import-control.xml?ref=94ac96df7bd244e009d1d150dc62a5b1843e89b8",
    "patch": "@@ -269,6 +269,11 @@\n     <allow pkg=\"org.slf4j\" />\n     <allow pkg=\"org.apache.kafka.common\" />\n     <allow pkg=\"org.apache.kafka.test\" />\n+    <allow pkg=\"com.fasterxml.jackson\" />\n+\n+    <subpackage name=\"log\">\n+      <allow pkg=\"org.apache.kafka.server.log\" />\n+    </subpackage>\n   </subpackage>\n \n   <subpackage name=\"shell\">"
  },
  {
    "sha": "e1bebe38f3db413de526c56ff63f521813d64bec",
    "filename": "checkstyle/suppressions.xml",
    "status": "modified",
    "additions": 2,
    "deletions": 0,
    "changes": 2,
    "blob_url": "https://github.com/apache/kafka/blob/94ac96df7bd244e009d1d150dc62a5b1843e89b8/checkstyle/suppressions.xml",
    "raw_url": "https://github.com/apache/kafka/raw/94ac96df7bd244e009d1d150dc62a5b1843e89b8/checkstyle/suppressions.xml",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/checkstyle/suppressions.xml?ref=94ac96df7bd244e009d1d150dc62a5b1843e89b8",
    "patch": "@@ -184,6 +184,8 @@\n               files=\"streams[\\\\/]src[\\\\/](generated|generated-test)[\\\\/].+.java$\"/>\n     <suppress checks=\"(NPathComplexity|ClassFanOutComplexity|CyclomaticComplexity|ClassDataAbstractionCoupling|FinalLocalVariable|LocalVariableName|MemberName|ParameterName|MethodLength|JavaNCSS|AvoidStarImport)\"\n               files=\"raft[\\\\/]src[\\\\/](generated|generated-test)[\\\\/].+.java$\"/>\n+    <suppress checks=\"(NPathComplexity|ClassFanOutComplexity|CyclomaticComplexity|ClassDataAbstractionCoupling|FinalLocalVariable|LocalVariableName|MemberName|ParameterName|MethodLength|JavaNCSS|AvoidStarImport)\"\n+              files=\"remote-storage[\\\\/]src[\\\\/](generated|generated-test)[\\\\/].+.java$\"/>\n \n     <suppress checks=\"ImportControl\" files=\"FetchResponseData.java\"/>\n     <suppress checks=\"ImportControl\" files=\"RecordsSerdeTest.java\"/>"
  },
  {
    "sha": "b17c03442b4d13c5caf79ce27458425b19af4aea",
    "filename": "clients/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogSegmentState.java",
    "status": "modified",
    "additions": 26,
    "deletions": 1,
    "changes": 27,
    "blob_url": "https://github.com/apache/kafka/blob/94ac96df7bd244e009d1d150dc62a5b1843e89b8/clients/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogSegmentState.java",
    "raw_url": "https://github.com/apache/kafka/raw/94ac96df7bd244e009d1d150dc62a5b1843e89b8/clients/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogSegmentState.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogSegmentState.java?ref=94ac96df7bd244e009d1d150dc62a5b1843e89b8",
    "patch": "@@ -21,14 +21,16 @@\n import java.util.Arrays;\n import java.util.Collections;\n import java.util.Map;\n+import java.util.Objects;\n import java.util.function.Function;\n import java.util.stream.Collectors;\n \n /**\n  * It indicates the state of the remote log segment. This will be based on the action executed on this\n  * segment by the remote log service implementation.\n  * <p>\n- * It goes through the below state transitions.\n+ * It goes through the below state transitions. Self transition is treated as valid. This allows updating with the\n+ * same state in case of retries and failover.\n  * <p>\n  * <pre>\n  * +---------------------+            +----------------------+\n@@ -87,4 +89,27 @@ public byte id() {\n     public static RemoteLogSegmentState forId(byte id) {\n         return STATE_TYPES.get(id);\n     }\n+\n+    public static boolean isValidTransition(RemoteLogSegmentState srcState, RemoteLogSegmentState targetState) {\n+        Objects.requireNonNull(targetState, \"targetState can not be null\");\n+\n+        if (srcState == null) {\n+            // If the source state is null, check the target state as the initial state viz DELETE_PARTITION_MARKED\n+            // Wanted to keep this logic simple here by taking null for srcState, instead of creating one more state like\n+            // COPY_SEGMENT_NOT_STARTED and have the null check by caller and pass that state.\n+            return targetState == COPY_SEGMENT_STARTED;\n+        } else if (srcState == targetState) {\n+            // Self transition is treated as valid. This is to maintain the idempotency for the state in case of retries\n+            // or failover.\n+            return true;\n+        } else if (srcState == COPY_SEGMENT_STARTED) {\n+            return targetState == COPY_SEGMENT_FINISHED || targetState == DELETE_SEGMENT_STARTED;\n+        } else if (srcState == COPY_SEGMENT_FINISHED) {\n+            return targetState == DELETE_SEGMENT_STARTED;\n+        } else if (srcState == DELETE_SEGMENT_STARTED) {\n+            return targetState == DELETE_SEGMENT_FINISHED;\n+        } else {\n+            return false;\n+        }\n+    }\n }"
  },
  {
    "sha": "56e737bc23f8f9320e7b6d9db4f8511b64f5e503",
    "filename": "clients/src/main/java/org/apache/kafka/server/log/remote/storage/RemotePartitionDeleteState.java",
    "status": "modified",
    "additions": 28,
    "deletions": 5,
    "changes": 33,
    "blob_url": "https://github.com/apache/kafka/blob/94ac96df7bd244e009d1d150dc62a5b1843e89b8/clients/src/main/java/org/apache/kafka/server/log/remote/storage/RemotePartitionDeleteState.java",
    "raw_url": "https://github.com/apache/kafka/raw/94ac96df7bd244e009d1d150dc62a5b1843e89b8/clients/src/main/java/org/apache/kafka/server/log/remote/storage/RemotePartitionDeleteState.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/server/log/remote/storage/RemotePartitionDeleteState.java?ref=94ac96df7bd244e009d1d150dc62a5b1843e89b8",
    "patch": "@@ -21,25 +21,27 @@\n import java.util.Arrays;\n import java.util.Collections;\n import java.util.Map;\n+import java.util.Objects;\n import java.util.function.Function;\n import java.util.stream.Collectors;\n \n /**\n  * It indicates the deletion state of the remote topic partition. This will be based on the action executed on this\n  * partition by the remote log service implementation.\n- * State transitions are mentioned below.\n+ * State transitions are mentioned below. Self transition is treated as valid. This allows updating with the\n+ * same state in case of retries and failover.\n  * <p>\n  * <PRE>\n  * +-------------------------+\n  * |DELETE_PARTITION_MARKED  |\n  * +-----------+-------------+\n- * |\n- * |\n+ *             |\n+ *             |\n  * +-----------v--------------+\n  * |DELETE_PARTITION_STARTED  |\n  * +-----------+--------------+\n- * |\n- * |\n+ *             |\n+ *             |\n  * +-----------v--------------+\n  * |DELETE_PARTITION_FINISHED |\n  * +--------------------------+\n@@ -83,4 +85,25 @@ public static RemotePartitionDeleteState forId(byte id) {\n         return STATE_TYPES.get(id);\n     }\n \n+    public static boolean isValidTransition(RemotePartitionDeleteState srcState,\n+                                            RemotePartitionDeleteState targetState) {\n+        Objects.requireNonNull(targetState, \"targetState can not be null\");\n+\n+        if (srcState == null) {\n+            // If the source state is null, check the target state as the initial state viz DELETE_PARTITION_MARKED\n+            // Wanted to keep this logic simple here by taking null for srcState, instead of creating one more state like\n+            // DELETE_PARTITION_NOT_MARKED and have the null check by caller and pass that state.\n+            return targetState == DELETE_PARTITION_MARKED;\n+        } else if (srcState == targetState) {\n+            // Self transition is treated as valid. This is to maintain the idempotency for the state in case of retries\n+            // or failover.\n+            return true;\n+        } else if (srcState == DELETE_PARTITION_MARKED) {\n+            return targetState == DELETE_PARTITION_STARTED;\n+        } else if (srcState == DELETE_PARTITION_STARTED) {\n+            return targetState == DELETE_PARTITION_FINISHED;\n+        } else {\n+            return false;\n+        }\n+    }\n }\n\\ No newline at end of file"
  },
  {
    "sha": "e5f4f56bcf789068aab58cf42e87a5faceb40489",
    "filename": "remote-storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/RemoteLogMetadataContext.java",
    "status": "added",
    "additions": 83,
    "deletions": 0,
    "changes": 83,
    "blob_url": "https://github.com/apache/kafka/blob/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/RemoteLogMetadataContext.java",
    "raw_url": "https://github.com/apache/kafka/raw/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/RemoteLogMetadataContext.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/remote-storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/RemoteLogMetadataContext.java?ref=94ac96df7bd244e009d1d150dc62a5b1843e89b8",
    "patch": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.server.log.remote.metadata.storage;\n+\n+import java.util.Objects;\n+\n+/**\n+ * The context associated with the record in remote log metadata topic. This contains api-key, version and the\n+ * payload object.\n+ * <br>\n+ * <p>\n+ * For example:\n+ * remote log segment metadata record will have\n+ * <pre>\n+ * <ul>\n+ *     <li>api key as: {@link org.apache.kafka.server.log.remote.metadata.storage.generated.RemoteLogSegmentMetadataRecord#apiKey()}</li>\n+ *     <li>version as: 0 (or current version) , and </li>\n+ *     <li>payload as: {@link org.apache.kafka.server.log.remote.storage.RemoteLogSegmentMetadata}</li>\n+ * </ul>\n+ * </pre>\n+ * <p>\n+ *\n+ * You can read more details in <a href=\"https://cwiki.apache.org/confluence/display/KAFKA/KIP-405%3A+Kafka+Tiered+Storage#KIP405:KafkaTieredStorage-MessageFormat\">KIP-405</a>\n+ */\n+public class RemoteLogMetadataContext {\n+    private final byte apiKey;\n+    private final byte version;\n+    private final Object payload;\n+\n+    public RemoteLogMetadataContext(byte apiKey, byte version, Object payload) {\n+        this.apiKey = apiKey;\n+        this.version = version;\n+        this.payload = payload;\n+    }\n+\n+    public byte apiKey() {\n+        return apiKey;\n+    }\n+\n+    public byte version() {\n+        return version;\n+    }\n+\n+    public Object payload() {\n+        return payload;\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+        if (this == o) return true;\n+        if (o == null || getClass() != o.getClass()) return false;\n+        RemoteLogMetadataContext that = (RemoteLogMetadataContext) o;\n+        return apiKey == that.apiKey && version == that.version && Objects.equals(payload, that.payload);\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+        return Objects.hash(apiKey, version, payload);\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return \"RemoteLogMetadataContext{\" +\n+               \"apiKey=\" + apiKey +\n+               \", version=\" + version +\n+               \", payload=\" + payload +\n+               '}';\n+    }\n+}"
  },
  {
    "sha": "bc20f4666a0c9e6169a3ea0bc201e677dae473b4",
    "filename": "remote-storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serde/RemoteLogMetadataContextSerde.java",
    "status": "added",
    "additions": 115,
    "deletions": 0,
    "changes": 115,
    "blob_url": "https://github.com/apache/kafka/blob/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serde/RemoteLogMetadataContextSerde.java",
    "raw_url": "https://github.com/apache/kafka/raw/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serde/RemoteLogMetadataContextSerde.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/remote-storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serde/RemoteLogMetadataContextSerde.java?ref=94ac96df7bd244e009d1d150dc62a5b1843e89b8",
    "patch": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.server.log.remote.metadata.storage.serde;\n+\n+import org.apache.kafka.server.log.remote.metadata.storage.RemoteLogMetadataContext;\n+import org.apache.kafka.common.protocol.ByteBufferAccessor;\n+import org.apache.kafka.common.protocol.Message;\n+import org.apache.kafka.common.protocol.ObjectSerializationCache;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.common.serialization.Serializer;\n+import org.apache.kafka.server.log.remote.metadata.storage.generated.RemoteLogSegmentMetadataRecord;\n+import org.apache.kafka.server.log.remote.metadata.storage.generated.RemoteLogSegmentMetadataRecordUpdate;\n+import org.apache.kafka.server.log.remote.metadata.storage.generated.RemotePartitionDeleteMetadataRecord;\n+\n+import java.nio.ByteBuffer;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+/**\n+ * Serialization and deserialization for {@link RemoteLogMetadataContext}. This is the root serdes for the messages\n+ * that are stored in internal remote log metadata topic.\n+ */\n+public class RemoteLogMetadataContextSerde implements Serde<RemoteLogMetadataContext> {\n+\n+    public static final byte REMOTE_LOG_SEGMENT_METADATA_API_KEY = (byte) new RemoteLogSegmentMetadataRecord().apiKey();\n+    public static final byte REMOTE_LOG_SEGMENT_METADATA_UPDATE_API_KEY = (byte) new RemoteLogSegmentMetadataRecordUpdate().apiKey();\n+    public static final byte REMOTE_PARTITION_DELETE_API_KEY = (byte) new RemotePartitionDeleteMetadataRecord().apiKey();\n+\n+    private final Map<Byte, RemoteLogMetadataSerdes> keyWithSerde;\n+    private final Deserializer<RemoteLogMetadataContext> rootDeserializer;\n+    private final Serializer<RemoteLogMetadataContext> rootSerializer;\n+\n+    public RemoteLogMetadataContextSerde() {\n+        keyWithSerde = createInternalSerde();\n+        rootSerializer = (topic, data) -> serialize(data);\n+        rootDeserializer = (topic, data) -> deserialize(data);\n+    }\n+\n+    private Map<Byte, RemoteLogMetadataSerdes> createInternalSerde() {\n+        Map<Byte, RemoteLogMetadataSerdes> serdes = new HashMap<>();\n+        serdes.put(REMOTE_LOG_SEGMENT_METADATA_API_KEY, new RemoteLogSegmentMetadataSerde());\n+        serdes.put(REMOTE_LOG_SEGMENT_METADATA_UPDATE_API_KEY, new RemoteLogSegmentMetadataUpdateSerde());\n+        serdes.put(REMOTE_PARTITION_DELETE_API_KEY, new RemotePartitionDeleteMetadataSerde());\n+        return serdes;\n+    }\n+\n+    private byte[] serialize(RemoteLogMetadataContext remoteLogMetadataContext) {\n+        RemoteLogMetadataSerdes serDe = keyWithSerde.get(remoteLogMetadataContext.apiKey());\n+        if (serDe == null) {\n+            throw new IllegalArgumentException(\"Serializer for apikey: \" + remoteLogMetadataContext.apiKey() +\n+                                               \" does not exist.\");\n+        }\n+\n+        @SuppressWarnings(\"unchecked\")\n+        Message message = serDe.serialize(remoteLogMetadataContext.payload());\n+\n+        return transformToBytes(message, remoteLogMetadataContext.apiKey(), remoteLogMetadataContext.version());\n+    }\n+\n+    private RemoteLogMetadataContext deserialize(byte[] data) {\n+        ByteBuffer byteBuffer = ByteBuffer.wrap(data);\n+        byte apiKey = byteBuffer.get();\n+        byte version = byteBuffer.get();\n+        RemoteLogMetadataSerdes serDe = keyWithSerde.get(apiKey);\n+        if (serDe == null) {\n+            throw new IllegalArgumentException(\"Deserializer for apikey: \" + apiKey + \" does not exist.\");\n+        }\n+\n+        Object deserializedObj = serDe.deserialize(version, byteBuffer);\n+        return new RemoteLogMetadataContext(apiKey, version, deserializedObj);\n+    }\n+\n+    private byte[] transformToBytes(Message message, byte apiKey, byte apiVersion) {\n+        ObjectSerializationCache cache = new ObjectSerializationCache();\n+        // add header containing apiKey and apiVersion\n+        // headerSize is 1 byte for apiKey and 1 byte for apiVersion\n+        int headerSize = 1 + 1;\n+        int messageSize = message.size(cache, apiVersion);\n+        ByteBufferAccessor writable = new ByteBufferAccessor(ByteBuffer.allocate(headerSize + messageSize));\n+\n+        //write apiKey and apiVersion\n+        writable.writeByte(apiKey);\n+        writable.writeByte(apiVersion);\n+\n+        //write the message\n+        message.write(writable, cache, apiVersion);\n+\n+        return writable.buffer().array();\n+    }\n+\n+    @Override\n+    public Serializer<RemoteLogMetadataContext> serializer() {\n+        return rootSerializer;\n+    }\n+\n+    @Override\n+    public Deserializer<RemoteLogMetadataContext> deserializer() {\n+        return rootDeserializer;\n+    }\n+}"
  },
  {
    "sha": "ec50fdc357a4e31cf2ec6b1c5269801eca6674f9",
    "filename": "remote-storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serde/RemoteLogMetadataSerdes.java",
    "status": "added",
    "additions": 58,
    "deletions": 0,
    "changes": 58,
    "blob_url": "https://github.com/apache/kafka/blob/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serde/RemoteLogMetadataSerdes.java",
    "raw_url": "https://github.com/apache/kafka/raw/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serde/RemoteLogMetadataSerdes.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/remote-storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serde/RemoteLogMetadataSerdes.java?ref=94ac96df7bd244e009d1d150dc62a5b1843e89b8",
    "patch": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.server.log.remote.metadata.storage.serde;\n+\n+import org.apache.kafka.common.protocol.Message;\n+\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * The default implementation of {@link org.apache.kafka.server.log.remote.storage.RemoteLogMetadataManager} stores all\n+ * the remote log metadata in an internal topic. Those messages can be {@link org.apache.kafka.server.log.remote.storage.RemoteLogSegmentMetadata},\n+ * {@link org.apache.kafka.server.log.remote.storage.RemoteLogSegmentMetadataUpdate}, and {@link org.apache.kafka.server.log.remote.storage.RemotePartitionDeleteMetadata}.\n+ * These messages are written in Kafka's protocol message format as mentioned in\n+ * <a href=\"https://cwiki.apache.org/confluence/display/KAFKA/KIP-405%3A+Kafka+Tiered+Storage#KIP405:KafkaTieredStorage-MessageFormat\">KIP-405</a>\n+ *\n+ * This interface is about serializing and deserializing these messages that are stored in remote log metadata internal\n+ * topic. There are respective implementations for the mentioned message types.\n+ *\n+ * @param <T> metadata type to be serialized/deserialized.\n+ *\n+ * @see RemoteLogSegmentMetadataSerde\n+ * @see RemoteLogSegmentMetadataUpdateSerde\n+ * @see RemotePartitionDeleteMetadataSerde\n+ *\n+ */\n+public interface RemoteLogMetadataSerdes<T> {\n+\n+    /**\n+     * Returns the message serialized for the given {@code metadata} object.\n+     *\n+     * @param metadata object to be serialized.\n+     * @return\n+     */\n+    Message serialize(T metadata);\n+\n+    /**\n+     * Return the deserialized object for the given payload and the version.\n+     * @param version version of the message payload.\n+     * @param data payload.\n+     * @return\n+     */\n+    T deserialize(byte version, ByteBuffer data);\n+\n+}"
  },
  {
    "sha": "50a13d99d48945f0acf0151383bc51c876fe3312",
    "filename": "remote-storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serde/RemoteLogSegmentMetadataSerde.java",
    "status": "added",
    "additions": 87,
    "deletions": 0,
    "changes": 87,
    "blob_url": "https://github.com/apache/kafka/blob/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serde/RemoteLogSegmentMetadataSerde.java",
    "raw_url": "https://github.com/apache/kafka/raw/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serde/RemoteLogSegmentMetadataSerde.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/remote-storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serde/RemoteLogSegmentMetadataSerde.java?ref=94ac96df7bd244e009d1d150dc62a5b1843e89b8",
    "patch": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.server.log.remote.metadata.storage.serde;\n+\n+import org.apache.kafka.common.TopicIdPartition;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.protocol.ByteBufferAccessor;\n+import org.apache.kafka.common.protocol.Message;\n+import org.apache.kafka.server.log.remote.metadata.storage.generated.RemoteLogSegmentMetadataRecord;\n+import org.apache.kafka.server.log.remote.storage.RemoteLogSegmentId;\n+import org.apache.kafka.server.log.remote.storage.RemoteLogSegmentMetadata;\n+import org.apache.kafka.server.log.remote.storage.RemoteLogSegmentMetadataUpdate;\n+import org.apache.kafka.server.log.remote.storage.RemoteLogSegmentState;\n+\n+import java.nio.ByteBuffer;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class RemoteLogSegmentMetadataSerde implements RemoteLogMetadataSerdes<RemoteLogSegmentMetadata> {\n+\n+    public Message serialize(RemoteLogSegmentMetadata data) {\n+        RemoteLogSegmentMetadataRecord record = new RemoteLogSegmentMetadataRecord()\n+                .setRemoteLogSegmentId(\n+                        new RemoteLogSegmentMetadataRecord.RemoteLogSegmentIdEntry()\n+                                .setTopicIdPartition(new RemoteLogSegmentMetadataRecord.TopicIdPartitionEntry()\n+                                        .setId(data.remoteLogSegmentId().topicIdPartition().topicId())\n+                                        .setName(data.remoteLogSegmentId().topicIdPartition().topicPartition()\n+                                                .topic())\n+                                        .setPartition(data.remoteLogSegmentId().topicIdPartition().topicPartition()\n+                                                .partition()))\n+                                .setId(data.remoteLogSegmentId().id()))\n+                .setStartOffset(data.startOffset())\n+                .setEndOffset(data.endOffset())\n+                .setBrokerId(data.brokerId())\n+                .setEventTimestamp(data.eventTimestamp())\n+                .setMaxTimestamp(data.maxTimestamp())\n+                .setSegmentSizeInBytes(data.segmentSizeInBytes())\n+                .setSegmentLeaderEpochs(data.segmentLeaderEpochs().entrySet().stream()\n+                        .map(entry -> new RemoteLogSegmentMetadataRecord.SegmentLeaderEpochEntry()\n+                                .setLeaderEpoch(entry.getKey())\n+                                .setOffset(entry.getValue())).collect(Collectors.toList()))\n+                .setRemoteLogSegmentState(data.state().id());\n+\n+        return record;\n+    }\n+\n+    @Override\n+    public RemoteLogSegmentMetadata deserialize(byte version, ByteBuffer byteBuffer) {\n+        RemoteLogSegmentMetadataRecord record = new RemoteLogSegmentMetadataRecord(\n+                new ByteBufferAccessor(byteBuffer), version);\n+\n+        RemoteLogSegmentId remoteLogSegmentId = buildRemoteLogSegmentId(record.remoteLogSegmentId());\n+        Map<Integer, Long> segmentLeaderEpochs = new HashMap<>();\n+        for (RemoteLogSegmentMetadataRecord.SegmentLeaderEpochEntry segmentLeaderEpoch : record.segmentLeaderEpochs()) {\n+            segmentLeaderEpochs.put(segmentLeaderEpoch.leaderEpoch(), segmentLeaderEpoch.offset());\n+        }\n+        RemoteLogSegmentMetadata remoteLogSegmentMetadata = new RemoteLogSegmentMetadata(remoteLogSegmentId,\n+                record.startOffset(), record.endOffset(), record.maxTimestamp(), record.brokerId(),\n+                record.eventTimestamp(), record.segmentSizeInBytes(), segmentLeaderEpochs);\n+        RemoteLogSegmentMetadataUpdate rlsmUpdate = new RemoteLogSegmentMetadataUpdate(remoteLogSegmentId,\n+                record.eventTimestamp(), RemoteLogSegmentState.forId(record.remoteLogSegmentState()),\n+                record.brokerId());\n+\n+        return remoteLogSegmentMetadata.createRemoteLogSegmentWithUpdates(rlsmUpdate);\n+    }\n+\n+    private RemoteLogSegmentId buildRemoteLogSegmentId(RemoteLogSegmentMetadataRecord.RemoteLogSegmentIdEntry entry) {\n+        TopicIdPartition topicIdPartition = new TopicIdPartition(entry.topicIdPartition().id(),\n+                new TopicPartition(entry.topicIdPartition().name(), entry.topicIdPartition().partition()));\n+        return new RemoteLogSegmentId(topicIdPartition, entry.id());\n+    }\n+}"
  },
  {
    "sha": "0bc12a9024728ede6a8221fd1365061698226a0b",
    "filename": "remote-storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serde/RemoteLogSegmentMetadataUpdateSerde.java",
    "status": "added",
    "additions": 56,
    "deletions": 0,
    "changes": 56,
    "blob_url": "https://github.com/apache/kafka/blob/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serde/RemoteLogSegmentMetadataUpdateSerde.java",
    "raw_url": "https://github.com/apache/kafka/raw/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serde/RemoteLogSegmentMetadataUpdateSerde.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/remote-storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serde/RemoteLogSegmentMetadataUpdateSerde.java?ref=94ac96df7bd244e009d1d150dc62a5b1843e89b8",
    "patch": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.server.log.remote.metadata.storage.serde;\n+\n+import org.apache.kafka.common.TopicIdPartition;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.protocol.ByteBufferAccessor;\n+import org.apache.kafka.common.protocol.Message;\n+import org.apache.kafka.server.log.remote.metadata.storage.generated.RemoteLogSegmentMetadataRecordUpdate;\n+import org.apache.kafka.server.log.remote.storage.RemoteLogSegmentId;\n+import org.apache.kafka.server.log.remote.storage.RemoteLogSegmentMetadataUpdate;\n+import org.apache.kafka.server.log.remote.storage.RemoteLogSegmentState;\n+\n+import java.nio.ByteBuffer;\n+\n+public class RemoteLogSegmentMetadataUpdateSerde implements RemoteLogMetadataSerdes<RemoteLogSegmentMetadataUpdate> {\n+\n+    public Message serialize(RemoteLogSegmentMetadataUpdate data) {\n+        RemoteLogSegmentMetadataRecordUpdate recordUpdate = new RemoteLogSegmentMetadataRecordUpdate()\n+                .setRemoteLogSegmentId(new RemoteLogSegmentMetadataRecordUpdate.RemoteLogSegmentIdEntry()\n+                        .setId(data.remoteLogSegmentId().id())\n+                        .setTopicIdPartition(new RemoteLogSegmentMetadataRecordUpdate.TopicIdPartitionEntry()\n+                                .setName(data.remoteLogSegmentId().topicIdPartition().topicPartition().topic())\n+                                .setPartition(\n+                                        data.remoteLogSegmentId().topicIdPartition().topicPartition().partition())\n+                                .setId(data.remoteLogSegmentId().topicIdPartition().topicId())))\n+                .setEventTimestamp(data.eventTimestamp())\n+                .setRemoteLogSegmentState(data.state().id());\n+        return recordUpdate;\n+    }\n+\n+    public RemoteLogSegmentMetadataUpdate deserialize(byte version, ByteBuffer byteBuffer) {\n+        RemoteLogSegmentMetadataRecordUpdate record = new RemoteLogSegmentMetadataRecordUpdate(\n+                new ByteBufferAccessor(byteBuffer), version);\n+        RemoteLogSegmentMetadataRecordUpdate.RemoteLogSegmentIdEntry entry = record.remoteLogSegmentId();\n+        TopicIdPartition topicIdPartition = new TopicIdPartition(entry.topicIdPartition().id(),\n+                new TopicPartition(entry.topicIdPartition().name(), entry.topicIdPartition().partition()));\n+\n+        return new RemoteLogSegmentMetadataUpdate(new RemoteLogSegmentId(topicIdPartition, entry.id()),\n+                record.eventTimestamp(), RemoteLogSegmentState.forId(record.remoteLogSegmentState()), record.brokerId());\n+    }\n+}"
  },
  {
    "sha": "e65b32c15c2e1abc2e0988ad62fe0651817f01e8",
    "filename": "remote-storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serde/RemotePartitionDeleteMetadataSerde.java",
    "status": "added",
    "additions": 54,
    "deletions": 0,
    "changes": 54,
    "blob_url": "https://github.com/apache/kafka/blob/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serde/RemotePartitionDeleteMetadataSerde.java",
    "raw_url": "https://github.com/apache/kafka/raw/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serde/RemotePartitionDeleteMetadataSerde.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/remote-storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/serde/RemotePartitionDeleteMetadataSerde.java?ref=94ac96df7bd244e009d1d150dc62a5b1843e89b8",
    "patch": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.server.log.remote.metadata.storage.serde;\n+\n+import org.apache.kafka.common.TopicIdPartition;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.protocol.ByteBufferAccessor;\n+import org.apache.kafka.common.protocol.Message;\n+import org.apache.kafka.server.log.remote.metadata.storage.generated.RemotePartitionDeleteMetadataRecord;\n+import org.apache.kafka.server.log.remote.storage.RemotePartitionDeleteMetadata;\n+import org.apache.kafka.server.log.remote.storage.RemotePartitionDeleteState;\n+\n+import java.nio.ByteBuffer;\n+\n+public final class RemotePartitionDeleteMetadataSerde implements RemoteLogMetadataSerdes<RemotePartitionDeleteMetadata> {\n+\n+    @Override\n+    public Message serialize(RemotePartitionDeleteMetadata data) {\n+        RemotePartitionDeleteMetadataRecord record = new RemotePartitionDeleteMetadataRecord()\n+                .setTopicIdPartition(new RemotePartitionDeleteMetadataRecord.TopicIdPartitionEntry()\n+                        .setName(data.topicIdPartition().topicPartition().topic())\n+                        .setPartition(data.topicIdPartition().topicPartition().partition())\n+                        .setId(data.topicIdPartition().topicId()))\n+                .setEventTimestamp(data.eventTimestamp())\n+                .setBrokerId(data.brokerId())\n+                .setRemotePartitionDeleteState(data.state().id());\n+        return record;\n+    }\n+\n+    public RemotePartitionDeleteMetadata deserialize(byte version, ByteBuffer byteBuffer) {\n+        RemotePartitionDeleteMetadataRecord record = new RemotePartitionDeleteMetadataRecord(\n+                new ByteBufferAccessor(byteBuffer), version);\n+        TopicIdPartition topicIdPartition = new TopicIdPartition(record.topicIdPartition().id(),\n+                new TopicPartition(record.topicIdPartition().name(), record.topicIdPartition().partition()));\n+\n+        return new RemotePartitionDeleteMetadata(topicIdPartition,\n+                RemotePartitionDeleteState.forId(record.remotePartitionDeleteState()),\n+                record.eventTimestamp(), record.brokerId());\n+    }\n+}"
  },
  {
    "sha": "e9af053f896c303b3176edade65823bc9531b8b0",
    "filename": "remote-storage/src/main/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteLogMetadataManager.java",
    "status": "added",
    "additions": 185,
    "deletions": 0,
    "changes": 185,
    "blob_url": "https://github.com/apache/kafka/blob/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/main/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteLogMetadataManager.java",
    "raw_url": "https://github.com/apache/kafka/raw/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/main/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteLogMetadataManager.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/remote-storage/src/main/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteLogMetadataManager.java?ref=94ac96df7bd244e009d1d150dc62a5b1843e89b8",
    "patch": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.server.log.remote.storage;\n+\n+import org.apache.kafka.common.TopicIdPartition;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+\n+/**\n+ * This class is an implementation of {@link RemoteLogMetadataManager} backed by inmemory store.\n+ */\n+public class InmemoryRemoteLogMetadataManager implements RemoteLogMetadataManager {\n+    private static final Logger log = LoggerFactory.getLogger(InmemoryRemoteLogMetadataManager.class);\n+\n+    private final ConcurrentMap<TopicIdPartition, RemotePartitionDeleteMetadata> idToPartitionDeleteMetadata =\n+            new ConcurrentHashMap<>();\n+\n+    private final ConcurrentMap<TopicIdPartition, RemoteLogMetadataCache> partitionToRemoteLogMetadataCache =\n+            new ConcurrentHashMap<>();\n+\n+    @Override\n+    public void addRemoteLogSegmentMetadata(RemoteLogSegmentMetadata remoteLogSegmentMetadata)\n+            throws RemoteStorageException {\n+        log.debug(\"Adding remote log segment : [{}]\", remoteLogSegmentMetadata);\n+        Objects.requireNonNull(remoteLogSegmentMetadata, \"remoteLogSegmentMetadata can not be null\");\n+\n+        // this method is allowed only to add remote log segment with the initial state(which is RemoteLogSegmentState.COPY_SEGMENT_STARTED)\n+        // but not to update the existing remote log segment metadata.\n+        if (remoteLogSegmentMetadata.state() != RemoteLogSegmentState.COPY_SEGMENT_STARTED) {\n+            throw new IllegalArgumentException(\"Given remoteLogSegmentMetadata should have state as \" + RemoteLogSegmentState.COPY_SEGMENT_STARTED\n+                    + \" but it contains state as: \" + remoteLogSegmentMetadata.state());\n+        }\n+\n+        RemoteLogSegmentId remoteLogSegmentId = remoteLogSegmentMetadata.remoteLogSegmentId();\n+\n+        RemoteLogMetadataCache remoteLogMetadataCache = partitionToRemoteLogMetadataCache\n+                .computeIfAbsent(remoteLogSegmentId.topicIdPartition(), id -> new RemoteLogMetadataCache());\n+\n+        remoteLogMetadataCache.addToInProgress(remoteLogSegmentMetadata);\n+    }\n+\n+    @Override\n+    public void updateRemoteLogSegmentMetadata(RemoteLogSegmentMetadataUpdate metadataUpdate)\n+            throws RemoteStorageException {\n+        log.debug(\"Updating remote log segment: [{}]\", metadataUpdate);\n+        Objects.requireNonNull(metadataUpdate, \"metadataUpdate can not be null\");\n+\n+        RemoteLogSegmentState targetState = metadataUpdate.state();\n+        // Callers should use putRemoteLogSegmentMetadata to add RemoteLogSegmentMetadata with state as\n+        // RemoteLogSegmentState.COPY_SEGMENT_STARTED.\n+        if (targetState == RemoteLogSegmentState.COPY_SEGMENT_STARTED) {\n+            throw new IllegalArgumentException(\"Given remoteLogSegmentMetadata should not have the state as: \"\n+                                               + RemoteLogSegmentState.COPY_SEGMENT_STARTED);\n+        }\n+\n+        RemoteLogSegmentId remoteLogSegmentId = metadataUpdate.remoteLogSegmentId();\n+        TopicIdPartition topicIdPartition = remoteLogSegmentId.topicIdPartition();\n+        RemoteLogMetadataCache remoteLogMetadataCache = partitionToRemoteLogMetadataCache.get(topicIdPartition);\n+        if (remoteLogMetadataCache == null) {\n+            throw new RemoteResourceNotFoundException(\"No metadata found for partition: \" + topicIdPartition);\n+        }\n+\n+        remoteLogMetadataCache.updateRemoteLogSegmentMetadata(metadataUpdate);\n+    }\n+\n+    @Override\n+    public Optional<RemoteLogSegmentMetadata> remoteLogSegmentMetadata(TopicIdPartition topicIdPartition,\n+                                                                       long offset,\n+                                                                       int epochForOffset)\n+            throws RemoteStorageException {\n+        Objects.requireNonNull(topicIdPartition, \"topicIdPartition can not be null\");\n+\n+        RemoteLogMetadataCache remoteLogMetadataCache = partitionToRemoteLogMetadataCache.get(topicIdPartition);\n+        if (remoteLogMetadataCache == null) {\n+            throw new RemoteResourceNotFoundException(\"No metadata found for the given partition: \" + topicIdPartition);\n+        }\n+\n+        return remoteLogMetadataCache.remoteLogSegmentMetadata(epochForOffset, offset);\n+    }\n+\n+    @Override\n+    public Optional<Long> highestLogOffset(TopicIdPartition topicIdPartition,\n+                                           int leaderEpoch) throws RemoteStorageException {\n+        Objects.requireNonNull(topicIdPartition, \"topicIdPartition can not be null\");\n+\n+        RemoteLogMetadataCache remoteLogMetadataCache = partitionToRemoteLogMetadataCache.get(topicIdPartition);\n+        if (remoteLogMetadataCache == null) {\n+            throw new RemoteResourceNotFoundException(\"No metadata found for partition: \" + topicIdPartition);\n+        }\n+\n+        return remoteLogMetadataCache.highestLogOffset(leaderEpoch);    \n+    }\n+\n+    @Override\n+    public void putRemotePartitionDeleteMetadata(RemotePartitionDeleteMetadata remotePartitionDeleteMetadata)\n+            throws RemoteStorageException {\n+        log.debug(\"Adding delete state with: [{}]\", remotePartitionDeleteMetadata);\n+        Objects.requireNonNull(remotePartitionDeleteMetadata, \"remotePartitionDeleteMetadata can not be null\");\n+\n+        TopicIdPartition topicIdPartition = remotePartitionDeleteMetadata.topicIdPartition();\n+\n+        RemotePartitionDeleteState targetState = remotePartitionDeleteMetadata.state();\n+        RemotePartitionDeleteMetadata existingMetadata = idToPartitionDeleteMetadata.get(topicIdPartition);\n+        RemotePartitionDeleteState existingState = existingMetadata != null ? existingMetadata.state() : null;\n+        if (!RemotePartitionDeleteState.isValidTransition(existingState, targetState)) {\n+            throw new IllegalStateException(\"Current state: \" + existingState + \", target state: \" + targetState);\n+        }\n+\n+        idToPartitionDeleteMetadata.put(topicIdPartition, remotePartitionDeleteMetadata);\n+\n+        if (targetState == RemotePartitionDeleteState.DELETE_PARTITION_FINISHED) {\n+            // remove the association for the partition.\n+            partitionToRemoteLogMetadataCache.remove(topicIdPartition);\n+            idToPartitionDeleteMetadata.remove(topicIdPartition);\n+        }\n+    }\n+\n+    @Override\n+    public Iterator<RemoteLogSegmentMetadata> listRemoteLogSegments(TopicIdPartition topicIdPartition)\n+            throws RemoteStorageException {\n+        RemoteLogMetadataCache remoteLogMetadataCache = partitionToRemoteLogMetadataCache.get(topicIdPartition);\n+        if (remoteLogMetadataCache == null) {\n+            throw new RemoteResourceNotFoundException(\"No metadata found for partition: \" + topicIdPartition);\n+        }\n+\n+        return remoteLogMetadataCache.listAllRemoteLogSegments();\n+    }\n+\n+    @Override\n+    public Iterator<RemoteLogSegmentMetadata> listRemoteLogSegments(TopicIdPartition topicIdPartition, int leaderEpoch)\n+            throws RemoteStorageException {\n+        Objects.requireNonNull(topicIdPartition, \"topicIdPartition can not be null\");\n+\n+        RemoteLogMetadataCache remoteLogMetadataCache = partitionToRemoteLogMetadataCache.get(topicIdPartition);\n+        if (remoteLogMetadataCache == null) {\n+            throw new RemoteResourceNotFoundException(\"No metadata found for partition: \" + topicIdPartition);\n+        }\n+\n+        return remoteLogMetadataCache.listRemoteLogSegments(leaderEpoch);\n+    }\n+\n+    @Override\n+    public void onPartitionLeadershipChanges(Set<TopicIdPartition> leaderPartitions,\n+                                             Set<TopicIdPartition> followerPartitions) {\n+        // It is not applicable for this implementation. This will track the segments that are added/updated as part of\n+        // this instance. It does not depend upon any leader or follower transitions.\n+    }\n+\n+    @Override\n+    public void onStopPartitions(Set<TopicIdPartition> partitions) {\n+        // It is not applicable for this implementation. This will track the segments that are added/updated as part of\n+        // this instance. It does not depend upon stopped partitions.\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+    }\n+\n+    @Override\n+    public void configure(Map<String, ?> configs) {\n+    }\n+}"
  },
  {
    "sha": "0f63f981282883ff919eef5bf6581349a41f6436",
    "filename": "remote-storage/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogMetadataCache.java",
    "status": "added",
    "additions": 172,
    "deletions": 0,
    "changes": 172,
    "blob_url": "https://github.com/apache/kafka/blob/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogMetadataCache.java",
    "raw_url": "https://github.com/apache/kafka/raw/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogMetadataCache.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/remote-storage/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogMetadataCache.java?ref=94ac96df7bd244e009d1d150dc62a5b1843e89b8",
    "patch": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.server.log.remote.storage;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.NavigableMap;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * This class provides an inmemory cache of remote log segment metadata. This maintains the lineage of segments\n+ * with respect to epoch evolution. It also keeps track of segments which are not considered to be copied to remote\n+ * storage.\n+ */\n+public class RemoteLogMetadataCache {\n+    private static final Logger log = LoggerFactory.getLogger(RemoteLogMetadataCache.class);\n+\n+    private final ConcurrentMap<RemoteLogSegmentId, RemoteLogSegmentMetadata> idToSegmentMetadata\n+            = new ConcurrentHashMap<>();\n+\n+    // It keeps the segments which are not yet reached to COPY_SEGMENT_FINISHED state.\n+    private final Set<RemoteLogSegmentId> remoteLogSegmentIdInProgress = new HashSet<>();\n+\n+    // It will have all the segments except with state as COPY_SEGMENT_STARTED.\n+    private final ConcurrentMap<Integer, NavigableMap<Long, RemoteLogSegmentId>> leaderEpochToOffsetToId\n+            = new ConcurrentHashMap<>();\n+\n+    private void addRemoteLogSegmentMetadata(RemoteLogSegmentMetadata remoteLogSegmentMetadata) {\n+        log.debug(\"Adding remote log segment metadata: [{}]\", remoteLogSegmentMetadata);\n+        idToSegmentMetadata.put(remoteLogSegmentMetadata.remoteLogSegmentId(), remoteLogSegmentMetadata);\n+        Map<Integer, Long> leaderEpochToOffset = remoteLogSegmentMetadata.segmentLeaderEpochs();\n+        for (Map.Entry<Integer, Long> entry : leaderEpochToOffset.entrySet()) {\n+            leaderEpochToOffsetToId.computeIfAbsent(entry.getKey(), k -> new ConcurrentSkipListMap<>())\n+                    .put(entry.getValue(), remoteLogSegmentMetadata.remoteLogSegmentId());\n+        }\n+    }\n+\n+    public Optional<RemoteLogSegmentMetadata> remoteLogSegmentMetadata(int leaderEpoch, long offset) {\n+        NavigableMap<Long, RemoteLogSegmentId> offsetToId = leaderEpochToOffsetToId.get(leaderEpoch);\n+        if (offsetToId == null || offsetToId.isEmpty()) {\n+            return Optional.empty();\n+        }\n+\n+        // look for floor entry as the given offset may exist in this entry.\n+        Map.Entry<Long, RemoteLogSegmentId> entry = offsetToId.floorEntry(offset);\n+        if (entry == null) {\n+            // if the offset is lower than the minimum offset available in metadata then return empty.\n+            return Optional.empty();\n+        }\n+\n+        RemoteLogSegmentMetadata metadata = idToSegmentMetadata.get(entry.getValue());\n+        // check whether the given offset with leaderEpoch exists in this segment.\n+        // check for epoch's offset boundaries with in this segment.\n+        //      1. get the next epoch's start offset -1 if exists\n+        //      2. if no next epoch exists, then segment end offset can be considered as epoch's relative end offset.\n+        Map.Entry<Integer, Long> nextEntry = metadata.segmentLeaderEpochs()\n+                .higherEntry(leaderEpoch);\n+        long epochEndOffset = (nextEntry != null) ? nextEntry.getValue() - 1 : metadata.endOffset();\n+\n+        // seek offset should be <= epoch's end offset.\n+        return (offset > epochEndOffset) ? Optional.empty() : Optional.of(metadata);\n+    }\n+\n+    public void updateRemoteLogSegmentMetadata(RemoteLogSegmentMetadataUpdate metadataUpdate)\n+            throws RemoteResourceNotFoundException {\n+        log.debug(\"Updating remote log segment metadata: [{}]\", metadataUpdate);\n+        RemoteLogSegmentId remoteLogSegmentId = metadataUpdate.remoteLogSegmentId();\n+        RemoteLogSegmentMetadata existingMetadata = idToSegmentMetadata.get(remoteLogSegmentId);\n+        if (existingMetadata == null) {\n+            throw new RemoteResourceNotFoundException(\"No remote log segment metadata found for : \"\n+                                                      + remoteLogSegmentId);\n+        }\n+\n+        RemoteLogSegmentState targetState = metadataUpdate.state();\n+        RemoteLogSegmentState existingState = existingMetadata.state();\n+        if (!RemoteLogSegmentState.isValidTransition(existingMetadata.state(), targetState)) {\n+            throw new IllegalStateException(\"Current state: \" + existingState + \", target state: \" + targetState);\n+        }\n+\n+        RemoteLogSegmentMetadata updatedMetadata = existingMetadata.createRemoteLogSegmentWithUpdates(metadataUpdate);\n+        idToSegmentMetadata.put(remoteLogSegmentId, updatedMetadata);\n+        if (targetState != RemoteLogSegmentState.COPY_SEGMENT_STARTED) {\n+            remoteLogSegmentIdInProgress.remove(remoteLogSegmentId);\n+            addRemoteLogSegmentMetadata(updatedMetadata);\n+        }\n+\n+        if (targetState == RemoteLogSegmentState.DELETE_SEGMENT_FINISHED) {\n+            log.debug(\"Cleaning up the state for : [{}]\", metadataUpdate);\n+            // remove this entry when the state is moved to delete_segment_finished\n+            Map<Integer, Long> leaderEpochs = existingMetadata.segmentLeaderEpochs();\n+            for (Map.Entry<Integer, Long> entry : leaderEpochs.entrySet()) {\n+                NavigableMap<Long, RemoteLogSegmentId> offsetToIds = leaderEpochToOffsetToId.get(entry.getKey());\n+                // remove the mappings where this segment is deleted.\n+                offsetToIds.values().remove(remoteLogSegmentId);\n+            }\n+\n+            // remove the segment-id mapping.\n+            idToSegmentMetadata.remove(remoteLogSegmentId);\n+        }\n+    }\n+\n+    public Iterator<RemoteLogSegmentMetadata> listAllRemoteLogSegments() {\n+        ArrayList<RemoteLogSegmentMetadata> list = new ArrayList<>(idToSegmentMetadata.values());\n+        list.addAll(remoteLogSegmentIdInProgress.stream().map(id -> idToSegmentMetadata.get(id))\n+                .collect(Collectors.toList()));\n+        list.sort(Comparator.comparingLong(RemoteLogSegmentMetadata::startOffset));\n+        return list.iterator();\n+    }\n+\n+    public Iterator<RemoteLogSegmentMetadata> listRemoteLogSegments(int leaderEpoch) {\n+        NavigableMap<Long, RemoteLogSegmentId> map = leaderEpochToOffsetToId.get(leaderEpoch);\n+        return map != null ? map.values().stream().map(id -> idToSegmentMetadata.get(id)).iterator()\n+                           : Collections.emptyIterator();\n+    }\n+\n+    public Optional<Long> highestLogOffset(int leaderEpoch) {\n+        NavigableMap<Long, RemoteLogSegmentId> offsetToSegmentId = leaderEpochToOffsetToId.get(leaderEpoch);\n+        if (offsetToSegmentId == null) {\n+            return Optional.empty();\n+        }\n+\n+        long max = 0L;\n+        for (RemoteLogSegmentId id : offsetToSegmentId.values()) {\n+            RemoteLogSegmentMetadata metadata = idToSegmentMetadata.get(id);\n+            Map.Entry<Integer, Long> nextEntry = metadata.segmentLeaderEpochs().higherEntry(leaderEpoch);\n+            // If there is an higher entry than the given leader epoch, end-offset of the given leader epoch is,\n+            // (next leader epoch's start-offset) -1.\n+            long nextVal = nextEntry != null ? nextEntry.getValue() - 1 : metadata.endOffset();\n+            max = Math.max(max, nextVal);\n+        }\n+\n+        return Optional.of(max);\n+    }\n+\n+    /**\n+     * This will be added to copy_in_progress metadata list. This will be removed from that list once it is moved to the\n+     * next state which can be COPY_SEGMENT_FINISHED or DELETE_SEGMENT_STARTED.\n+     *\n+     * @param remoteLogSegmentMetadata\n+     */\n+    public void addToInProgress(RemoteLogSegmentMetadata remoteLogSegmentMetadata) {\n+        log.debug(\"Adding to inprogress state: [{}]\", remoteLogSegmentMetadata);\n+        idToSegmentMetadata.put(remoteLogSegmentMetadata.remoteLogSegmentId(), remoteLogSegmentMetadata);\n+        remoteLogSegmentIdInProgress.add(remoteLogSegmentMetadata.remoteLogSegmentId());\n+    }\n+}"
  },
  {
    "sha": "146de7abe4442bc2c617f704f428c970c9b25e36",
    "filename": "remote-storage/src/main/resources/META-INF/services/org.apache.kafka.common.config.provider.ConfigProvider",
    "status": "added",
    "additions": 16,
    "deletions": 0,
    "changes": 16,
    "blob_url": "https://github.com/apache/kafka/blob/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/main/resources/META-INF/services/org.apache.kafka.common.config.provider.ConfigProvider",
    "raw_url": "https://github.com/apache/kafka/raw/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/main/resources/META-INF/services/org.apache.kafka.common.config.provider.ConfigProvider",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/remote-storage/src/main/resources/META-INF/services/org.apache.kafka.common.config.provider.ConfigProvider?ref=94ac96df7bd244e009d1d150dc62a5b1843e89b8",
    "patch": "@@ -0,0 +1,16 @@\n+ # Licensed to the Apache Software Foundation (ASF) under one or more\n+ # contributor license agreements. See the NOTICE file distributed with\n+ # this work for additional information regarding copyright ownership.\n+ # The ASF licenses this file to You under the Apache License, Version 2.0\n+ # (the \"License\"); you may not use this file except in compliance with\n+ # the License. You may obtain a copy of the License at\n+ #\n+ #    http://www.apache.org/licenses/LICENSE-2.0\n+ #\n+ # Unless required by applicable law or agreed to in writing, software\n+ # distributed under the License is distributed on an \"AS IS\" BASIS,\n+ # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ # See the License for the specific language governing permissions and\n+ # limitations under the License.\n+\n+org.apache.kafka.common.config.provider.FileConfigProvider"
  },
  {
    "sha": "1c297547e98d3ec843fc751ba884fe2244ac149b",
    "filename": "remote-storage/src/main/resources/message/RemoteLogSegmentMetadata.json",
    "status": "added",
    "additions": 126,
    "deletions": 0,
    "changes": 126,
    "blob_url": "https://github.com/apache/kafka/blob/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/main/resources/message/RemoteLogSegmentMetadata.json",
    "raw_url": "https://github.com/apache/kafka/raw/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/main/resources/message/RemoteLogSegmentMetadata.json",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/remote-storage/src/main/resources/message/RemoteLogSegmentMetadata.json?ref=94ac96df7bd244e009d1d150dc62a5b1843e89b8",
    "patch": "@@ -0,0 +1,126 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 0,\n+  \"type\": \"data\",\n+  \"name\": \"RemoteLogSegmentMetadataRecord\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"none\",\n+  \"fields\": [\n+    {\n+      \"name\": \"RemoteLogSegmentId\",\n+      \"type\": \"RemoteLogSegmentIdEntry\",\n+      \"versions\": \"0+\",\n+      \"about\": \"Unique representation of the remote log segment\",\n+      \"fields\": [\n+        {\n+          \"name\": \"TopicIdPartition\",\n+          \"type\": \"TopicIdPartitionEntry\",\n+          \"versions\": \"0+\",\n+          \"about\": \"Represents unique topic partition\",\n+          \"fields\": [\n+            {\n+              \"name\": \"Name\",\n+              \"type\": \"string\",\n+              \"versions\": \"0+\",\n+              \"about\": \"Topic name\"\n+            },\n+            {\n+              \"name\": \"Id\",\n+              \"type\": \"uuid\",\n+              \"versions\": \"0+\",\n+              \"about\": \"Unique identifier of the topic\"\n+            },\n+            {\n+              \"name\": \"Partition\",\n+              \"type\": \"int32\",\n+              \"versions\": \"0+\",\n+              \"about\": \"Partition number\"\n+            }\n+          ]\n+        },\n+        {\n+          \"name\": \"Id\",\n+          \"type\": \"uuid\",\n+          \"versions\": \"0+\",\n+          \"about\": \"Unique identifier of the remote log segment\"\n+        }\n+      ]\n+    },\n+    {\n+      \"name\": \"StartOffset\",\n+      \"type\": \"int64\",\n+      \"versions\": \"0+\",\n+      \"about\": \"Start offset  of the segment.\"\n+    },\n+    {\n+      \"name\": \"EndOffset\",\n+      \"type\": \"int64\",\n+      \"versions\": \"0+\",\n+      \"about\": \"End offset  of the segment.\"\n+    },\n+    {\n+      \"name\": \"BrokerId\",\n+      \"type\": \"int32\",\n+      \"versions\": \"0+\",\n+      \"about\": \"Broker id from which this event is generated.\"\n+    },\n+    {\n+      \"name\": \"MaxTimestamp\",\n+      \"type\": \"int64\",\n+      \"versions\": \"0+\",\n+      \"about\": \"Maximum timestamp with in this segment.\"\n+    },\n+    {\n+      \"name\": \"EventTimestamp\",\n+      \"type\": \"int64\",\n+      \"versions\": \"0+\",\n+      \"about\": \"Event timestamp of this segment.\"\n+    },\n+    {\n+      \"name\": \"SegmentLeaderEpochs\",\n+      \"type\": \"[]SegmentLeaderEpochEntry\",\n+      \"versions\": \"0+\",\n+      \"about\": \"Leader epoch cache.\",\n+      \"fields\": [\n+        {\n+          \"name\": \"LeaderEpoch\",\n+          \"type\": \"int32\",\n+          \"versions\": \"0+\",\n+          \"about\": \"Leader epoch\"\n+        },\n+        {\n+          \"name\": \"Offset\",\n+          \"type\": \"int64\",\n+          \"versions\": \"0+\",\n+          \"about\": \"Start offset for the leader epoch\"\n+        }\n+      ]\n+    },\n+    {\n+      \"name\": \"SegmentSizeInBytes\",\n+      \"type\": \"int32\",\n+      \"versions\": \"0+\",\n+      \"about\": \"Segment size in bytes\"\n+    },\n+    {\n+      \"name\": \"RemoteLogSegmentState\",\n+      \"type\": \"int8\",\n+      \"versions\": \"0+\",\n+      \"about\": \"State of the remote log segment\"\n+    }\n+  ]\n+}\n\\ No newline at end of file"
  },
  {
    "sha": "673fea862a199d428f5d9f540c18249173168de3",
    "filename": "remote-storage/src/main/resources/message/RemoteLogSegmentMetadataUpdate.json",
    "status": "added",
    "additions": 83,
    "deletions": 0,
    "changes": 83,
    "blob_url": "https://github.com/apache/kafka/blob/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/main/resources/message/RemoteLogSegmentMetadataUpdate.json",
    "raw_url": "https://github.com/apache/kafka/raw/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/main/resources/message/RemoteLogSegmentMetadataUpdate.json",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/remote-storage/src/main/resources/message/RemoteLogSegmentMetadataUpdate.json?ref=94ac96df7bd244e009d1d150dc62a5b1843e89b8",
    "patch": "@@ -0,0 +1,83 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 1,\n+  \"type\": \"data\",\n+  \"name\": \"RemoteLogSegmentMetadataRecordUpdate\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"none\",\n+  \"fields\": [\n+    {\n+      \"name\": \"RemoteLogSegmentId\",\n+      \"type\": \"RemoteLogSegmentIdEntry\",\n+      \"versions\": \"0+\",\n+      \"about\": \"Unique representation of the remote log segment\",\n+      \"fields\": [\n+        {\n+          \"name\": \"TopicIdPartition\",\n+          \"type\": \"TopicIdPartitionEntry\",\n+          \"versions\": \"0+\",\n+          \"about\": \"Represents unique topic partition\",\n+          \"fields\": [\n+            {\n+              \"name\": \"Name\",\n+              \"type\": \"string\",\n+              \"versions\": \"0+\",\n+              \"about\": \"Topic name\"\n+            },\n+            {\n+              \"name\": \"Id\",\n+              \"type\": \"uuid\",\n+              \"versions\": \"0+\",\n+              \"about\": \"Unique identifier of the topic\"\n+            },\n+            {\n+              \"name\": \"Partition\",\n+              \"type\": \"int32\",\n+              \"versions\": \"0+\",\n+              \"about\": \"Partition number\"\n+            }\n+          ]\n+        },\n+        {\n+          \"name\": \"Id\",\n+          \"type\": \"uuid\",\n+          \"versions\": \"0+\",\n+          \"about\": \"Unique identifier of the remote log segment\"\n+        }\n+      ]\n+    },\n+    {\n+      \"name\": \"BrokerId\",\n+      \"type\": \"int32\",\n+      \"versions\": \"0+\",\n+      \"about\": \"Broker id from which this event is generated.\"\n+    },\n+    {\n+      \"name\": \"EventTimestamp\",\n+      \"type\": \"int64\",\n+      \"versions\": \"0+\",\n+      \"about\": \"Event timestamp of this segment.\"\n+    },\n+    {\n+      \"name\": \"RemoteLogSegmentState\",\n+      \"type\": \"int8\",\n+      \"versions\": \"0+\",\n+      \"about\": \"State of the remote segment\"\n+    }\n+  ]\n+}\n+ \n\\ No newline at end of file"
  },
  {
    "sha": "f391dc42bb58f4359a259245de2f05a510d3d011",
    "filename": "remote-storage/src/main/resources/message/RemotePartitionDeleteMetadata.json",
    "status": "added",
    "additions": 68,
    "deletions": 0,
    "changes": 68,
    "blob_url": "https://github.com/apache/kafka/blob/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/main/resources/message/RemotePartitionDeleteMetadata.json",
    "raw_url": "https://github.com/apache/kafka/raw/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/main/resources/message/RemotePartitionDeleteMetadata.json",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/remote-storage/src/main/resources/message/RemotePartitionDeleteMetadata.json?ref=94ac96df7bd244e009d1d150dc62a5b1843e89b8",
    "patch": "@@ -0,0 +1,68 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 2,\n+  \"type\": \"data\",\n+  \"name\": \"RemotePartitionDeleteMetadataRecord\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"none\",\n+  \"fields\": [\n+    {\n+      \"name\": \"TopicIdPartition\",\n+      \"type\": \"TopicIdPartitionEntry\",\n+      \"versions\": \"0+\",\n+      \"about\": \"Represents unique topic partition\",\n+      \"fields\": [\n+        {\n+          \"name\": \"Name\",\n+          \"type\": \"string\",\n+          \"versions\": \"0+\",\n+          \"about\": \"Topic name\"\n+        },\n+        {\n+          \"name\": \"Id\",\n+          \"type\": \"uuid\",\n+          \"versions\": \"0+\",\n+          \"about\": \"Unique identifier of the topic\"\n+        },\n+        {\n+          \"name\": \"Partition\",\n+          \"type\": \"int32\",\n+          \"versions\": \"0+\",\n+          \"about\": \"Partition number\"\n+        }\n+      ]\n+    },\n+    {\n+      \"name\": \"BrokerId\",\n+      \"type\": \"int32\",\n+      \"versions\": \"0+\",\n+      \"about\": \"Broker (controller or leader) id from which this event is created. DELETE_PARTITION_MARKED is sent by the controller. DELETE_PARTITION_STARTED and DELETE_PARTITION_FINISHED are sent by remote log metadata topic partition leader.\"\n+    },\n+    {\n+      \"name\": \"EventTimestamp\",\n+      \"type\": \"int64\",\n+      \"versions\": \"0+\",\n+      \"about\": \"Event timestamp of this segment.\"\n+    },\n+    {\n+      \"name\": \"RemotePartitionDeleteState\",\n+      \"type\": \"int8\",\n+      \"versions\": \"0+\",\n+      \"about\": \"Deletion state of the remote partition\"\n+    }\n+  ]\n+}\n\\ No newline at end of file"
  },
  {
    "sha": "090e14e343e9a057c9b5e3594473fcce0da4dbdd",
    "filename": "remote-storage/src/test/java/org/apache/kafka/server/log/remote/metadata/storage/RemoteLogMetadataSerdesTest.java",
    "status": "added",
    "additions": 153,
    "deletions": 0,
    "changes": 153,
    "blob_url": "https://github.com/apache/kafka/blob/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/test/java/org/apache/kafka/server/log/remote/metadata/storage/RemoteLogMetadataSerdesTest.java",
    "raw_url": "https://github.com/apache/kafka/raw/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/test/java/org/apache/kafka/server/log/remote/metadata/storage/RemoteLogMetadataSerdesTest.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/remote-storage/src/test/java/org/apache/kafka/server/log/remote/metadata/storage/RemoteLogMetadataSerdesTest.java?ref=94ac96df7bd244e009d1d150dc62a5b1843e89b8",
    "patch": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.server.log.remote.metadata.storage;\n+\n+import org.apache.kafka.common.TopicIdPartition;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.Uuid;\n+import org.apache.kafka.server.log.remote.metadata.storage.serde.RemoteLogMetadataContextSerde;\n+import org.apache.kafka.server.log.remote.storage.RemoteLogSegmentId;\n+import org.apache.kafka.server.log.remote.storage.RemoteLogSegmentMetadata;\n+import org.apache.kafka.server.log.remote.storage.RemoteLogSegmentMetadataUpdate;\n+import org.apache.kafka.server.log.remote.storage.RemoteLogSegmentState;\n+import org.apache.kafka.server.log.remote.storage.RemotePartitionDeleteMetadata;\n+import org.apache.kafka.server.log.remote.storage.RemotePartitionDeleteState;\n+import org.junit.jupiter.api.Assertions;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class RemoteLogMetadataSerdesTest {\n+\n+    public static final String TOPIC = \"foo\";\n+    private final TopicIdPartition fooTp0 = new TopicIdPartition(Uuid.randomUuid(), new TopicPartition(TOPIC, 0));\n+\n+    @Test\n+    public void testRemoteLogSegmentMetadataSerde() throws Exception {\n+        // Create RemoteLogMetadataContext for RemoteLogSegmentMetadata\n+        // Deserialize the bytes and get RemoteLogSegmentMetadata and check it is as expected.\n+        RemoteLogSegmentMetadata rlsm = createRemoteLogSegmentMetadata();\n+\n+        RemoteLogMetadataContext remoteLogMetadataContext = new RemoteLogMetadataContext(\n+                RemoteLogMetadataContextSerde.REMOTE_LOG_SEGMENT_METADATA_API_KEY, (byte) 0, rlsm);\n+        doTestRemoteLogContextSerde(remoteLogMetadataContext);\n+    }\n+\n+    @Test\n+    public void testRemoteLogSegmentMetadataSerdeWithWrongApiKey() throws Exception {\n+        RemoteLogSegmentMetadata rlsm = createRemoteLogSegmentMetadata();\n+        // set the wrong API key for the given RemoteLogSegmentMetadata on RemoteLogMetadataContext and expect an\n+        // exception is thrown while serializing.\n+        byte[] wrongApiKeys = {RemoteLogMetadataContextSerde.REMOTE_LOG_SEGMENT_METADATA_UPDATE_API_KEY,\n+            RemoteLogMetadataContextSerde.REMOTE_PARTITION_DELETE_API_KEY};\n+\n+        doTestWrongAPIKeySerde(rlsm, wrongApiKeys);\n+    }\n+\n+    @Test\n+    public void testRemoteLogSegmentMetadataUpdateSerde() throws Exception {\n+        // Create RemoteLogMetadataContext for RemoteLogSegmentMetadataUpdate\n+        RemoteLogSegmentMetadataUpdate rlsmUpdate = createRemoteLogSegmentMetadataUpdate();\n+\n+        RemoteLogMetadataContext remoteLogMetadataContext = new RemoteLogMetadataContext(\n+            RemoteLogMetadataContextSerde.REMOTE_LOG_SEGMENT_METADATA_UPDATE_API_KEY, (byte) 0, rlsmUpdate);\n+\n+        doTestRemoteLogContextSerde(remoteLogMetadataContext);\n+    }\n+\n+    @Test\n+    public void testRemoteLogSegmentMetadataUpdateSerdeWithWrongApiKey() throws Exception {\n+        RemoteLogSegmentMetadataUpdate rlsm = createRemoteLogSegmentMetadataUpdate();\n+        // set the wrong API key for the given RemoteLogSegmentMetadataUpdate on RemoteLogMetadataContext and expect an\n+        // exception is thrown while serializing.\n+        byte[] wrongApiKeys = {RemoteLogMetadataContextSerde.REMOTE_LOG_SEGMENT_METADATA_API_KEY,\n+            RemoteLogMetadataContextSerde.REMOTE_PARTITION_DELETE_API_KEY};\n+\n+        doTestWrongAPIKeySerde(rlsm, wrongApiKeys);\n+    }\n+\n+    @Test\n+    public void testRemotePartitionDeleteMetadataSerde() throws Exception {\n+        // Create RemoteLogMetadataContext for RemotePartitionDeleteMetadata\n+        RemotePartitionDeleteMetadata remotePartitionDeleteMetadata = createRemotePartitionDeleteMetadata();\n+\n+        RemoteLogMetadataContext remoteLogMetadataContext = new RemoteLogMetadataContext(\n+                RemoteLogMetadataContextSerde.REMOTE_PARTITION_DELETE_API_KEY, (byte) 0, remotePartitionDeleteMetadata);\n+\n+        doTestRemoteLogContextSerde(remoteLogMetadataContext);\n+    }\n+\n+    @Test\n+    public void testRemotePartitionDeleteMetadataSerdeWithWrongApiKey() throws Exception {\n+        RemotePartitionDeleteMetadata remotePartitionDeleteMetadata = createRemotePartitionDeleteMetadata();\n+        // set the wrong API key for the given RemotePartitionDeleteMetadata on RemoteLogMetadataContext and expect an\n+        // exception is thrown while serializing.\n+        byte[] wrongApiKeys = {RemoteLogMetadataContextSerde.REMOTE_LOG_SEGMENT_METADATA_API_KEY,\n+            RemoteLogMetadataContextSerde.REMOTE_LOG_SEGMENT_METADATA_UPDATE_API_KEY};\n+\n+        doTestWrongAPIKeySerde(remotePartitionDeleteMetadata, wrongApiKeys);\n+    }\n+\n+    private RemoteLogSegmentMetadata createRemoteLogSegmentMetadata() {\n+        Map<Integer, Long> segLeaderEpochs = new HashMap<>();\n+        segLeaderEpochs.put(0, 0L);\n+        segLeaderEpochs.put(1, 20L);\n+        segLeaderEpochs.put(2, 80L);\n+        RemoteLogSegmentId remoteLogSegmentId = new RemoteLogSegmentId(fooTp0, Uuid.randomUuid());\n+        return new RemoteLogSegmentMetadata(remoteLogSegmentId, 0L, 100L, -1L, 2,\n+                System.currentTimeMillis(), 1024, segLeaderEpochs);\n+    }\n+\n+    private RemoteLogSegmentMetadataUpdate createRemoteLogSegmentMetadataUpdate() {\n+        RemoteLogSegmentId remoteLogSegmentId = new RemoteLogSegmentId(fooTp0, Uuid.randomUuid());\n+        return new RemoteLogSegmentMetadataUpdate(remoteLogSegmentId,\n+                System.currentTimeMillis(), RemoteLogSegmentState.COPY_SEGMENT_FINISHED, 0);\n+    }\n+\n+    private RemotePartitionDeleteMetadata createRemotePartitionDeleteMetadata() {\n+        return new RemotePartitionDeleteMetadata(fooTp0,\n+                RemotePartitionDeleteState.DELETE_PARTITION_MARKED,\n+                System.currentTimeMillis(), 0);\n+    }\n+\n+    private void doTestRemoteLogContextSerde(RemoteLogMetadataContext remoteLogMetadataContext) {\n+        RemoteLogMetadataContextSerde remoteLogMetadataContextSerde = new RemoteLogMetadataContextSerde();\n+\n+        // serialize context and get the bytes.\n+        byte[] contextSerBytes = remoteLogMetadataContextSerde.serializer().serialize(TOPIC, remoteLogMetadataContext);\n+\n+        // Deserialize the bytes and check the RemoteLogMetadataContext object is as expected.\n+        RemoteLogMetadataContext deserializedRlmContext = remoteLogMetadataContextSerde.deserializer()\n+                .deserialize(TOPIC, contextSerBytes);\n+        Assertions.assertEquals(remoteLogMetadataContext, deserializedRlmContext);\n+    }\n+\n+    private void doTestWrongAPIKeySerde(Object object, byte[] wrongApiKeys) {\n+        for (byte wrongApiKey : wrongApiKeys) {\n+            RemoteLogMetadataContext remoteLogMetadataContext = new RemoteLogMetadataContext(wrongApiKey, (byte) 0,\n+                    object);\n+\n+            // serialize context, receive an exception as it has the wrong api key.\n+            Assertions.assertThrows(Exception.class, () -> {\n+                byte[] contextSerBytes = new RemoteLogMetadataContextSerde().serializer()\n+                        .serialize(TOPIC, remoteLogMetadataContext);\n+            });\n+        }\n+    }\n+\n+}"
  },
  {
    "sha": "4e7f0351e0694d96243de2fc6c520124340ba99c",
    "filename": "remote-storage/src/test/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteLogMetadataManagerTest.java",
    "status": "added",
    "additions": 234,
    "deletions": 0,
    "changes": 234,
    "blob_url": "https://github.com/apache/kafka/blob/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/test/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteLogMetadataManagerTest.java",
    "raw_url": "https://github.com/apache/kafka/raw/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/test/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteLogMetadataManagerTest.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/remote-storage/src/test/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteLogMetadataManagerTest.java?ref=94ac96df7bd244e009d1d150dc62a5b1843e89b8",
    "patch": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.server.log.remote.storage;\n+\n+import org.apache.kafka.common.TopicIdPartition;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.Uuid;\n+import org.junit.jupiter.api.Assertions;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+\n+public class InmemoryRemoteLogMetadataManagerTest {\n+\n+    private static final TopicIdPartition TP0 = new TopicIdPartition(Uuid.randomUuid(), new TopicPartition(\"foo\", 0));\n+    private static final int SEG_SIZE = 1024 * 1024;\n+    private static final int BROKER_ID = 0;\n+\n+    @Test\n+    public void testRLMMFetchSegment() throws Exception {\n+        InmemoryRemoteLogMetadataManager rlmm = new InmemoryRemoteLogMetadataManager();\n+        int brokerId = 0;\n+        // Create remote log segment metadata and add them to RLMM.\n+\n+        // segment 0\n+        // 0-100\n+        // leader epochs (0,0), (1,20), (2,80)\n+        Map<Integer, Long> seg0leaderEpochs = new HashMap<>();\n+        seg0leaderEpochs.put(0, 0L);\n+        seg0leaderEpochs.put(1, 20L);\n+        seg0leaderEpochs.put(2, 80L);\n+        RemoteLogSegmentId segIdFooTp0s0e100 = new RemoteLogSegmentId(TP0, Uuid.randomUuid());\n+        RemoteLogSegmentMetadata segMetFooTp0s0e100 = new RemoteLogSegmentMetadata(segIdFooTp0s0e100, 0L, 100L, -1L, BROKER_ID,\n+                System.currentTimeMillis(), SEG_SIZE, seg0leaderEpochs);\n+        rlmm.addRemoteLogSegmentMetadata(segMetFooTp0s0e100);\n+\n+        // wWe should not get this as the segment is still gettign copied and it is not yet considered successful until\n+        // it reaches RemoteLogSegmentState.COPY_SEGMENT_FINISHED.\n+        Assertions.assertFalse(rlmm.remoteLogSegmentMetadata(TP0, 40, 1).isPresent());\n+\n+        RemoteLogSegmentMetadataUpdate segMetFooTp0s0e100Update = new RemoteLogSegmentMetadataUpdate(segIdFooTp0s0e100,\n+                System.currentTimeMillis(), RemoteLogSegmentState.COPY_SEGMENT_FINISHED, brokerId);\n+        rlmm.updateRemoteLogSegmentMetadata(segMetFooTp0s0e100Update);\n+\n+        // segment 1\n+        // 100 - 200\n+        // no changes in leadership with in this segment\n+        // leader epochs (2, 101)\n+        Map<Integer, Long> seg1leaderEpochs = new HashMap<>();\n+        seg1leaderEpochs.put(2, 101L);\n+        RemoteLogSegmentId segIdFooTp0s101e200 = new RemoteLogSegmentId(TP0, Uuid.randomUuid());\n+        RemoteLogSegmentMetadata segMetFooTp0s101e200 = new RemoteLogSegmentMetadata(segIdFooTp0s101e200, 101L, 200L, -1L, BROKER_ID,\n+                System.currentTimeMillis(), SEG_SIZE, seg1leaderEpochs);\n+        rlmm.addRemoteLogSegmentMetadata(segMetFooTp0s101e200);\n+        RemoteLogSegmentMetadataUpdate segMetFooTp0s101e200Update = new RemoteLogSegmentMetadataUpdate(segIdFooTp0s101e200,\n+                System.currentTimeMillis(), RemoteLogSegmentState.COPY_SEGMENT_FINISHED, brokerId);\n+        rlmm.updateRemoteLogSegmentMetadata(segMetFooTp0s101e200Update);\n+\n+        // segment 2\n+        // 201 - 300\n+        // moved to epoch 3 in between\n+        // leader epochs (2, 201), (3, 240)\n+        Map<Integer, Long> seg2leaderEpochs = new HashMap<>();\n+        seg2leaderEpochs.put(2, 201L);\n+        seg2leaderEpochs.put(3, 240L);\n+        RemoteLogSegmentId segIdFooTp0s101e300 = new RemoteLogSegmentId(TP0, Uuid.randomUuid());\n+        RemoteLogSegmentMetadata segMetFooTp0s101e300 = new RemoteLogSegmentMetadata(segIdFooTp0s101e300, 201L, 300L, -1L, 3,\n+                System.currentTimeMillis(), SEG_SIZE, seg2leaderEpochs);\n+        rlmm.addRemoteLogSegmentMetadata(segMetFooTp0s101e300);\n+        RemoteLogSegmentMetadataUpdate segMetFooTp0s101e300Update = new RemoteLogSegmentMetadataUpdate(segIdFooTp0s101e300,\n+                System.currentTimeMillis(), RemoteLogSegmentState.COPY_SEGMENT_FINISHED, brokerId);\n+        rlmm.updateRemoteLogSegmentMetadata(segMetFooTp0s101e300Update);\n+\n+        // segment 3\n+        // 250 - 400\n+        // leader epochs (3, 250), (4, 370)\n+        Map<Integer, Long> seg3leaderEpochs = new HashMap<>();\n+        seg3leaderEpochs.put(3, 250L);\n+        seg3leaderEpochs.put(4, 370L);\n+        RemoteLogSegmentId segIdFooTp0s250e400 = new RemoteLogSegmentId(TP0, Uuid.randomUuid());\n+        RemoteLogSegmentMetadata segMetFooTp0s250e400 = new RemoteLogSegmentMetadata(segIdFooTp0s250e400, 250L, 400L, -1L, BROKER_ID,\n+                System.currentTimeMillis(), SEG_SIZE, seg3leaderEpochs);\n+        rlmm.addRemoteLogSegmentMetadata(segMetFooTp0s250e400);\n+        RemoteLogSegmentMetadataUpdate segMetFooTp0s250e400Update = new RemoteLogSegmentMetadataUpdate(segIdFooTp0s250e400,\n+                System.currentTimeMillis(), RemoteLogSegmentState.COPY_SEGMENT_FINISHED, brokerId);\n+        rlmm.updateRemoteLogSegmentMetadata(segMetFooTp0s250e400Update);\n+\n+        //////////////////////////////////////////////////////////////////////////////////////////\n+        //  Search for RLMM.remoteLogSegmentMetadata(TP, offset, leaderEpoch)  for different\n+        // epochs and offsets\n+        //////////////////////////////////////////////////////////////////////////////////////////\n+\n+        // Search for offset 40, epoch 1\n+        Optional<RemoteLogSegmentMetadata> segO40E1 = rlmm.remoteLogSegmentMetadata(TP0, 40, 1);\n+        Assertions.assertEquals(segMetFooTp0s0e100.createRemoteLogSegmentWithUpdates(segMetFooTp0s0e100Update),\n+                segO40E1.orElse(null));\n+\n+        // Search for offset 110, epoch 2\n+        Optional<RemoteLogSegmentMetadata> segO110E2 = rlmm.remoteLogSegmentMetadata(TP0, 110, 2);\n+        Assertions.assertEquals(segMetFooTp0s101e200.createRemoteLogSegmentWithUpdates(segMetFooTp0s101e200Update),\n+                segO110E2.orElse(null));\n+\n+        // Search for offset 110, epoch 1, and it should not exist\n+        Optional<RemoteLogSegmentMetadata> segO110E1 = rlmm.remoteLogSegmentMetadata(TP0, 110, 1);\n+        Assertions.assertFalse(segO110E1.isPresent());\n+\n+        // Search for offset 240, epoch 3\n+        Optional<RemoteLogSegmentMetadata> segO2400E3 = rlmm.remoteLogSegmentMetadata(TP0, 240, 3);\n+        Assertions.assertEquals(segMetFooTp0s101e300.createRemoteLogSegmentWithUpdates(segMetFooTp0s101e300Update),\n+                segO2400E3.orElse(null));\n+\n+        // Search for offset 250, epoch 3\n+        Optional<RemoteLogSegmentMetadata> segO2500E3 = rlmm.remoteLogSegmentMetadata(TP0, 250, 3);\n+        Assertions.assertEquals(segMetFooTp0s250e400.createRemoteLogSegmentWithUpdates(segMetFooTp0s250e400Update),\n+                segO2500E3.orElse(null));\n+\n+        //search for highest offset for leader epoch 3\n+        Optional<Long> highestOffsetForEpoch3 = rlmm.highestLogOffset(TP0, 3);\n+        Assertions.assertEquals(369, highestOffsetForEpoch3.get());\n+\n+        // Search for offset 375, epoch 4\n+        Optional<RemoteLogSegmentMetadata> segO3750E4 = rlmm.remoteLogSegmentMetadata(TP0, 375, 4);\n+        Assertions.assertEquals(segMetFooTp0s250e400.createRemoteLogSegmentWithUpdates(segMetFooTp0s250e400Update),\n+                segO3750E4.orElse(null));\n+\n+        // Search for offset 401, epoch 4\n+        Optional<RemoteLogSegmentMetadata> segO4010E4 = rlmm.remoteLogSegmentMetadata(TP0, 401, 4);\n+        Assertions.assertFalse(segO4010E4.isPresent());\n+\n+        //////////////////////////////////////////////////////////////////////////////////////////\n+        //  Search for RLMM.highestLogOffset(TP, leaderEpoch)  for all the leader epochs\n+        //////////////////////////////////////////////////////////////////////////////////////////\n+\n+        //search for highest offset for leader epoch 0\n+        Optional<Long> highestOffsetForEpoch0 = rlmm.highestLogOffset(TP0, 0);\n+        Assertions.assertEquals(19, highestOffsetForEpoch0.orElse(null));\n+\n+        //search for highest offset for leader epoch 1\n+        Optional<Long> highestOffsetForEpoch1 = rlmm.highestLogOffset(TP0, 1);\n+        Assertions.assertEquals(79, highestOffsetForEpoch1.orElse(null));\n+\n+        //search for highest offset for leader epoch 2\n+        Optional<Long> highestOffsetForEpoch2 = rlmm.highestLogOffset(TP0, 2);\n+        Assertions.assertEquals(239, highestOffsetForEpoch2.orElse(null));\n+\n+        //search for highest offset for leader epoch 4\n+        Optional<Long> highestOffsetForEpoch4 = rlmm.highestLogOffset(TP0, 4);\n+        Assertions.assertEquals(400, highestOffsetForEpoch4.orElse(null));\n+\n+        // Update segment with state as DELETE_SEGMENT_STARTED.\n+        // It should be available when we search for that segment.\n+        RemoteLogSegmentMetadataUpdate rlsmUpdate = new RemoteLogSegmentMetadataUpdate(segIdFooTp0s0e100,\n+                System.currentTimeMillis(), RemoteLogSegmentState.DELETE_SEGMENT_STARTED, 0);\n+        rlmm.updateRemoteLogSegmentMetadata(rlsmUpdate);\n+\n+        Optional<RemoteLogSegmentMetadata> seg01s10e0 = rlmm.remoteLogSegmentMetadata(TP0, 10, 0);\n+        Assertions.assertEquals(segMetFooTp0s0e100.createRemoteLogSegmentWithUpdates(rlsmUpdate),\n+                seg01s10e0.orElse(null));\n+\n+        // Update segment with state as DELETE_SEGMENT_FINISHED.\n+        // It should not be available when we search for that segment.\n+        rlmm.updateRemoteLogSegmentMetadata(new RemoteLogSegmentMetadataUpdate(segIdFooTp0s0e100, System.currentTimeMillis(),\n+                RemoteLogSegmentState.DELETE_SEGMENT_FINISHED, 0));\n+        Assertions.assertFalse(rlmm.remoteLogSegmentMetadata(TP0, 10, 0).isPresent());\n+    }\n+\n+    @Test\n+    public void testRemotePartitionDeletion() throws Exception {\n+        InmemoryRemoteLogMetadataManager rlmm = new InmemoryRemoteLogMetadataManager();\n+\n+        // Create remote log segment metadata and add them to RLMM.\n+\n+        // segment 0\n+        // 0-100\n+        // leader epochs (0,0), (1,20), (2,80)\n+        Map<Integer, Long> seg0leaderEpochs = new HashMap<>();\n+        seg0leaderEpochs.put(0, 0L);\n+        seg0leaderEpochs.put(1, 20L);\n+        seg0leaderEpochs.put(2, 50L);\n+        seg0leaderEpochs.put(3, 80L);\n+        RemoteLogSegmentId segIdFooTp0s0e100 = new RemoteLogSegmentId(TP0, Uuid.randomUuid());\n+        RemoteLogSegmentMetadata segMetFooTp0s0e100 = new RemoteLogSegmentMetadata(segIdFooTp0s0e100, 0L, 100L, -1L, BROKER_ID,\n+                System.currentTimeMillis(), SEG_SIZE, seg0leaderEpochs);\n+        rlmm.addRemoteLogSegmentMetadata(segMetFooTp0s0e100);\n+        RemoteLogSegmentMetadataUpdate segMetFooTp0s0e100Update = new RemoteLogSegmentMetadataUpdate(segIdFooTp0s0e100,\n+                System.currentTimeMillis(), RemoteLogSegmentState.COPY_SEGMENT_FINISHED, 0);\n+        rlmm.updateRemoteLogSegmentMetadata(segMetFooTp0s0e100Update);\n+\n+        RemoteLogSegmentMetadata expectedSegMetFooTp0s0e100 = segMetFooTp0s0e100.createRemoteLogSegmentWithUpdates(segMetFooTp0s0e100Update);\n+\n+        // Check that the seg exists in RLMM\n+        Optional<RemoteLogSegmentMetadata> seg0s0e100 = rlmm.remoteLogSegmentMetadata(TP0, 30L, 1);\n+        Assertions.assertEquals(expectedSegMetFooTp0s0e100, seg0s0e100.orElse(null));\n+\n+        // Mark the partition for deletion. RLMM should clear all its internal state for that partition.\n+        rlmm.putRemotePartitionDeleteMetadata(createRemotePartitionDeleteMetadata(RemotePartitionDeleteState.DELETE_PARTITION_MARKED));\n+\n+        Optional<RemoteLogSegmentMetadata> seg0s0e100AfterDelMark = rlmm.remoteLogSegmentMetadata(TP0, 30L, 1);\n+        Assertions.assertEquals(expectedSegMetFooTp0s0e100, seg0s0e100AfterDelMark.orElse(null));\n+\n+        // Set the partition deletion state as started. Partition and segments should still be accessible as they are not\n+        // yet deleted.\n+        rlmm.putRemotePartitionDeleteMetadata(createRemotePartitionDeleteMetadata(RemotePartitionDeleteState.DELETE_PARTITION_STARTED));\n+\n+        Optional<RemoteLogSegmentMetadata> seg0s0e100AfterDelStart = rlmm.remoteLogSegmentMetadata(TP0, 30L, 1);\n+        Assertions.assertEquals(expectedSegMetFooTp0s0e100, seg0s0e100AfterDelStart.orElse(null));\n+\n+        // Set the partition deletion state as finished. RLMM should clear all its internal state for that partition.\n+        rlmm.putRemotePartitionDeleteMetadata(createRemotePartitionDeleteMetadata(RemotePartitionDeleteState.DELETE_PARTITION_FINISHED));\n+\n+        Assertions.assertThrows(RemoteResourceNotFoundException.class,\n+            () -> rlmm.remoteLogSegmentMetadata(TP0, 30L, 1));\n+    }\n+\n+    private RemotePartitionDeleteMetadata createRemotePartitionDeleteMetadata(RemotePartitionDeleteState state) {\n+        return new RemotePartitionDeleteMetadata(TP0, state, System.currentTimeMillis(), BROKER_ID);\n+    }\n+}"
  },
  {
    "sha": "b5ae45cc7966c7ac057b83de0cc3f60447a14776",
    "filename": "remote-storage/src/test/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteStorageManager.java",
    "status": "added",
    "additions": 171,
    "deletions": 0,
    "changes": 171,
    "blob_url": "https://github.com/apache/kafka/blob/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/test/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteStorageManager.java",
    "raw_url": "https://github.com/apache/kafka/raw/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/test/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteStorageManager.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/remote-storage/src/test/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteStorageManager.java?ref=94ac96df7bd244e009d1d150dc62a5b1843e89b8",
    "patch": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.server.log.remote.storage;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.file.Files;\n+import java.util.Collections;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+/**\n+ * This class is an implementation of {@link RemoteStorageManager} backed by inmemory store.\n+ */\n+public class InmemoryRemoteStorageManager implements RemoteStorageManager {\n+    private static final Logger log = LoggerFactory.getLogger(InmemoryRemoteStorageManager.class);\n+\n+    // map of key to log data, which can be segment or any of its indexes.\n+    private Map<String, byte[]> keyToLogData = new ConcurrentHashMap<>();\n+\n+    public InmemoryRemoteStorageManager() {\n+    }\n+\n+    static String generateKeyForSegment(RemoteLogSegmentMetadata remoteLogSegmentMetadata) {\n+        return remoteLogSegmentMetadata.remoteLogSegmentId().id().toString() + \".segment\";\n+    }\n+\n+    static String generateKeyForIndex(RemoteLogSegmentMetadata remoteLogSegmentMetadata,\n+                                      IndexType indexType) {\n+        return remoteLogSegmentMetadata.remoteLogSegmentId().id().toString() + \".\" + indexType.toString();\n+    }\n+\n+    // visible for testing.\n+    boolean containsKey(String key) {\n+        return keyToLogData.containsKey(key);\n+    }\n+\n+    @Override\n+    public void copyLogSegmentData(RemoteLogSegmentMetadata remoteLogSegmentMetadata,\n+                                   LogSegmentData logSegmentData)\n+            throws RemoteStorageException {\n+        log.debug(\"copying log segment and indexes for : {}\", remoteLogSegmentMetadata);\n+        Objects.requireNonNull(remoteLogSegmentMetadata, \"remoteLogSegmentMetadata can not be null\");\n+        Objects.requireNonNull(logSegmentData, \"logSegmentData can not be null\");\n+        try {\n+            keyToLogData.put(generateKeyForSegment(remoteLogSegmentMetadata),\n+                    Files.readAllBytes(logSegmentData.logSegment().toPath()));\n+            keyToLogData.put(generateKeyForIndex(remoteLogSegmentMetadata, IndexType.Offset),\n+                    Files.readAllBytes(logSegmentData.offsetIndex().toPath()));\n+            keyToLogData.put(generateKeyForIndex(remoteLogSegmentMetadata, IndexType.Timestamp),\n+                    Files.readAllBytes(logSegmentData.timeIndex().toPath()));\n+            keyToLogData.put(generateKeyForIndex(remoteLogSegmentMetadata, IndexType.Transaction),\n+                    Files.readAllBytes(logSegmentData.txnIndex().toPath()));\n+            keyToLogData.put(generateKeyForIndex(remoteLogSegmentMetadata, IndexType.LeaderEpoch),\n+                    logSegmentData.leaderEpochIndex().array());\n+            keyToLogData.put(generateKeyForIndex(remoteLogSegmentMetadata, IndexType.ProducerSnapshot),\n+                    Files.readAllBytes(logSegmentData.producerSnapshotIndex().toPath()));\n+        } catch (IOException e) {\n+            throw new RemoteStorageException(e.getMessage(), e);\n+        }\n+        log.debug(\"copied log segment and indexes for : {} successfully.\", remoteLogSegmentMetadata);\n+    }\n+\n+    @Override\n+    public InputStream fetchLogSegment(RemoteLogSegmentMetadata remoteLogSegmentMetadata,\n+                                       int startPosition)\n+            throws RemoteStorageException {\n+        log.debug(\"Received fetch segment request at start position: [{}] for [{}]\", startPosition, remoteLogSegmentMetadata);\n+        Objects.requireNonNull(remoteLogSegmentMetadata, \"remoteLogSegmentMetadata can not be null\");\n+\n+        return fetchLogSegment(remoteLogSegmentMetadata, startPosition, Integer.MAX_VALUE);\n+    }\n+\n+    @Override\n+    public InputStream fetchLogSegment(RemoteLogSegmentMetadata remoteLogSegmentMetadata,\n+                                       int startPosition,\n+                                       int endPosition) throws RemoteStorageException {\n+        log.debug(\"Received fetch segment request at start position: [{}] and end position: [{}] for segment [{}]\",\n+                startPosition, endPosition, remoteLogSegmentMetadata);\n+\n+        Objects.requireNonNull(remoteLogSegmentMetadata, \"remoteLogSegmentMetadata can not be null\");\n+\n+        if (startPosition < 0 || endPosition < 0) {\n+            throw new IllegalArgumentException(\"Given start position or end position must not be negative.\");\n+        }\n+\n+        if (endPosition < startPosition) {\n+            throw new IllegalArgumentException(\"end position must be greater than start position\");\n+        }\n+\n+        String key = generateKeyForSegment(remoteLogSegmentMetadata);\n+        byte[] segment = keyToLogData.get(key);\n+\n+        if (segment == null) {\n+            throw new RemoteResourceNotFoundException(\"No remote log segment found with start offset:\"\n+                                                      + remoteLogSegmentMetadata.startOffset() + \" and id: \"\n+                                                      + remoteLogSegmentMetadata.remoteLogSegmentId());\n+        }\n+\n+        if (startPosition >= segment.length) {\n+            throw new IllegalArgumentException(\"start position: \" + startPosition\n+                                               + \" must be less than the length of the segment: \" + segment.length);\n+        }\n+\n+        // check for boundaries like given end position is more than the length, length should never be more than the\n+        // existing segment size.\n+        int length = Math.min(segment.length - 1, endPosition) - startPosition + 1;\n+        log.debug(\"Length of the segment to be sent: [{}], for segment: [{}]\", length, remoteLogSegmentMetadata);\n+\n+        return new ByteArrayInputStream(segment, startPosition, length);\n+    }\n+\n+    @Override\n+    public InputStream fetchIndex(RemoteLogSegmentMetadata remoteLogSegmentMetadata,\n+                                  IndexType indexType) throws RemoteStorageException {\n+        log.debug(\"Received fetch request for index type: [{}], segment [{}]\", indexType, remoteLogSegmentMetadata);\n+        Objects.requireNonNull(remoteLogSegmentMetadata, \"remoteLogSegmentMetadata can not be null\");\n+        Objects.requireNonNull(indexType, \"indexType can not be null\");\n+\n+        String key = generateKeyForIndex(remoteLogSegmentMetadata, indexType);\n+        byte[] index = keyToLogData.get(key);\n+        if (index == null) {\n+            throw new RemoteResourceNotFoundException(\"No remote log segment index found with start offset:\"\n+                                                      + remoteLogSegmentMetadata.startOffset() + \" and id: \"\n+                                                      + remoteLogSegmentMetadata.remoteLogSegmentId());\n+        }\n+\n+        return new ByteArrayInputStream(index);\n+    }\n+\n+    @Override\n+    public void deleteLogSegmentData(RemoteLogSegmentMetadata remoteLogSegmentMetadata) throws RemoteStorageException {\n+        log.info(\"Deleting log segment for: [{}]\", remoteLogSegmentMetadata);\n+        Objects.requireNonNull(remoteLogSegmentMetadata, \"remoteLogSegmentMetadata can not be null\");\n+        String segmentKey = generateKeyForSegment(remoteLogSegmentMetadata);\n+        keyToLogData.remove(segmentKey);\n+        for (IndexType indexType : IndexType.values()) {\n+            String key = generateKeyForIndex(remoteLogSegmentMetadata, indexType);\n+            keyToLogData.remove(key);\n+        }\n+        log.info(\"Deleted log segment successfully for: [{}]\", remoteLogSegmentMetadata);\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+        keyToLogData = Collections.emptyMap();\n+    }\n+\n+    @Override\n+    public void configure(Map<String, ?> configs) {\n+    }\n+}"
  },
  {
    "sha": "5110763f299577f96ae2d1fc4aa7aa4f6a887398",
    "filename": "remote-storage/src/test/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteStorageManagerTest.java",
    "status": "added",
    "additions": 241,
    "deletions": 0,
    "changes": 241,
    "blob_url": "https://github.com/apache/kafka/blob/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/test/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteStorageManagerTest.java",
    "raw_url": "https://github.com/apache/kafka/raw/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/test/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteStorageManagerTest.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/remote-storage/src/test/java/org/apache/kafka/server/log/remote/storage/InmemoryRemoteStorageManagerTest.java?ref=94ac96df7bd244e009d1d150dc62a5b1843e89b8",
    "patch": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.server.log.remote.storage;\n+\n+import org.apache.kafka.common.TopicIdPartition;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.Uuid;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.jupiter.api.Assertions;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.SeekableByteChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.Collections;\n+import java.util.Random;\n+\n+public class InmemoryRemoteStorageManagerTest {\n+\n+    private static final TopicPartition TP = new TopicPartition(\"foo\", 1);\n+    private static final File DIR = TestUtils.tempDirectory(\"inmem-rsm-\");\n+\n+    @Test\n+    public void testCopyLogSegment() throws Exception {\n+        InmemoryRemoteStorageManager rsm = new InmemoryRemoteStorageManager();\n+        RemoteLogSegmentMetadata rlsm = createRemoteLogSegmentMetadata();\n+        LogSegmentData logSegmentData = createLogSegmentData();\n+        // Copy all the segment data.\n+        rsm.copyLogSegmentData(rlsm, logSegmentData);\n+\n+        // Check that the segment data exists in inmemory RSM.\n+        boolean containsSegment = rsm.containsKey(InmemoryRemoteStorageManager.generateKeyForSegment(rlsm));\n+        Assertions.assertTrue(containsSegment);\n+\n+        // Check that the indexes exist in inmemory RSM.\n+        for (RemoteStorageManager.IndexType indexType : RemoteStorageManager.IndexType.values()) {\n+            boolean containsIndex = rsm.containsKey(InmemoryRemoteStorageManager.generateKeyForIndex(rlsm, indexType));\n+            Assertions.assertTrue(containsIndex);\n+        }\n+    }\n+\n+    private RemoteLogSegmentMetadata createRemoteLogSegmentMetadata() {\n+        TopicIdPartition topicPartition = new TopicIdPartition(Uuid.randomUuid(), TP);\n+        RemoteLogSegmentId id = new RemoteLogSegmentId(topicPartition, Uuid.randomUuid());\n+        return new RemoteLogSegmentMetadata(id, 100L, 200L, System.currentTimeMillis(), 0,\n+                System.currentTimeMillis(), 100, Collections.singletonMap(1, 100L));\n+    }\n+\n+    @Test\n+    public void testFetchLogSegmentIndexes() throws Exception {\n+        InmemoryRemoteStorageManager rsm = new InmemoryRemoteStorageManager();\n+        RemoteLogSegmentMetadata rlsm = createRemoteLogSegmentMetadata();\n+        int segSize = 100;\n+        LogSegmentData logSegmentData = createLogSegmentData(segSize);\n+\n+        // Copy the segment\n+        rsm.copyLogSegmentData(rlsm, logSegmentData);\n+\n+        // Check segment data exists for the copied segment.\n+        try (InputStream segmentStream = rsm.fetchLogSegment(rlsm, 0)) {\n+            checkContentSame(segmentStream, logSegmentData.logSegment().toPath());\n+        }\n+\n+        // Check all segment indexes exist for the copied segment.\n+        try (InputStream offsetIndexStream = rsm.fetchIndex(rlsm, RemoteStorageManager.IndexType.Offset)) {\n+            checkContentSame(offsetIndexStream, logSegmentData.offsetIndex().toPath());\n+        }\n+\n+        try (InputStream timestampIndexStream = rsm.fetchIndex(rlsm, RemoteStorageManager.IndexType.Timestamp)) {\n+            checkContentSame(timestampIndexStream, logSegmentData.timeIndex().toPath());\n+        }\n+\n+        try (InputStream txnIndexStream = rsm.fetchIndex(rlsm, RemoteStorageManager.IndexType.Transaction)) {\n+            checkContentSame(txnIndexStream, logSegmentData.txnIndex().toPath());\n+        }\n+\n+        try (InputStream producerSnapshotStream = rsm\n+                .fetchIndex(rlsm, RemoteStorageManager.IndexType.ProducerSnapshot)) {\n+            checkContentSame(producerSnapshotStream, logSegmentData.producerSnapshotIndex().toPath());\n+        }\n+\n+        try (InputStream leaderEpochIndexStream = rsm.fetchIndex(rlsm, RemoteStorageManager.IndexType.LeaderEpoch)) {\n+            ByteBuffer leaderEpochIndex = logSegmentData.leaderEpochIndex();\n+            Assertions.assertEquals(leaderEpochIndex,\n+                    readAsByteBuffer(leaderEpochIndexStream, leaderEpochIndex.array().length));\n+        }\n+    }\n+\n+    @Test\n+    public void testFetchSegmentsForRange() throws Exception {\n+        InmemoryRemoteStorageManager rsm = new InmemoryRemoteStorageManager();\n+        RemoteLogSegmentMetadata rlsm = createRemoteLogSegmentMetadata();\n+        int segSize = 100;\n+        LogSegmentData logSegmentData = createLogSegmentData(segSize);\n+        Path path = logSegmentData.logSegment().toPath();\n+\n+        // Copy the segment\n+        rsm.copyLogSegmentData(rlsm, logSegmentData);\n+\n+        // 1. Fetch segment for startPos at 0\n+        doTestFetchForRange(rsm, rlsm, path, 0, 40);\n+\n+        // 2. Fetch segment for start and end positions as start and end of the segment.\n+        doTestFetchForRange(rsm, rlsm, path, 0, segSize);\n+\n+        // 3. Fetch segment for endPos at the end of segment.\n+        doTestFetchForRange(rsm, rlsm, path, 90, segSize - 90);\n+\n+        // 4. Fetch segment only for the start position.\n+        doTestFetchForRange(rsm, rlsm, path, 0, 1);\n+\n+        // 5. Fetch segment only for the end position.\n+        doTestFetchForRange(rsm, rlsm, path, segSize - 1, 1);\n+\n+        // 6. Fetch for any range other than boundaries.\n+        doTestFetchForRange(rsm, rlsm, path, 3, 90);\n+    }\n+\n+    private void doTestFetchForRange(InmemoryRemoteStorageManager rsm, RemoteLogSegmentMetadata rlsm, Path path,\n+                                     int startPos, int len) throws Exception {\n+        // Read from the segment for the expected range.\n+        ByteBuffer expectedSegRangeBytes = ByteBuffer.allocate(len);\n+        try (SeekableByteChannel seekableByteChannel = Files.newByteChannel(path)) {\n+            seekableByteChannel.position(startPos).read(expectedSegRangeBytes);\n+        }\n+        expectedSegRangeBytes.rewind();\n+\n+        // Fetch from inmemory RSM for the same range\n+        ByteBuffer fetchedSegRangeBytes = ByteBuffer.allocate(len);\n+        try (InputStream segmentRangeStream = rsm.fetchLogSegment(rlsm, startPos, startPos + len - 1)) {\n+            Utils.readFully(segmentRangeStream, fetchedSegRangeBytes);\n+        }\n+        fetchedSegRangeBytes.rewind();\n+        Assertions.assertEquals(expectedSegRangeBytes, fetchedSegRangeBytes);\n+    }\n+\n+    @Test\n+    public void testFetchInvalidRange() throws Exception {\n+        InmemoryRemoteStorageManager rsm = new InmemoryRemoteStorageManager();\n+        RemoteLogSegmentMetadata rlsm = createRemoteLogSegmentMetadata();\n+        int segSize = 100;\n+        LogSegmentData logSegmentData = createLogSegmentData(segSize);\n+\n+        // Copy the segment\n+        rsm.copyLogSegmentData(rlsm, logSegmentData);\n+\n+        // Check fetch segments with invalid ranges like startPos < endPos\n+        Assertions.assertThrows(Exception.class, () -> rsm.fetchLogSegment(rlsm, 2, 1));\n+\n+        // Check fetch segments with invalid ranges like startPos or endPos as negative.\n+        Assertions.assertThrows(Exception.class, () -> rsm.fetchLogSegment(rlsm, -1, 0));\n+        Assertions.assertThrows(Exception.class, () -> rsm.fetchLogSegment(rlsm, -2, -1));\n+    }\n+\n+    @Test\n+    public void testDeleteSegment() throws Exception {\n+        InmemoryRemoteStorageManager rsm = new InmemoryRemoteStorageManager();\n+        RemoteLogSegmentMetadata rlsm = createRemoteLogSegmentMetadata();\n+        LogSegmentData logSegmentData = createLogSegmentData();\n+\n+        // Copy a log segment.\n+        rsm.copyLogSegmentData(rlsm, logSegmentData);\n+\n+        // Check that the copied segment exists in rsm and it is same.\n+        try (InputStream segmentStream = rsm.fetchLogSegment(rlsm, 0)) {\n+            checkContentSame(segmentStream, logSegmentData.logSegment().toPath());\n+        }\n+\n+        // Delete segment and check that it does not exist in RSM.\n+        rsm.deleteLogSegmentData(rlsm);\n+\n+        // Check that the segment data does not exist.\n+        Assertions.assertThrows(RemoteResourceNotFoundException.class, () -> rsm.fetchLogSegment(rlsm, 0));\n+\n+        // Check that the segment data does not exist for range.\n+        Assertions.assertThrows(RemoteResourceNotFoundException.class, () -> rsm.fetchLogSegment(rlsm, 0, 1));\n+\n+        // Check that all the indexes are not found.\n+        for (RemoteStorageManager.IndexType indexType : RemoteStorageManager.IndexType.values()) {\n+            Assertions.assertThrows(RemoteResourceNotFoundException.class, () -> rsm.fetchIndex(rlsm, indexType));\n+        }\n+    }\n+\n+    private void checkContentSame(InputStream segmentStream, Path path) throws IOException {\n+        byte[] segmentBytes = Files.readAllBytes(path);\n+        ByteBuffer byteBuffer = readAsByteBuffer(segmentStream, segmentBytes.length);\n+        Assertions.assertEquals(ByteBuffer.wrap(segmentBytes), byteBuffer);\n+    }\n+\n+    private ByteBuffer readAsByteBuffer(InputStream segmentStream,\n+                                        int len) throws IOException {\n+        ByteBuffer byteBuffer = ByteBuffer.wrap(new byte[len]);\n+        Utils.readFully(segmentStream, byteBuffer);\n+        byteBuffer.rewind();\n+        return byteBuffer;\n+    }\n+\n+    private LogSegmentData createLogSegmentData() throws Exception {\n+        return createLogSegmentData(100);\n+    }\n+\n+    private LogSegmentData createLogSegmentData(int segSize) throws Exception {\n+        int prefix = Math.abs(new Random().nextInt());\n+        File segment = new File(DIR, prefix + \".seg\");\n+        Files.write(segment.toPath(), TestUtils.randomBytes(segSize));\n+\n+        File offsetIndex = new File(DIR, prefix + \".oi\");\n+        Files.write(offsetIndex.toPath(), TestUtils.randomBytes(10));\n+\n+        File timeIndex = new File(DIR, prefix + \".ti\");\n+        Files.write(timeIndex.toPath(), TestUtils.randomBytes(10));\n+\n+        File txnIndex = new File(DIR, prefix + \".txni\");\n+        Files.write(txnIndex.toPath(), TestUtils.randomBytes(10));\n+\n+        File producerSnapshotIndex = new File(DIR, prefix + \".psi\");\n+        Files.write(producerSnapshotIndex.toPath(), TestUtils.randomBytes(10));\n+\n+        ByteBuffer leaderEpochIndex = ByteBuffer.wrap(TestUtils.randomBytes(10));\n+        return new LogSegmentData(segment, offsetIndex, timeIndex, txnIndex, producerSnapshotIndex, leaderEpochIndex);\n+    }\n+}"
  },
  {
    "sha": "d78408b583080f47861ba7606a47d089834b3f7f",
    "filename": "remote-storage/src/test/java/org/apache/kafka/server/log/remote/storage/RemoteLogMetadataCacheTest.java",
    "status": "added",
    "additions": 149,
    "deletions": 0,
    "changes": 149,
    "blob_url": "https://github.com/apache/kafka/blob/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/test/java/org/apache/kafka/server/log/remote/storage/RemoteLogMetadataCacheTest.java",
    "raw_url": "https://github.com/apache/kafka/raw/94ac96df7bd244e009d1d150dc62a5b1843e89b8/remote-storage/src/test/java/org/apache/kafka/server/log/remote/storage/RemoteLogMetadataCacheTest.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/remote-storage/src/test/java/org/apache/kafka/server/log/remote/storage/RemoteLogMetadataCacheTest.java?ref=94ac96df7bd244e009d1d150dc62a5b1843e89b8",
    "patch": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.server.log.remote.storage;\n+\n+import org.apache.kafka.common.TopicIdPartition;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.Uuid;\n+import org.junit.jupiter.api.Assertions;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+\n+public class RemoteLogMetadataCacheTest {\n+\n+    private static final TopicIdPartition TP0 = new TopicIdPartition(Uuid.randomUuid(),\n+            new TopicPartition(\"foo\", 0));\n+    private static final int SEG_SIZE = 1024 * 1024;\n+    private static final int BROKER_ID = 0;\n+\n+    @Test\n+    public void testCacheSegmentsWithDifferentStates() throws Exception {\n+        RemoteLogMetadataCache cache = new RemoteLogMetadataCache();\n+\n+        // Add segments with different states and check cache.remoteLogSegmentMetadata(int leaderEpoch, long offset)\n+        // cache.listRemoteLogSegments(int leaderEpoch), and cache.listAllRemoteLogSegments().\n+\n+        // =============================================================================================================\n+        // 1.Create a segment with state COPY_SEGMENT_STARTED, and check for searching that segment and listing the\n+        // segments.\n+        // ==============================================================================================================\n+        Map<Integer, Long> seg0leaderEpochs = Collections.singletonMap(0, 0L);\n+        RemoteLogSegmentId seg0Id = new RemoteLogSegmentId(TP0, Uuid.randomUuid());\n+        RemoteLogSegmentMetadata segCopyInProgress = new RemoteLogSegmentMetadata(seg0Id, 0L, 50L, -1L, BROKER_ID,\n+                System.currentTimeMillis(), SEG_SIZE, seg0leaderEpochs);\n+        cache.addToInProgress(segCopyInProgress);\n+\n+        // This segment should not be available as the state is not reached to COPY_SEGMENT_FINISHED.\n+        Optional<RemoteLogSegmentMetadata> seg0s0e0 = cache.remoteLogSegmentMetadata(0, 0);\n+        Assertions.assertFalse(seg0s0e0.isPresent());\n+\n+        // cache.listRemoteLogSegments(0) should not contain the above segment, it will be empty.\n+        Assertions.assertFalse(cache.listRemoteLogSegments(0).hasNext());\n+        // But cache.listRemoteLogSegments() should contain the above segment.\n+        checkContainsAll(cache.listAllRemoteLogSegments(), Collections.singletonList(segCopyInProgress));\n+\n+        // =============================================================================================================\n+        // 2.Create a segment and move it to state COPY_SEGMENT_FINISHED. and check for searching that segment and\n+        // listing the segments.\n+        // ==============================================================================================================\n+        Map<Integer, Long> seg1leaderEpochs = Collections.singletonMap(0, 101L);\n+        RemoteLogSegmentId seg1Id = new RemoteLogSegmentId(TP0, Uuid.randomUuid());\n+        RemoteLogSegmentMetadata seg1 = new RemoteLogSegmentMetadata(seg1Id, 101L, 200L, -1L, BROKER_ID,\n+                System.currentTimeMillis(), SEG_SIZE, seg1leaderEpochs);\n+        cache.addToInProgress(seg1);\n+        RemoteLogSegmentMetadataUpdate seg1Update = new RemoteLogSegmentMetadataUpdate(seg1Id,\n+                System.currentTimeMillis(), RemoteLogSegmentState.COPY_SEGMENT_FINISHED, BROKER_ID);\n+        cache.updateRemoteLogSegmentMetadata(seg1Update);\n+        RemoteLogSegmentMetadata segCopyFinished = seg1.createRemoteLogSegmentWithUpdates(seg1Update);\n+\n+        // Search should return the above segment.\n+        Optional<RemoteLogSegmentMetadata> seg1S150 = cache.remoteLogSegmentMetadata(0, 150);\n+        Assertions.assertEquals(seg1.createRemoteLogSegmentWithUpdates(seg1Update), seg1S150.orElse(null));\n+\n+        // cache.listRemoteLogSegments(0) should not contain the above segment.\n+        checkContainsAll(cache.listRemoteLogSegments(0), Collections.singletonList(segCopyFinished));\n+        // But cache.listRemoteLogSegments() should contain both the segments.\n+        checkContainsAll(cache.listAllRemoteLogSegments(), Arrays.asList(segCopyInProgress, segCopyFinished));\n+\n+        // =============================================================================================================\n+        // 3.Create a segment and move it to state DELETE_SEGMENT_STARTED, and check for searching that segment and\n+        // listing the segments.\n+        // ==============================================================================================================\n+        Map<Integer, Long> seg2leaderEpochs = Collections.singletonMap(0, 201L);\n+        RemoteLogSegmentId seg2Id = new RemoteLogSegmentId(TP0, Uuid.randomUuid());\n+        RemoteLogSegmentMetadata seg2 = new RemoteLogSegmentMetadata(seg2Id, 201L, 300L, -1L, BROKER_ID,\n+                System.currentTimeMillis(), SEG_SIZE, seg2leaderEpochs);\n+        cache.addToInProgress(seg2);\n+        RemoteLogSegmentMetadataUpdate seg2Update = new RemoteLogSegmentMetadataUpdate(seg2Id,\n+                System.currentTimeMillis(), RemoteLogSegmentState.DELETE_SEGMENT_STARTED, BROKER_ID);\n+        cache.updateRemoteLogSegmentMetadata(seg2Update);\n+        RemoteLogSegmentMetadata segDeleteStarted = seg2.createRemoteLogSegmentWithUpdates(seg2Update);\n+\n+        // Search should return the above segment.\n+        Optional<RemoteLogSegmentMetadata> seg2S250 = cache.remoteLogSegmentMetadata(0, 250);\n+        Assertions.assertEquals(seg2.createRemoteLogSegmentWithUpdates(seg2Update), seg2S250.orElse(null));\n+\n+        // cache.listRemoteLogSegments(0) should contain the above segment.\n+        checkContainsAll(cache.listRemoteLogSegments(0), Arrays.asList(segCopyFinished, segDeleteStarted));\n+        // But cache.listRemoteLogSegments() should contain all the segments.\n+        checkContainsAll(cache.listAllRemoteLogSegments(),\n+                Arrays.asList(segCopyInProgress, segCopyFinished, segDeleteStarted));\n+\n+        // =============================================================================================================\n+        // 4.Create a segment and move it to state DELETE_SEGMENT_FINISHED, and check for searching that segment and\n+        // listing the segments.\n+        // ==============================================================================================================\n+        Map<Integer, Long> seg3leaderEpochs = Collections.singletonMap(0, 301L);\n+        RemoteLogSegmentId seg3Id = new RemoteLogSegmentId(TP0, Uuid.randomUuid());\n+        RemoteLogSegmentMetadata seg3 = new RemoteLogSegmentMetadata(seg3Id, 301L, 400L, -1L, BROKER_ID,\n+                System.currentTimeMillis(), SEG_SIZE, seg3leaderEpochs);\n+        cache.addToInProgress(seg3);\n+        RemoteLogSegmentMetadataUpdate seg3Update1 = new RemoteLogSegmentMetadataUpdate(seg3Id,\n+                System.currentTimeMillis(), RemoteLogSegmentState.DELETE_SEGMENT_STARTED, BROKER_ID);\n+        cache.updateRemoteLogSegmentMetadata(seg3Update1);\n+\n+        // Search should return the above segment.\n+        Optional<RemoteLogSegmentMetadata> seg3S350 = cache.remoteLogSegmentMetadata(0, 350);\n+        Assertions.assertEquals(seg3.createRemoteLogSegmentWithUpdates(seg3Update1), seg3S350.orElse(null));\n+\n+        RemoteLogSegmentMetadataUpdate seg3Update2 = new RemoteLogSegmentMetadataUpdate(seg3Id,\n+                System.currentTimeMillis(), RemoteLogSegmentState.DELETE_SEGMENT_FINISHED, BROKER_ID);\n+        cache.updateRemoteLogSegmentMetadata(seg3Update2);\n+\n+        // cache.listRemoteLogSegments(0) should not contain the above segment.\n+        checkContainsAll(cache.listRemoteLogSegments(0), Arrays.asList(segCopyFinished, segDeleteStarted));\n+        // But cache.listRemoteLogSegments() should not contain both the segments as it should have been removed.\n+        checkContainsAll(cache.listAllRemoteLogSegments(),\n+                Arrays.asList(segCopyInProgress, segCopyFinished, segDeleteStarted));\n+    }\n+\n+    private void checkContainsAll(Iterator<RemoteLogSegmentMetadata> allSegments,\n+                                  List<RemoteLogSegmentMetadata> expectedSegments) {\n+        Set<RemoteLogSegmentMetadata> set = new HashSet<>();\n+        allSegments.forEachRemaining(metadata -> set.add(metadata));\n+        Assertions.assertTrue(set.containsAll(expectedSegments));\n+    }\n+\n+}"
  },
  {
    "sha": "c9a85e652a9690392f7515b6241870c896f81227",
    "filename": "settings.gradle",
    "status": "modified",
    "additions": 1,
    "deletions": 0,
    "changes": 1,
    "blob_url": "https://github.com/apache/kafka/blob/94ac96df7bd244e009d1d150dc62a5b1843e89b8/settings.gradle",
    "raw_url": "https://github.com/apache/kafka/raw/94ac96df7bd244e009d1d150dc62a5b1843e89b8/settings.gradle",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/settings.gradle?ref=94ac96df7bd244e009d1d150dc62a5b1843e89b8",
    "patch": "@@ -29,6 +29,7 @@ include 'clients',\n     'log4j-appender',\n     'metadata',\n     'raft',\n+    'remote-storage',\n     'shell',\n     'streams',\n     'streams:examples',"
  }
]
