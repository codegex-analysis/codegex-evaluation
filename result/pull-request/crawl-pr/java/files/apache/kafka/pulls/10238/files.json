[
  {
    "sha": "fffe71a9db451a93503be0e8e520257d1014f959",
    "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/apache/kafka/blob/931fc4f610db5162b05497c8e0278494e8195b32/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java",
    "raw_url": "https://github.com/apache/kafka/raw/931fc4f610db5162b05497c8e0278494e8195b32/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java?ref=931fc4f610db5162b05497c8e0278494e8195b32",
    "patch": "@@ -626,7 +626,7 @@ private WorkerTask buildWorkerTask(ClusterConfigState configState,\n             // Note we pass the configState as it performs dynamic transformations under the covers\n             return new WorkerSourceTask(id, (SourceTask) task, statusListener, initialState, keyConverter, valueConverter,\n                     headerConverter, transformationChain, producer, admin, topicCreationGroups,\n-                    offsetReader, offsetWriter, config, configState, metrics, loader, time, retryWithToleranceOperator, herder.statusBackingStore());\n+                    offsetReader, offsetWriter, config, configState, metrics, loader, time, retryWithToleranceOperator, herder.statusBackingStore(), executor);\n         } else if (task instanceof SinkTask) {\n             TransformationChain<SinkRecord> transformationChain = new TransformationChain<>(connConfig.<SinkRecord>transformations(), retryWithToleranceOperator);\n             log.info(\"Initializing: {}\", transformationChain);"
  },
  {
    "sha": "48660f3b332333a80f3547c4540282313afb0b6c",
    "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java",
    "status": "modified",
    "additions": 29,
    "deletions": 9,
    "changes": 38,
    "blob_url": "https://github.com/apache/kafka/blob/931fc4f610db5162b05497c8e0278494e8195b32/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java",
    "raw_url": "https://github.com/apache/kafka/raw/931fc4f610db5162b05497c8e0278494e8195b32/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java?ref=931fc4f610db5162b05497c8e0278494e8195b32",
    "patch": "@@ -60,6 +60,7 @@\n import java.util.Map;\n import java.util.concurrent.CountDownLatch;\n import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Executor;\n import java.util.concurrent.Future;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.TimeoutException;\n@@ -86,6 +87,7 @@\n     private final TopicAdmin admin;\n     private final CloseableOffsetStorageReader offsetReader;\n     private final OffsetStorageWriter offsetWriter;\n+    private final Executor closeExecutor;\n     private final SourceTaskMetricsGroup sourceTaskMetricsGroup;\n     private final AtomicReference<Exception> producerSendException;\n     private final boolean isTopicTrackingEnabled;\n@@ -123,7 +125,8 @@ public WorkerSourceTask(ConnectorTaskId id,\n                             ClassLoader loader,\n                             Time time,\n                             RetryWithToleranceOperator retryWithToleranceOperator,\n-                            StatusBackingStore statusBackingStore) {\n+                            StatusBackingStore statusBackingStore,\n+                            Executor closeExecutor) {\n \n         super(id, statusListener, initialState, loader, connectMetrics,\n                 retryWithToleranceOperator, time, statusBackingStore);\n@@ -139,6 +142,7 @@ public WorkerSourceTask(ConnectorTaskId id,\n         this.admin = admin;\n         this.offsetReader = offsetReader;\n         this.offsetWriter = offsetWriter;\n+        this.closeExecutor = closeExecutor;\n \n         this.toSend = null;\n         this.lastSendFailed = false;\n@@ -171,13 +175,9 @@ protected void close() {\n                 log.warn(\"Could not stop task\", t);\n             }\n         }\n-        if (producer != null) {\n-            try {\n-                producer.close(Duration.ofSeconds(30));\n-            } catch (Throwable t) {\n-                log.warn(\"Could not close producer\", t);\n-            }\n-        }\n+\n+        closeProducer(Duration.ofSeconds(30));\n+\n         if (admin != null) {\n             try {\n                 admin.close(Duration.ofSeconds(30));\n@@ -202,6 +202,14 @@ public void removeMetrics() {\n     public void cancel() {\n         super.cancel();\n         offsetReader.close();\n+        // We proactively close the producer here as the main work thread for the task may\n+        // be blocked indefinitely in a call to Producer::send if automatic topic creation is\n+        // not enabled on either the connector or the Kafka cluster. Closing the producer should\n+        // unblock it in that case and allow shutdown to proceed normally.\n+        // With a duration of 0, the producer's own shutdown logic should be fairly quick,\n+        // but closing user-pluggable classes like interceptors may lag indefinitely. So, we\n+        // call close on a separate thread in order to avoid blocking the herder's tick thread.\n+        closeExecutor.execute(() -> closeProducer(Duration.ZERO));\n     }\n \n     @Override\n@@ -259,6 +267,16 @@ public void execute() {\n         }\n     }\n \n+    private void closeProducer(Duration duration) {\n+        if (producer != null) {\n+            try {\n+                producer.close(duration);\n+            } catch (Throwable t) {\n+                log.warn(\"Could not close producer for {}\", id, t);\n+            }\n+        }\n+    }\n+\n     private void maybeThrowProducerSendException() {\n         if (producerSendException.get() != null) {\n             throw new ConnectException(\n@@ -488,7 +506,9 @@ public boolean commitOffsets() {\n             while (!outstandingMessages.isEmpty()) {\n                 try {\n                     long timeoutMs = timeout - time.milliseconds();\n-                    if (timeoutMs <= 0) {\n+                    // If the task has been cancelled, no more records will be sent from the producer; in that case, if any outstanding messages remain,\n+                    // we can stop flushing immediately\n+                    if (isCancelled() || timeoutMs <= 0) {\n                         log.error(\"{} Failed to flush, timed out while waiting for producer to flush outstanding {} messages\", this, outstandingMessages.size());\n                         finishFailedFlush();\n                         recordCommitFailure(time.milliseconds() - started, null);"
  },
  {
    "sha": "a717af23432205f6b41b2b2e59b52d68ce905dfd",
    "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerTask.java",
    "status": "modified",
    "additions": 4,
    "deletions": 0,
    "changes": 4,
    "blob_url": "https://github.com/apache/kafka/blob/931fc4f610db5162b05497c8e0278494e8195b32/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerTask.java",
    "raw_url": "https://github.com/apache/kafka/raw/931fc4f610db5162b05497c8e0278494e8195b32/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerTask.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerTask.java?ref=931fc4f610db5162b05497c8e0278494e8195b32",
    "patch": "@@ -156,6 +156,10 @@ protected boolean isStopping() {\n         return stopping;\n     }\n \n+    protected boolean isCancelled() {\n+        return cancelled;\n+    }\n+\n     private void doClose() {\n         try {\n             close();"
  },
  {
    "sha": "5cd794e7c83b1d0c185a22d30126c27ac78e86e6",
    "filename": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/ConnectWorkerIntegrationTest.java",
    "status": "modified",
    "additions": 41,
    "deletions": 4,
    "changes": 45,
    "blob_url": "https://github.com/apache/kafka/blob/931fc4f610db5162b05497c8e0278494e8195b32/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ConnectWorkerIntegrationTest.java",
    "raw_url": "https://github.com/apache/kafka/raw/931fc4f610db5162b05497c8e0278494e8195b32/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ConnectWorkerIntegrationTest.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ConnectWorkerIntegrationTest.java?ref=931fc4f610db5162b05497c8e0278494e8195b32",
    "patch": "@@ -65,24 +65,27 @@\n     private static final long OFFSET_COMMIT_INTERVAL_MS = TimeUnit.SECONDS.toMillis(30);\n     private static final int NUM_WORKERS = 3;\n     private static final int NUM_TASKS = 4;\n+    private static final int MESSAGES_PER_POLL = 10;\n     private static final String CONNECTOR_NAME = \"simple-source\";\n     private static final String TOPIC_NAME = \"test-topic\";\n \n     private EmbeddedConnectCluster.Builder connectBuilder;\n     private EmbeddedConnectCluster connect;\n-    Map<String, String> workerProps = new HashMap<>();\n-    Properties brokerProps = new Properties();\n+    private Map<String, String> workerProps;\n+    private Properties brokerProps;\n \n     @Rule\n     public TestRule watcher = ConnectIntegrationTestUtils.newTestWatcher(log);\n \n     @Before\n     public void setup() {\n         // setup Connect worker properties\n+        workerProps = new HashMap<>();\n         workerProps.put(OFFSET_COMMIT_INTERVAL_MS_CONFIG, String.valueOf(OFFSET_COMMIT_INTERVAL_MS));\n         workerProps.put(CONNECTOR_CLIENT_POLICY_CLASS_CONFIG, \"All\");\n \n         // setup Kafka broker properties\n+        brokerProps = new Properties();\n         brokerProps.put(\"auto.create.topics.enable\", String.valueOf(false));\n \n         // build a Connect cluster backed by Kafka and Zk\n@@ -288,14 +291,48 @@ public void testTaskStatuses() throws Exception {\n                 decreasedNumTasks, \"Connector task statuses did not update in time.\");\n     }\n \n+    @Test\n+    public void testSourceTaskNotBlockedOnShutdownWithNonExistentTopic() throws Exception {\n+        // When automatic topic creation is disabled on the broker\n+        brokerProps.put(\"auto.create.topics.enable\", \"false\");\n+        connect = connectBuilder\n+            .brokerProps(brokerProps)\n+            .numWorkers(1)\n+            .numBrokers(1)\n+            .build();\n+        connect.start();\n+\n+        connect.assertions().assertAtLeastNumWorkersAreUp(1, \"Initial group of workers did not start in time.\");\n+\n+        // and when the connector is not configured to create topics\n+        Map<String, String> props = defaultSourceConnectorProps(\"nonexistenttopic\");\n+        props.remove(DEFAULT_TOPIC_CREATION_PREFIX + REPLICATION_FACTOR_CONFIG);\n+        props.remove(DEFAULT_TOPIC_CREATION_PREFIX + PARTITIONS_CONFIG);\n+        props.put(\"throughput\", \"-1\");\n+\n+        ConnectorHandle connector = RuntimeHandles.get().connectorHandle(CONNECTOR_NAME);\n+        connector.expectedRecords(NUM_TASKS * MESSAGES_PER_POLL);\n+        connect.configureConnector(CONNECTOR_NAME, props);\n+        connect.assertions().assertConnectorAndExactlyNumTasksAreRunning(CONNECTOR_NAME,\n+            NUM_TASKS, \"Connector tasks did not start in time\");\n+        connector.awaitRecords(TimeUnit.MINUTES.toMillis(1));\n+\n+        // Then if we delete the connector, it and each of its tasks should be stopped by the framework\n+        // even though the producer is blocked because there is no topic\n+        StartAndStopLatch stopCounter = connector.expectedStops(1);\n+        connect.deleteConnector(CONNECTOR_NAME);\n+\n+        assertTrue(\"Connector and all tasks were not stopped in time\", stopCounter.await(1, TimeUnit.MINUTES));\n+    }\n+\n     private Map<String, String> defaultSourceConnectorProps(String topic) {\n         // setup up props for the source connector\n         Map<String, String> props = new HashMap<>();\n         props.put(CONNECTOR_CLASS_CONFIG, MonitorableSourceConnector.class.getSimpleName());\n         props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));\n         props.put(TOPIC_CONFIG, topic);\n-        props.put(\"throughput\", String.valueOf(10));\n-        props.put(\"messages.per.poll\", String.valueOf(10));\n+        props.put(\"throughput\", \"10\");\n+        props.put(\"messages.per.poll\", String.valueOf(MESSAGES_PER_POLL));\n         props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());\n         props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());\n         props.put(DEFAULT_TOPIC_CREATION_PREFIX + REPLICATION_FACTOR_CONFIG, String.valueOf(1));"
  },
  {
    "sha": "ffc9e7a7a4fb093e72278c1b4bd3899853bb8c03",
    "filename": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/ConnectorHandle.java",
    "status": "modified",
    "additions": 4,
    "deletions": 4,
    "changes": 8,
    "blob_url": "https://github.com/apache/kafka/blob/931fc4f610db5162b05497c8e0278494e8195b32/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ConnectorHandle.java",
    "raw_url": "https://github.com/apache/kafka/raw/931fc4f610db5162b05497c8e0278494e8195b32/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ConnectorHandle.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ConnectorHandle.java?ref=931fc4f610db5162b05497c8e0278494e8195b32",
    "patch": "@@ -292,8 +292,8 @@ public StartAndStopLatch expectedStarts(int expectedStarts, boolean includeTasks\n      * {@link StartAndStopLatch#await(long, TimeUnit)} to wait up to a specified duration for the\n      * connector and all tasks to be started at least the specified number of times.\n      *\n-     * <p>This method does not track the number of times the connector and tasks are stopped, and\n-     * only tracks the number of times the connector and tasks are <em>started</em>.\n+     * <p>This method does not track the number of times the connector and tasks are started, and\n+     * only tracks the number of times the connector and tasks are <em>stopped</em>.\n      *\n      * @param expectedStops the minimum number of starts that are expected once this method is\n      *                      called\n@@ -315,8 +315,8 @@ public StartAndStopLatch expectedStops(int expectedStops) {\n      * {@link StartAndStopLatch#await(long, TimeUnit)} to wait up to a specified duration for the\n      * connector and all tasks to be started at least the specified number of times.\n      *\n-     * <p>This method does not track the number of times the connector and tasks are stopped, and\n-     * only tracks the number of times the connector and tasks are <em>started</em>.\n+     * <p>This method does not track the number of times the connector and tasks are started, and\n+     * only tracks the number of times the connector and tasks are <em>stopped</em>.\n      *\n      * @param expectedStops the minimum number of starts that are expected once this method is\n      *                      called"
  },
  {
    "sha": "aaada374cb2b6a30d6529ef1a54f9b846a1c11d1",
    "filename": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/MonitorableSourceConnector.java",
    "status": "modified",
    "additions": 1,
    "deletions": 0,
    "changes": 1,
    "blob_url": "https://github.com/apache/kafka/blob/931fc4f610db5162b05497c8e0278494e8195b32/connect/runtime/src/test/java/org/apache/kafka/connect/integration/MonitorableSourceConnector.java",
    "raw_url": "https://github.com/apache/kafka/raw/931fc4f610db5162b05497c8e0278494e8195b32/connect/runtime/src/test/java/org/apache/kafka/connect/integration/MonitorableSourceConnector.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/test/java/org/apache/kafka/connect/integration/MonitorableSourceConnector.java?ref=931fc4f610db5162b05497c8e0278494e8195b32",
    "patch": "@@ -130,6 +130,7 @@ public void start(Map<String, String> props) {\n                     throttler.throttle();\n                 }\n                 taskHandle.record(batchSize);\n+                log.info(\"Returning batch of {} records\", batchSize);\n                 return LongStream.range(0, batchSize)\n                         .mapToObj(i -> new SourceRecord(\n                                 Collections.singletonMap(\"task.id\", taskId),"
  },
  {
    "sha": "daa9eddac9ca93393d07616f749b57233fcfac43",
    "filename": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskTest.java",
    "status": "modified",
    "additions": 2,
    "deletions": 1,
    "changes": 3,
    "blob_url": "https://github.com/apache/kafka/blob/931fc4f610db5162b05497c8e0278494e8195b32/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskTest.java",
    "raw_url": "https://github.com/apache/kafka/raw/931fc4f610db5162b05497c8e0278494e8195b32/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskTest.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskTest.java?ref=931fc4f610db5162b05497c8e0278494e8195b32",
    "patch": "@@ -77,6 +77,7 @@\n import java.util.Collections;\n import java.util.HashMap;\n import java.util.Map;\n+import java.util.concurrent.Executor;\n \n import static java.util.Collections.emptyMap;\n import static java.util.Collections.singletonList;\n@@ -563,7 +564,7 @@ private void createSourceTask(TargetState initialState, RetryWithToleranceOperat\n                 producer, admin, null,\n                 offsetReader, offsetWriter, workerConfig,\n                 ClusterConfigState.EMPTY, metrics, pluginLoader, time, retryWithToleranceOperator,\n-                statusBackingStore);\n+                statusBackingStore, (Executor) Runnable::run);\n     }\n \n     private ConsumerRecords<byte[], byte[]> records(ConsumerRecord<byte[], byte[]> record) {"
  },
  {
    "sha": "5cdfab96d22ee24ce0e16abd66902f28064a1bd0",
    "filename": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskWithTopicCreationTest.java",
    "status": "modified",
    "additions": 2,
    "deletions": 1,
    "changes": 3,
    "blob_url": "https://github.com/apache/kafka/blob/931fc4f610db5162b05497c8e0278494e8195b32/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskWithTopicCreationTest.java",
    "raw_url": "https://github.com/apache/kafka/raw/931fc4f610db5162b05497c8e0278494e8195b32/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskWithTopicCreationTest.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskWithTopicCreationTest.java?ref=931fc4f610db5162b05497c8e0278494e8195b32",
    "patch": "@@ -80,6 +80,7 @@\n import java.util.HashMap;\n import java.util.Map;\n import java.util.Set;\n+import java.util.concurrent.Executor;\n \n import static java.util.Collections.emptyMap;\n import static java.util.Collections.singletonList;\n@@ -581,7 +582,7 @@ private void createSourceTask(TargetState initialState, RetryWithToleranceOperat\n                 producer, admin, TopicCreationGroup.configuredGroups(sourceConfig),\n                 offsetReader, offsetWriter, workerConfig,\n                 ClusterConfigState.EMPTY, metrics, pluginLoader, time, retryWithToleranceOperator,\n-                statusBackingStore);\n+                statusBackingStore, (Executor) Runnable::run);\n     }\n \n     private ConsumerRecords<byte[], byte[]> records(ConsumerRecord<byte[], byte[]> record) {"
  },
  {
    "sha": "0f9e6562fb99f5ee6a6103b53c7a445ea3d8ae1d",
    "filename": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskTest.java",
    "status": "modified",
    "additions": 4,
    "deletions": 1,
    "changes": 5,
    "blob_url": "https://github.com/apache/kafka/blob/931fc4f610db5162b05497c8e0278494e8195b32/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskTest.java",
    "raw_url": "https://github.com/apache/kafka/raw/931fc4f610db5162b05497c8e0278494e8195b32/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskTest.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskTest.java?ref=931fc4f610db5162b05497c8e0278494e8195b32",
    "patch": "@@ -197,7 +197,7 @@ private void createWorkerTask(TargetState initialState, Converter keyConverter,\n         workerTask = new WorkerSourceTask(taskId, sourceTask, statusListener, initialState, keyConverter, valueConverter, headerConverter,\n                 transformationChain, producer, admin, null,\n                 offsetReader, offsetWriter, config, clusterConfigState, metrics, plugins.delegatingLoader(), Time.SYSTEM,\n-                RetryWithToleranceOperatorTest.NOOP_OPERATOR, statusBackingStore);\n+                RetryWithToleranceOperatorTest.NOOP_OPERATOR, statusBackingStore, Runnable::run);\n     }\n \n     @Test\n@@ -710,6 +710,9 @@ public void testCancel() {\n         offsetReader.close();\n         PowerMock.expectLastCall();\n \n+        producer.close(Duration.ZERO);\n+        PowerMock.expectLastCall();\n+\n         PowerMock.replayAll();\n \n         workerTask.cancel();"
  },
  {
    "sha": "d26faa40707dd72cc46017d9ed3d1a99a3932598",
    "filename": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskWithTopicCreationTest.java",
    "status": "modified",
    "additions": 4,
    "deletions": 1,
    "changes": 5,
    "blob_url": "https://github.com/apache/kafka/blob/931fc4f610db5162b05497c8e0278494e8195b32/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskWithTopicCreationTest.java",
    "raw_url": "https://github.com/apache/kafka/raw/931fc4f610db5162b05497c8e0278494e8195b32/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskWithTopicCreationTest.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskWithTopicCreationTest.java?ref=931fc4f610db5162b05497c8e0278494e8195b32",
    "patch": "@@ -222,7 +222,7 @@ private void createWorkerTask(TargetState initialState, Converter keyConverter,\n         workerTask = new WorkerSourceTask(taskId, sourceTask, statusListener, initialState, keyConverter, valueConverter, headerConverter,\n                 transformationChain, producer, admin, TopicCreationGroup.configuredGroups(sourceConfig),\n                 offsetReader, offsetWriter, config, clusterConfigState, metrics, plugins.delegatingLoader(), Time.SYSTEM,\n-                RetryWithToleranceOperatorTest.NOOP_OPERATOR, statusBackingStore);\n+                RetryWithToleranceOperatorTest.NOOP_OPERATOR, statusBackingStore, Runnable::run);\n     }\n \n     @Test\n@@ -754,6 +754,9 @@ public void testCancel() {\n         offsetReader.close();\n         PowerMock.expectLastCall();\n \n+        producer.close(Duration.ZERO);\n+        PowerMock.expectLastCall();\n+\n         PowerMock.replayAll();\n \n         workerTask.cancel();"
  },
  {
    "sha": "f3c9167ae4d7d81f95d6fe4f80fba89b21bb1def",
    "filename": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerTest.java",
    "status": "modified",
    "additions": 3,
    "deletions": 1,
    "changes": 4,
    "blob_url": "https://github.com/apache/kafka/blob/931fc4f610db5162b05497c8e0278494e8195b32/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerTest.java",
    "raw_url": "https://github.com/apache/kafka/raw/931fc4f610db5162b05497c8e0278494e8195b32/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerTest.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerTest.java?ref=931fc4f610db5162b05497c8e0278494e8195b32",
    "patch": "@@ -94,6 +94,7 @@\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ConcurrentMap;\n import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Executor;\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.TimeUnit;\n \n@@ -1482,7 +1483,8 @@ private void expectNewWorkerTask() throws Exception {\n                 EasyMock.eq(pluginLoader),\n                 anyObject(Time.class),\n                 anyObject(RetryWithToleranceOperator.class),\n-                anyObject(StatusBackingStore.class))\n+                anyObject(StatusBackingStore.class),\n+                anyObject(Executor.class))\n                 .andReturn(workerTask);\n     }\n     /* Name here needs to be unique as we are testing the aliasing mechanism */"
  },
  {
    "sha": "f0b959efb2ff5dd16c65a524df315a0a00fef344",
    "filename": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerWithTopicCreationTest.java",
    "status": "modified",
    "additions": 3,
    "deletions": 1,
    "changes": 4,
    "blob_url": "https://github.com/apache/kafka/blob/931fc4f610db5162b05497c8e0278494e8195b32/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerWithTopicCreationTest.java",
    "raw_url": "https://github.com/apache/kafka/raw/931fc4f610db5162b05497c8e0278494e8195b32/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerWithTopicCreationTest.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerWithTopicCreationTest.java?ref=931fc4f610db5162b05497c8e0278494e8195b32",
    "patch": "@@ -85,6 +85,7 @@\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ConcurrentMap;\n import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Executor;\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.TimeUnit;\n \n@@ -1390,7 +1391,8 @@ private void expectNewWorkerTask() throws Exception {\n                 EasyMock.eq(pluginLoader),\n                 anyObject(Time.class),\n                 anyObject(RetryWithToleranceOperator.class),\n-                anyObject(StatusBackingStore.class))\n+                anyObject(StatusBackingStore.class),\n+                anyObject(Executor.class))\n                 .andReturn(workerTask);\n     }\n "
  }
]
