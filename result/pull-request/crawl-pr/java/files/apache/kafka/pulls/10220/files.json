[
  {
    "sha": "7d6374c5b0adc91f0cd10853f5d3201c210423f5",
    "filename": "checkstyle/import-control-core.xml",
    "status": "modified",
    "additions": 16,
    "deletions": 0,
    "changes": 16,
    "blob_url": "https://github.com/apache/kafka/blob/b028ae999e53a8a78ee6fc59f4410740978e4c91/checkstyle/import-control-core.xml",
    "raw_url": "https://github.com/apache/kafka/raw/b028ae999e53a8a78ee6fc59f4410740978e4c91/checkstyle/import-control-core.xml",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/checkstyle/import-control-core.xml?ref=b028ae999e53a8a78ee6fc59f4410740978e4c91",
    "patch": "@@ -43,6 +43,19 @@\n   <disallow class=\"com.yammer.metrics.Metrics\" />\n   <allow pkg=\"com.yammer.metrics\"/>\n \n+  <subpackage name=\"testkit\">\n+    <allow pkg=\"kafka.metrics\"/>\n+    <allow pkg=\"kafka.raft\"/>\n+    <allow pkg=\"kafka.server\"/>\n+    <allow pkg=\"kafka.tools\"/>\n+    <allow pkg=\"org.apache.kafka.clients\"/>\n+    <allow pkg=\"org.apache.kafka.controller\"/>\n+    <allow pkg=\"org.apache.kafka.raft\"/>\n+    <allow pkg=\"org.apache.kafka.test\"/>\n+    <allow pkg=\"org.apache.kafka.metadata\" />\n+    <allow pkg=\"org.apache.kafka.metalog\" />\n+  </subpackage>\n+\n   <subpackage name=\"tools\">\n     <allow pkg=\"org.apache.kafka.clients.admin\" />\n     <allow pkg=\"kafka.admin\" />\n@@ -73,6 +86,9 @@\n     </subpackage>\n     <subpackage name=\"junit\">\n       <allow pkg=\"kafka.test\"/>\n+      <allow pkg=\"kafka.testkit\"/>\n+      <allow pkg=\"org.apache.kafka.clients\"/>\n+      <allow pkg=\"org.apache.kafka.metadata\" />\n     </subpackage>\n   </subpackage>\n </import-control>"
  },
  {
    "sha": "db578d2fcd78ee9efd34391e4473b0015581aa73",
    "filename": "core/src/main/scala/kafka/raft/RaftManager.scala",
    "status": "modified",
    "additions": 3,
    "deletions": 2,
    "changes": 5,
    "blob_url": "https://github.com/apache/kafka/blob/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/main/scala/kafka/raft/RaftManager.scala",
    "raw_url": "https://github.com/apache/kafka/raw/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/main/scala/kafka/raft/RaftManager.scala",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/raft/RaftManager.scala?ref=b028ae999e53a8a78ee6fc59f4410740978e4c91",
    "patch": "@@ -110,7 +110,8 @@ class KafkaRaftManager[T](\n   topicPartition: TopicPartition,\n   time: Time,\n   metrics: Metrics,\n-  threadNamePrefixOpt: Option[String]\n+  threadNamePrefixOpt: Option[String],\n+  val controllerQuorumVotersFuture: CompletableFuture[util.Map[Integer, AddressSpec]]\n ) extends RaftManager[T] with Logging {\n \n   private val raftConfig = new RaftConfig(config)\n@@ -131,7 +132,7 @@ class KafkaRaftManager[T](\n \n   def startup(): Unit = {\n     // Update the voter endpoints (if valid) with what's in RaftConfig\n-    val voterAddresses: util.Map[Integer, AddressSpec] = raftConfig.quorumVoterConnections\n+    val voterAddresses: util.Map[Integer, AddressSpec] = controllerQuorumVotersFuture.get()\n     for (voterAddressEntry <- voterAddresses.entrySet.asScala) {\n       voterAddressEntry.getValue match {\n         case spec: InetAddressSpec =>"
  },
  {
    "sha": "3c6139cc964cf00811470483ab8d70150d0a96e3",
    "filename": "core/src/main/scala/kafka/server/BrokerServer.scala",
    "status": "modified",
    "additions": 3,
    "deletions": 2,
    "changes": 5,
    "blob_url": "https://github.com/apache/kafka/blob/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/main/scala/kafka/server/BrokerServer.scala",
    "raw_url": "https://github.com/apache/kafka/raw/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/main/scala/kafka/server/BrokerServer.scala",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/server/BrokerServer.scala?ref=b028ae999e53a8a78ee6fc59f4410740978e4c91",
    "patch": "@@ -45,6 +45,7 @@ import org.apache.kafka.common.{ClusterResource, Endpoint, KafkaException}\n import org.apache.kafka.metadata.{BrokerState, VersionRange}\n import org.apache.kafka.metalog.MetaLogManager\n import org.apache.kafka.raft.RaftConfig\n+import org.apache.kafka.raft.RaftConfig.AddressSpec\n import org.apache.kafka.server.authorizer.Authorizer\n \n import scala.collection.{Map, Seq}\n@@ -61,7 +62,7 @@ class BrokerServer(\n                     val metrics: Metrics,\n                     val threadNamePrefix: Option[String],\n                     val initialOfflineDirs: Seq[String],\n-                    val controllerQuorumVotersFuture: CompletableFuture[util.List[String]],\n+                    val controllerQuorumVotersFuture: CompletableFuture[util.Map[Integer, AddressSpec]],\n                     val supportedFeatures: util.Map[String, VersionRange]\n                   ) extends KafkaBroker {\n \n@@ -178,7 +179,7 @@ class BrokerServer(\n       tokenCache = new DelegationTokenCache(ScramMechanism.mechanismNames)\n       credentialProvider = new CredentialProvider(ScramMechanism.mechanismNames, tokenCache)\n \n-      val controllerNodes = RaftConfig.quorumVoterStringsToNodes(controllerQuorumVotersFuture.get()).asScala\n+      val controllerNodes = RaftConfig.voterConnectionsToNodes(controllerQuorumVotersFuture.get()).asScala\n       val controllerNodeProvider = RaftControllerNodeProvider(metaLogManager, config, controllerNodes)\n \n       clientToControllerChannelManager = BrokerToControllerChannelManager("
  },
  {
    "sha": "dc7fb4aeda5e5b17f9b3619a1090e9482f0bac7b",
    "filename": "core/src/main/scala/kafka/server/ControllerServer.scala",
    "status": "modified",
    "additions": 3,
    "deletions": 3,
    "changes": 6,
    "blob_url": "https://github.com/apache/kafka/blob/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/main/scala/kafka/server/ControllerServer.scala",
    "raw_url": "https://github.com/apache/kafka/raw/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/main/scala/kafka/server/ControllerServer.scala",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/server/ControllerServer.scala?ref=b028ae999e53a8a78ee6fc59f4410740978e4c91",
    "patch": "@@ -40,6 +40,7 @@ import org.apache.kafka.controller.{Controller, QuorumController, QuorumControll\n import org.apache.kafka.metadata.{ApiMessageAndVersion, VersionRange}\n import org.apache.kafka.metalog.MetaLogManager\n import org.apache.kafka.raft.RaftConfig\n+import org.apache.kafka.raft.RaftConfig.AddressSpec\n import org.apache.kafka.server.authorizer.Authorizer\n \n import scala.jdk.CollectionConverters._\n@@ -55,7 +56,7 @@ class ControllerServer(\n                         val time: Time,\n                         val metrics: Metrics,\n                         val threadNamePrefix: Option[String],\n-                        val controllerQuorumVotersFuture: CompletableFuture[util.List[String]]\n+                        val controllerQuorumVotersFuture: CompletableFuture[util.Map[Integer, AddressSpec]]\n                       ) extends Logging with KafkaMetricsGroup {\n   import kafka.server.Server._\n \n@@ -157,8 +158,7 @@ class ControllerServer(\n \n \n       quotaManagers = QuotaFactory.instantiate(config, metrics, time, threadNamePrefix.getOrElse(\"\"))\n-      val controllerNodes =\n-        RaftConfig.quorumVoterStringsToNodes(controllerQuorumVotersFuture.get()).asScala\n+      val controllerNodes = RaftConfig.voterConnectionsToNodes(controllerQuorumVotersFuture.get()).asScala\n       controllerApis = new ControllerApis(socketServer.dataPlaneRequestChannel,\n         authorizer,\n         quotaManagers,"
  },
  {
    "sha": "2d95b2afca54a6f0ea24808671dd3b4f5c00fd04",
    "filename": "core/src/main/scala/kafka/server/KafkaRaftServer.scala",
    "status": "modified",
    "additions": 5,
    "deletions": 2,
    "changes": 7,
    "blob_url": "https://github.com/apache/kafka/blob/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/main/scala/kafka/server/KafkaRaftServer.scala",
    "raw_url": "https://github.com/apache/kafka/raw/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/main/scala/kafka/server/KafkaRaftServer.scala",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/server/KafkaRaftServer.scala?ref=b028ae999e53a8a78ee6fc59f4410740978e4c91",
    "patch": "@@ -28,6 +28,7 @@ import kafka.utils.{CoreUtils, Logging, Mx4jLoader, VerifiableProperties}\n import org.apache.kafka.common.TopicPartition\n import org.apache.kafka.common.utils.{AppInfoParser, Time}\n import org.apache.kafka.metadata.ApiMessageAndVersion\n+import org.apache.kafka.raft.RaftConfig\n import org.apache.kafka.raft.metadata.{MetaLogRaftShim, MetadataRecordSerde}\n \n import scala.collection.Seq\n@@ -59,7 +60,8 @@ class KafkaRaftServer(\n     metaProps.clusterId.toString\n   )\n \n-  private val controllerQuorumVotersFuture = CompletableFuture.completedFuture(config.quorumVoters)\n+  private val controllerQuorumVotersFuture = CompletableFuture.completedFuture(\n+    RaftConfig.parseVoterConnections(config.quorumVoters))\n \n   private val raftManager = new KafkaRaftManager[ApiMessageAndVersion](\n     metaProps,\n@@ -68,7 +70,8 @@ class KafkaRaftServer(\n     KafkaRaftServer.MetadataPartition,\n     time,\n     metrics,\n-    threadNamePrefix\n+    threadNamePrefix,\n+    controllerQuorumVotersFuture\n   )\n \n   private val metaLogShim = new MetaLogRaftShim(raftManager.kafkaRaftClient, config.nodeId)"
  },
  {
    "sha": "242d53735a49c553a83e7b3da92c960a973c19ab",
    "filename": "core/src/main/scala/kafka/tools/TestRaftServer.scala",
    "status": "modified",
    "additions": 4,
    "deletions": 4,
    "changes": 8,
    "blob_url": "https://github.com/apache/kafka/blob/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/main/scala/kafka/tools/TestRaftServer.scala",
    "raw_url": "https://github.com/apache/kafka/raw/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/main/scala/kafka/tools/TestRaftServer.scala",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/tools/TestRaftServer.scala?ref=b028ae999e53a8a78ee6fc59f4410740978e4c91",
    "patch": "@@ -18,8 +18,7 @@\n package kafka.tools\n \n import java.util.concurrent.atomic.{AtomicInteger, AtomicLong}\n-import java.util.concurrent.{CountDownLatch, LinkedBlockingDeque, TimeUnit}\n-\n+import java.util.concurrent.{CompletableFuture, CountDownLatch, LinkedBlockingDeque, TimeUnit}\n import joptsimple.OptionException\n import kafka.network.SocketServer\n import kafka.raft.{KafkaRaftManager, RaftManager}\n@@ -37,7 +36,7 @@ import org.apache.kafka.common.security.token.delegation.internals.DelegationTok\n import org.apache.kafka.common.utils.{Time, Utils}\n import org.apache.kafka.common.{TopicPartition, Uuid, protocol}\n import org.apache.kafka.raft.BatchReader.Batch\n-import org.apache.kafka.raft.{BatchReader, RaftClient, RecordSerde}\n+import org.apache.kafka.raft.{BatchReader, RaftClient, RaftConfig, RecordSerde}\n \n import scala.jdk.CollectionConverters._\n \n@@ -85,7 +84,8 @@ class TestRaftServer(\n       partition,\n       time,\n       metrics,\n-      Some(threadNamePrefix)\n+      Some(threadNamePrefix),\n+      CompletableFuture.completedFuture(RaftConfig.parseVoterConnections(config.quorumVoters))\n     )\n \n     workloadGenerator = new RaftWorkloadGenerator("
  },
  {
    "sha": "cac986fdc317cbc8705755a59be9263c9afb1b09",
    "filename": "core/src/test/java/kafka/test/ClusterInstance.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/apache/kafka/blob/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/test/java/kafka/test/ClusterInstance.java",
    "raw_url": "https://github.com/apache/kafka/raw/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/test/java/kafka/test/ClusterInstance.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/java/kafka/test/ClusterInstance.java?ref=b028ae999e53a8a78ee6fc59f4410740978e4c91",
    "patch": "@@ -29,7 +29,7 @@\n \n     enum ClusterType {\n         ZK,\n-        // RAFT\n+        RAFT\n     }\n \n     /**"
  },
  {
    "sha": "7d07d44d0abf311ac4610d6fe783f68b8f03745c",
    "filename": "core/src/test/java/kafka/test/ClusterTestExtensionsTest.java",
    "status": "modified",
    "additions": 4,
    "deletions": 4,
    "changes": 8,
    "blob_url": "https://github.com/apache/kafka/blob/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/test/java/kafka/test/ClusterTestExtensionsTest.java",
    "raw_url": "https://github.com/apache/kafka/raw/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/test/java/kafka/test/ClusterTestExtensionsTest.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/java/kafka/test/ClusterTestExtensionsTest.java?ref=b028ae999e53a8a78ee6fc59f4410740978e4c91",
    "patch": "@@ -86,20 +86,20 @@ public void testClusterTemplate() {\n             @ClusterConfigProperty(key = \"foo\", value = \"bar\"),\n             @ClusterConfigProperty(key = \"spam\", value = \"eggs\")\n         }),\n-        @ClusterTest(name = \"cluster-tests-2\", clusterType = Type.ZK, serverProperties = {\n+        @ClusterTest(name = \"cluster-tests-2\", clusterType = Type.RAFT, serverProperties = {\n             @ClusterConfigProperty(key = \"foo\", value = \"baz\"),\n             @ClusterConfigProperty(key = \"spam\", value = \"eggz\")\n         })\n     })\n     public void testClusterTests() {\n-        if (clusterInstance.config().name().filter(name -> name.equals(\"cluster-tests-1\")).isPresent()) {\n+        if (clusterInstance.clusterType().equals(ClusterInstance.ClusterType.ZK)) {\n             Assertions.assertEquals(clusterInstance.config().serverProperties().getProperty(\"foo\"), \"bar\");\n             Assertions.assertEquals(clusterInstance.config().serverProperties().getProperty(\"spam\"), \"eggs\");\n-        } else if (clusterInstance.config().name().filter(name -> name.equals(\"cluster-tests-2\")).isPresent()) {\n+        } else if (clusterInstance.clusterType().equals(ClusterInstance.ClusterType.RAFT)) {\n             Assertions.assertEquals(clusterInstance.config().serverProperties().getProperty(\"foo\"), \"baz\");\n             Assertions.assertEquals(clusterInstance.config().serverProperties().getProperty(\"spam\"), \"eggz\");\n         } else {\n-            Assertions.fail(\"Unknown cluster config \" + clusterInstance.config().name());\n+            Assertions.fail(\"Unknown cluster type \" + clusterInstance.clusterType());\n         }\n     }\n "
  },
  {
    "sha": "bfed4893dbd8f5b1b3994c384af82c59ebc8c456",
    "filename": "core/src/test/java/kafka/test/annotation/Type.java",
    "status": "modified",
    "additions": 35,
    "deletions": 4,
    "changes": 39,
    "blob_url": "https://github.com/apache/kafka/blob/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/test/java/kafka/test/annotation/Type.java",
    "raw_url": "https://github.com/apache/kafka/raw/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/test/java/kafka/test/annotation/Type.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/java/kafka/test/annotation/Type.java?ref=b028ae999e53a8a78ee6fc59f4410740978e4c91",
    "patch": "@@ -17,12 +17,43 @@\n \n package kafka.test.annotation;\n \n+import kafka.test.ClusterConfig;\n+import kafka.test.junit.RaftClusterInvocationContext;\n+import kafka.test.junit.ZkClusterInvocationContext;\n+import org.junit.jupiter.api.extension.TestTemplateInvocationContext;\n+\n+import java.util.function.Consumer;\n+\n /**\n  * The type of cluster config being requested. Used by {@link kafka.test.ClusterConfig} and the test annotations.\n  */\n public enum Type {\n-    // RAFT,\n-    ZK,\n-    BOTH,\n-    DEFAULT\n+    RAFT {\n+        @Override\n+        public void invocationContexts(ClusterConfig config, Consumer<TestTemplateInvocationContext> invocationConsumer) {\n+            invocationConsumer.accept(new RaftClusterInvocationContext(config.copyOf()));\n+        }\n+    },\n+    ZK {\n+        @Override\n+        public void invocationContexts(ClusterConfig config, Consumer<TestTemplateInvocationContext> invocationConsumer) {\n+            invocationConsumer.accept(new ZkClusterInvocationContext(config.copyOf()));\n+        }\n+    },\n+    BOTH {\n+        @Override\n+        public void invocationContexts(ClusterConfig config, Consumer<TestTemplateInvocationContext> invocationConsumer) {\n+            invocationConsumer.accept(new RaftClusterInvocationContext(config.copyOf()));\n+            invocationConsumer.accept(new ZkClusterInvocationContext(config.copyOf()));\n+\n+        }\n+    },\n+    DEFAULT {\n+        @Override\n+        public void invocationContexts(ClusterConfig config, Consumer<TestTemplateInvocationContext> invocationConsumer) {\n+            throw new IllegalStateException(\"Cannot create invocation contexts for DEFAULT type\");\n+        }\n+    };\n+\n+    public abstract void invocationContexts(ClusterConfig config, Consumer<TestTemplateInvocationContext> invocationConsumer);\n }"
  },
  {
    "sha": "ced1e6fcbd8b7421cac40769617342bb7db72ee6",
    "filename": "core/src/test/java/kafka/test/junit/ClusterTestExtensions.java",
    "status": "modified",
    "additions": 4,
    "deletions": 15,
    "changes": 19,
    "blob_url": "https://github.com/apache/kafka/blob/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/test/java/kafka/test/junit/ClusterTestExtensions.java",
    "raw_url": "https://github.com/apache/kafka/raw/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/test/java/kafka/test/junit/ClusterTestExtensions.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/java/kafka/test/junit/ClusterTestExtensions.java?ref=b028ae999e53a8a78ee6fc59f4410740978e4c91",
    "patch": "@@ -128,13 +128,7 @@ private void processClusterTemplate(ExtensionContext context, ClusterTemplate an\n             generatedClusterConfigs.add(ClusterConfig.defaultClusterBuilder().build());\n         }\n \n-        generatedClusterConfigs.forEach(config -> {\n-            if (config.clusterType() == Type.ZK) {\n-                testInvocations.accept(new ZkClusterInvocationContext(config.copyOf()));\n-            } else {\n-                throw new IllegalStateException(\"Unknown cluster type \" + config.clusterType());\n-            }\n-        });\n+        generatedClusterConfigs.forEach(config -> config.clusterType().invocationContexts(config, testInvocations));\n     }\n \n     private void generateClusterConfigurations(ExtensionContext context, String generateClustersMethods, ClusterGenerator generator) {\n@@ -198,14 +192,9 @@ private void processClusterTest(ClusterTest annot, ClusterTestDefaults defaults,\n             properties.put(property.key(), property.value());\n         }\n \n-        switch (type) {\n-            case ZK:\n-            case BOTH:\n-                ClusterConfig config = builder.build();\n-                config.serverProperties().putAll(properties);\n-                testInvocations.accept(new ZkClusterInvocationContext(config));\n-                break;\n-        }\n+        ClusterConfig config = builder.build();\n+        config.serverProperties().putAll(properties);\n+        type.invocationContexts(config, testInvocations);\n     }\n \n     private ClusterTestDefaults getClusterTestDefaults(Class<?> testClass) {"
  },
  {
    "sha": "0c3c5f47d6c4f8e633406ff441f53176d34e0964",
    "filename": "core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java",
    "status": "added",
    "additions": 196,
    "deletions": 0,
    "changes": 196,
    "blob_url": "https://github.com/apache/kafka/blob/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java",
    "raw_url": "https://github.com/apache/kafka/raw/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java?ref=b028ae999e53a8a78ee6fc59f4410740978e4c91",
    "patch": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.test.junit;\n+\n+import kafka.network.SocketServer;\n+import kafka.server.BrokerServer;\n+import kafka.server.ControllerServer;\n+import kafka.test.ClusterConfig;\n+import kafka.test.ClusterInstance;\n+import kafka.testkit.KafkaClusterTestKit;\n+import kafka.testkit.TestKitNodes;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.common.network.ListenerName;\n+import org.apache.kafka.metadata.BrokerState;\n+import org.junit.jupiter.api.extension.AfterTestExecutionCallback;\n+import org.junit.jupiter.api.extension.BeforeTestExecutionCallback;\n+import org.junit.jupiter.api.extension.Extension;\n+import org.junit.jupiter.api.extension.TestTemplateInvocationContext;\n+\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Wraps a {@link KafkaClusterTestKit} inside lifecycle methods for a test invocation. Each instance of this\n+ * class is provided with a configuration for the cluster.\n+ *\n+ * This context also provides parameter resolvers for:\n+ *\n+ * <ul>\n+ *     <li>ClusterConfig (the same instance passed to the constructor)</li>\n+ *     <li>ClusterInstance (includes methods to expose underlying SocketServer-s)</li>\n+ *     <li>IntegrationTestHelper (helper methods)</li>\n+ * </ul>\n+ */\n+public class RaftClusterInvocationContext implements TestTemplateInvocationContext {\n+\n+    private final ClusterConfig clusterConfig;\n+    private final AtomicReference<KafkaClusterTestKit> clusterReference;\n+\n+    public RaftClusterInvocationContext(ClusterConfig clusterConfig) {\n+        this.clusterConfig = clusterConfig;\n+        this.clusterReference = new AtomicReference<>();\n+    }\n+\n+    @Override\n+    public String getDisplayName(int invocationIndex) {\n+        String clusterDesc = clusterConfig.nameTags().entrySet().stream()\n+            .map(Object::toString)\n+            .collect(Collectors.joining(\", \"));\n+        return String.format(\"[Quorum %d] %s\", invocationIndex, clusterDesc);\n+    }\n+\n+    @Override\n+    public List<Extension> getAdditionalExtensions() {\n+        return Arrays.asList(\n+            (BeforeTestExecutionCallback) context -> {\n+                KafkaClusterTestKit.Builder builder = new KafkaClusterTestKit.Builder(\n+                    new TestKitNodes.Builder().\n+                        setNumKip500BrokerNodes(clusterConfig.numBrokers()).\n+                        setNumControllerNodes(clusterConfig.numControllers()).build());\n+\n+                // Copy properties into the TestKit builder\n+                clusterConfig.serverProperties().forEach((key, value) -> builder.setConfigProp(key.toString(), value.toString()));\n+                // KAFKA-12512 need to pass security protocol and listener name here\n+                KafkaClusterTestKit cluster = builder.build();\n+                clusterReference.set(cluster);\n+                cluster.format();\n+                cluster.startup();\n+                kafka.utils.TestUtils.waitUntilTrue(\n+                    () -> cluster.kip500Brokers().get(0).currentState() == BrokerState.RUNNING,\n+                    () -> \"Broker never made it to RUNNING state.\",\n+                    org.apache.kafka.test.TestUtils.DEFAULT_MAX_WAIT_MS,\n+                    100L);\n+            },\n+            (AfterTestExecutionCallback) context -> clusterReference.get().close(),\n+            new ClusterInstanceParameterResolver(new RaftClusterInstance(clusterReference, clusterConfig)),\n+            new GenericParameterResolver<>(clusterConfig, ClusterConfig.class)\n+        );\n+    }\n+\n+    public static class RaftClusterInstance implements ClusterInstance {\n+\n+        private final AtomicReference<KafkaClusterTestKit> clusterReference;\n+        private final ClusterConfig clusterConfig;\n+        final AtomicBoolean started = new AtomicBoolean(false);\n+        final AtomicBoolean stopped = new AtomicBoolean(false);\n+\n+        RaftClusterInstance(AtomicReference<KafkaClusterTestKit> clusterReference, ClusterConfig clusterConfig) {\n+            this.clusterReference = clusterReference;\n+            this.clusterConfig = clusterConfig;\n+        }\n+\n+        @Override\n+        public String bootstrapServers() {\n+            return clusterReference.get().clientProperties().getProperty(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG);\n+        }\n+\n+        @Override\n+        public Collection<SocketServer> brokerSocketServers() {\n+            return clusterReference.get().kip500Brokers().values().stream()\n+                .map(BrokerServer::socketServer)\n+                .collect(Collectors.toList());\n+        }\n+\n+        @Override\n+        public ListenerName clientListener() {\n+            return ListenerName.normalised(\"EXTERNAL\");\n+        }\n+\n+        @Override\n+        public Collection<SocketServer> controllerSocketServers() {\n+            return clusterReference.get().controllers().values().stream()\n+                .map(ControllerServer::socketServer)\n+                .collect(Collectors.toList());\n+        }\n+\n+        @Override\n+        public SocketServer anyBrokerSocketServer() {\n+            return clusterReference.get().kip500Brokers().values().stream()\n+                .map(BrokerServer::socketServer)\n+                .findFirst()\n+                .orElseThrow(() -> new RuntimeException(\"No broker SocketServers found\"));\n+        }\n+\n+        @Override\n+        public SocketServer anyControllerSocketServer() {\n+            return clusterReference.get().controllers().values().stream()\n+                .map(ControllerServer::socketServer)\n+                .findFirst()\n+                .orElseThrow(() -> new RuntimeException(\"No controller SocketServers found\"));\n+        }\n+\n+        @Override\n+        public ClusterType clusterType() {\n+            return ClusterType.RAFT;\n+        }\n+\n+        @Override\n+        public ClusterConfig config() {\n+            return clusterConfig;\n+        }\n+\n+        @Override\n+        public KafkaClusterTestKit getUnderlying() {\n+            return clusterReference.get();\n+        }\n+\n+        @Override\n+        public Admin createAdminClient(Properties configOverrides) {\n+            return Admin.create(clusterReference.get().clientProperties());\n+        }\n+\n+        @Override\n+        public void start() {\n+            if (started.compareAndSet(false, true)) {\n+                try {\n+                    clusterReference.get().startup();\n+                } catch (Exception e) {\n+                    throw new RuntimeException(\"Failed to start Raft server\", e);\n+                }\n+            }\n+        }\n+\n+        @Override\n+        public void stop() {\n+            if (stopped.compareAndSet(false, true)) {\n+                try {\n+                    clusterReference.get().close();\n+                } catch (Exception e) {\n+                    throw new RuntimeException(\"Failed to stop Raft server\", e);\n+                }\n+            }\n+        }\n+    }\n+}\n\\ No newline at end of file"
  },
  {
    "sha": "0b5859404b346f9836cb735da94f891ff96b5065",
    "filename": "core/src/test/java/kafka/testkit/BrokerNode.java",
    "status": "added",
    "additions": 99,
    "deletions": 0,
    "changes": 99,
    "blob_url": "https://github.com/apache/kafka/blob/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/test/java/kafka/testkit/BrokerNode.java",
    "raw_url": "https://github.com/apache/kafka/raw/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/test/java/kafka/testkit/BrokerNode.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/java/kafka/testkit/BrokerNode.java?ref=b028ae999e53a8a78ee6fc59f4410740978e4c91",
    "patch": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.testkit;\n+\n+import org.apache.kafka.common.Uuid;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+public class BrokerNode implements TestKitNode {\n+    public static class Builder {\n+        private int id = -1;\n+        private Uuid incarnationId = null;\n+        private String metadataDirectory = null;\n+        private List<String> logDataDirectories = null;\n+\n+        public Builder setId(int id) {\n+            this.id = id;\n+            return this;\n+        }\n+\n+        public Builder setLogDirectories(List<String> logDataDirectories) {\n+            this.logDataDirectories = logDataDirectories;\n+            return this;\n+        }\n+\n+        public Builder setMetadataDirectory(String metadataDirectory) {\n+            this.metadataDirectory = metadataDirectory;\n+            return this;\n+        }\n+\n+        public BrokerNode build() {\n+            if (id == -1) {\n+                throw new RuntimeException(\"You must set the node id\");\n+            }\n+            if (incarnationId == null) {\n+                incarnationId = Uuid.randomUuid();\n+            }\n+            if (logDataDirectories == null) {\n+                logDataDirectories = Collections.\n+                    singletonList(String.format(\"broker_%d_data0\", id));\n+            }\n+            if (metadataDirectory == null) {\n+                metadataDirectory = logDataDirectories.get(0);\n+            }\n+            return new BrokerNode(id, incarnationId, metadataDirectory,\n+                logDataDirectories);\n+        }\n+    }\n+\n+    private final int id;\n+    private final Uuid incarnationId;\n+    private final String metadataDirectory;\n+    private final List<String> logDataDirectories;\n+\n+    BrokerNode(int id,\n+               Uuid incarnationId,\n+               String metadataDirectory,\n+               List<String> logDataDirectories) {\n+        this.id = id;\n+        this.incarnationId = incarnationId;\n+        this.metadataDirectory = metadataDirectory;\n+        this.logDataDirectories = new ArrayList<>(logDataDirectories);\n+    }\n+\n+    @Override\n+    public int id() {\n+        return id;\n+    }\n+\n+    public Uuid incarnationId() {\n+        return incarnationId;\n+    }\n+\n+    @Override\n+    public String metadataDirectory() {\n+        return metadataDirectory;\n+    }\n+\n+    public List<String> logDataDirectories() {\n+        return logDataDirectories;\n+    }\n+}"
  },
  {
    "sha": "be6c8067f1f7f19600c48e5c76e2b5f06386f5cd",
    "filename": "core/src/test/java/kafka/testkit/ControllerNode.java",
    "status": "added",
    "additions": 63,
    "deletions": 0,
    "changes": 63,
    "blob_url": "https://github.com/apache/kafka/blob/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/test/java/kafka/testkit/ControllerNode.java",
    "raw_url": "https://github.com/apache/kafka/raw/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/test/java/kafka/testkit/ControllerNode.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/java/kafka/testkit/ControllerNode.java?ref=b028ae999e53a8a78ee6fc59f4410740978e4c91",
    "patch": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.testkit;\n+\n+public class ControllerNode implements TestKitNode {\n+    public static class Builder {\n+        private int id = -1;\n+        private String metadataDirectory = null;\n+\n+        public Builder setId(int id) {\n+            this.id = id;\n+            return this;\n+        }\n+\n+        public Builder setMetadataDirectory() {\n+            this.metadataDirectory = metadataDirectory;\n+            return this;\n+        }\n+\n+        public ControllerNode build() {\n+            if (id == -1) {\n+                throw new RuntimeException(\"You must set the node id\");\n+            }\n+            if (metadataDirectory == null) {\n+                metadataDirectory = String.format(\"controller_%d\", id);\n+            }\n+            return new ControllerNode(id, metadataDirectory);\n+        }\n+    }\n+\n+    private final int id;\n+    private final String metadataDirectory;\n+\n+    ControllerNode(int id, String metadataDirectory) {\n+        this.id = id;\n+        this.metadataDirectory = metadataDirectory;\n+    }\n+\n+    @Override\n+    public int id() {\n+        return id;\n+    }\n+\n+    @Override\n+    public String metadataDirectory() {\n+        return metadataDirectory;\n+    }\n+}"
  },
  {
    "sha": "bbcd37b2d86f3cf132fa258037569bceded24601",
    "filename": "core/src/test/java/kafka/testkit/KafkaClusterTestKit.java",
    "status": "added",
    "additions": 494,
    "deletions": 0,
    "changes": 494,
    "blob_url": "https://github.com/apache/kafka/blob/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/test/java/kafka/testkit/KafkaClusterTestKit.java",
    "raw_url": "https://github.com/apache/kafka/raw/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/test/java/kafka/testkit/KafkaClusterTestKit.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/java/kafka/testkit/KafkaClusterTestKit.java?ref=b028ae999e53a8a78ee6fc59f4410740978e4c91",
    "patch": "@@ -0,0 +1,494 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.testkit;\n+\n+import kafka.raft.KafkaRaftManager;\n+import kafka.server.BrokerServer;\n+import kafka.server.ControllerServer;\n+import kafka.server.KafkaConfig;\n+import kafka.server.KafkaConfig$;\n+import kafka.server.KafkaRaftServer;\n+import kafka.server.MetaProperties;\n+import kafka.server.Server;\n+import kafka.tools.StorageTool;\n+import kafka.utils.Logging;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.metrics.Metrics;\n+import org.apache.kafka.common.network.ListenerName;\n+import org.apache.kafka.common.utils.ThreadUtils;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.controller.Controller;\n+import org.apache.kafka.metadata.ApiMessageAndVersion;\n+import org.apache.kafka.metalog.MetaLogManager;\n+import org.apache.kafka.raft.RaftConfig;\n+import org.apache.kafka.raft.metadata.MetaLogRaftShim;\n+import org.apache.kafka.raft.metadata.MetadataRecordSerde;\n+import org.apache.kafka.test.TestUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Option;\n+import scala.collection.JavaConverters;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.PrintStream;\n+import java.net.InetSocketAddress;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.AbstractMap.SimpleImmutableEntry;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Properties;\n+import java.util.TreeMap;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+\n+\n+@SuppressWarnings(\"deprecation\") // Needed for Scala 2.12 compatibility\n+public class KafkaClusterTestKit implements AutoCloseable {\n+    private final static Logger log = LoggerFactory.getLogger(KafkaClusterTestKit.class);\n+\n+    /**\n+     * This class manages a future which is completed with the proper value for\n+     * controller.quorum.voters once the randomly assigned ports for all the controllers are\n+     * known.\n+     */\n+    private static class ControllerQuorumVotersFutureManager implements AutoCloseable {\n+        private final int expectedControllers;\n+        private final CompletableFuture<Map<Integer, RaftConfig.AddressSpec>> future = new CompletableFuture<>();\n+        private final Map<Integer, Integer> controllerPorts = new TreeMap<>();\n+\n+        ControllerQuorumVotersFutureManager(int expectedControllers) {\n+            this.expectedControllers = expectedControllers;\n+        }\n+\n+        synchronized void registerPort(int nodeId, int port) {\n+            controllerPorts.put(nodeId, port);\n+            if (controllerPorts.size() >= expectedControllers) {\n+                future.complete(controllerPorts.entrySet().stream().\n+                    collect(Collectors.toMap(\n+                        Map.Entry::getKey,\n+                        entry -> new RaftConfig.InetAddressSpec(new InetSocketAddress(\"localhost\", entry.getValue()))\n+                    )));\n+            }\n+        }\n+\n+        void fail(Throwable e) {\n+            future.completeExceptionally(e);\n+        }\n+\n+        @Override\n+        public void close() {\n+            future.cancel(true);\n+        }\n+    }\n+\n+    public static class Builder {\n+        private TestKitNodes nodes;\n+        private Map<String, String> configProps = new HashMap<>();\n+\n+        public Builder(TestKitNodes nodes) {\n+            this.nodes = nodes;\n+        }\n+\n+        public Builder setConfigProp(String key, String value) {\n+            this.configProps.put(key, value);\n+            return this;\n+        }\n+\n+        public KafkaClusterTestKit build() throws Exception {\n+            Map<Integer, ControllerServer> controllers = new HashMap<>();\n+            Map<Integer, BrokerServer> kip500Brokers = new HashMap<>();\n+            Map<Integer, KafkaRaftManager> raftManagers = new HashMap<>();\n+            String uninitializedQuorumVotersString = nodes.controllerNodes().keySet().stream().\n+                map(controllerNode -> String.format(\"%d@0.0.0.0:0\", controllerNode)).\n+                collect(Collectors.joining(\",\"));\n+            /*\n+              Number of threads = Total number of brokers + Total number of controllers + Total number of Raft Managers\n+                                = Total number of brokers + Total number of controllers * 2\n+                                  (Raft Manager per broker/controller)\n+             */\n+            int numOfExecutorThreads = (nodes.brokerNodes().size() + nodes.controllerNodes().size()) * 2;\n+            ExecutorService executorService = null;\n+            ControllerQuorumVotersFutureManager connectFutureManager =\n+                new ControllerQuorumVotersFutureManager(nodes.controllerNodes().size());\n+            File baseDirectory = null;\n+\n+            try {\n+                baseDirectory = TestUtils.tempDirectory();\n+                nodes = nodes.copyWithAbsolutePaths(baseDirectory.getAbsolutePath());\n+                executorService = Executors.newFixedThreadPool(numOfExecutorThreads,\n+                    ThreadUtils.createThreadFactory(\"KafkaClusterTestKit%d\", false));\n+                for (ControllerNode node : nodes.controllerNodes().values()) {\n+                    Map<String, String> props = new HashMap<>(configProps);\n+                    props.put(KafkaConfig$.MODULE$.ProcessRolesProp(), \"controller\");\n+                    props.put(KafkaConfig$.MODULE$.NodeIdProp(),\n+                        Integer.toString(node.id()));\n+                    props.put(KafkaConfig$.MODULE$.MetadataLogDirProp(),\n+                        node.metadataDirectory());\n+                    props.put(KafkaConfig$.MODULE$.ListenerSecurityProtocolMapProp(),\n+                        \"CONTROLLER:PLAINTEXT\");\n+                    props.put(KafkaConfig$.MODULE$.ListenersProp(),\n+                        \"CONTROLLER://localhost:0\");\n+                    props.put(KafkaConfig$.MODULE$.ControllerListenerNamesProp(),\n+                        \"CONTROLLER\");\n+                    // Note: we can't accurately set controller.quorum.voters yet, since we don't\n+                    // yet know what ports each controller will pick.  Set it to a dummy string \\\n+                    // for now as a placeholder.\n+                    props.put(RaftConfig.QUORUM_VOTERS_CONFIG, uninitializedQuorumVotersString);\n+                    setupNodeDirectories(baseDirectory, node.metadataDirectory(), Collections.emptyList());\n+                    KafkaConfig config = new KafkaConfig(props, false, Option.empty());\n+\n+                    String threadNamePrefix = String.format(\"controller%d_\", node.id());\n+                    MetaProperties metaProperties = MetaProperties.apply(nodes.clusterId(), node.id());\n+                    TopicPartition metadataPartition = new TopicPartition(KafkaRaftServer.MetadataTopic(), 0);\n+                    KafkaRaftManager<ApiMessageAndVersion> raftManager = new KafkaRaftManager<>(\n+                        metaProperties, config, new MetadataRecordSerde(), metadataPartition,\n+                        Time.SYSTEM, new Metrics(), Option.apply(threadNamePrefix), connectFutureManager.future);\n+                    MetaLogManager metaLogShim = new MetaLogRaftShim(raftManager.kafkaRaftClient(), config.nodeId());\n+                    ControllerServer controller = new ControllerServer(\n+                        nodes.controllerProperties(node.id()),\n+                        config,\n+                        metaLogShim,\n+                        raftManager,\n+                        Time.SYSTEM,\n+                        new Metrics(),\n+                        Option.apply(threadNamePrefix),\n+                        connectFutureManager.future\n+                    );\n+                    controllers.put(node.id(), controller);\n+                    controller.socketServerFirstBoundPortFuture().whenComplete((port, e) -> {\n+                        if (e != null) {\n+                            connectFutureManager.fail(e);\n+                        } else {\n+                            connectFutureManager.registerPort(node.id(), port);\n+                        }\n+                    });\n+                    raftManagers.put(node.id(), raftManager);\n+                }\n+                for (BrokerNode node : nodes.brokerNodes().values()) {\n+                    Map<String, String> props = new HashMap<>(configProps);\n+                    props.put(KafkaConfig$.MODULE$.ProcessRolesProp(), \"broker\");\n+                    props.put(KafkaConfig$.MODULE$.BrokerIdProp(),\n+                        Integer.toString(node.id()));\n+                    props.put(KafkaConfig$.MODULE$.MetadataLogDirProp(),\n+                        node.metadataDirectory());\n+                    props.put(KafkaConfig$.MODULE$.LogDirsProp(),\n+                        String.join(\",\", node.logDataDirectories()));\n+                    props.put(KafkaConfig$.MODULE$.ListenerSecurityProtocolMapProp(),\n+                        \"EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT\");\n+                    props.put(KafkaConfig$.MODULE$.ListenersProp(),\n+                        \"EXTERNAL://localhost:0\");\n+                    props.put(KafkaConfig$.MODULE$.InterBrokerListenerNameProp(),\n+                        nodes.interBrokerListenerName().value());\n+                    props.put(KafkaConfig$.MODULE$.ControllerListenerNamesProp(),\n+                        \"CONTROLLER\");\n+\n+                    setupNodeDirectories(baseDirectory, node.metadataDirectory(),\n+                        node.logDataDirectories());\n+\n+                    // Just like above, we set a placeholder voter list here until we\n+                    // find out what ports the controllers picked.\n+                    props.put(RaftConfig.QUORUM_VOTERS_CONFIG, uninitializedQuorumVotersString);\n+                    KafkaConfig config = new KafkaConfig(props, false, Option.empty());\n+\n+                    String threadNamePrefix = String.format(\"broker%d_\", node.id());\n+                    MetaProperties metaProperties = MetaProperties.apply(nodes.clusterId(), node.id());\n+                    TopicPartition metadataPartition = new TopicPartition(KafkaRaftServer.MetadataTopic(), 0);\n+                    KafkaRaftManager<ApiMessageAndVersion> raftManager = new KafkaRaftManager<>(\n+                            metaProperties, config, new MetadataRecordSerde(), metadataPartition,\n+                            Time.SYSTEM, new Metrics(), Option.apply(threadNamePrefix), connectFutureManager.future);\n+                    MetaLogManager metaLogShim = new MetaLogRaftShim(raftManager.kafkaRaftClient(), config.nodeId());\n+                    BrokerServer broker = new BrokerServer(\n+                        config,\n+                        nodes.brokerProperties(node.id()),\n+                        metaLogShim,\n+                        Time.SYSTEM,\n+                        new Metrics(),\n+                        Option.apply(threadNamePrefix),\n+                        JavaConverters.asScalaBuffer(Collections.<String>emptyList()).toSeq(),\n+                        connectFutureManager.future,\n+                        Server.SUPPORTED_FEATURES()\n+                    );\n+                    kip500Brokers.put(node.id(), broker);\n+                    raftManagers.put(node.id(), raftManager);\n+                }\n+            } catch (Exception e) {\n+                if (executorService != null) {\n+                    executorService.shutdownNow();\n+                    executorService.awaitTermination(5, TimeUnit.MINUTES);\n+                }\n+                for (ControllerServer controller : controllers.values()) {\n+                    controller.shutdown();\n+                }\n+                for (BrokerServer brokerServer : kip500Brokers.values()) {\n+                    brokerServer.shutdown();\n+                }\n+                for (KafkaRaftManager raftManager : raftManagers.values()) {\n+                    raftManager.shutdown();\n+                }\n+                connectFutureManager.close();\n+                if (baseDirectory != null) {\n+                    Utils.delete(baseDirectory);\n+                }\n+                throw e;\n+            }\n+            return new KafkaClusterTestKit(executorService, nodes, controllers,\n+                kip500Brokers, raftManagers, connectFutureManager, baseDirectory);\n+        }\n+\n+        static private void setupNodeDirectories(File baseDirectory,\n+                                                 String metadataDirectory,\n+                                                 Collection<String> logDataDirectories) throws Exception {\n+            Files.createDirectories(new File(baseDirectory, \"local\").toPath());\n+            Files.createDirectories(Paths.get(metadataDirectory));\n+            for (String logDataDirectory : logDataDirectories) {\n+                Files.createDirectories(Paths.get(logDataDirectory));\n+            }\n+        }\n+    }\n+\n+    private final ExecutorService executorService;\n+    private final TestKitNodes nodes;\n+    private final Map<Integer, ControllerServer> controllers;\n+    private final Map<Integer, BrokerServer> kip500Brokers;\n+    private final Map<Integer, KafkaRaftManager> raftManagers;\n+    private final ControllerQuorumVotersFutureManager controllerQuorumVotersFutureManager;\n+    private final File baseDirectory;\n+\n+    private KafkaClusterTestKit(ExecutorService executorService,\n+                                TestKitNodes nodes,\n+                                Map<Integer, ControllerServer> controllers,\n+                                Map<Integer, BrokerServer> kip500Brokers,\n+                                Map<Integer, KafkaRaftManager> raftManagers,\n+                                ControllerQuorumVotersFutureManager controllerQuorumVotersFutureManager,\n+                                File baseDirectory) {\n+        this.executorService = executorService;\n+        this.nodes = nodes;\n+        this.controllers = controllers;\n+        this.kip500Brokers = kip500Brokers;\n+        this.raftManagers = raftManagers;\n+        this.controllerQuorumVotersFutureManager = controllerQuorumVotersFutureManager;\n+        this.baseDirectory = baseDirectory;\n+    }\n+\n+    public void format() throws Exception {\n+        List<Future<?>> futures = new ArrayList<>();\n+        try {\n+            for (Entry<Integer, ControllerServer> entry : controllers.entrySet()) {\n+                int nodeId = entry.getKey();\n+                ControllerServer controller = entry.getValue();\n+                formatNodeAndLog(nodes.controllerProperties(nodeId), controller.config().metadataLogDir(),\n+                    controller, futures::add);\n+            }\n+            for (Entry<Integer, BrokerServer> entry : kip500Brokers.entrySet()) {\n+                int nodeId = entry.getKey();\n+                BrokerServer broker = entry.getValue();\n+                formatNodeAndLog(nodes.brokerProperties(nodeId), broker.config().metadataLogDir(),\n+                    broker, futures::add);\n+            }\n+            for (Future<?> future: futures) {\n+                future.get();\n+            }\n+        } catch (Exception e) {\n+            for (Future<?> future: futures) {\n+                future.cancel(true);\n+            }\n+            throw e;\n+        }\n+    }\n+\n+    private void formatNodeAndLog(MetaProperties properties, String metadataLogDir, Logging loggingMixin,\n+                                  Consumer<Future<?>> futureConsumer) {\n+        futureConsumer.accept(executorService.submit(() -> {\n+            try (ByteArrayOutputStream stream = new ByteArrayOutputStream()) {\n+                try (PrintStream out = new PrintStream(stream)) {\n+                    StorageTool.formatCommand(out,\n+                            JavaConverters.asScalaBuffer(Collections.singletonList(metadataLogDir)).toSeq(),\n+                            properties,\n+                            false);\n+                } finally {\n+                    for (String line : stream.toString().split(String.format(\"%n\"))) {\n+                        loggingMixin.info(() -> line);\n+                    }\n+                }\n+            } catch (IOException e) {\n+                throw new RuntimeException(e);\n+            }\n+        }));\n+    }\n+\n+    public void startup() throws ExecutionException, InterruptedException {\n+        List<Future<?>> futures = new ArrayList<>();\n+        try {\n+            for (ControllerServer controller : controllers.values()) {\n+                futures.add(executorService.submit(controller::startup));\n+            }\n+            for (KafkaRaftManager raftManager : raftManagers.values()) {\n+                futures.add(controllerQuorumVotersFutureManager.future.thenRunAsync(raftManager::startup));\n+            }\n+            for (BrokerServer broker : kip500Brokers.values()) {\n+                futures.add(executorService.submit(broker::startup));\n+            }\n+            for (Future<?> future: futures) {\n+                future.get();\n+            }\n+        } catch (Exception e) {\n+            for (Future<?> future: futures) {\n+                future.cancel(true);\n+            }\n+            throw e;\n+        }\n+    }\n+\n+    /**\n+     * Wait for a controller to mark all the brokers as ready (registered and unfenced).\n+     */\n+    public void waitForReadyBrokers() throws ExecutionException, InterruptedException {\n+        // We can choose any controller, not just the active controller.\n+        // If we choose a standby controller, we will wait slightly longer.\n+        ControllerServer controllerServer = controllers.values().iterator().next();\n+        Controller controller = controllerServer.controller();\n+        controller.waitForReadyBrokers(kip500Brokers.size()).get();\n+    }\n+\n+    public Properties controllerClientProperties() throws ExecutionException, InterruptedException {\n+        Properties properties = new Properties();\n+        if (!controllers.isEmpty()) {\n+            Collection<Node> controllerNodes = RaftConfig.voterConnectionsToNodes(\n+                controllerQuorumVotersFutureManager.future.get());\n+\n+            StringBuilder bld = new StringBuilder();\n+            String prefix = \"\";\n+            for (Node node : controllerNodes) {\n+                bld.append(prefix).append(node.id()).append('@');\n+                bld.append(node.host()).append(\":\").append(node.port());\n+                prefix = \",\";\n+            }\n+            properties.setProperty(RaftConfig.QUORUM_VOTERS_CONFIG, bld.toString());\n+            properties.setProperty(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG,\n+                controllerNodes.stream().map(n -> n.host() + \":\" + n.port()).\n+                    collect(Collectors.joining(\",\")));\n+        }\n+        return properties;\n+    }\n+\n+    public Properties clientProperties() {\n+        Properties properties = new Properties();\n+        if (!kip500Brokers.isEmpty()) {\n+            StringBuilder bld = new StringBuilder();\n+            String prefix = \"\";\n+            for (Entry<Integer, BrokerServer> entry : kip500Brokers.entrySet()) {\n+                int brokerId = entry.getKey();\n+                BrokerServer broker = entry.getValue();\n+                ListenerName listenerName = nodes.externalListenerName();\n+                int port = broker.boundPort(listenerName);\n+                if (port <= 0) {\n+                    throw new RuntimeException(\"Broker \" + brokerId + \" does not yet \" +\n+                        \"have a bound port for \" + listenerName + \".  Did you start \" +\n+                        \"the cluster yet?\");\n+                }\n+                bld.append(prefix).append(\"localhost:\").append(port);\n+                prefix = \",\";\n+            }\n+            properties.setProperty(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, bld.toString());\n+        }\n+        return properties;\n+    }\n+\n+    public Map<Integer, ControllerServer> controllers() {\n+        return controllers;\n+    }\n+\n+    public Map<Integer, BrokerServer> kip500Brokers() {\n+        return kip500Brokers;\n+    }\n+\n+    public Map<Integer, KafkaRaftManager> raftManagers() {\n+        return raftManagers;\n+    }\n+\n+    public TestKitNodes nodes() {\n+        return nodes;\n+    }\n+\n+    @Override\n+    public void close() throws Exception {\n+        List<Entry<String, Future<?>>> futureEntries = new ArrayList<>();\n+        try {\n+            controllerQuorumVotersFutureManager.close();\n+            for (Entry<Integer, BrokerServer> entry : kip500Brokers.entrySet()) {\n+                int brokerId = entry.getKey();\n+                BrokerServer broker = entry.getValue();\n+                futureEntries.add(new SimpleImmutableEntry<>(\"broker\" + brokerId,\n+                    executorService.submit(broker::shutdown)));\n+            }\n+            waitForAllFutures(futureEntries);\n+            futureEntries.clear();\n+            for (Entry<Integer, ControllerServer> entry : controllers.entrySet()) {\n+                int controllerId = entry.getKey();\n+                ControllerServer controller = entry.getValue();\n+                futureEntries.add(new SimpleImmutableEntry<>(\"controller\" + controllerId,\n+                    executorService.submit(controller::shutdown)));\n+            }\n+            waitForAllFutures(futureEntries);\n+            futureEntries.clear();\n+            for (Entry<Integer, KafkaRaftManager> entry : raftManagers.entrySet()) {\n+                int raftManagerId = entry.getKey();\n+                KafkaRaftManager raftManager = entry.getValue();\n+                futureEntries.add(new SimpleImmutableEntry<>(\"raftManager\" + raftManagerId,\n+                    executorService.submit(raftManager::shutdown)));\n+            }\n+            waitForAllFutures(futureEntries);\n+            futureEntries.clear();\n+            Utils.delete(baseDirectory);\n+        } catch (Exception e) {\n+            for (Entry<String, Future<?>> entry : futureEntries) {\n+                entry.getValue().cancel(true);\n+            }\n+            throw e;\n+        } finally {\n+            executorService.shutdownNow();\n+            executorService.awaitTermination(1, TimeUnit.DAYS);\n+        }\n+    }\n+\n+    private void waitForAllFutures(List<Entry<String, Future<?>>> futureEntries)\n+            throws Exception {\n+        for (Entry<String, Future<?>> entry : futureEntries) {\n+            log.debug(\"waiting for {} to shut down.\", entry.getKey());\n+            entry.getValue().get();\n+            log.debug(\"{} successfully shut down.\", entry.getKey());\n+        }\n+    }\n+}"
  },
  {
    "sha": "a5423d135f9cb94ab1f7eb045278b3fc096eb7ad",
    "filename": "core/src/test/java/kafka/testkit/TestKitNode.java",
    "status": "added",
    "additions": 23,
    "deletions": 0,
    "changes": 23,
    "blob_url": "https://github.com/apache/kafka/blob/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/test/java/kafka/testkit/TestKitNode.java",
    "raw_url": "https://github.com/apache/kafka/raw/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/test/java/kafka/testkit/TestKitNode.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/java/kafka/testkit/TestKitNode.java?ref=b028ae999e53a8a78ee6fc59f4410740978e4c91",
    "patch": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.testkit;\n+\n+public interface TestKitNode {\n+    int id();\n+    String metadataDirectory();\n+}"
  },
  {
    "sha": "bbac96f330d22bd3528b44f0f03d1328604a013d",
    "filename": "core/src/test/java/kafka/testkit/TestKitNodes.java",
    "status": "added",
    "additions": 181,
    "deletions": 0,
    "changes": 181,
    "blob_url": "https://github.com/apache/kafka/blob/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/test/java/kafka/testkit/TestKitNodes.java",
    "raw_url": "https://github.com/apache/kafka/raw/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/test/java/kafka/testkit/TestKitNodes.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/java/kafka/testkit/TestKitNodes.java?ref=b028ae999e53a8a78ee6fc59f4410740978e4c91",
    "patch": "@@ -0,0 +1,181 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.testkit;\n+\n+import kafka.server.MetaProperties;\n+import org.apache.kafka.common.Uuid;\n+import org.apache.kafka.common.network.ListenerName;\n+\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.NavigableMap;\n+import java.util.TreeMap;\n+\n+public class TestKitNodes {\n+    public static class Builder {\n+        private Uuid clusterId = null;\n+        private final NavigableMap<Integer, ControllerNode> controllerNodes = new TreeMap<>();\n+        private final NavigableMap<Integer, BrokerNode> brokerNodes = new TreeMap<>();\n+\n+        public Builder setClusterId(Uuid clusterId) {\n+            this.clusterId = clusterId;\n+            return this;\n+        }\n+\n+        public Builder addNodes(TestKitNode[] nodes) {\n+            for (TestKitNode node : nodes) {\n+                addNode(node);\n+            }\n+            return this;\n+        }\n+\n+        public Builder addNode(TestKitNode node) {\n+            if (node instanceof ControllerNode) {\n+                ControllerNode controllerNode = (ControllerNode) node;\n+                controllerNodes.put(node.id(), controllerNode);\n+            } else if (node instanceof BrokerNode) {\n+                BrokerNode brokerNode = (BrokerNode) node;\n+                brokerNodes.put(node.id(), brokerNode);\n+            } else {\n+                throw new RuntimeException(\"Can't handle TestKitNode subclass \" +\n+                        node.getClass().getSimpleName());\n+            }\n+            return this;\n+        }\n+\n+        public Builder setNumControllerNodes(int numControllerNodes) {\n+            if (numControllerNodes < 0) {\n+                throw new RuntimeException(\"Invalid negative value for numControllerNodes\");\n+            }\n+\n+            while (controllerNodes.size() > numControllerNodes) {\n+                controllerNodes.pollFirstEntry();\n+            }\n+            while (controllerNodes.size() < numControllerNodes) {\n+                int nextId = 3000;\n+                if (!controllerNodes.isEmpty()) {\n+                    nextId = controllerNodes.lastKey() + 1;\n+                }\n+                controllerNodes.put(nextId, new ControllerNode.Builder().\n+                    setId(nextId).build());\n+            }\n+            return this;\n+        }\n+\n+        public Builder setNumKip500BrokerNodes(int numBrokerNodes) {\n+            if (numBrokerNodes < 0) {\n+                throw new RuntimeException(\"Invalid negative value for numBrokerNodes\");\n+            }\n+            while (brokerNodes.size() > numBrokerNodes) {\n+                brokerNodes.pollFirstEntry();\n+            }\n+            while (brokerNodes.size() < numBrokerNodes) {\n+                int nextId = 0;\n+                if (!brokerNodes.isEmpty()) {\n+                    nextId = brokerNodes.lastKey() + 1;\n+                }\n+                brokerNodes.put(nextId, new BrokerNode.Builder().\n+                    setId(nextId).build());\n+            }\n+            return this;\n+        }\n+\n+        public TestKitNodes build() {\n+            if (clusterId == null) {\n+                clusterId = Uuid.randomUuid();\n+            }\n+            return new TestKitNodes(clusterId, controllerNodes, brokerNodes);\n+        }\n+    }\n+\n+    private final Uuid clusterId;\n+    private final NavigableMap<Integer, ControllerNode> controllerNodes;\n+    private final NavigableMap<Integer, BrokerNode> brokerNodes;\n+\n+    private TestKitNodes(Uuid clusterId,\n+                         NavigableMap<Integer, ControllerNode> controllerNodes,\n+                         NavigableMap<Integer, BrokerNode> brokerNodes) {\n+        this.clusterId = clusterId;\n+        this.controllerNodes = controllerNodes;\n+        this.brokerNodes = brokerNodes;\n+    }\n+\n+    public Uuid clusterId() {\n+        return clusterId;\n+    }\n+\n+    public Map<Integer, ControllerNode> controllerNodes() {\n+        return controllerNodes;\n+    }\n+\n+    public NavigableMap<Integer, BrokerNode> brokerNodes() {\n+        return brokerNodes;\n+    }\n+\n+    public MetaProperties controllerProperties(int id) {\n+        return MetaProperties.apply(clusterId, id);\n+    }\n+\n+    public MetaProperties brokerProperties(int id) {\n+        return MetaProperties.apply(clusterId, id);\n+    }\n+\n+    public ListenerName interBrokerListenerName() {\n+        return new ListenerName(\"EXTERNAL\");\n+    }\n+\n+    public ListenerName externalListenerName() {\n+        return new ListenerName(\"EXTERNAL\");\n+    }\n+\n+    public TestKitNodes copyWithAbsolutePaths(String baseDirectory) {\n+        NavigableMap<Integer, ControllerNode> newControllerNodes = new TreeMap<>();\n+        NavigableMap<Integer, BrokerNode> newBrokerNodes = new TreeMap<>();\n+        for (Entry<Integer, ControllerNode> entry : controllerNodes.entrySet()) {\n+            ControllerNode node = entry.getValue();\n+            newControllerNodes.put(entry.getKey(), new ControllerNode(node.id(),\n+                absolutize(baseDirectory, node.metadataDirectory())));\n+        }\n+        for (Entry<Integer, BrokerNode> entry : brokerNodes.entrySet()) {\n+            BrokerNode node = entry.getValue();\n+            newBrokerNodes.put(entry.getKey(), new BrokerNode(node.id(),\n+                node.incarnationId(), absolutize(baseDirectory, node.metadataDirectory()),\n+                absolutize(baseDirectory, node.logDataDirectories())));\n+        }\n+        return new TestKitNodes(clusterId, newControllerNodes, newBrokerNodes);\n+    }\n+\n+    private static List<String> absolutize(String base, Collection<String> directories) {\n+        List<String> newDirectories = new ArrayList<>();\n+        for (String directory : directories) {\n+            newDirectories.add(absolutize(base, directory));\n+        }\n+        return newDirectories;\n+    }\n+\n+    private static String absolutize(String base, String directory) {\n+        if (Paths.get(directory).isAbsolute()) {\n+            return directory;\n+        }\n+        return Paths.get(base, directory).toAbsolutePath().toString();\n+    }\n+}"
  },
  {
    "sha": "f7fb7364a3c385914ee421f8877da0845ac90108",
    "filename": "core/src/test/resources/log4j.properties",
    "status": "modified",
    "additions": 3,
    "deletions": 2,
    "changes": 5,
    "blob_url": "https://github.com/apache/kafka/blob/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/test/resources/log4j.properties",
    "raw_url": "https://github.com/apache/kafka/raw/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/test/resources/log4j.properties",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/resources/log4j.properties?ref=b028ae999e53a8a78ee6fc59f4410740978e4c91",
    "patch": "@@ -18,8 +18,9 @@ log4j.appender.stdout=org.apache.log4j.ConsoleAppender\n log4j.appender.stdout.layout=org.apache.log4j.PatternLayout\n log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c:%L)%n\n \n-log4j.logger.kafka=ERROR\n-log4j.logger.org.apache.kafka=ERROR\n+log4j.logger.kafka=WARN\n+log4j.logger.org.apache.kafka=WARN\n+\n \n # zkclient can be verbose, during debugging it is common to adjust it separately\n log4j.logger.org.apache.zookeeper=WARN"
  },
  {
    "sha": "6833c8047468f7ba23f67495c6440b50a8c8c881",
    "filename": "core/src/test/scala/integration/kafka/server/RaftClusterTest.scala",
    "status": "added",
    "additions": 216,
    "deletions": 0,
    "changes": 216,
    "blob_url": "https://github.com/apache/kafka/blob/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/test/scala/integration/kafka/server/RaftClusterTest.scala",
    "raw_url": "https://github.com/apache/kafka/raw/b028ae999e53a8a78ee6fc59f4410740978e4c91/core/src/test/scala/integration/kafka/server/RaftClusterTest.scala",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/scala/integration/kafka/server/RaftClusterTest.scala?ref=b028ae999e53a8a78ee6fc59f4410740978e4c91",
    "patch": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.testkit.{KafkaClusterTestKit, TestKitNodes}\n+import kafka.utils.TestUtils\n+import org.apache.kafka.clients.admin.{Admin, NewTopic}\n+import org.apache.kafka.metadata.BrokerState\n+import org.junit.jupiter.api.{Test, Timeout}\n+import org.junit.jupiter.api.Assertions._\n+\n+import java.util\n+import java.util.Collections\n+import java.util.concurrent.TimeUnit\n+\n+@Timeout(120000)\n+class RaftClusterTest {\n+\n+  @Test\n+  def testCreateClusterAndClose(): Unit = {\n+    val cluster = new KafkaClusterTestKit.Builder(\n+      new TestKitNodes.Builder().\n+        setNumKip500BrokerNodes(1).\n+        setNumControllerNodes(1).build()).build()\n+    try {\n+      cluster.format()\n+      cluster.startup()\n+    } finally {\n+      cluster.close()\n+    }\n+  }\n+\n+  @Test\n+  def testCreateClusterAndWaitForBrokerInRunningState(): Unit = {\n+    val cluster = new KafkaClusterTestKit.Builder(\n+      new TestKitNodes.Builder().\n+        setNumKip500BrokerNodes(3).\n+        setNumControllerNodes(3).build()).build()\n+    try {\n+      cluster.format()\n+      cluster.startup()\n+      TestUtils.waitUntilTrue(() => cluster.kip500Brokers().get(0).currentState() == BrokerState.RUNNING,\n+        \"Broker never made it to RUNNING state.\")\n+      TestUtils.waitUntilTrue(() => cluster.raftManagers().get(0).kafkaRaftClient.leaderAndEpoch().leaderId.isPresent,\n+        \"RaftManager was not initialized.\")\n+      val admin = Admin.create(cluster.clientProperties())\n+      try {\n+        assertEquals(cluster.nodes().clusterId().toString,\n+          admin.describeCluster().clusterId().get())\n+      } finally {\n+        admin.close()\n+      }\n+    } finally {\n+      cluster.close()\n+    }\n+  }\n+\n+  @Test\n+  def testCreateClusterAndCreateListDeleteTopic(): Unit = {\n+    val cluster = new KafkaClusterTestKit.Builder(\n+      new TestKitNodes.Builder().\n+        setNumKip500BrokerNodes(3).\n+        setNumControllerNodes(3).build()).build()\n+    try {\n+      cluster.format()\n+      cluster.startup()\n+      cluster.waitForReadyBrokers()\n+      TestUtils.waitUntilTrue(() => cluster.kip500Brokers().get(0).currentState() == BrokerState.RUNNING,\n+        \"Broker never made it to RUNNING state.\")\n+      TestUtils.waitUntilTrue(() => cluster.raftManagers().get(0).kafkaRaftClient.leaderAndEpoch().leaderId.isPresent,\n+        \"RaftManager was not initialized.\")\n+\n+      val admin = Admin.create(cluster.clientProperties())\n+      try {\n+        // Create a test topic\n+        val newTopic = Collections.singletonList(new NewTopic(\"test-topic\", 1, 3.toShort))\n+        val createTopicResult = admin.createTopics(newTopic)\n+        createTopicResult.all().get(60, TimeUnit.SECONDS)\n+\n+        // List created topic\n+        TestUtils.waitUntilTrue(() => {\n+          val listTopicsResult = admin.listTopics()\n+          val result = listTopicsResult.names().get(5, TimeUnit.SECONDS).size() == newTopic.size()\n+          if (result) {\n+            newTopic forEach(topic => {\n+              assertTrue(listTopicsResult.names().get().contains(topic.name()))\n+            })\n+          }\n+          result\n+        }, \"Topics created were not listed.\")\n+\n+        // Delete topic\n+        val deleteResult = admin.deleteTopics(Collections.singletonList(\"test-topic\"))\n+        deleteResult.all().get(60, TimeUnit.SECONDS)\n+\n+        // List again\n+        TestUtils.waitUntilTrue(() => {\n+          val listTopicsResult = admin.listTopics()\n+          val result = listTopicsResult.names().get(5, TimeUnit.SECONDS).size() != newTopic.size()\n+          if (result) {\n+            newTopic forEach(topic => {\n+              assertFalse(listTopicsResult.names().get().contains(topic.name()))\n+            })\n+          }\n+          result\n+        }, \"Topic was not removed from list.\")\n+\n+      } finally {\n+        admin.close()\n+      }\n+    } finally {\n+      cluster.close()\n+    }\n+  }\n+\n+  @Test\n+  def testCreateClusterAndCreateAndManyTopics(): Unit = {\n+    val cluster = new KafkaClusterTestKit.Builder(\n+      new TestKitNodes.Builder().\n+        setNumKip500BrokerNodes(3).\n+        setNumControllerNodes(3).build()).build()\n+    try {\n+      cluster.format()\n+      cluster.startup()\n+      cluster.waitForReadyBrokers()\n+      TestUtils.waitUntilTrue(() => cluster.kip500Brokers().get(0).currentState() == BrokerState.RUNNING,\n+        \"Broker never made it to RUNNING state.\")\n+      TestUtils.waitUntilTrue(() => cluster.raftManagers().get(0).kafkaRaftClient.leaderAndEpoch().leaderId.isPresent,\n+        \"RaftManager was not initialized.\")\n+      val admin = Admin.create(cluster.clientProperties())\n+      try {\n+        // Create many topics\n+        val newTopic = new util.ArrayList[NewTopic]()\n+        newTopic.add(new NewTopic(\"test-topic-1\", 1, 3.toShort))\n+        newTopic.add(new NewTopic(\"test-topic-2\", 1, 3.toShort))\n+        newTopic.add(new NewTopic(\"test-topic-3\", 1, 3.toShort))\n+        val createTopicResult = admin.createTopics(newTopic)\n+        createTopicResult.all().get(60, TimeUnit.SECONDS)\n+\n+        // List created topic\n+        TestUtils.waitUntilTrue(() => {\n+          val listTopicsResult = admin.listTopics()\n+          val result = listTopicsResult.names().get(5, TimeUnit.SECONDS).size() == newTopic.size()\n+          if (result) {\n+            newTopic forEach(topic => {\n+              assertTrue(listTopicsResult.names().get().contains(topic.name()))\n+            })\n+          }\n+          result\n+        }, \"Topics created were not listed.\")\n+      } finally {\n+        admin.close()\n+      }\n+    } finally {\n+      cluster.close()\n+    }\n+  }\n+\n+  @Test\n+  def testCreateClusterAndCreateAndManyTopicsWithManyPartitions(): Unit = {\n+    val cluster = new KafkaClusterTestKit.Builder(\n+      new TestKitNodes.Builder().\n+        setNumKip500BrokerNodes(3).\n+        setNumControllerNodes(3).build()).build()\n+    try {\n+      cluster.format()\n+      cluster.startup()\n+      cluster.waitForReadyBrokers()\n+      TestUtils.waitUntilTrue(() => cluster.kip500Brokers().get(0).currentState() == BrokerState.RUNNING,\n+        \"Broker never made it to RUNNING state.\")\n+      TestUtils.waitUntilTrue(() => cluster.raftManagers().get(0).kafkaRaftClient.leaderAndEpoch().leaderId.isPresent,\n+        \"RaftManager was not initialized.\")\n+      val admin = Admin.create(cluster.clientProperties())\n+      try {\n+        // Create many topics\n+        val newTopic = new util.ArrayList[NewTopic]()\n+        newTopic.add(new NewTopic(\"test-topic-1\", 3, 3.toShort))\n+        newTopic.add(new NewTopic(\"test-topic-2\", 3, 3.toShort))\n+        newTopic.add(new NewTopic(\"test-topic-3\", 3, 3.toShort))\n+        val createTopicResult = admin.createTopics(newTopic)\n+        createTopicResult.all().get(60, TimeUnit.SECONDS)\n+\n+        // List created topic\n+        TestUtils.waitUntilTrue(() => {\n+          val listTopicsResult = admin.listTopics()\n+          val result = listTopicsResult.names().get(5, TimeUnit.SECONDS).size() == newTopic.size()\n+          if (result) {\n+            newTopic forEach(topic => {\n+              assertTrue(listTopicsResult.names().get().contains(topic.name()))\n+            })\n+          }\n+          result\n+        }, \"Topics created were not listed.\")\n+      } finally {\n+        admin.close()\n+      }\n+    } finally {\n+      cluster.close()\n+    }\n+  }\n+}"
  },
  {
    "sha": "0833df0bb2328fbe306f3348041ae3dd63b6c6f7",
    "filename": "raft/src/main/java/org/apache/kafka/raft/RaftConfig.java",
    "status": "modified",
    "additions": 9,
    "deletions": 4,
    "changes": 13,
    "blob_url": "https://github.com/apache/kafka/blob/b028ae999e53a8a78ee6fc59f4410740978e4c91/raft/src/main/java/org/apache/kafka/raft/RaftConfig.java",
    "raw_url": "https://github.com/apache/kafka/raw/b028ae999e53a8a78ee6fc59f4410740978e4c91/raft/src/main/java/org/apache/kafka/raft/RaftConfig.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/raft/src/main/java/org/apache/kafka/raft/RaftConfig.java?ref=b028ae999e53a8a78ee6fc59f4410740978e4c91",
    "patch": "@@ -28,6 +28,7 @@\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n+import java.util.Objects;\n import java.util.Set;\n import java.util.stream.Collectors;\n \n@@ -241,12 +242,16 @@ private static Integer parseVoterId(String idString) {\n     }\n \n     public static List<Node> quorumVoterStringsToNodes(List<String> voters) {\n-        return parseVoterConnections(voters).entrySet().stream()\n+        return voterConnectionsToNodes(parseVoterConnections(voters));\n+    }\n+\n+    public static List<Node> voterConnectionsToNodes(Map<Integer, RaftConfig.AddressSpec> voterConnections) {\n+        return voterConnections.entrySet().stream()\n+            .filter(Objects::nonNull)\n             .filter(connection -> connection.getValue() instanceof InetAddressSpec)\n             .map(connection -> {\n-                InetAddressSpec inetAddressSpec = InetAddressSpec.class.cast(connection.getValue());\n-                return new Node(connection.getKey(), inetAddressSpec.address.getHostName(),\n-                    inetAddressSpec.address.getPort());\n+                InetAddressSpec spec = (InetAddressSpec) connection.getValue();\n+                return new Node(connection.getKey(), spec.address.getHostName(), spec.address.getPort());\n             })\n             .collect(Collectors.toList());\n     }"
  }
]
