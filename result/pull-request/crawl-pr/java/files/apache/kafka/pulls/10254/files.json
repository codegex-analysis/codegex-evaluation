[
  {
    "sha": "ee9664a8b1ca7f6e7dd964a6ffff8e680ab0754a",
    "filename": "checkstyle/import-control-core.xml",
    "status": "modified",
    "additions": 16,
    "deletions": 0,
    "changes": 16,
    "blob_url": "https://github.com/apache/kafka/blob/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/checkstyle/import-control-core.xml",
    "raw_url": "https://github.com/apache/kafka/raw/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/checkstyle/import-control-core.xml",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/checkstyle/import-control-core.xml?ref=ecc84e8015cf079ef14daf0deaff0a688fde4e0a",
    "patch": "@@ -43,6 +43,19 @@\n   <disallow class=\"com.yammer.metrics.Metrics\" />\n   <allow pkg=\"com.yammer.metrics\"/>\n \n+  <subpackage name=\"testkit\">\n+    <allow pkg=\"kafka.metrics\"/>\n+    <allow pkg=\"kafka.raft\"/>\n+    <allow pkg=\"kafka.server\"/>\n+    <allow pkg=\"kafka.tools\"/>\n+    <allow pkg=\"org.apache.kafka.clients\"/>\n+    <allow pkg=\"org.apache.kafka.controller\"/>\n+    <allow pkg=\"org.apache.kafka.raft\"/>\n+    <allow pkg=\"org.apache.kafka.test\"/>\n+    <allow pkg=\"org.apache.kafka.metadata\" />\n+    <allow pkg=\"org.apache.kafka.metalog\" />\n+  </subpackage>\n+\n   <subpackage name=\"tools\">\n     <allow pkg=\"org.apache.kafka.clients.admin\" />\n     <allow pkg=\"kafka.admin\" />\n@@ -71,6 +84,9 @@\n     </subpackage>\n     <subpackage name=\"junit\">\n       <allow pkg=\"kafka.test\"/>\n+      <allow pkg=\"kafka.testkit\"/>\n+      <allow pkg=\"org.apache.kafka.clients\"/>\n+      <allow pkg=\"org.apache.kafka.metadata\" />\n     </subpackage>\n   </subpackage>\n </import-control>"
  },
  {
    "sha": "254771b4c6b020791b3291e51238e99d8682a097",
    "filename": "core/src/main/scala/kafka/raft/RaftManager.scala",
    "status": "modified",
    "additions": 3,
    "deletions": 2,
    "changes": 5,
    "blob_url": "https://github.com/apache/kafka/blob/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/main/scala/kafka/raft/RaftManager.scala",
    "raw_url": "https://github.com/apache/kafka/raw/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/main/scala/kafka/raft/RaftManager.scala",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/raft/RaftManager.scala?ref=ecc84e8015cf079ef14daf0deaff0a688fde4e0a",
    "patch": "@@ -104,7 +104,8 @@ class KafkaRaftManager[T](\n   topicPartition: TopicPartition,\n   time: Time,\n   metrics: Metrics,\n-  threadNamePrefixOpt: Option[String]\n+  threadNamePrefixOpt: Option[String],\n+  val controllerQuorumVotersFuture: CompletableFuture[util.List[String]]\n ) extends RaftManager[T] with Logging {\n \n   private val raftConfig = new RaftConfig(config)\n@@ -125,7 +126,7 @@ class KafkaRaftManager[T](\n \n   def startup(): Unit = {\n     // Update the voter endpoints (if valid) with what's in RaftConfig\n-    val voterAddresses: util.Map[Integer, AddressSpec] = raftConfig.quorumVoterConnections\n+    val voterAddresses: util.Map[Integer, AddressSpec] = RaftConfig.parseVoterConnections(controllerQuorumVotersFuture.get())\n     for (voterAddressEntry <- voterAddresses.entrySet.asScala) {\n       voterAddressEntry.getValue match {\n         case spec: InetAddressSpec =>"
  },
  {
    "sha": "595eb7bb64586e297dc202f34f2bb807080985ff",
    "filename": "core/src/main/scala/kafka/server/BrokerServer.scala",
    "status": "modified",
    "additions": 2,
    "deletions": 1,
    "changes": 3,
    "blob_url": "https://github.com/apache/kafka/blob/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/main/scala/kafka/server/BrokerServer.scala",
    "raw_url": "https://github.com/apache/kafka/raw/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/main/scala/kafka/server/BrokerServer.scala",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/server/BrokerServer.scala?ref=ecc84e8015cf079ef14daf0deaff0a688fde4e0a",
    "patch": "@@ -113,6 +113,7 @@ class BrokerServer(\n   var metadataCache: RaftMetadataCache = null\n \n   var quotaManagers: QuotaFactory.QuotaManagers = null\n+\n   var quotaCache: ClientQuotaCache = null\n \n   private var _brokerTopicStats: BrokerTopicStats = null\n@@ -325,7 +326,7 @@ class BrokerServer(\n \n       // Start processing requests once we've caught up on the metadata log, recovered logs if necessary,\n       // and started all services that we previously delayed starting.\n-      val raftSupport = RaftSupport(forwardingManager, metadataCache)\n+      val raftSupport = RaftSupport(forwardingManager, metadataCache, quotaCache)\n       dataPlaneRequestProcessor = new KafkaApis(socketServer.dataPlaneRequestChannel, raftSupport,\n         replicaManager, groupCoordinator, transactionCoordinator, autoTopicCreationManager,\n         config.nodeId, config, configRepository, metadataCache, metrics, authorizer, quotaManagers,"
  },
  {
    "sha": "6311a4026e88693b636a1d62ca971e18bc27eab9",
    "filename": "core/src/main/scala/kafka/server/KafkaApis.scala",
    "status": "modified",
    "additions": 41,
    "deletions": 28,
    "changes": 69,
    "blob_url": "https://github.com/apache/kafka/blob/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/main/scala/kafka/server/KafkaApis.scala",
    "raw_url": "https://github.com/apache/kafka/raw/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/main/scala/kafka/server/KafkaApis.scala",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/server/KafkaApis.scala?ref=ecc84e8015cf079ef14daf0deaff0a688fde4e0a",
    "patch": "@@ -23,7 +23,6 @@ import java.util\n import java.util.concurrent.ConcurrentHashMap\n import java.util.concurrent.atomic.AtomicInteger\n import java.util.{Collections, Optional}\n-\n import kafka.admin.AdminUtils\n import kafka.api.{ApiVersion, ElectLeadersRequestOps, KAFKA_0_11_0_IV0, KAFKA_2_3_IV0}\n import kafka.common.OffsetAndMetadata\n@@ -65,6 +64,7 @@ import org.apache.kafka.common.message.{AddOffsetsToTxnResponseData, AlterClient\n import org.apache.kafka.common.metrics.Metrics\n import org.apache.kafka.common.network.{ListenerName, Send}\n import org.apache.kafka.common.protocol.{ApiKeys, Errors}\n+import org.apache.kafka.common.quota.ClientQuotaEntity\n import org.apache.kafka.common.record._\n import org.apache.kafka.common.replica.ClientMetadata\n import org.apache.kafka.common.replica.ClientMetadata.DefaultClientMetadata\n@@ -3023,37 +3023,50 @@ class KafkaApis(val requestChannel: RequestChannel,\n   }\n \n   def handleDescribeClientQuotasRequest(request: RequestChannel.Request): Unit = {\n-    val zkSupport = metadataSupport.requireZkOrThrow(KafkaApis.notYetSupported(request))\n     val describeClientQuotasRequest = request.body[DescribeClientQuotasRequest]\n \n-    if (authHelper.authorize(request.context, DESCRIBE_CONFIGS, CLUSTER, CLUSTER_NAME)) {\n-      val result = zkSupport.adminManager.describeClientQuotas(describeClientQuotasRequest.filter)\n-\n-      val entriesData = result.iterator.map { case (quotaEntity, quotaValues) =>\n-        val entityData = quotaEntity.entries.asScala.iterator.map { case (entityType, entityName) =>\n-          new DescribeClientQuotasResponseData.EntityData()\n-            .setEntityType(entityType)\n-            .setEntityName(entityName)\n-        }.toBuffer\n-\n-        val valueData = quotaValues.iterator.map { case (key, value) =>\n-          new DescribeClientQuotasResponseData.ValueData()\n-            .setKey(key)\n-            .setValue(value)\n-        }.toBuffer\n-\n-        new DescribeClientQuotasResponseData.EntryData()\n-          .setEntity(entityData.asJava)\n-          .setValues(valueData.asJava)\n-      }.toBuffer\n-\n-      requestHelper.sendResponseMaybeThrottle(request, requestThrottleMs =>\n-        new DescribeClientQuotasResponse(new DescribeClientQuotasResponseData()\n-          .setThrottleTimeMs(requestThrottleMs)\n-          .setEntries(entriesData.asJava)))\n-    } else {\n+    if (!authHelper.authorize(request.context, DESCRIBE_CONFIGS, CLUSTER, CLUSTER_NAME)) {\n       requestHelper.sendResponseMaybeThrottle(request, requestThrottleMs =>\n         describeClientQuotasRequest.getErrorResponse(requestThrottleMs, Errors.CLUSTER_AUTHORIZATION_FAILED.exception))\n+    } else {\n+      metadataSupport match {\n+        case ZkSupport(adminManager, controller, zkClient, forwardingManager, metadataCache) =>\n+          val result = adminManager.describeClientQuotas(describeClientQuotasRequest.filter)\n+\n+          val entriesData = result.iterator.map { case (quotaEntity, quotaValues) =>\n+            val entityData = quotaEntity.entries.asScala.iterator.map { case (entityType, entityName) =>\n+              new DescribeClientQuotasResponseData.EntityData()\n+                .setEntityType(entityType)\n+                .setEntityName(entityName)\n+            }.toBuffer\n+\n+            val valueData = quotaValues.iterator.map { case (key, value) =>\n+              new DescribeClientQuotasResponseData.ValueData()\n+                .setKey(key)\n+                .setValue(value)\n+            }.toBuffer\n+\n+            new DescribeClientQuotasResponseData.EntryData()\n+              .setEntity(entityData.asJava)\n+              .setValues(valueData.asJava)\n+          }.toBuffer\n+\n+          requestHelper.sendResponseMaybeThrottle(request, requestThrottleMs =>\n+            new DescribeClientQuotasResponse(new DescribeClientQuotasResponseData()\n+              .setThrottleTimeMs(requestThrottleMs)\n+              .setEntries(entriesData.asJava)))\n+        case RaftSupport(fwdMgr, metadataCache, quotaCache) =>\n+          val result = quotaCache.describeClientQuotas(\n+            describeClientQuotasRequest.filter().components().asScala.toSeq,\n+            describeClientQuotasRequest.filter().strict())\n+          val resultAsJava = new util.HashMap[ClientQuotaEntity, util.Map[String, java.lang.Double]](result.size)\n+          result.foreach { case (entity, quotas) =>\n+            resultAsJava.put(entity, quotas.map { case (key, quota) => key -> Double.box(quota)}.asJava)\n+          }\n+          requestHelper.sendResponseMaybeThrottle(request, requestThrottleMs =>\n+            DescribeClientQuotasResponse.fromQuotaEntities(resultAsJava, requestThrottleMs)\n+          )\n+      }\n     }\n   }\n "
  },
  {
    "sha": "644ba37a980c5114d93e3485faadc3c1620a0146",
    "filename": "core/src/main/scala/kafka/server/KafkaRaftServer.scala",
    "status": "modified",
    "additions": 2,
    "deletions": 1,
    "changes": 3,
    "blob_url": "https://github.com/apache/kafka/blob/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/main/scala/kafka/server/KafkaRaftServer.scala",
    "raw_url": "https://github.com/apache/kafka/raw/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/main/scala/kafka/server/KafkaRaftServer.scala",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/server/KafkaRaftServer.scala?ref=ecc84e8015cf079ef14daf0deaff0a688fde4e0a",
    "patch": "@@ -68,7 +68,8 @@ class KafkaRaftServer(\n     KafkaRaftServer.MetadataPartition,\n     time,\n     metrics,\n-    threadNamePrefix\n+    threadNamePrefix,\n+    controllerQuorumVotersFuture\n   )\n \n   private val metaLogShim = new MetaLogRaftShim(raftManager.kafkaRaftClient, config.nodeId)"
  },
  {
    "sha": "3970f4988330c0438e181849ed05ed2d60cfdd87",
    "filename": "core/src/main/scala/kafka/server/MetadataSupport.scala",
    "status": "modified",
    "additions": 3,
    "deletions": 2,
    "changes": 5,
    "blob_url": "https://github.com/apache/kafka/blob/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/main/scala/kafka/server/MetadataSupport.scala",
    "raw_url": "https://github.com/apache/kafka/raw/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/main/scala/kafka/server/MetadataSupport.scala",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/server/MetadataSupport.scala?ref=ecc84e8015cf079ef14daf0deaff0a688fde4e0a",
    "patch": "@@ -19,7 +19,7 @@ package kafka.server\n \n import kafka.controller.KafkaController\n import kafka.network.RequestChannel\n-import kafka.server.metadata.RaftMetadataCache\n+import kafka.server.metadata.{ClientQuotaCache, RaftMetadataCache}\n import kafka.zk.{AdminZkClient, KafkaZkClient}\n import org.apache.kafka.common.requests.AbstractResponse\n \n@@ -91,7 +91,8 @@ case class ZkSupport(adminManager: ZkAdminManager,\n   override def controllerId: Option[Int] =  metadataCache.getControllerId\n }\n \n-case class RaftSupport(fwdMgr: ForwardingManager, metadataCache: RaftMetadataCache) extends MetadataSupport {\n+case class RaftSupport(fwdMgr: ForwardingManager, metadataCache: RaftMetadataCache, quotaCache: ClientQuotaCache)\n+    extends MetadataSupport {\n   override val forwardingManager: Option[ForwardingManager] = Some(fwdMgr)\n   override def requireZkOrThrow(createException: => Exception): ZkSupport = throw createException\n   override def requireRaftOrThrow(createException: => Exception): RaftSupport = this"
  },
  {
    "sha": "3a148e63bd95a0c740aca469f62369211e1d82c1",
    "filename": "core/src/main/scala/kafka/server/metadata/ClientQuotaCache.scala",
    "status": "modified",
    "additions": 11,
    "deletions": 1,
    "changes": 12,
    "blob_url": "https://github.com/apache/kafka/blob/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/main/scala/kafka/server/metadata/ClientQuotaCache.scala",
    "raw_url": "https://github.com/apache/kafka/raw/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/main/scala/kafka/server/metadata/ClientQuotaCache.scala",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/server/metadata/ClientQuotaCache.scala?ref=ecc84e8015cf079ef14daf0deaff0a688fde4e0a",
    "patch": "@@ -122,6 +122,14 @@ class ClientQuotaCache {\n       entityFilters.put(entityType, entityMatch)\n     }\n \n+    // Special case for non-strict empty filter, match everything\n+    if (filters.isEmpty && !strict) {\n+      val allResults: Map[QuotaEntity, Map[String, Double]] = quotaCache.map {\n+        entry => entry._1 -> entry._2.toMap\n+      }.toMap\n+      return allResults\n+    }\n+\n     if (entityFilters.isEmpty) {\n       return Map.empty\n     }\n@@ -130,7 +138,7 @@ class ClientQuotaCache {\n     val matchingEntities: Set[QuotaEntity] = if (entityFilters.contains(ClientQuotaEntity.IP)) {\n       if (entityFilters.size > 1) {\n         throw new InvalidRequestException(\"Invalid entity filter component combination, IP filter component should \" +\n-          \"not be used with user or clientId filter component.\")\n+          \"not be used with User or ClientId filter component.\")\n       }\n       val ipMatch = entityFilters.get(ClientQuotaEntity.IP)\n       ipMatch.fold(Set.empty[QuotaEntity]) {\n@@ -294,4 +302,6 @@ class ClientQuotaCache {\n         updateCacheIndexPartial(ipEntityIndex, DefaultIp)\n     }\n   }\n+\n+  override def toString = s\"ClientQuotaCache($quotaCache)\"\n }"
  },
  {
    "sha": "fbb703ec7bbc5fc91148da91ca31541efdb5f7a6",
    "filename": "core/src/main/scala/kafka/server/metadata/ClientQuotaMetadataManager.scala",
    "status": "modified",
    "additions": 3,
    "deletions": 3,
    "changes": 6,
    "blob_url": "https://github.com/apache/kafka/blob/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/main/scala/kafka/server/metadata/ClientQuotaMetadataManager.scala",
    "raw_url": "https://github.com/apache/kafka/raw/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/main/scala/kafka/server/metadata/ClientQuotaMetadataManager.scala",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/server/metadata/ClientQuotaMetadataManager.scala?ref=ecc84e8015cf079ef14daf0deaff0a688fde4e0a",
    "patch": "@@ -121,16 +121,16 @@ class ClientQuotaMetadataManager(private[metadata] val quotaManagers: QuotaManag\n       return\n     }\n \n-    // Update the cache\n-    quotaCache.updateQuotaCache(ipEntity, quotaRecord.key, quotaRecord.value, quotaRecord.remove)\n-\n     // Convert the value to an appropriate Option for the quota manager\n     val newValue = if (quotaRecord.remove()) {\n       None\n     } else {\n       Some(quotaRecord.value).map(_.toInt)\n     }\n     connectionQuotas.updateIpConnectionRateQuota(inetAddress, newValue)\n+\n+    // Update the cache\n+    quotaCache.updateQuotaCache(ipEntity, quotaRecord.key, quotaRecord.value, quotaRecord.remove)\n   }\n \n   def handleUserClientQuota(quotaEntity: QuotaEntity, quotaRecord: QuotaRecord): Unit = {"
  },
  {
    "sha": "75b27b9cddab2f4176168f32f54f594fc508e7b0",
    "filename": "core/src/main/scala/kafka/tools/TestRaftServer.scala",
    "status": "modified",
    "additions": 3,
    "deletions": 3,
    "changes": 6,
    "blob_url": "https://github.com/apache/kafka/blob/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/main/scala/kafka/tools/TestRaftServer.scala",
    "raw_url": "https://github.com/apache/kafka/raw/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/main/scala/kafka/tools/TestRaftServer.scala",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/tools/TestRaftServer.scala?ref=ecc84e8015cf079ef14daf0deaff0a688fde4e0a",
    "patch": "@@ -18,8 +18,7 @@\n package kafka.tools\n \n import java.util.concurrent.atomic.{AtomicInteger, AtomicLong}\n-import java.util.concurrent.{CountDownLatch, LinkedBlockingDeque, TimeUnit}\n-\n+import java.util.concurrent.{CompletableFuture, CountDownLatch, LinkedBlockingDeque, TimeUnit}\n import joptsimple.OptionException\n import kafka.network.SocketServer\n import kafka.raft.{KafkaRaftManager, RaftManager}\n@@ -85,7 +84,8 @@ class TestRaftServer(\n       partition,\n       time,\n       metrics,\n-      Some(threadNamePrefix)\n+      Some(threadNamePrefix),\n+      CompletableFuture.completedFuture(config.quorumVoters)\n     )\n \n     workloadGenerator = new RaftWorkloadGenerator("
  },
  {
    "sha": "cac986fdc317cbc8705755a59be9263c9afb1b09",
    "filename": "core/src/test/java/kafka/test/ClusterInstance.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/apache/kafka/blob/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/java/kafka/test/ClusterInstance.java",
    "raw_url": "https://github.com/apache/kafka/raw/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/java/kafka/test/ClusterInstance.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/java/kafka/test/ClusterInstance.java?ref=ecc84e8015cf079ef14daf0deaff0a688fde4e0a",
    "patch": "@@ -29,7 +29,7 @@\n \n     enum ClusterType {\n         ZK,\n-        // RAFT\n+        RAFT\n     }\n \n     /**"
  },
  {
    "sha": "7d07d44d0abf311ac4610d6fe783f68b8f03745c",
    "filename": "core/src/test/java/kafka/test/ClusterTestExtensionsTest.java",
    "status": "modified",
    "additions": 4,
    "deletions": 4,
    "changes": 8,
    "blob_url": "https://github.com/apache/kafka/blob/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/java/kafka/test/ClusterTestExtensionsTest.java",
    "raw_url": "https://github.com/apache/kafka/raw/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/java/kafka/test/ClusterTestExtensionsTest.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/java/kafka/test/ClusterTestExtensionsTest.java?ref=ecc84e8015cf079ef14daf0deaff0a688fde4e0a",
    "patch": "@@ -86,20 +86,20 @@ public void testClusterTemplate() {\n             @ClusterConfigProperty(key = \"foo\", value = \"bar\"),\n             @ClusterConfigProperty(key = \"spam\", value = \"eggs\")\n         }),\n-        @ClusterTest(name = \"cluster-tests-2\", clusterType = Type.ZK, serverProperties = {\n+        @ClusterTest(name = \"cluster-tests-2\", clusterType = Type.RAFT, serverProperties = {\n             @ClusterConfigProperty(key = \"foo\", value = \"baz\"),\n             @ClusterConfigProperty(key = \"spam\", value = \"eggz\")\n         })\n     })\n     public void testClusterTests() {\n-        if (clusterInstance.config().name().filter(name -> name.equals(\"cluster-tests-1\")).isPresent()) {\n+        if (clusterInstance.clusterType().equals(ClusterInstance.ClusterType.ZK)) {\n             Assertions.assertEquals(clusterInstance.config().serverProperties().getProperty(\"foo\"), \"bar\");\n             Assertions.assertEquals(clusterInstance.config().serverProperties().getProperty(\"spam\"), \"eggs\");\n-        } else if (clusterInstance.config().name().filter(name -> name.equals(\"cluster-tests-2\")).isPresent()) {\n+        } else if (clusterInstance.clusterType().equals(ClusterInstance.ClusterType.RAFT)) {\n             Assertions.assertEquals(clusterInstance.config().serverProperties().getProperty(\"foo\"), \"baz\");\n             Assertions.assertEquals(clusterInstance.config().serverProperties().getProperty(\"spam\"), \"eggz\");\n         } else {\n-            Assertions.fail(\"Unknown cluster config \" + clusterInstance.config().name());\n+            Assertions.fail(\"Unknown cluster type \" + clusterInstance.clusterType());\n         }\n     }\n "
  },
  {
    "sha": "c5746b2acfc01c20370b0fee0dacfd0f550de8d8",
    "filename": "core/src/test/java/kafka/test/annotation/Type.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/apache/kafka/blob/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/java/kafka/test/annotation/Type.java",
    "raw_url": "https://github.com/apache/kafka/raw/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/java/kafka/test/annotation/Type.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/java/kafka/test/annotation/Type.java?ref=ecc84e8015cf079ef14daf0deaff0a688fde4e0a",
    "patch": "@@ -21,7 +21,7 @@\n  * The type of cluster config being requested. Used by {@link kafka.test.ClusterConfig} and the test annotations.\n  */\n public enum Type {\n-    // RAFT,\n+    RAFT,\n     ZK,\n     BOTH,\n     DEFAULT"
  },
  {
    "sha": "cccb4c21479b2876c64ebd724f3e9b0c64fa2e38",
    "filename": "core/src/test/java/kafka/test/junit/ClusterTestExtensions.java",
    "status": "modified",
    "additions": 29,
    "deletions": 6,
    "changes": 35,
    "blob_url": "https://github.com/apache/kafka/blob/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/java/kafka/test/junit/ClusterTestExtensions.java",
    "raw_url": "https://github.com/apache/kafka/raw/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/java/kafka/test/junit/ClusterTestExtensions.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/java/kafka/test/junit/ClusterTestExtensions.java?ref=ecc84e8015cf079ef14daf0deaff0a688fde4e0a",
    "patch": "@@ -129,10 +129,20 @@ private void processClusterTemplate(ExtensionContext context, ClusterTemplate an\n         }\n \n         generatedClusterConfigs.forEach(config -> {\n-            if (config.clusterType() == Type.ZK) {\n-                testInvocations.accept(new ZkClusterInvocationContext(config.copyOf()));\n-            } else {\n-                throw new IllegalStateException(\"Unknown cluster type \" + config.clusterType());\n+            switch (config.clusterType()) {\n+                case RAFT:\n+                    testInvocations.accept(new RaftClusterInvocationContext(config.copyOf()));\n+                    break;\n+                case BOTH:\n+                    testInvocations.accept(new RaftClusterInvocationContext(config.copyOf()));\n+                    testInvocations.accept(new ZkClusterInvocationContext(config.copyOf()));\n+                    break;\n+                case ZK:\n+                    testInvocations.accept(new ZkClusterInvocationContext(config.copyOf()));\n+                    break;\n+                case DEFAULT: // Should not see DEFAULT here\n+                default:\n+                    throw new IllegalStateException(\"Unknown cluster type \" + config.clusterType());\n             }\n         });\n     }\n@@ -199,12 +209,25 @@ private void processClusterTest(ClusterTest annot, ClusterTestDefaults defaults,\n         }\n \n         switch (type) {\n-            case ZK:\n-            case BOTH:\n+            case ZK: {\n                 ClusterConfig config = builder.build();\n                 config.serverProperties().putAll(properties);\n                 testInvocations.accept(new ZkClusterInvocationContext(config));\n                 break;\n+            } case RAFT: {\n+                ClusterConfig config = builder.build();\n+                config.serverProperties().putAll(properties);\n+                testInvocations.accept(new RaftClusterInvocationContext(config));\n+                break;\n+            } case BOTH: {\n+                ClusterConfig zkConfig = builder.build();\n+                zkConfig.serverProperties().putAll(properties);\n+                testInvocations.accept(new ZkClusterInvocationContext(zkConfig));\n+                ClusterConfig raftConfig = builder.build();\n+                raftConfig.serverProperties().putAll(properties);\n+                testInvocations.accept(new RaftClusterInvocationContext(raftConfig));\n+                break;\n+            }\n         }\n     }\n "
  },
  {
    "sha": "385ee2030bc94b5beece38560133f74f83c986e6",
    "filename": "core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java",
    "status": "added",
    "additions": 196,
    "deletions": 0,
    "changes": 196,
    "blob_url": "https://github.com/apache/kafka/blob/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java",
    "raw_url": "https://github.com/apache/kafka/raw/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java?ref=ecc84e8015cf079ef14daf0deaff0a688fde4e0a",
    "patch": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.test.junit;\n+\n+import kafka.network.SocketServer;\n+import kafka.server.BrokerServer;\n+import kafka.server.ControllerServer;\n+import kafka.test.ClusterConfig;\n+import kafka.test.ClusterInstance;\n+import kafka.testkit.KafkaClusterTestKit;\n+import kafka.testkit.TestKitNodes;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.common.network.ListenerName;\n+import org.apache.kafka.metadata.BrokerState;\n+import org.junit.jupiter.api.extension.AfterTestExecutionCallback;\n+import org.junit.jupiter.api.extension.BeforeTestExecutionCallback;\n+import org.junit.jupiter.api.extension.Extension;\n+import org.junit.jupiter.api.extension.TestTemplateInvocationContext;\n+\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Wraps a {@link KafkaClusterTestKit} inside lifecycle methods for a test invocation. Each instance of this\n+ * class is provided with a configuration for the cluster.\n+ *\n+ * This context also provides parameter resolvers for:\n+ *\n+ * <ul>\n+ *     <li>ClusterConfig (the same instance passed to the constructor)</li>\n+ *     <li>ClusterInstance (includes methods to expose underlying SocketServer-s)</li>\n+ *     <li>IntegrationTestHelper (helper methods)</li>\n+ * </ul>\n+ */\n+public class RaftClusterInvocationContext implements TestTemplateInvocationContext {\n+\n+    private final ClusterConfig clusterConfig;\n+    private final AtomicReference<KafkaClusterTestKit> clusterReference;\n+\n+    public RaftClusterInvocationContext(ClusterConfig clusterConfig) {\n+        this.clusterConfig = clusterConfig;\n+        this.clusterReference = new AtomicReference<>();\n+    }\n+\n+    @Override\n+    public String getDisplayName(int invocationIndex) {\n+        String clusterDesc = clusterConfig.nameTags().entrySet().stream()\n+            .map(Object::toString)\n+            .collect(Collectors.joining(\", \"));\n+        return String.format(\"[Quorum %d] %s\", invocationIndex, clusterDesc);\n+    }\n+\n+    @Override\n+    public List<Extension> getAdditionalExtensions() {\n+        return Arrays.asList(\n+            (BeforeTestExecutionCallback) context -> {\n+                KafkaClusterTestKit.Builder builder = new KafkaClusterTestKit.Builder(\n+                    new TestKitNodes.Builder().\n+                        setNumKip500BrokerNodes(clusterConfig.numBrokers()).\n+                        setNumControllerNodes(clusterConfig.numControllers()).build());\n+\n+                // Copy properties into the TestKit builder\n+                clusterConfig.serverProperties().forEach((key, value) -> builder.setConfigProp(key.toString(), value.toString()));\n+                // TODO how to pass down security protocol and listener name?\n+                KafkaClusterTestKit cluster = builder.build();\n+                clusterReference.set(cluster);\n+                cluster.format();\n+                cluster.startup();\n+                kafka.utils.TestUtils.waitUntilTrue(\n+                    () -> cluster.kip500Brokers().get(0).currentState() == BrokerState.RUNNING,\n+                    () -> \"Broker never made it to RUNNING state.\",\n+                    org.apache.kafka.test.TestUtils.DEFAULT_MAX_WAIT_MS,\n+                    100L);\n+            },\n+            (AfterTestExecutionCallback) context -> clusterReference.get().close(),\n+            new ClusterInstanceParameterResolver(new RaftClusterInstance(clusterReference, clusterConfig)),\n+            new GenericParameterResolver<>(clusterConfig, ClusterConfig.class)\n+        );\n+    }\n+\n+    public static class RaftClusterInstance implements ClusterInstance {\n+\n+        private final AtomicReference<KafkaClusterTestKit> clusterReference;\n+        private final ClusterConfig clusterConfig;\n+        final AtomicBoolean started = new AtomicBoolean(false);\n+        final AtomicBoolean stopped = new AtomicBoolean(false);\n+\n+        RaftClusterInstance(AtomicReference<KafkaClusterTestKit> clusterReference, ClusterConfig clusterConfig) {\n+            this.clusterReference = clusterReference;\n+            this.clusterConfig = clusterConfig;\n+        }\n+\n+        @Override\n+        public String bootstrapServers() {\n+            return clusterReference.get().clientProperties().getProperty(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG);\n+        }\n+\n+        @Override\n+        public Collection<SocketServer> brokerSocketServers() {\n+            return clusterReference.get().kip500Brokers().values().stream()\n+                .map(BrokerServer::socketServer)\n+                .collect(Collectors.toList());\n+        }\n+\n+        @Override\n+        public ListenerName clientListener() {\n+            return ListenerName.normalised(\"EXTERNAL\");\n+        }\n+\n+        @Override\n+        public Collection<SocketServer> controllerSocketServers() {\n+            return clusterReference.get().controllers().values().stream()\n+                .map(ControllerServer::socketServer)\n+                .collect(Collectors.toList());\n+        }\n+\n+        @Override\n+        public SocketServer anyBrokerSocketServer() {\n+            return clusterReference.get().kip500Brokers().values().stream()\n+                .map(BrokerServer::socketServer)\n+                .findFirst()\n+                .orElseThrow(() -> new RuntimeException(\"No broker SocketServers found\"));\n+        }\n+\n+        @Override\n+        public SocketServer anyControllerSocketServer() {\n+            return clusterReference.get().controllers().values().stream()\n+                .map(ControllerServer::socketServer)\n+                .findFirst()\n+                .orElseThrow(() -> new RuntimeException(\"No controller SocketServers found\"));\n+        }\n+\n+        @Override\n+        public ClusterType clusterType() {\n+            return ClusterType.RAFT;\n+        }\n+\n+        @Override\n+        public ClusterConfig config() {\n+            return clusterConfig;\n+        }\n+\n+        @Override\n+        public Object getUnderlying() {\n+            return clusterReference.get();\n+        }\n+\n+        @Override\n+        public Admin createAdminClient(Properties configOverrides) {\n+            return Admin.create(clusterReference.get().clientProperties());\n+        }\n+\n+        @Override\n+        public void start() {\n+            if (started.compareAndSet(false, true)) {\n+                try {\n+                    clusterReference.get().startup();\n+                } catch (Exception e) {\n+                    throw new RuntimeException(\"Failed to start up Raft server\", e);\n+                }\n+            }\n+        }\n+\n+        @Override\n+        public void stop() {\n+            if (stopped.compareAndSet(false, true)) {\n+                try {\n+                    clusterReference.get().close();\n+                } catch (Exception e) {\n+                    throw new RuntimeException(\"Failed to stop up Raft server\", e);\n+                }\n+            }\n+        }\n+    }\n+}\n\\ No newline at end of file"
  },
  {
    "sha": "be6c8067f1f7f19600c48e5c76e2b5f06386f5cd",
    "filename": "core/src/test/java/kafka/testkit/ControllerNode.java",
    "status": "added",
    "additions": 63,
    "deletions": 0,
    "changes": 63,
    "blob_url": "https://github.com/apache/kafka/blob/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/java/kafka/testkit/ControllerNode.java",
    "raw_url": "https://github.com/apache/kafka/raw/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/java/kafka/testkit/ControllerNode.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/java/kafka/testkit/ControllerNode.java?ref=ecc84e8015cf079ef14daf0deaff0a688fde4e0a",
    "patch": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.testkit;\n+\n+public class ControllerNode implements TestKitNode {\n+    public static class Builder {\n+        private int id = -1;\n+        private String metadataDirectory = null;\n+\n+        public Builder setId(int id) {\n+            this.id = id;\n+            return this;\n+        }\n+\n+        public Builder setMetadataDirectory() {\n+            this.metadataDirectory = metadataDirectory;\n+            return this;\n+        }\n+\n+        public ControllerNode build() {\n+            if (id == -1) {\n+                throw new RuntimeException(\"You must set the node id\");\n+            }\n+            if (metadataDirectory == null) {\n+                metadataDirectory = String.format(\"controller_%d\", id);\n+            }\n+            return new ControllerNode(id, metadataDirectory);\n+        }\n+    }\n+\n+    private final int id;\n+    private final String metadataDirectory;\n+\n+    ControllerNode(int id, String metadataDirectory) {\n+        this.id = id;\n+        this.metadataDirectory = metadataDirectory;\n+    }\n+\n+    @Override\n+    public int id() {\n+        return id;\n+    }\n+\n+    @Override\n+    public String metadataDirectory() {\n+        return metadataDirectory;\n+    }\n+}"
  },
  {
    "sha": "50638b4682726227e751232d303d3ca8819340a8",
    "filename": "core/src/test/java/kafka/testkit/KafkaClusterTestKit.java",
    "status": "added",
    "additions": 499,
    "deletions": 0,
    "changes": 499,
    "blob_url": "https://github.com/apache/kafka/blob/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/java/kafka/testkit/KafkaClusterTestKit.java",
    "raw_url": "https://github.com/apache/kafka/raw/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/java/kafka/testkit/KafkaClusterTestKit.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/java/kafka/testkit/KafkaClusterTestKit.java?ref=ecc84e8015cf079ef14daf0deaff0a688fde4e0a",
    "patch": "@@ -0,0 +1,499 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.testkit;\n+\n+import kafka.raft.KafkaRaftManager;\n+import kafka.server.KafkaConfig;\n+import kafka.server.KafkaConfig$;\n+import kafka.server.Server;\n+import kafka.server.BrokerServer;\n+import kafka.server.ControllerServer;\n+import kafka.server.MetaProperties;\n+import kafka.tools.StorageTool;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.metrics.Metrics;\n+import org.apache.kafka.common.network.ListenerName;\n+import org.apache.kafka.common.utils.ThreadUtils;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.controller.Controller;\n+import org.apache.kafka.metadata.ApiMessageAndVersion;\n+import org.apache.kafka.metalog.MetaLogManager;\n+import org.apache.kafka.raft.RaftConfig;\n+import org.apache.kafka.raft.metadata.MetaLogRaftShim;\n+import org.apache.kafka.raft.metadata.MetadataRecordSerde;\n+import org.apache.kafka.test.TestUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Option;\n+import scala.collection.JavaConverters;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.PrintStream;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.AbstractMap.SimpleImmutableEntry;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Properties;\n+import java.util.TreeMap;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+\n+@SuppressWarnings(\"deprecation\") // Needed for Scala 2.12 compatibility\n+public class KafkaClusterTestKit implements AutoCloseable {\n+    private final static Logger log = LoggerFactory.getLogger(KafkaClusterTestKit.class);\n+\n+    /**\n+     * This class manages a future which is completed with the proper value for\n+     * controller.quorum.voters once the randomly assigned ports for all the controllers are\n+     * known.\n+     */\n+    private static class ControllerQuorumVotersFutureManager implements AutoCloseable {\n+        private final int expectedControllers;\n+        private final CompletableFuture<List<String>> future = new CompletableFuture<>();\n+        private final Map<Integer, Integer> controllerPorts = new TreeMap<>();\n+\n+        ControllerQuorumVotersFutureManager(int expectedControllers) {\n+            this.expectedControllers = expectedControllers;\n+        }\n+\n+        synchronized void registerPort(int nodeId, int port) {\n+            controllerPorts.put(nodeId, port);\n+            if (controllerPorts.size() >= expectedControllers) {\n+                future.complete(controllerPorts.entrySet().stream().\n+                    map(e -> String.format(\"%d@localhost:%d\", e.getKey(), e.getValue())).\n+                    collect(Collectors.toList()));\n+            }\n+        }\n+\n+        void fail(Throwable e) {\n+            future.completeExceptionally(e);\n+        }\n+\n+        @Override\n+        public void close() {\n+            future.cancel(true);\n+        }\n+    }\n+\n+    public static class Builder {\n+        private TestKitNodes nodes;\n+        private Map<String, String> configProps = new HashMap<>();\n+\n+        public Builder(TestKitNodes nodes) {\n+            this.nodes = nodes;\n+        }\n+\n+        public Builder setConfigProp(String key, String value) {\n+            this.configProps.put(key, value);\n+            return this;\n+        }\n+\n+        public KafkaClusterTestKit build() throws Exception {\n+            Map<Integer, ControllerServer> controllers = new HashMap<>();\n+            Map<Integer, BrokerServer> kip500Brokers = new HashMap<>();\n+            Map<Integer, KafkaRaftManager> raftManagers = new HashMap<>();\n+            String dummyQuorumVotersString = nodes.controllerNodes().keySet().stream().\n+                map(controllerNode -> String.format(\"%d@0.0.0.0:0\", controllerNode)).\n+                collect(Collectors.joining(\",\"));\n+            /*\n+              Number of threads = Total number of brokers + Total number of controllers + Total number of Raft Managers\n+                                = Total number of brokers + Total number of controllers * 2\n+                                  (Raft Manager per broker/controller)\n+             */\n+            int numOfExecutorThreads = (nodes.brokerNodes().size() + nodes.controllerNodes().size()) * 2;\n+            ExecutorService executorService = null;\n+            ControllerQuorumVotersFutureManager connectFutureManager =\n+                new ControllerQuorumVotersFutureManager(nodes.controllerNodes().size());\n+            File baseDirectory = null;\n+\n+            try {\n+                baseDirectory = TestUtils.tempDirectory();\n+                nodes = nodes.copyWithAbsolutePaths(baseDirectory.getAbsolutePath());\n+                executorService = Executors.newFixedThreadPool(numOfExecutorThreads,\n+                    ThreadUtils.createThreadFactory(\"KafkaClusterTestKit%d\", false));\n+                Time time = Time.SYSTEM;\n+                for (ControllerNode node : nodes.controllerNodes().values()) {\n+                    Map<String, String> props = new HashMap<>(configProps);\n+                    props.put(KafkaConfig$.MODULE$.ProcessRolesProp(), \"controller\");\n+                    props.put(KafkaConfig$.MODULE$.NodeIdProp(),\n+                        Integer.toString(node.id()));\n+                    props.put(KafkaConfig$.MODULE$.MetadataLogDirProp(),\n+                        node.metadataDirectory());\n+                    props.put(KafkaConfig$.MODULE$.ListenerSecurityProtocolMapProp(),\n+                        \"CONTROLLER:PLAINTEXT\");\n+                    props.put(KafkaConfig$.MODULE$.ListenersProp(),\n+                        \"CONTROLLER://localhost:0\");\n+                    props.put(KafkaConfig$.MODULE$.ControllerListenerNamesProp(),\n+                        \"CONTROLLER\");\n+                    // Note: we can't accurately set controller.quorum.voters yet, since we don't\n+                    // yet know what ports each controller will pick.  Set it to a dummy string \\\n+                    // for now as a placeholder.\n+                    props.put(RaftConfig.QUORUM_VOTERS_CONFIG, dummyQuorumVotersString);\n+                    setupNodeDirectories(baseDirectory, node.metadataDirectory(), Collections.emptyList());\n+                    KafkaConfig config = new KafkaConfig(props, false, Option.empty());\n+\n+                    String threadNamePrefix = String.format(\"controller%d_\", node.id());\n+                    MetaProperties metaProperties = MetaProperties.apply(nodes.clusterId(), node.id());\n+                    TopicPartition metadataPartition = new TopicPartition(\"@metadata\", 0);\n+                    KafkaRaftManager<ApiMessageAndVersion> raftManager = new KafkaRaftManager<>(\n+                        metaProperties, config, new MetadataRecordSerde(), metadataPartition,\n+                        Time.SYSTEM, new Metrics(), Option.apply(threadNamePrefix), connectFutureManager.future);\n+                    MetaLogManager metaLogShim = new MetaLogRaftShim(raftManager.kafkaRaftClient(), config.nodeId());\n+                    ControllerServer controller = new ControllerServer(\n+                        nodes.controllerProperties(node.id()),\n+                        config,\n+                        metaLogShim,\n+                        raftManager,\n+                        time,\n+                        new Metrics(),\n+                        Option.apply(threadNamePrefix),\n+                        connectFutureManager.future\n+                    );\n+                    controllers.put(node.id(), controller);\n+                    controller.socketServerFirstBoundPortFuture().whenComplete((port, e) -> {\n+                        if (e != null) {\n+                            connectFutureManager.fail(e);\n+                        } else {\n+                            connectFutureManager.registerPort(node.id(), port);\n+                        }\n+                    });\n+                    raftManagers.put(node.id(), raftManager);\n+                }\n+                for (Kip500BrokerNode node : nodes.brokerNodes().values()) {\n+                    Map<String, String> props = new HashMap<>(configProps);\n+                    props.put(KafkaConfig$.MODULE$.ProcessRolesProp(), \"broker\");\n+                    props.put(KafkaConfig$.MODULE$.BrokerIdProp(),\n+                        Integer.toString(node.id()));\n+                    props.put(KafkaConfig$.MODULE$.MetadataLogDirProp(),\n+                        node.metadataDirectory());\n+                    props.put(KafkaConfig$.MODULE$.LogDirsProp(),\n+                        String.join(\",\", node.logDataDirectories()));\n+                    props.put(KafkaConfig$.MODULE$.ListenerSecurityProtocolMapProp(),\n+                        \"EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT\");\n+                    props.put(KafkaConfig$.MODULE$.ListenersProp(),\n+                        \"EXTERNAL://localhost:0\");\n+                    props.put(KafkaConfig$.MODULE$.InterBrokerListenerNameProp(),\n+                        nodes.interBrokerListenerName().value());\n+                    props.put(KafkaConfig$.MODULE$.ControllerListenerNamesProp(),\n+                        \"CONTROLLER\");\n+\n+                    setupNodeDirectories(baseDirectory, node.metadataDirectory(),\n+                        node.logDataDirectories());\n+\n+                    // Just like above, we set a placeholder voter list here until we\n+                    // find out what ports the controllers picked.\n+                    props.put(RaftConfig.QUORUM_VOTERS_CONFIG, dummyQuorumVotersString);\n+                    KafkaConfig config = new KafkaConfig(props, false, Option.empty());\n+\n+                    String threadNamePrefix = String.format(\"broker%d_\", node.id());\n+                    MetaProperties metaProperties = MetaProperties.apply(nodes.clusterId(), node.id());\n+                    TopicPartition metadataPartition = new TopicPartition(\"@metadata\", 0);\n+                    KafkaRaftManager<ApiMessageAndVersion> raftManager = new KafkaRaftManager<>(\n+                            metaProperties, config, new MetadataRecordSerde(), metadataPartition,\n+                            Time.SYSTEM, new Metrics(), Option.apply(threadNamePrefix), connectFutureManager.future);\n+                    MetaLogManager metaLogShim = new MetaLogRaftShim(raftManager.kafkaRaftClient(), config.nodeId());\n+                    BrokerServer broker = new BrokerServer(\n+                        config,\n+                        nodes.brokerProperties(node.id()),\n+                        metaLogShim,\n+                        time,\n+                        new Metrics(),\n+                        Option.apply(threadNamePrefix),\n+                        JavaConverters.asScalaBuffer(Collections.<String>emptyList()).toSeq(),\n+                        connectFutureManager.future,\n+                        Server.SUPPORTED_FEATURES()\n+                    );\n+                    kip500Brokers.put(node.id(), broker);\n+                    raftManagers.put(node.id(), raftManager);\n+                }\n+            } catch (Exception e) {\n+                if (executorService != null) {\n+                    executorService.shutdownNow();\n+                    executorService.awaitTermination(1, TimeUnit.DAYS);\n+                }\n+                for (ControllerServer controller : controllers.values()) {\n+                    controller.shutdown();\n+                }\n+                for (BrokerServer brokerServer : kip500Brokers.values()) {\n+                    brokerServer.shutdown();\n+                }\n+                for (KafkaRaftManager raftManager : raftManagers.values()) {\n+                    raftManager.shutdown();\n+                }\n+                connectFutureManager.close();\n+                if (baseDirectory != null) {\n+                    Utils.delete(baseDirectory);\n+                }\n+                throw e;\n+            }\n+            return new KafkaClusterTestKit(executorService, nodes, controllers,\n+                kip500Brokers, raftManagers, connectFutureManager, baseDirectory);\n+        }\n+\n+        static private void setupNodeDirectories(File baseDirectory,\n+                                                 String metadataDirectory,\n+                                                 Collection<String> logDataDirectories) throws Exception {\n+            Files.createDirectories(new File(baseDirectory, \"local\").toPath());\n+            Files.createDirectories(Paths.get(metadataDirectory));\n+            for (String logDataDirectory : logDataDirectories) {\n+                Files.createDirectories(Paths.get(logDataDirectory));\n+            }\n+        }\n+    }\n+\n+    private final ExecutorService executorService;\n+    private final TestKitNodes nodes;\n+    private final Map<Integer, ControllerServer> controllers;\n+    private final Map<Integer, BrokerServer> kip500Brokers;\n+    private final Map<Integer, KafkaRaftManager> raftManagers;\n+    private final ControllerQuorumVotersFutureManager controllerQuorumVotersFutureManager;\n+    private final File baseDirectory;\n+\n+    private KafkaClusterTestKit(ExecutorService executorService,\n+                                TestKitNodes nodes,\n+                                Map<Integer, ControllerServer> controllers,\n+                                Map<Integer, BrokerServer> kip500Brokers,\n+                                Map<Integer, KafkaRaftManager> raftManagers,\n+                                ControllerQuorumVotersFutureManager controllerQuorumVotersFutureManager,\n+                                File baseDirectory) {\n+        this.executorService = executorService;\n+        this.nodes = nodes;\n+        this.controllers = controllers;\n+        this.kip500Brokers = kip500Brokers;\n+        this.raftManagers = raftManagers;\n+        this.controllerQuorumVotersFutureManager = controllerQuorumVotersFutureManager;\n+        this.baseDirectory = baseDirectory;\n+    }\n+\n+    public void format() throws Exception {\n+        List<Future<?>> futures = new ArrayList<>();\n+        try {\n+            for (Entry<Integer, ControllerServer> entry : controllers.entrySet()) {\n+                int nodeId = entry.getKey();\n+                ControllerServer controller = entry.getValue();\n+                futures.add(executorService.submit(() -> {\n+                    try (ByteArrayOutputStream stream = new ByteArrayOutputStream()) {\n+                        try (PrintStream out = new PrintStream(stream)) {\n+                            StorageTool.formatCommand(out,\n+                                JavaConverters.asScalaBuffer(Collections.singletonList(\n+                                    controller.config().metadataLogDir())).toSeq(),\n+                                nodes.controllerProperties(nodeId),\n+                                false);\n+                        } finally {\n+                            for (String line : stream.toString().split(String.format(\"%n\"))) {\n+                                controller.info(() -> line);\n+                            }\n+                        }\n+                    } catch (IOException e) {\n+                        throw new RuntimeException(e);\n+                    }\n+                }));\n+            }\n+            for (Entry<Integer, BrokerServer> entry : kip500Brokers.entrySet()) {\n+                int nodeId = entry.getKey();\n+                BrokerServer broker = entry.getValue();\n+                futures.add(executorService.submit(() -> {\n+                    try (ByteArrayOutputStream stream = new ByteArrayOutputStream()) {\n+                        try (PrintStream out = new PrintStream(stream)) {\n+                            StorageTool.formatCommand(out,\n+                                JavaConverters.asScalaBuffer(Collections.singletonList(\n+                                    broker.config().metadataLogDir())).toSeq(),\n+                                nodes.brokerProperties(nodeId),\n+                                false);\n+                        } finally {\n+                            for (String line : stream.toString().split(String.format(\"%n\"))) {\n+                                broker.info(() -> line);\n+                            }\n+                        }\n+                    } catch (IOException e) {\n+                        throw new RuntimeException(e);\n+                    }\n+                }));\n+            }\n+            for (Future<?> future: futures) {\n+                future.get();\n+            }\n+        } catch (Exception e) {\n+            for (Future<?> future: futures) {\n+                future.cancel(true);\n+            }\n+            throw e;\n+        }\n+    }\n+\n+    public void startup() throws ExecutionException, InterruptedException {\n+        List<Future<?>> futures = new ArrayList<>();\n+        try {\n+            for (ControllerServer controller : controllers.values()) {\n+                futures.add(executorService.submit(controller::startup));\n+            }\n+            for (KafkaRaftManager raftManager : raftManagers.values()) {\n+                futures.add(controllerQuorumVotersFutureManager.future.thenRunAsync(raftManager::startup));\n+            }\n+            for (BrokerServer broker : kip500Brokers.values()) {\n+                futures.add(executorService.submit(broker::startup));\n+            }\n+            for (Future<?> future: futures) {\n+                future.get();\n+            }\n+        } catch (Exception e) {\n+            for (Future<?> future: futures) {\n+                future.cancel(true);\n+            }\n+            throw e;\n+        }\n+    }\n+\n+    /**\n+     * Wait for a controller to mark all the brokers as ready (registered and unfenced).\n+     */\n+    public void waitForReadyBrokers() throws ExecutionException, InterruptedException {\n+        // We can choose any controller, not just the active controller.\n+        // If we choose a standby controller, we will wait slightly longer.\n+        ControllerServer controllerServer = controllers.values().iterator().next();\n+        Controller controller = controllerServer.controller();\n+        controller.waitForReadyBrokers(kip500Brokers.size()).get();\n+    }\n+\n+    public Properties controllerClientProperties() throws ExecutionException, InterruptedException {\n+        Properties properties = new Properties();\n+        if (!controllers.isEmpty()) {\n+            Collection<Node> controllerNodes = RaftConfig.quorumVoterStringsToNodes(\n+                    controllerQuorumVotersFutureManager.future.get());\n+\n+            StringBuilder bld = new StringBuilder();\n+            String prefix = \"\";\n+            for (Node node : controllerNodes) {\n+                bld.append(prefix).append(node.id()).append('@');\n+                bld.append(node.host()).append(\":\").append(node.port());\n+                prefix = \",\";\n+            }\n+            properties.setProperty(RaftConfig.QUORUM_VOTERS_CONFIG, bld.toString());\n+            properties.setProperty(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG,\n+                controllerNodes.stream().map(n -> n.host() + \":\" + n.port()).\n+                    collect(Collectors.joining(\",\")));\n+        }\n+        return properties;\n+    }\n+\n+    public Properties clientProperties() {\n+        Properties properties = new Properties();\n+        if (!kip500Brokers.isEmpty()) {\n+            StringBuilder bld = new StringBuilder();\n+            String prefix = \"\";\n+            for (Entry<Integer, BrokerServer> entry : kip500Brokers.entrySet()) {\n+                int brokerId = entry.getKey();\n+                BrokerServer broker = entry.getValue();\n+                ListenerName listenerName = nodes.externalListenerName();\n+                int port = broker.boundPort(listenerName);\n+                if (port <= 0) {\n+                    throw new RuntimeException(\"Broker \" + brokerId + \" does not yet \" +\n+                        \"have a bound port for \" + listenerName + \".  Did you start \" +\n+                        \"the cluster yet?\");\n+                }\n+                bld.append(prefix).append(\"localhost:\").append(port);\n+                prefix = \",\";\n+            }\n+            properties.setProperty(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, bld.toString());\n+        }\n+        return properties;\n+    }\n+\n+    public Map<Integer, ControllerServer> controllers() {\n+        return controllers;\n+    }\n+\n+    public Map<Integer, BrokerServer> kip500Brokers() {\n+        return kip500Brokers;\n+    }\n+\n+    public Map<Integer, KafkaRaftManager> raftManagers() {\n+        return raftManagers;\n+    }\n+\n+    public TestKitNodes nodes() {\n+        return nodes;\n+    }\n+\n+    @Override\n+    public void close() throws Exception {\n+        List<Entry<String, Future<?>>> futureEntries = new ArrayList<>();\n+        try {\n+            controllerQuorumVotersFutureManager.close();\n+            for (Entry<Integer, BrokerServer> entry : kip500Brokers.entrySet()) {\n+                int brokerId = entry.getKey();\n+                BrokerServer broker = entry.getValue();\n+                futureEntries.add(new SimpleImmutableEntry<>(\"broker\" + brokerId,\n+                    executorService.submit(broker::shutdown)));\n+            }\n+            waitForAllFutures(futureEntries);\n+            futureEntries.clear();\n+            for (Entry<Integer, ControllerServer> entry : controllers.entrySet()) {\n+                int controllerId = entry.getKey();\n+                ControllerServer controller = entry.getValue();\n+                futureEntries.add(new SimpleImmutableEntry<>(\"controller\" + controllerId,\n+                    executorService.submit(controller::shutdown)));\n+            }\n+            waitForAllFutures(futureEntries);\n+            futureEntries.clear();\n+            for (Entry<Integer, KafkaRaftManager> entry : raftManagers.entrySet()) {\n+                int raftManagerId = entry.getKey();\n+                KafkaRaftManager raftManager = entry.getValue();\n+                futureEntries.add(new SimpleImmutableEntry<>(\"raftManager\" + raftManagerId,\n+                    executorService.submit(raftManager::shutdown)));\n+            }\n+            waitForAllFutures(futureEntries);\n+            futureEntries.clear();\n+            Utils.delete(baseDirectory);\n+        } catch (Exception e) {\n+            for (Entry<String, Future<?>> entry : futureEntries) {\n+                entry.getValue().cancel(true);\n+            }\n+            throw e;\n+        } finally {\n+            executorService.shutdownNow();\n+            executorService.awaitTermination(1, TimeUnit.DAYS);\n+        }\n+    }\n+\n+    private void waitForAllFutures(List<Entry<String, Future<?>>> futureEntries)\n+            throws Exception {\n+        for (Entry<String, Future<?>> entry : futureEntries) {\n+            log.debug(\"waiting for {} to shut down.\", entry.getKey());\n+            entry.getValue().get();\n+            log.debug(\"{} successfully shut down.\", entry.getKey());\n+        }\n+    }\n+}"
  },
  {
    "sha": "c48b1e7e190b9bc9ba33d292114b532ceb5d4d05",
    "filename": "core/src/test/java/kafka/testkit/Kip500BrokerNode.java",
    "status": "added",
    "additions": 94,
    "deletions": 0,
    "changes": 94,
    "blob_url": "https://github.com/apache/kafka/blob/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/java/kafka/testkit/Kip500BrokerNode.java",
    "raw_url": "https://github.com/apache/kafka/raw/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/java/kafka/testkit/Kip500BrokerNode.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/java/kafka/testkit/Kip500BrokerNode.java?ref=ecc84e8015cf079ef14daf0deaff0a688fde4e0a",
    "patch": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.testkit;\n+\n+import org.apache.kafka.common.Uuid;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+public class Kip500BrokerNode implements TestKitNode {\n+    public static class Builder {\n+        private int id = -1;\n+        private Uuid incarnationId = null;\n+        private String metadataDirectory = null;\n+        private List<String> logDataDirectories = null;\n+\n+        public Builder setId(int id) {\n+            this.id = id;\n+            return this;\n+        }\n+\n+        public Builder setLogDirectories(List<String> logDataDirectories) {\n+            this.logDataDirectories = logDataDirectories;\n+            return this;\n+        }\n+\n+        public Kip500BrokerNode build() {\n+            if (id == -1) {\n+                throw new RuntimeException(\"You must set the node id\");\n+            }\n+            if (incarnationId == null) {\n+                incarnationId = Uuid.randomUuid();\n+            }\n+            if (metadataDirectory == null) {\n+                metadataDirectory = String.format(\"kip500broker_%d_meta\", id);\n+            }\n+            if (logDataDirectories == null) {\n+                logDataDirectories  = Collections.\n+                    singletonList(String.format(\"kip500broker_%d_data0\", id));\n+            }\n+            return new Kip500BrokerNode(id, incarnationId, metadataDirectory,\n+                logDataDirectories);\n+        }\n+    }\n+\n+    private final int id;\n+    private final Uuid incarnationId;\n+    private final String metadataDirectory;\n+    private final List<String> logDataDirectories;\n+\n+    Kip500BrokerNode(int id,\n+                     Uuid incarnationId,\n+                     String metadataDirectory,\n+                     List<String> logDataDirectories) {\n+        this.id = id;\n+        this.incarnationId = incarnationId;\n+        this.metadataDirectory = metadataDirectory;\n+        this.logDataDirectories = new ArrayList<>(logDataDirectories);\n+    }\n+\n+    @Override\n+    public int id() {\n+        return id;\n+    }\n+\n+    public Uuid incarnationId() {\n+        return incarnationId;\n+    }\n+\n+    @Override\n+    public String metadataDirectory() {\n+        return metadataDirectory;\n+    }\n+\n+    public List<String> logDataDirectories() {\n+        return logDataDirectories;\n+    }\n+}"
  },
  {
    "sha": "a5423d135f9cb94ab1f7eb045278b3fc096eb7ad",
    "filename": "core/src/test/java/kafka/testkit/TestKitNode.java",
    "status": "added",
    "additions": 23,
    "deletions": 0,
    "changes": 23,
    "blob_url": "https://github.com/apache/kafka/blob/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/java/kafka/testkit/TestKitNode.java",
    "raw_url": "https://github.com/apache/kafka/raw/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/java/kafka/testkit/TestKitNode.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/java/kafka/testkit/TestKitNode.java?ref=ecc84e8015cf079ef14daf0deaff0a688fde4e0a",
    "patch": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.testkit;\n+\n+public interface TestKitNode {\n+    int id();\n+    String metadataDirectory();\n+}"
  },
  {
    "sha": "0388168a10c3ce575dd15431a6a08efb447839eb",
    "filename": "core/src/test/java/kafka/testkit/TestKitNodes.java",
    "status": "added",
    "additions": 187,
    "deletions": 0,
    "changes": 187,
    "blob_url": "https://github.com/apache/kafka/blob/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/java/kafka/testkit/TestKitNodes.java",
    "raw_url": "https://github.com/apache/kafka/raw/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/java/kafka/testkit/TestKitNodes.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/java/kafka/testkit/TestKitNodes.java?ref=ecc84e8015cf079ef14daf0deaff0a688fde4e0a",
    "patch": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.testkit;\n+\n+import kafka.server.MetaProperties;\n+import org.apache.kafka.common.Uuid;\n+import org.apache.kafka.common.network.ListenerName;\n+\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.NavigableMap;\n+import java.util.TreeMap;\n+\n+public class TestKitNodes {\n+    public static class Builder {\n+        private Uuid clusterId = null;\n+        private final NavigableMap<Integer, ControllerNode> controllerNodes = new TreeMap<>();\n+        private final NavigableMap<Integer, Kip500BrokerNode> kip500BrokerNodes = new TreeMap<>();\n+\n+        public Builder setClusterId(Uuid clusterId) {\n+            this.clusterId = clusterId;\n+            return this;\n+        }\n+\n+        public Builder addNodes(TestKitNode[] nodes) {\n+            for (TestKitNode node : nodes) {\n+                addNode(node);\n+            }\n+            return this;\n+        }\n+\n+        public Builder addNode(TestKitNode node) {\n+            if (node instanceof ControllerNode) {\n+                ControllerNode controllerNode = (ControllerNode) node;\n+                controllerNodes.put(node.id(), controllerNode);\n+            } else if (node instanceof Kip500BrokerNode) {\n+                Kip500BrokerNode brokerNode = (Kip500BrokerNode) node;\n+                kip500BrokerNodes.put(node.id(), brokerNode);\n+            } else {\n+                throw new RuntimeException(\"Can't handle TestKitNode subclass \" +\n+                        node.getClass().getSimpleName());\n+            }\n+            return this;\n+        }\n+\n+        public Builder setNumControllerNodes(int numControllerNodes) {\n+            if (numControllerNodes < 0) {\n+                throw new RuntimeException(\"Invalid negative value for numControllerNodes\");\n+            }\n+            while (controllerNodes.size() > numControllerNodes) {\n+                Iterator<Entry<Integer, ControllerNode>> iter =\n+                    controllerNodes.entrySet().iterator();\n+                iter.next();\n+                iter.remove();\n+            }\n+            while (controllerNodes.size() < numControllerNodes) {\n+                int nextId = 3000;\n+                if (!controllerNodes.isEmpty()) {\n+                    nextId = controllerNodes.lastKey() + 1;\n+                }\n+                controllerNodes.put(nextId, new ControllerNode.Builder().\n+                    setId(nextId).build());\n+            }\n+            return this;\n+        }\n+\n+        public Builder setNumKip500BrokerNodes(int numBrokerNodes) {\n+            if (numBrokerNodes < 0) {\n+                throw new RuntimeException(\"Invalid negative value for numBrokerNodes\");\n+            }\n+            while (kip500BrokerNodes.size() > numBrokerNodes) {\n+                Iterator<Entry<Integer, Kip500BrokerNode>> iter =\n+                    kip500BrokerNodes.entrySet().iterator();\n+                iter.next();\n+                iter.remove();\n+            }\n+            while (kip500BrokerNodes.size() < numBrokerNodes) {\n+                int nextId = 0;\n+                if (!kip500BrokerNodes.isEmpty()) {\n+                    nextId = kip500BrokerNodes.lastKey() + 1;\n+                }\n+                kip500BrokerNodes.put(nextId, new Kip500BrokerNode.Builder().\n+                    setId(nextId).build());\n+            }\n+            return this;\n+        }\n+\n+        public TestKitNodes build() {\n+            if (clusterId == null) {\n+                clusterId = Uuid.randomUuid();\n+            }\n+            return new TestKitNodes(clusterId, controllerNodes, kip500BrokerNodes);\n+        }\n+    }\n+\n+    private final Uuid clusterId;\n+    private final NavigableMap<Integer, ControllerNode> controllerNodes;\n+    private final NavigableMap<Integer, Kip500BrokerNode> brokerNodes;\n+\n+    private TestKitNodes(Uuid clusterId,\n+                         NavigableMap<Integer, ControllerNode> controllerNodes,\n+                         NavigableMap<Integer, Kip500BrokerNode> brokerNodes) {\n+        this.clusterId = clusterId;\n+        this.controllerNodes = controllerNodes;\n+        this.brokerNodes = brokerNodes;\n+    }\n+\n+    public Uuid clusterId() {\n+        return clusterId;\n+    }\n+\n+    public Map<Integer, ControllerNode> controllerNodes() {\n+        return controllerNodes;\n+    }\n+\n+    public NavigableMap<Integer, Kip500BrokerNode> brokerNodes() {\n+        return brokerNodes;\n+    }\n+\n+    public MetaProperties controllerProperties(int id) {\n+        return MetaProperties.apply(clusterId, id);\n+    }\n+\n+    public MetaProperties brokerProperties(int id) {\n+        return MetaProperties.apply(clusterId, id);\n+    }\n+\n+    public ListenerName interBrokerListenerName() {\n+        return new ListenerName(\"EXTERNAL\");\n+    }\n+\n+    public ListenerName externalListenerName() {\n+        return new ListenerName(\"EXTERNAL\");\n+    }\n+\n+    public TestKitNodes copyWithAbsolutePaths(String baseDirectory) {\n+        NavigableMap<Integer, ControllerNode> newControllerNodes = new TreeMap<>();\n+        NavigableMap<Integer, Kip500BrokerNode> newBrokerNodes = new TreeMap<>();\n+        for (Entry<Integer, ControllerNode> entry : controllerNodes.entrySet()) {\n+            ControllerNode node = entry.getValue();\n+            newControllerNodes.put(entry.getKey(), new ControllerNode(node.id(),\n+                absolutize(baseDirectory, node.metadataDirectory())));\n+        }\n+        for (Entry<Integer, Kip500BrokerNode> entry : brokerNodes.entrySet()) {\n+            Kip500BrokerNode node = entry.getValue();\n+            newBrokerNodes.put(entry.getKey(), new Kip500BrokerNode(node.id(),\n+                node.incarnationId(), absolutize(baseDirectory, node.metadataDirectory()),\n+                absolutize(baseDirectory, node.logDataDirectories())));\n+        }\n+        return new TestKitNodes(clusterId, newControllerNodes, newBrokerNodes);\n+    }\n+\n+    private static List<String> absolutize(String base, Collection<String> directories) {\n+        List<String> newDirectories = new ArrayList<>();\n+        for (String directory : directories) {\n+            newDirectories.add(absolutize(base, directory));\n+        }\n+        return newDirectories;\n+    }\n+\n+    private static String absolutize(String base, String directory) {\n+        if (Paths.get(directory).isAbsolute()) {\n+            return directory;\n+        }\n+        return Paths.get(base, directory).toAbsolutePath().toString();\n+    }\n+}"
  },
  {
    "sha": "4c238cdc59ca82b7a64ea549e90cf1fcbf7d8030",
    "filename": "core/src/test/resources/log4j.properties",
    "status": "modified",
    "additions": 2,
    "deletions": 2,
    "changes": 4,
    "blob_url": "https://github.com/apache/kafka/blob/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/resources/log4j.properties",
    "raw_url": "https://github.com/apache/kafka/raw/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/resources/log4j.properties",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/resources/log4j.properties?ref=ecc84e8015cf079ef14daf0deaff0a688fde4e0a",
    "patch": "@@ -18,8 +18,8 @@ log4j.appender.stdout=org.apache.log4j.ConsoleAppender\n log4j.appender.stdout.layout=org.apache.log4j.PatternLayout\n log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c:%L)%n\n \n-log4j.logger.kafka=ERROR\n-log4j.logger.org.apache.kafka=ERROR\n+log4j.logger.kafka=INFO\n+log4j.logger.org.apache.kafka=INFO\n \n # zkclient can be verbose, during debugging it is common to adjust it separately\n log4j.logger.org.apache.zookeeper=WARN"
  },
  {
    "sha": "98fdc3401bd0b29e98593643a9f98c1d144a72af",
    "filename": "core/src/test/scala/integration/kafka/server/RaftClusterTest.scala",
    "status": "added",
    "additions": 286,
    "deletions": 0,
    "changes": 286,
    "blob_url": "https://github.com/apache/kafka/blob/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/scala/integration/kafka/server/RaftClusterTest.scala",
    "raw_url": "https://github.com/apache/kafka/raw/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/scala/integration/kafka/server/RaftClusterTest.scala",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/scala/integration/kafka/server/RaftClusterTest.scala?ref=ecc84e8015cf079ef14daf0deaff0a688fde4e0a",
    "patch": "@@ -0,0 +1,286 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.testkit.{KafkaClusterTestKit, TestKitNodes}\n+import kafka.utils.TestUtils\n+import org.apache.kafka.clients.admin.{Admin, NewTopic}\n+import org.apache.kafka.common.quota.{ClientQuotaAlteration, ClientQuotaEntity, ClientQuotaFilter, ClientQuotaFilterComponent}\n+import org.apache.kafka.metadata.BrokerState\n+import org.junit.jupiter.api.{Test, Timeout}\n+import org.junit.jupiter.api.Assertions._\n+\n+import java.util\n+import java.util.Collections\n+import java.util.concurrent.TimeUnit\n+import scala.jdk.CollectionConverters._\n+\n+@Timeout(120000)\n+class RaftClusterTest {\n+\n+  @Test\n+  def testCreateClusterAndClose(): Unit = {\n+    val cluster = new KafkaClusterTestKit.Builder(\n+      new TestKitNodes.Builder().\n+        setNumKip500BrokerNodes(1).\n+        setNumControllerNodes(1).build()).build()\n+    try {\n+      cluster.format()\n+      cluster.startup()\n+    } finally {\n+      cluster.close()\n+    }\n+  }\n+\n+  @Test\n+  def testCreateClusterAndWaitForBrokerInRunningState(): Unit = {\n+    val cluster = new KafkaClusterTestKit.Builder(\n+      new TestKitNodes.Builder().\n+        setNumKip500BrokerNodes(3).\n+        setNumControllerNodes(3).build()).build()\n+    try {\n+      cluster.format()\n+      cluster.startup()\n+      TestUtils.waitUntilTrue(() => cluster.kip500Brokers().get(0).currentState() == BrokerState.RUNNING,\n+        \"Broker never made it to RUNNING state.\")\n+      TestUtils.waitUntilTrue(() => cluster.raftManagers().get(0).kafkaRaftClient.leaderAndEpoch().leaderId.isPresent,\n+      \"RaftManager was not initialized.\")\n+      val admin = Admin.create(cluster.clientProperties())\n+      try {\n+        assertEquals(cluster.nodes().clusterId().toString,\n+          admin.describeCluster().clusterId().get())\n+      } finally {\n+        admin.close()\n+      }\n+    } finally {\n+      cluster.close()\n+    }\n+  }\n+\n+  @Test\n+  def testCreateClusterAndCreateAndListTopic(): Unit = {\n+    val cluster = new KafkaClusterTestKit.Builder(\n+      new TestKitNodes.Builder().\n+        setNumKip500BrokerNodes(3).\n+        setNumControllerNodes(3).build()).build()\n+    try {\n+      cluster.format()\n+      cluster.startup()\n+      cluster.waitForReadyBrokers()\n+      TestUtils.waitUntilTrue(() => cluster.kip500Brokers().get(0).currentState() == BrokerState.RUNNING,\n+        \"Broker never made it to RUNNING state.\")\n+      TestUtils.waitUntilTrue(() => cluster.raftManagers().get(0).kafkaRaftClient.leaderAndEpoch().leaderId.isPresent,\n+        \"RaftManager was not initialized.\")\n+      val admin = Admin.create(cluster.clientProperties())\n+      try {\n+        // Create a test topic\n+        val newTopic = Collections.singletonList(new NewTopic(\"test-topic\", 1, 3.toShort))\n+        val createTopicResult = admin.createTopics(newTopic)\n+        createTopicResult.all().get(60, TimeUnit.SECONDS)\n+\n+        // List created topic\n+        TestUtils.waitUntilTrue(() => {\n+          val listTopicsResult = admin.listTopics()\n+          val result = listTopicsResult.names().get(5, TimeUnit.SECONDS).size() == newTopic.size()\n+          if (result) {\n+            newTopic forEach(topic => {\n+              assertTrue(listTopicsResult.names().get().contains(topic.name()))\n+            })\n+          }\n+          result\n+        }, \"Topics created were not listed.\")\n+      } finally {\n+        admin.close()\n+      }\n+    } finally {\n+      cluster.close()\n+    }\n+  }\n+\n+  @Test\n+  def testCreateClusterAndCreateAndManyTopics(): Unit = {\n+    val cluster = new KafkaClusterTestKit.Builder(\n+      new TestKitNodes.Builder().\n+        setNumKip500BrokerNodes(3).\n+        setNumControllerNodes(3).build()).build()\n+    try {\n+      cluster.format()\n+      cluster.startup()\n+      cluster.waitForReadyBrokers()\n+      TestUtils.waitUntilTrue(() => cluster.kip500Brokers().get(0).currentState() == BrokerState.RUNNING,\n+        \"Broker never made it to RUNNING state.\")\n+      TestUtils.waitUntilTrue(() => cluster.raftManagers().get(0).kafkaRaftClient.leaderAndEpoch().leaderId.isPresent,\n+        \"RaftManager was not initialized.\")\n+      val admin = Admin.create(cluster.clientProperties())\n+      try {\n+        // Create many topics\n+        val newTopic = new util.ArrayList[NewTopic]()\n+        newTopic.add(new NewTopic(\"test-topic-1\", 1, 3.toShort))\n+        newTopic.add(new NewTopic(\"test-topic-2\", 1, 3.toShort))\n+        newTopic.add(new NewTopic(\"test-topic-3\", 1, 3.toShort))\n+        val createTopicResult = admin.createTopics(newTopic)\n+        createTopicResult.all().get(60, TimeUnit.SECONDS)\n+\n+        // List created topic\n+        TestUtils.waitUntilTrue(() => {\n+          val listTopicsResult = admin.listTopics()\n+          val result = listTopicsResult.names().get(5, TimeUnit.SECONDS).size() == newTopic.size()\n+          if (result) {\n+            newTopic forEach(topic => {\n+              assertTrue(listTopicsResult.names().get().contains(topic.name()))\n+            })\n+          }\n+          result\n+        }, \"Topics created were not listed.\")\n+      } finally {\n+        admin.close()\n+      }\n+    } finally {\n+      cluster.close()\n+    }\n+  }\n+\n+  @Test\n+  def testCreateClusterAndCreateAndManyTopicsWithManyPartitions(): Unit = {\n+    val cluster = new KafkaClusterTestKit.Builder(\n+      new TestKitNodes.Builder().\n+        setNumKip500BrokerNodes(3).\n+        setNumControllerNodes(3).build()).build()\n+    try {\n+      cluster.format()\n+      cluster.startup()\n+      cluster.waitForReadyBrokers()\n+      TestUtils.waitUntilTrue(() => cluster.kip500Brokers().get(0).currentState() == BrokerState.RUNNING,\n+        \"Broker never made it to RUNNING state.\")\n+      TestUtils.waitUntilTrue(() => cluster.raftManagers().get(0).kafkaRaftClient.leaderAndEpoch().leaderId.isPresent,\n+        \"RaftManager was not initialized.\")\n+      val admin = Admin.create(cluster.clientProperties())\n+      try {\n+        // Create many topics\n+        val newTopic = new util.ArrayList[NewTopic]()\n+        newTopic.add(new NewTopic(\"test-topic-1\", 3, 3.toShort))\n+        newTopic.add(new NewTopic(\"test-topic-2\", 3, 3.toShort))\n+        newTopic.add(new NewTopic(\"test-topic-3\", 3, 3.toShort))\n+        val createTopicResult = admin.createTopics(newTopic)\n+        createTopicResult.all().get(60, TimeUnit.SECONDS)\n+\n+        // List created topic\n+        TestUtils.waitUntilTrue(() => {\n+          val listTopicsResult = admin.listTopics()\n+          val result = listTopicsResult.names().get(5, TimeUnit.SECONDS).size() == newTopic.size()\n+          if (result) {\n+            newTopic forEach(topic => {\n+              assertTrue(listTopicsResult.names().get().contains(topic.name()))\n+            })\n+          }\n+          result\n+        }, \"Topics created were not listed.\")\n+      } finally {\n+        admin.close()\n+      }\n+    } finally {\n+      cluster.close()\n+    }\n+  }\n+\n+  @Test\n+  def testClientQuotas(): Unit = {\n+    val cluster = new KafkaClusterTestKit.Builder(\n+      new TestKitNodes.Builder().\n+        setNumKip500BrokerNodes(1).\n+        setNumControllerNodes(1).build()).build()\n+    try {\n+      cluster.format()\n+      cluster.startup()\n+      TestUtils.waitUntilTrue(() => cluster.kip500Brokers().get(0).currentState() == BrokerState.RUNNING,\n+        \"Broker never made it to RUNNING state.\")\n+      val admin = Admin.create(cluster.clientProperties())\n+      try {\n+        val entity = new ClientQuotaEntity(Map(\"user\" -> \"testkit\").asJava)\n+        var filter = ClientQuotaFilter.containsOnly(\n+          List(ClientQuotaFilterComponent.ofEntity(\"user\", \"testkit\")).asJava)\n+\n+        def alterThenDescribe(entity: ClientQuotaEntity,\n+                              quotas: Seq[ClientQuotaAlteration.Op],\n+                              filter: ClientQuotaFilter,\n+                              expectCount: Int): java.util.Map[ClientQuotaEntity, java.util.Map[String, java.lang.Double]] = {\n+          admin.alterClientQuotas(Seq(new ClientQuotaAlteration(entity, quotas.asJava)).asJava)\n+          val (describeResult, ok) = TestUtils.computeUntilTrue(admin.describeClientQuotas(filter).entities().get()) {\n+            results => results.getOrDefault(entity, java.util.Collections.emptyMap[String, java.lang.Double]()).size() == expectCount\n+          }\n+          assertTrue(ok, \"Broker never saw new client quotas\")\n+          describeResult\n+        }\n+\n+        var describeResult = alterThenDescribe(entity,\n+          Seq(new ClientQuotaAlteration.Op(\"request_percentage\", 0.99)), filter, 1)\n+        assertEquals(0.99, describeResult.get(entity).get(\"request_percentage\"), 1e-6)\n+\n+        describeResult = alterThenDescribe(entity, Seq(\n+          new ClientQuotaAlteration.Op(\"request_percentage\", 0.97),\n+          new ClientQuotaAlteration.Op(\"unknown_quota\", 100),\n+          new ClientQuotaAlteration.Op(\"producer_byte_rate\", 10000),\n+          new ClientQuotaAlteration.Op(\"consumer_byte_rate\", 10001)\n+        ), filter, 3)\n+        assertEquals(0.97, describeResult.get(entity).get(\"request_percentage\"), 1e-6)\n+        assertEquals(10000.0, describeResult.get(entity).get(\"producer_byte_rate\"), 1e-6)\n+        assertEquals(10001.0, describeResult.get(entity).get(\"consumer_byte_rate\"), 1e-6)\n+\n+        describeResult = alterThenDescribe(entity, Seq(\n+          new ClientQuotaAlteration.Op(\"request_percentage\", 0.95),\n+          new ClientQuotaAlteration.Op(\"producer_byte_rate\", null),\n+          new ClientQuotaAlteration.Op(\"consumer_byte_rate\", null)\n+        ), filter, 1)\n+        assertEquals(0.95, describeResult.get(entity).get(\"request_percentage\"), 1e-6)\n+\n+        describeResult = alterThenDescribe(entity, Seq(\n+          new ClientQuotaAlteration.Op(\"request_percentage\", null)), filter, 0)\n+\n+        describeResult = alterThenDescribe(entity,\n+          Seq(new ClientQuotaAlteration.Op(\"producer_byte_rate\", 9999)), filter, 1)\n+        assertEquals(9999.0, describeResult.get(entity).get(\"producer_byte_rate\"), 1e-6)\n+\n+        // Add another quota for a different entity with same user part\n+        val entity2 = new ClientQuotaEntity(Map(\"user\" -> \"testkit\", \"client-id\" -> \"some-client\").asJava)\n+        filter = ClientQuotaFilter.containsOnly(\n+          List(\n+            ClientQuotaFilterComponent.ofEntity(\"user\", \"testkit\"),\n+            ClientQuotaFilterComponent.ofEntity(\"client-id\", \"some-client\"),\n+          ).asJava)\n+        describeResult = alterThenDescribe(entity2,\n+          Seq(new ClientQuotaAlteration.Op(\"producer_byte_rate\", 9998)), filter, 1)\n+        assertEquals(9998.0, describeResult.get(entity2).get(\"producer_byte_rate\"), 1e-6)\n+\n+        // non-strict match\n+        filter = ClientQuotaFilter.contains(\n+          List(ClientQuotaFilterComponent.ofEntity(\"user\", \"testkit\")).asJava)\n+\n+        val (describeResult2, ok) = TestUtils.computeUntilTrue(admin.describeClientQuotas(filter).entities().get()) {\n+          results => results.size() == 2\n+        }\n+        assertTrue(ok, \"Broker never saw two client quotas\")\n+        assertEquals(9999.0, describeResult2.get(entity).get(\"producer_byte_rate\"), 1e-6)\n+        assertEquals(9998.0, describeResult2.get(entity2).get(\"producer_byte_rate\"), 1e-6)\n+      } finally {\n+        admin.close()\n+      }\n+    } finally {\n+      cluster.close()\n+    }\n+  }\n+}"
  },
  {
    "sha": "78f1e8e5b0092308ea17e9079e508e53180a0353",
    "filename": "core/src/test/scala/unit/kafka/server/ClientQuotasRequestTest.scala",
    "status": "modified",
    "additions": 9,
    "deletions": 5,
    "changes": 14,
    "blob_url": "https://github.com/apache/kafka/blob/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/scala/unit/kafka/server/ClientQuotasRequestTest.scala",
    "raw_url": "https://github.com/apache/kafka/raw/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/scala/unit/kafka/server/ClientQuotasRequestTest.scala",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/scala/unit/kafka/server/ClientQuotasRequestTest.scala?ref=ecc84e8015cf079ef14daf0deaff0a688fde4e0a",
    "patch": "@@ -19,6 +19,7 @@ package kafka.server\n \n import integration.kafka.server.IntegrationTestUtils\n import kafka.test.ClusterInstance\n+import kafka.test.ClusterInstance.ClusterType\n \n import java.net.InetAddress\n import org.apache.kafka.clients.admin.{ScramCredentialInfo, ScramMechanism, UserScramCredentialUpsertion}\n@@ -38,7 +39,7 @@ import kafka.test.junit.ClusterTestExtensions\n \n import scala.jdk.CollectionConverters._\n \n-@ClusterTestDefaults(clusterType = Type.ZK)\n+@ClusterTestDefaults(clusterType = Type.BOTH)\n @ExtendWith(value = Array(classOf[ClusterTestExtensions]))\n class ClientQuotasRequestTest(cluster: ClusterInstance) {\n   private val ConsumerByteRateProp = QuotaConfigs.CONSUMER_BYTE_RATE_OVERRIDE_CONFIG\n@@ -174,7 +175,7 @@ class ClientQuotasRequestTest(cluster: ClusterInstance) {\n     ))\n   }\n \n-  @ClusterTest\n+  @ClusterTest(clusterType = Type.ZK) // No SCRAM for Raft yet\n   def testClientQuotasForScramUsers(): Unit = {\n     val userName = \"user\"\n \n@@ -399,7 +400,7 @@ class ClientQuotasRequestTest(cluster: ClusterInstance) {\n       val result = describeClientQuotas(filter)\n       val (expectedMatches, _) = (matchUserClientEntities ++ matchIpEntities).partition(e => partition(e._1))\n       assertEquals(expectedMatchSize, expectedMatches.size)  // for test verification\n-      assertEquals(expectedMatchSize, result.size)\n+      assertEquals(expectedMatchSize, result.size, s\"Failed to match $expectedMatchSize entities for $filter\")\n       val expectedMatchesMap = expectedMatches.toMap\n       matchUserClientEntities.foreach { case (entity, expectedValue) =>\n         if (expectedMatchesMap.contains(entity)) {\n@@ -561,6 +562,9 @@ class ClientQuotasRequestTest(cluster: ClusterInstance) {\n \n   private def describeClientQuotas(filter: ClientQuotaFilter) = {\n     val result = new KafkaFutureImpl[java.util.Map[ClientQuotaEntity, java.util.Map[String, java.lang.Double]]]\n+    if (cluster.clusterType() == ClusterType.RAFT) {\n+      Thread.sleep(3000) // need to do this since updates are now async!\n+    }\n     sendDescribeClientQuotasRequest(filter).complete(result)\n     try result.get catch {\n       case e: ExecutionException => throw e.getCause\n@@ -570,7 +574,7 @@ class ClientQuotasRequestTest(cluster: ClusterInstance) {\n   private def sendDescribeClientQuotasRequest(filter: ClientQuotaFilter): DescribeClientQuotasResponse = {\n     val request = new DescribeClientQuotasRequest.Builder(filter).build()\n     IntegrationTestUtils.connectAndReceive[DescribeClientQuotasResponse](request,\n-      destination = cluster.anyControllerSocketServer(),\n+      destination = cluster.anyBrokerSocketServer(),\n       listenerName = cluster.clientListener())\n   }\n \n@@ -598,7 +602,7 @@ class ClientQuotasRequestTest(cluster: ClusterInstance) {\n   private def sendAlterClientQuotasRequest(entries: Iterable[ClientQuotaAlteration], validateOnly: Boolean): AlterClientQuotasResponse = {\n     val request = new AlterClientQuotasRequest.Builder(entries.asJavaCollection, validateOnly).build()\n     IntegrationTestUtils.connectAndReceive[AlterClientQuotasResponse](request,\n-      destination = cluster.anyControllerSocketServer(),\n+      destination = cluster.anyBrokerSocketServer(),\n       listenerName = cluster.clientListener())\n   }\n "
  },
  {
    "sha": "e1cebf911b097d0f90f3057e012dc21c31fb477a",
    "filename": "core/src/test/scala/unit/kafka/server/KafkaApisTest.scala",
    "status": "modified",
    "additions": 3,
    "deletions": 3,
    "changes": 6,
    "blob_url": "https://github.com/apache/kafka/blob/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/scala/unit/kafka/server/KafkaApisTest.scala",
    "raw_url": "https://github.com/apache/kafka/raw/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/core/src/test/scala/unit/kafka/server/KafkaApisTest.scala",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/scala/unit/kafka/server/KafkaApisTest.scala?ref=ecc84e8015cf079ef14daf0deaff0a688fde4e0a",
    "patch": "@@ -23,7 +23,6 @@ import java.util\n import java.util.Arrays.asList\n import java.util.concurrent.TimeUnit\n import java.util.{Collections, Optional, Properties, Random}\n-\n import kafka.api.{ApiVersion, KAFKA_0_10_2_IV0, KAFKA_2_2_IV1, LeaderAndIsr}\n import kafka.cluster.{Broker, Partition}\n import kafka.controller.KafkaController\n@@ -33,7 +32,7 @@ import kafka.coordinator.transaction.{InitProducerIdResult, TransactionCoordinat\n import kafka.log.AppendOrigin\n import kafka.network.RequestChannel\n import kafka.server.QuotaFactory.QuotaManagers\n-import kafka.server.metadata.{CachedConfigRepository, ConfigRepository, RaftMetadataCache}\n+import kafka.server.metadata.{CachedConfigRepository, ClientQuotaCache, ConfigRepository, RaftMetadataCache}\n import kafka.utils.{MockTime, TestUtils}\n import kafka.zk.KafkaZkClient\n import org.apache.kafka.clients.admin.AlterConfigOp.OpType\n@@ -107,6 +106,7 @@ class KafkaApisTest {\n   private val replicaQuotaManager: ReplicationQuotaManager = EasyMock.createNiceMock(classOf[ReplicationQuotaManager])\n   private val quotas = QuotaManagers(clientQuotaManager, clientQuotaManager, clientRequestQuotaManager,\n     clientControllerQuotaManager, replicaQuotaManager, replicaQuotaManager, replicaQuotaManager, None)\n+  private val quotaCache = new ClientQuotaCache()\n   private val fetchManager: FetchManager = EasyMock.createNiceMock(classOf[FetchManager])\n   private val brokerTopicStats = new BrokerTopicStats\n   private val clusterId = \"clusterId\"\n@@ -149,7 +149,7 @@ class KafkaApisTest {\n       // with a RaftMetadataCache instance\n       metadataCache match {\n         case raftMetadataCache: RaftMetadataCache =>\n-          RaftSupport(forwardingManager, raftMetadataCache)\n+          RaftSupport(forwardingManager, raftMetadataCache, quotaCache)\n         case _ => throw new IllegalStateException(\"Test must set an instance of RaftMetadataCache\")\n       }\n     } else {"
  },
  {
    "sha": "d96e0ccc3c51b462fdcadc3680249d5f0af6e1aa",
    "filename": "metadata/src/main/java/org/apache/kafka/controller/ClientQuotaControlManager.java",
    "status": "modified",
    "additions": 21,
    "deletions": 12,
    "changes": 33,
    "blob_url": "https://github.com/apache/kafka/blob/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/metadata/src/main/java/org/apache/kafka/controller/ClientQuotaControlManager.java",
    "raw_url": "https://github.com/apache/kafka/raw/ecc84e8015cf079ef14daf0deaff0a688fde4e0a/metadata/src/main/java/org/apache/kafka/controller/ClientQuotaControlManager.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/metadata/src/main/java/org/apache/kafka/controller/ClientQuotaControlManager.java?ref=ecc84e8015cf079ef14daf0deaff0a688fde4e0a",
    "patch": "@@ -170,8 +170,10 @@ private void alterClientQuotaEntity(\n             }\n         });\n \n-        outputRecords.addAll(newRecords);\n-        outputResults.put(entity, ApiError.NONE);\n+        // Only add the records to outputRecords if there were no errors\n+        if (outputResults.putIfAbsent(entity, ApiError.NONE) == null) {\n+            outputRecords.addAll(newRecords);\n+        }\n     }\n \n     private ApiError configKeysForEntityType(Map<String, String> entity, Map<String, ConfigDef.ConfigKey> output) {\n@@ -182,18 +184,23 @@ private ApiError configKeysForEntityType(Map<String, String> entity, Map<String,\n         boolean hasIp = entity.containsKey(ClientQuotaEntity.IP);\n \n         final Map<String, ConfigDef.ConfigKey> configKeys;\n-        if (hasUser && hasClientId && !hasIp) {\n+        if (hasIp) {\n+            if (hasUser || hasClientId) {\n+                return new ApiError(Errors.INVALID_REQUEST, \"Invalid quota entity combination, IP entity should\" +\n+                    \"not be combined with User or ClientId\");\n+            } else {\n+                if (isValidIpEntity(entity.get(ClientQuotaEntity.IP))) {\n+                    configKeys = QuotaConfigs.ipConfigs().configKeys();\n+                } else {\n+                    return new ApiError(Errors.INVALID_REQUEST, entity.get(ClientQuotaEntity.IP) + \" is not a valid IP or resolvable host.\");\n+                }\n+            }\n+        } else if (hasUser && hasClientId) {\n             configKeys = QuotaConfigs.userConfigs().configKeys();\n-        } else if (hasUser && !hasClientId && !hasIp) {\n+        } else if (hasUser) {\n             configKeys = QuotaConfigs.userConfigs().configKeys();\n-        } else if (!hasUser && hasClientId && !hasIp) {\n+        } else if (hasClientId) {\n             configKeys = QuotaConfigs.clientConfigs().configKeys();\n-        } else if (!hasUser && !hasClientId && hasIp) {\n-            if (isValidIpEntity(entity.get(ClientQuotaEntity.IP))) {\n-                configKeys = QuotaConfigs.ipConfigs().configKeys();\n-            } else {\n-                return new ApiError(Errors.INVALID_REQUEST, entity.get(ClientQuotaEntity.IP) + \" is not a valid IP or resolvable host.\");\n-            }\n         } else {\n             return new ApiError(Errors.INVALID_REQUEST, \"Invalid empty client quota entity\");\n         }\n@@ -214,6 +221,8 @@ private ApiError validateQuotaKeyValue(Map<String, ConfigDef.ConfigKey> validKey\n         switch (configKey.type()) {\n             case DOUBLE:\n                 break;\n+            case SHORT:\n+            case INT:\n             case LONG:\n                 Double epsilon = 1e-6;\n                 Long longValue = Double.valueOf(value + epsilon).longValue();\n@@ -253,7 +262,7 @@ private ApiError validateEntity(ClientQuotaEntity entity, Map<String, String> va\n             String entityType = entityEntry.getKey();\n             String entityName = entityEntry.getValue();\n             if (validatedEntityMap.containsKey(entityType)) {\n-                return new ApiError(Errors.INVALID_REQUEST, \"Invalid empty client quota entity, duplicate entity entry \" + entityType);\n+                return new ApiError(Errors.INVALID_REQUEST, \"Invalid client quota entity, duplicate entity entry \" + entityType);\n             }\n             if (Objects.equals(entityType, ClientQuotaEntity.USER)) {\n                 validatedEntityMap.put(ClientQuotaEntity.USER, entityName);"
  }
]
