[
  {
    "sha": "7a1678445a83f3e964f4d8ea22a4f261f4264225",
    "filename": "clients/src/main/java/org/apache/kafka/common/requests/FetchRequest.java",
    "status": "modified",
    "additions": 10,
    "deletions": 5,
    "changes": 15,
    "blob_url": "https://github.com/apache/kafka/blob/603da4d86b155321aa18cc2cda1b8659ea31bc2d/clients/src/main/java/org/apache/kafka/common/requests/FetchRequest.java",
    "raw_url": "https://github.com/apache/kafka/raw/603da4d86b155321aa18cc2cda1b8659ea31bc2d/clients/src/main/java/org/apache/kafka/common/requests/FetchRequest.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/requests/FetchRequest.java?ref=603da4d86b155321aa18cc2cda1b8659ea31bc2d",
    "patch": "@@ -296,11 +296,16 @@ public AbstractResponse getErrorResponse(int throttleTimeMs, Throwable e) {\n         // may not be any partitions at all in the response.  For this reason, the top-level error code\n         // is essential for them.\n         Errors error = Errors.forException(e);\n-        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n-        for (Map.Entry<TopicPartition, PartitionData> entry : fetchData.entrySet()) {\n-            responseData.put(entry.getKey(), FetchResponse.partitionResponse(entry.getKey().partition(), error));\n-        }\n-        return FetchResponse.of(error, throttleTimeMs, data.sessionId(), responseData);\n+        FetchResponseData responseData = new FetchResponseData()\n+            .setThrottleTimeMs(throttleTimeMs)\n+            .setErrorCode(error.code())\n+            .setSessionId(data.sessionId());\n+        data.topics().forEach(topic -> {\n+            FetchResponseData.FetchableTopicResponse topicResponses = new FetchResponseData.FetchableTopicResponse().setTopic(topic.topic());\n+            responseData.responses().add(topicResponses);\n+            topic.partitions().forEach(partition -> topicResponses.partitions().add(FetchResponse.partitionResponse(partition.partition(), error)));\n+        });\n+        return new FetchResponse(responseData);\n     }\n \n     public int replicaId() {"
  },
  {
    "sha": "493b558b618938c7405e18edee2d05121e4543f5",
    "filename": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java",
    "status": "modified",
    "additions": 10,
    "deletions": 47,
    "changes": 57,
    "blob_url": "https://github.com/apache/kafka/blob/603da4d86b155321aa18cc2cda1b8659ea31bc2d/clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java",
    "raw_url": "https://github.com/apache/kafka/raw/603da4d86b155321aa18cc2cda1b8659ea31bc2d/clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java?ref=603da4d86b155321aa18cc2cda1b8659ea31bc2d",
    "patch": "@@ -26,9 +26,7 @@\n import org.apache.kafka.common.record.Records;\n \n import java.nio.ByteBuffer;\n-import java.util.ArrayList;\n import java.util.HashMap;\n-import java.util.Iterator;\n import java.util.LinkedHashMap;\n import java.util.List;\n import java.util.Map;\n@@ -122,17 +120,20 @@ public static FetchResponse parse(ByteBuffer buffer, short version) {\n     /**\n      * Convenience method to find the size of a response.\n      *\n-     * @param version       The version of the response to use.\n-     * @param partIterator  The partition iterator.\n-     * @return              The response size in bytes.\n+     * @param version        The version of the response to use.\n+     * @param topicResponses data\n+     * @return               The response size in bytes.\n      */\n     public static int sizeOf(short version,\n-                             Iterator<Map.Entry<TopicPartition, FetchResponseData.PartitionData>> partIterator) {\n+                             List<FetchResponseData.FetchableTopicResponse> topicResponses) {\n         // Since the throttleTimeMs and metadata field sizes are constant and fixed, we can\n         // use arbitrary values here without affecting the result.\n-        FetchResponseData data = toMessage(Errors.NONE, 0, INVALID_SESSION_ID, partIterator);\n-        ObjectSerializationCache cache = new ObjectSerializationCache();\n-        return 4 + data.size(cache, version);\n+        return 4 + new FetchResponseData()\n+            .setErrorCode(Errors.NONE.code())\n+            .setThrottleTimeMs(0)\n+            .setSessionId(INVALID_SESSION_ID)\n+            .setResponses(topicResponses)\n+            .size(new ObjectSerializationCache(), version);\n     }\n \n     @Override\n@@ -188,42 +189,4 @@ public static Records recordsOrFail(FetchResponseData.PartitionData partition) {\n     public static int recordsSize(FetchResponseData.PartitionData partition) {\n         return partition.records() == null ? 0 : partition.records().sizeInBytes();\n     }\n-\n-    public static FetchResponse of(Errors error,\n-                                   int throttleTimeMs,\n-                                   int sessionId,\n-                                   LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData) {\n-        return new FetchResponse(toMessage(error, throttleTimeMs, sessionId, responseData.entrySet().iterator()));\n-    }\n-\n-    private static FetchResponseData toMessage(Errors error,\n-                                               int throttleTimeMs,\n-                                               int sessionId,\n-                                               Iterator<Map.Entry<TopicPartition, FetchResponseData.PartitionData>> partIterator) {\n-        List<FetchResponseData.FetchableTopicResponse> topicResponseList = new ArrayList<>();\n-        partIterator.forEachRemaining(entry -> {\n-            FetchResponseData.PartitionData partitionData = entry.getValue();\n-            // Since PartitionData alone doesn't know the partition ID, we set it here\n-            partitionData.setPartitionIndex(entry.getKey().partition());\n-            // We have to keep the order of input topic-partition. Hence, we batch the partitions only if the last\n-            // batch is in the same topic group.\n-            FetchResponseData.FetchableTopicResponse previousTopic = topicResponseList.isEmpty() ? null\n-                : topicResponseList.get(topicResponseList.size() - 1);\n-            if (previousTopic != null && previousTopic.topic().equals(entry.getKey().topic()))\n-                previousTopic.partitions().add(partitionData);\n-            else {\n-                List<FetchResponseData.PartitionData> partitionResponses = new ArrayList<>();\n-                partitionResponses.add(partitionData);\n-                topicResponseList.add(new FetchResponseData.FetchableTopicResponse()\n-                    .setTopic(entry.getKey().topic())\n-                    .setPartitions(partitionResponses));\n-            }\n-        });\n-\n-        return new FetchResponseData()\n-            .setThrottleTimeMs(throttleTimeMs)\n-            .setErrorCode(error.code())\n-            .setSessionId(sessionId)\n-            .setResponses(topicResponseList);\n-    }\n }\n\\ No newline at end of file"
  },
  {
    "sha": "b689720a46b2032ac5cee8daf1a95599de1bf017",
    "filename": "clients/src/test/java/org/apache/kafka/clients/FetchSessionHandlerTest.java",
    "status": "modified",
    "additions": 39,
    "deletions": 32,
    "changes": 71,
    "blob_url": "https://github.com/apache/kafka/blob/603da4d86b155321aa18cc2cda1b8659ea31bc2d/clients/src/test/java/org/apache/kafka/clients/FetchSessionHandlerTest.java",
    "raw_url": "https://github.com/apache/kafka/raw/603da4d86b155321aa18cc2cda1b8659ea31bc2d/clients/src/test/java/org/apache/kafka/clients/FetchSessionHandlerTest.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/FetchSessionHandlerTest.java?ref=603da4d86b155321aa18cc2cda1b8659ea31bc2d",
    "patch": "@@ -27,6 +27,7 @@\n \n import java.util.ArrayList;\n import java.util.Arrays;\n+import java.util.Collections;\n import java.util.Comparator;\n import java.util.Iterator;\n import java.util.LinkedHashMap;\n@@ -163,12 +164,21 @@ private static void assertListEquals(List<TopicPartition> expected, List<TopicPa\n         }\n     }\n \n-    private static LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> respMap(RespEntry... entries) {\n-        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> map = new LinkedHashMap<>();\n+    private static FetchResponse respMap(Errors error,\n+                                         int throttleTimeMs,\n+                                         int sessionId,\n+                                         RespEntry... entries) {\n+        List<FetchResponseData.FetchableTopicResponse> topicResponses = new ArrayList<>();\n         for (RespEntry entry : entries) {\n-            map.put(entry.part, entry.data);\n+            topicResponses.add(new FetchResponseData.FetchableTopicResponse()\n+                .setTopic(entry.part.topic())\n+                .setPartitions(Collections.singletonList(entry.data)));\n         }\n-        return map;\n+        return new FetchResponse(new FetchResponseData()\n+            .setErrorCode(error.code())\n+            .setThrottleTimeMs(throttleTimeMs)\n+            .setSessionId(sessionId)\n+            .setResponses(topicResponses));\n     }\n \n     /**\n@@ -190,10 +200,9 @@ public void testSessionless() {\n         assertEquals(INVALID_SESSION_ID, data.metadata().sessionId());\n         assertEquals(INITIAL_EPOCH, data.metadata().epoch());\n \n-        FetchResponse resp = FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID,\n-            respMap(new RespEntry(\"foo\", 0, 0, 0),\n-                    new RespEntry(\"foo\", 1, 0, 0))\n-            );\n+        FetchResponse resp = respMap(Errors.NONE, 0, INVALID_SESSION_ID,\n+                new RespEntry(\"foo\", 0, 0, 0),\n+                new RespEntry(\"foo\", 1, 0, 0));\n         handler.handleResponse(resp);\n \n         FetchSessionHandler.Builder builder2 = handler.newBuilder();\n@@ -224,9 +233,9 @@ public void testIncrementals() {\n         assertEquals(INVALID_SESSION_ID, data.metadata().sessionId());\n         assertEquals(INITIAL_EPOCH, data.metadata().epoch());\n \n-        FetchResponse resp = FetchResponse.of(Errors.NONE, 0, 123,\n-            respMap(new RespEntry(\"foo\", 0, 10, 20),\n-                    new RespEntry(\"foo\", 1, 10, 20)));\n+        FetchResponse resp = respMap(Errors.NONE, 0, 123,\n+            new RespEntry(\"foo\", 0, 10, 20),\n+            new RespEntry(\"foo\", 1, 10, 20));\n         handler.handleResponse(resp);\n \n         // Test an incremental fetch request which adds one partition and modifies another.\n@@ -247,14 +256,13 @@ public void testIncrementals() {\n                 new ReqEntry(\"foo\", 1, 10, 120, 210)),\n             data2.toSend());\n \n-        FetchResponse resp2 = FetchResponse.of(Errors.NONE, 0, 123,\n-            respMap(new RespEntry(\"foo\", 1, 20, 20)));\n+        FetchResponse resp2 = respMap(Errors.NONE, 0, 123,\n+            new RespEntry(\"foo\", 1, 20, 20));\n         handler.handleResponse(resp2);\n \n         // Skip building a new request.  Test that handling an invalid fetch session epoch response results\n         // in a request which closes the session.\n-        FetchResponse resp3 = FetchResponse.of(Errors.INVALID_FETCH_SESSION_EPOCH,\n-                0, INVALID_SESSION_ID, respMap());\n+        FetchResponse resp3 = respMap(Errors.INVALID_FETCH_SESSION_EPOCH, 0, INVALID_SESSION_ID);\n         handler.handleResponse(resp3);\n \n         FetchSessionHandler.Builder builder4 = handler.newBuilder();\n@@ -309,10 +317,10 @@ public void testIncrementalPartitionRemoval() {\n             data.toSend(), data.sessionPartitions());\n         assertTrue(data.metadata().isFull());\n \n-        FetchResponse resp = FetchResponse.of(Errors.NONE, 0, 123,\n-            respMap(new RespEntry(\"foo\", 0, 10, 20),\n-                    new RespEntry(\"foo\", 1, 10, 20),\n-                    new RespEntry(\"bar\", 0, 10, 20)));\n+        FetchResponse resp = respMap(Errors.NONE, 0, 123,\n+            new RespEntry(\"foo\", 0, 10, 20),\n+                new RespEntry(\"foo\", 1, 10, 20),\n+                new RespEntry(\"bar\", 0, 10, 20));\n         handler.handleResponse(resp);\n \n         // Test an incremental fetch request which removes two partitions.\n@@ -333,8 +341,7 @@ public void testIncrementalPartitionRemoval() {\n \n         // A FETCH_SESSION_ID_NOT_FOUND response triggers us to close the session.\n         // The next request is a session establishing FULL request.\n-        FetchResponse resp2 = FetchResponse.of(Errors.FETCH_SESSION_ID_NOT_FOUND,\n-                0, INVALID_SESSION_ID, respMap());\n+        FetchResponse resp2 = respMap(Errors.FETCH_SESSION_ID_NOT_FOUND, 0, INVALID_SESSION_ID);\n         handler.handleResponse(resp2);\n         FetchSessionHandler.Builder builder3 = handler.newBuilder();\n         builder3.add(new TopicPartition(\"foo\", 0),\n@@ -350,10 +357,10 @@ public void testIncrementalPartitionRemoval() {\n     @Test\n     public void testVerifyFullFetchResponsePartitions() throws Exception {\n         FetchSessionHandler handler = new FetchSessionHandler(LOG_CONTEXT, 1);\n-        String issue = handler.verifyFullFetchResponsePartitions(FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID,\n-            respMap(new RespEntry(\"foo\", 0, 10, 20),\n-                new RespEntry(\"foo\", 1, 10, 20),\n-                new RespEntry(\"bar\", 0, 10, 20))));\n+        String issue = handler.verifyFullFetchResponsePartitions(respMap(Errors.NONE, 0, INVALID_SESSION_ID,\n+            new RespEntry(\"foo\", 0, 10, 20),\n+            new RespEntry(\"foo\", 1, 10, 20),\n+            new RespEntry(\"bar\", 0, 10, 20)));\n         assertTrue(issue.contains(\"extra\"));\n         assertFalse(issue.contains(\"omitted\"));\n         FetchSessionHandler.Builder builder = handler.newBuilder();\n@@ -364,14 +371,14 @@ public void testVerifyFullFetchResponsePartitions() throws Exception {\n         builder.add(new TopicPartition(\"bar\", 0),\n             new FetchRequest.PartitionData(20, 120, 220, Optional.empty()));\n         builder.build();\n-        String issue2 = handler.verifyFullFetchResponsePartitions(FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID,\n-            respMap(new RespEntry(\"foo\", 0, 10, 20),\n-                new RespEntry(\"foo\", 1, 10, 20),\n-                new RespEntry(\"bar\", 0, 10, 20))));\n+        String issue2 = handler.verifyFullFetchResponsePartitions(respMap(Errors.NONE, 0, INVALID_SESSION_ID,\n+            new RespEntry(\"foo\", 0, 10, 20),\n+            new RespEntry(\"foo\", 1, 10, 20),\n+            new RespEntry(\"bar\", 0, 10, 20)));\n         assertTrue(issue2 == null);\n-        String issue3 = handler.verifyFullFetchResponsePartitions(FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID,\n-            respMap(new RespEntry(\"foo\", 0, 10, 20),\n-                new RespEntry(\"foo\", 1, 10, 20))));\n+        String issue3 = handler.verifyFullFetchResponsePartitions(respMap(Errors.NONE, 0, INVALID_SESSION_ID,\n+            new RespEntry(\"foo\", 0, 10, 20),\n+            new RespEntry(\"foo\", 1, 10, 20)));\n         assertFalse(issue3.contains(\"extra\"));\n         assertTrue(issue3.contains(\"omitted\"));\n     }"
  },
  {
    "sha": "2f9c112bcc77cdc64833ec5f7106c7cc966a9958",
    "filename": "clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java",
    "status": "modified",
    "additions": 10,
    "deletions": 6,
    "changes": 16,
    "blob_url": "https://github.com/apache/kafka/blob/603da4d86b155321aa18cc2cda1b8659ea31bc2d/clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java",
    "raw_url": "https://github.com/apache/kafka/raw/603da4d86b155321aa18cc2cda1b8659ea31bc2d/clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java?ref=603da4d86b155321aa18cc2cda1b8659ea31bc2d",
    "patch": "@@ -108,7 +108,6 @@\n import java.util.HashMap;\n import java.util.HashSet;\n import java.util.Iterator;\n-import java.util.LinkedHashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n@@ -2277,7 +2276,7 @@ private ListOffsetsResponse listOffsetsResponse(Map<TopicPartition, Long> partit\n     }\n \n     private FetchResponse fetchResponse(Map<TopicPartition, FetchInfo> fetches) {\n-        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> tpResponses = new LinkedHashMap<>();\n+        List<FetchResponseData.FetchableTopicResponse> topicResponses = new ArrayList<>();\n         for (Map.Entry<TopicPartition, FetchInfo> fetchEntry : fetches.entrySet()) {\n             TopicPartition partition = fetchEntry.getKey();\n             long fetchOffset = fetchEntry.getValue().offset;\n@@ -2294,14 +2293,19 @@ private FetchResponse fetchResponse(Map<TopicPartition, FetchInfo> fetches) {\n                     builder.append(0L, (\"key-\" + i).getBytes(), (\"value-\" + i).getBytes());\n                 records = builder.build();\n             }\n-            tpResponses.put(partition,\n-                new FetchResponseData.PartitionData()\n+            topicResponses.add(new FetchResponseData.FetchableTopicResponse()\n+                .setTopic(partition.topic())\n+                .setPartitions(Collections.singletonList(new FetchResponseData.PartitionData()\n                     .setPartitionIndex(partition.partition())\n                     .setHighWatermark(highWatermark)\n                     .setLogStartOffset(logStartOffset)\n-                    .setRecords(records));\n+                    .setRecords(records))));\n         }\n-        return FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, tpResponses);\n+        return new FetchResponse(new FetchResponseData()\n+            .setErrorCode(Errors.NONE.code())\n+            .setThrottleTimeMs(0)\n+            .setSessionId(INVALID_SESSION_ID)\n+            .setResponses(topicResponses));\n     }\n \n     private FetchResponse fetchResponse(TopicPartition partition, long fetchOffset, int count) {"
  },
  {
    "sha": "1cc20e2b7696d2533a9a2ce3da86fbe40f014284",
    "filename": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java",
    "status": "modified",
    "additions": 161,
    "deletions": 94,
    "changes": 255,
    "blob_url": "https://github.com/apache/kafka/blob/603da4d86b155321aa18cc2cda1b8659ea31bc2d/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java",
    "raw_url": "https://github.com/apache/kafka/raw/603da4d86b155321aa18cc2cda1b8659ea31bc2d/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java?ref=603da4d86b155321aa18cc2cda1b8659ea31bc2d",
    "patch": "@@ -1271,18 +1271,22 @@ public void testFetchPositionAfterException() {\n \n         assertEquals(1, fetcher.sendFetches());\n \n-\n-        Map<TopicPartition, FetchResponseData.PartitionData> partitions = new LinkedHashMap<>();\n-        partitions.put(tp1, new FetchResponseData.PartitionData()\n-                .setPartitionIndex(tp1.partition())\n-                .setHighWatermark(100)\n-                .setRecords(records));\n-        partitions.put(tp0, new FetchResponseData.PartitionData()\n-                .setPartitionIndex(tp0.partition())\n-                .setErrorCode(Errors.OFFSET_OUT_OF_RANGE.code())\n-                .setHighWatermark(100));\n-        client.prepareResponse(FetchResponse.of(Errors.NONE,\n-            0, INVALID_SESSION_ID, new LinkedHashMap<>(partitions)));\n+        List<FetchResponseData.PartitionData> partitionResponses = new ArrayList<>();\n+        partitionResponses.add(new FetchResponseData.PartitionData()\n+            .setPartitionIndex(tp1.partition())\n+            .setHighWatermark(100)\n+            .setRecords(records));\n+        partitionResponses.add(new FetchResponseData.PartitionData()\n+            .setPartitionIndex(tp0.partition())\n+            .setErrorCode(Errors.OFFSET_OUT_OF_RANGE.code())\n+            .setHighWatermark(100));\n+        client.prepareResponse(new FetchResponse(new FetchResponseData()\n+            .setErrorCode(Errors.NONE.code())\n+            .setThrottleTimeMs(0)\n+            .setSessionId(INVALID_SESSION_ID)\n+            .setResponses(Collections.singletonList(new FetchResponseData.FetchableTopicResponse()\n+                .setTopic(topicName)\n+                .setPartitions(partitionResponses)))));\n         consumerClient.poll(time.timer(0));\n \n         List<ConsumerRecord<byte[], byte[]>> allFetchedRecords = new ArrayList<>();\n@@ -1322,29 +1326,34 @@ public void testCompletedFetchRemoval() {\n \n         assertEquals(1, fetcher.sendFetches());\n \n-        Map<TopicPartition, FetchResponseData.PartitionData> partitions = new LinkedHashMap<>();\n-        partitions.put(tp1, new FetchResponseData.PartitionData()\n-                .setPartitionIndex(tp1.partition())\n-                .setHighWatermark(100)\n-                .setRecords(records));\n-        partitions.put(tp0, new FetchResponseData.PartitionData()\n-                .setPartitionIndex(tp0.partition())\n-                .setErrorCode(Errors.OFFSET_OUT_OF_RANGE.code())\n-                .setHighWatermark(100));\n-        partitions.put(tp2, new FetchResponseData.PartitionData()\n-                .setPartitionIndex(tp2.partition())\n-                .setHighWatermark(100)\n-                .setLastStableOffset(4)\n-                .setLogStartOffset(0)\n-                .setRecords(nextRecords));\n-        partitions.put(tp3, new FetchResponseData.PartitionData()\n-                .setPartitionIndex(tp3.partition())\n-                .setHighWatermark(100)\n-                .setLastStableOffset(4)\n-                .setLogStartOffset(0)\n-                .setRecords(partialRecords));\n-        client.prepareResponse(FetchResponse.of(Errors.NONE,\n-                0, INVALID_SESSION_ID, new LinkedHashMap<>(partitions)));\n+        List<FetchResponseData.PartitionData> partitionResponses = new ArrayList<>();\n+        partitionResponses.add(new FetchResponseData.PartitionData()\n+            .setPartitionIndex(tp1.partition())\n+            .setHighWatermark(100)\n+            .setRecords(records));\n+        partitionResponses.add(new FetchResponseData.PartitionData()\n+            .setPartitionIndex(tp0.partition())\n+            .setErrorCode(Errors.OFFSET_OUT_OF_RANGE.code())\n+            .setHighWatermark(100));\n+        partitionResponses.add(new FetchResponseData.PartitionData()\n+            .setPartitionIndex(tp2.partition())\n+            .setHighWatermark(100)\n+            .setLastStableOffset(4)\n+            .setLogStartOffset(0)\n+            .setRecords(nextRecords));\n+        partitionResponses.add(new FetchResponseData.PartitionData()\n+            .setPartitionIndex(tp3.partition())\n+            .setHighWatermark(100)\n+            .setLastStableOffset(4)\n+            .setLogStartOffset(0)\n+            .setRecords(partialRecords));\n+        client.prepareResponse(new FetchResponse(new FetchResponseData()\n+            .setErrorCode(Errors.NONE.code())\n+            .setThrottleTimeMs(0)\n+            .setSessionId(INVALID_SESSION_ID)\n+            .setResponses(Collections.singletonList(new FetchResponseData.FetchableTopicResponse()\n+                .setTopic(topicName)\n+                .setPartitions(partitionResponses)))));\n         consumerClient.poll(time.timer(0));\n \n         List<ConsumerRecord<byte[], byte[]>> fetchedRecords = new ArrayList<>();\n@@ -1416,12 +1425,18 @@ public void testSeekBeforeException() {\n         subscriptions.seekUnvalidated(tp1, new SubscriptionState.FetchPosition(1, Optional.empty(), metadata.currentLeader(tp1)));\n \n         assertEquals(1, fetcher.sendFetches());\n-        partitions = new HashMap<>();\n-        partitions.put(tp1, new FetchResponseData.PartitionData()\n-                        .setPartitionIndex(tp1.partition())\n-                        .setErrorCode(Errors.OFFSET_OUT_OF_RANGE.code())\n-                        .setHighWatermark(100));\n-        client.prepareResponse(FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, new LinkedHashMap<>(partitions)));\n+        List<FetchResponseData.PartitionData> partitionResponses = new ArrayList<>();\n+        partitionResponses.add(new FetchResponseData.PartitionData()\n+            .setPartitionIndex(tp1.partition())\n+            .setErrorCode(Errors.OFFSET_OUT_OF_RANGE.code())\n+            .setHighWatermark(100));\n+        client.prepareResponse(new FetchResponse(new FetchResponseData()\n+            .setErrorCode(Errors.NONE.code())\n+            .setThrottleTimeMs(0)\n+            .setSessionId(INVALID_SESSION_ID)\n+            .setResponses(Collections.singletonList(new FetchResponseData.FetchableTopicResponse()\n+                .setTopic(topicName)\n+                .setPartitions(partitionResponses)))));\n         consumerClient.poll(time.timer(0));\n         assertEquals(1, fetcher.fetchedRecords().get(tp0).size());\n \n@@ -2299,7 +2314,7 @@ public void testFetchResponseMetrics() {\n         }\n \n         assertEquals(1, fetcher.sendFetches());\n-        client.prepareResponse(FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, fetchPartitionData));\n+        client.prepareResponse(fetchResponse(Errors.NONE, 0, INVALID_SESSION_ID, fetchPartitionData));\n         consumerClient.poll(time.timer(0));\n \n         Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>> fetchedRecords = fetchedRecords();\n@@ -2358,21 +2373,26 @@ public void testFetchResponseMetricsWithOnePartitionError() {\n             builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n         MemoryRecords records = builder.build();\n \n-        Map<TopicPartition, FetchResponseData.PartitionData> partitions = new HashMap<>();\n-        partitions.put(tp0, new FetchResponseData.PartitionData()\n+        List<FetchResponseData.PartitionData> partitionResponses = new ArrayList<>();\n+        partitionResponses.add(new FetchResponseData.PartitionData()\n                 .setPartitionIndex(tp0.partition())\n                 .setHighWatermark(100)\n                 .setLogStartOffset(0)\n                 .setRecords(records));\n-        partitions.put(tp1, new FetchResponseData.PartitionData()\n+        partitionResponses.add(new FetchResponseData.PartitionData()\n                 .setPartitionIndex(tp1.partition())\n                 .setErrorCode(Errors.OFFSET_OUT_OF_RANGE.code())\n                 .setHighWatermark(100)\n                 .setLogStartOffset(0));\n \n         assertEquals(1, fetcher.sendFetches());\n-        client.prepareResponse(FetchResponse.of(Errors.NONE,\n-                0, INVALID_SESSION_ID, new LinkedHashMap<>(partitions)));\n+        client.prepareResponse(new FetchResponse(new FetchResponseData()\n+            .setErrorCode(Errors.NONE.code())\n+            .setThrottleTimeMs(0)\n+            .setSessionId(INVALID_SESSION_ID)\n+            .setResponses(Collections.singletonList(new FetchResponseData.FetchableTopicResponse()\n+                .setTopic(topicName)\n+                .setPartitions(partitionResponses)))));\n         consumerClient.poll(time.timer(0));\n         fetcher.fetchedRecords();\n \n@@ -2406,19 +2426,25 @@ public void testFetchResponseMetricsWithOnePartitionAtTheWrongOffset() {\n             builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n         MemoryRecords records = builder.build();\n \n-        Map<TopicPartition, FetchResponseData.PartitionData> partitions = new HashMap<>();\n-        partitions.put(tp0, new FetchResponseData.PartitionData()\n-                .setPartitionIndex(tp0.partition())\n-                .setHighWatermark(100)\n-                .setLogStartOffset(0)\n-                .setRecords(records));\n-        partitions.put(tp1, new FetchResponseData.PartitionData()\n-                .setPartitionIndex(tp1.partition())\n-                .setHighWatermark(100)\n-                .setLogStartOffset(0)\n-                .setRecords(MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"val\".getBytes()))));\n-\n-        client.prepareResponse(FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, new LinkedHashMap<>(partitions)));\n+        List<FetchResponseData.PartitionData> partitionResponses = new ArrayList<>();\n+        partitionResponses.add(new FetchResponseData.PartitionData()\n+            .setPartitionIndex(tp0.partition())\n+            .setHighWatermark(100)\n+            .setLogStartOffset(0)\n+            .setRecords(records));\n+        partitionResponses.add(new FetchResponseData.PartitionData()\n+            .setPartitionIndex(tp1.partition())\n+            .setHighWatermark(100)\n+            .setLogStartOffset(0)\n+            .setRecords(MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"val\".getBytes()))));\n+\n+        client.prepareResponse(new FetchResponse(new FetchResponseData()\n+            .setErrorCode(Errors.NONE.code())\n+            .setThrottleTimeMs(0)\n+            .setSessionId(INVALID_SESSION_ID)\n+            .setResponses(Collections.singletonList(new FetchResponseData.FetchableTopicResponse()\n+                .setTopic(topicName)\n+                .setPartitions(partitionResponses)))));\n         consumerClient.poll(time.timer(0));\n         fetcher.fetchedRecords();\n \n@@ -3252,19 +3278,25 @@ public void testConsumingViaIncrementalFetchRequests() {\n         subscriptions.seekValidated(tp1, new SubscriptionState.FetchPosition(1, Optional.empty(), metadata.currentLeader(tp1)));\n \n         // Fetch some records and establish an incremental fetch session.\n-        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> partitions1 = new LinkedHashMap<>();\n-        partitions1.put(tp0, new FetchResponseData.PartitionData()\n+        List<FetchResponseData.PartitionData> partitionResponses1 = new ArrayList<>();\n+        partitionResponses1.add(new FetchResponseData.PartitionData()\n                 .setPartitionIndex(tp0.partition())\n                 .setHighWatermark(2)\n                 .setLastStableOffset(2)\n                 .setLogStartOffset(0)\n                 .setRecords(this.records));\n-        partitions1.put(tp1, new FetchResponseData.PartitionData()\n+        partitionResponses1.add(new FetchResponseData.PartitionData()\n                 .setPartitionIndex(tp1.partition())\n                 .setHighWatermark(100)\n                 .setLogStartOffset(0)\n                 .setRecords(emptyRecords));\n-        FetchResponse resp1 = FetchResponse.of(Errors.NONE, 0, 123, partitions1);\n+        FetchResponse resp1 = new FetchResponse(new FetchResponseData()\n+            .setErrorCode(Errors.NONE.code())\n+            .setThrottleTimeMs(0)\n+            .setSessionId(123)\n+            .setResponses(Collections.singletonList(new FetchResponseData.FetchableTopicResponse()\n+                .setTopic(topicName)\n+                .setPartitions(partitionResponses1))));\n         client.prepareResponse(resp1);\n         assertEquals(1, fetcher.sendFetches());\n         assertFalse(fetcher.hasCompletedFetches());\n@@ -3289,8 +3321,10 @@ public void testConsumingViaIncrementalFetchRequests() {\n         assertEquals(4L, subscriptions.position(tp0).offset);\n \n         // The second response contains no new records.\n-        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> partitions2 = new LinkedHashMap<>();\n-        FetchResponse resp2 = FetchResponse.of(Errors.NONE, 0, 123, partitions2);\n+        FetchResponse resp2 = new FetchResponse(new FetchResponseData()\n+            .setErrorCode(Errors.NONE.code())\n+            .setThrottleTimeMs(0)\n+            .setSessionId(123));\n         client.prepareResponse(resp2);\n         assertEquals(1, fetcher.sendFetches());\n         consumerClient.poll(time.timer(0));\n@@ -3300,14 +3334,20 @@ public void testConsumingViaIncrementalFetchRequests() {\n         assertEquals(1L, subscriptions.position(tp1).offset);\n \n         // The third response contains some new records for tp0.\n-        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> partitions3 = new LinkedHashMap<>();\n-        partitions3.put(tp0, new FetchResponseData.PartitionData()\n-                .setPartitionIndex(tp0.partition())\n-                .setHighWatermark(100)\n-                .setLastStableOffset(4)\n-                .setLogStartOffset(0)\n-                .setRecords(this.nextRecords));\n-        FetchResponse resp3 = FetchResponse.of(Errors.NONE, 0, 123, partitions3);\n+        List<FetchResponseData.PartitionData> partitionResponses3 = new ArrayList<>();\n+        partitionResponses3.add(new FetchResponseData.PartitionData()\n+            .setPartitionIndex(tp0.partition())\n+            .setHighWatermark(100)\n+            .setLastStableOffset(4)\n+            .setLogStartOffset(0)\n+            .setRecords(this.nextRecords));\n+        FetchResponse resp3 = new FetchResponse(new FetchResponseData()\n+            .setErrorCode(Errors.NONE.code())\n+            .setThrottleTimeMs(0)\n+            .setSessionId(123)\n+            .setResponses(Collections.singletonList(new FetchResponseData.FetchableTopicResponse()\n+                .setTopic(topicName)\n+                .setPartitions(partitionResponses3))));\n         client.prepareResponse(resp3);\n         assertEquals(1, fetcher.sendFetches());\n         consumerClient.poll(time.timer(0));\n@@ -3414,18 +3454,24 @@ private void verifySessionPartitions() {\n                     if (!client.requests().isEmpty()) {\n                         ClientRequest request = client.requests().peek();\n                         FetchRequest fetchRequest = (FetchRequest) request.requestBuilder().build();\n-                        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseMap = new LinkedHashMap<>();\n+                        List<FetchResponseData.PartitionData> partitionResponses = new ArrayList<>();\n                         for (Map.Entry<TopicPartition, FetchRequest.PartitionData> entry : fetchRequest.fetchData().entrySet()) {\n                             TopicPartition tp = entry.getKey();\n                             long offset = entry.getValue().fetchOffset;\n-                            responseMap.put(tp, new FetchResponseData.PartitionData()\n-                                    .setPartitionIndex(tp.partition())\n-                                    .setHighWatermark(offset + 2)\n-                                    .setLastStableOffset(offset + 2)\n-                                    .setLogStartOffset(0)\n-                                    .setRecords(buildRecords(offset, 2, offset)));\n+                            partitionResponses.add(new FetchResponseData.PartitionData()\n+                                .setPartitionIndex(tp.partition())\n+                                .setHighWatermark(offset + 2)\n+                                .setLastStableOffset(offset + 2)\n+                                .setLogStartOffset(0)\n+                                .setRecords(buildRecords(offset, 2, offset)));\n                         }\n-                        client.respondToRequest(request, FetchResponse.of(Errors.NONE, 0, 123, responseMap));\n+                        client.respondToRequest(request, new FetchResponse(new FetchResponseData()\n+                            .setErrorCode(Errors.NONE.code())\n+                            .setThrottleTimeMs(0)\n+                            .setSessionId(123)\n+                            .setResponses(Collections.singletonList(new FetchResponseData.FetchableTopicResponse()\n+                                .setTopic(topicName)\n+                                .setPartitions(partitionResponses)))));\n                         consumerClient.poll(time.timer(0));\n                     }\n                 }\n@@ -3480,15 +3526,21 @@ public void testFetcherSessionEpochUpdate() throws Exception {\n                         assertTrue(epoch == 0 || epoch == nextEpoch,\n                             String.format(\"Unexpected epoch expected %d got %d\", nextEpoch, epoch));\n                         nextEpoch++;\n-                        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseMap = new LinkedHashMap<>();\n-                        responseMap.put(tp0, new FetchResponseData.PartitionData()\n-                                .setPartitionIndex(tp0.partition())\n-                                .setHighWatermark(nextOffset + 2)\n-                                .setLastStableOffset(nextOffset + 2)\n-                                .setLogStartOffset(0)\n-                                .setRecords(buildRecords(nextOffset, 2, nextOffset)));\n+                        List<FetchResponseData.PartitionData> partitionResponses = new ArrayList<>();\n+                        partitionResponses.add(new FetchResponseData.PartitionData()\n+                            .setPartitionIndex(tp0.partition())\n+                            .setHighWatermark(nextOffset + 2)\n+                            .setLastStableOffset(nextOffset + 2)\n+                            .setLogStartOffset(0)\n+                            .setRecords(buildRecords(nextOffset, 2, nextOffset)));\n                         nextOffset += 2;\n-                        client.respondToRequest(request, FetchResponse.of(Errors.NONE, 0, 123, responseMap));\n+                        client.respondToRequest(request, new FetchResponse(new FetchResponseData()\n+                            .setErrorCode(Errors.NONE.code())\n+                            .setThrottleTimeMs(0)\n+                            .setSessionId(123)\n+                            .setResponses(Collections.singletonList(new FetchResponseData.FetchableTopicResponse()\n+                                .setTopic(topicName)\n+                                .setPartitions(partitionResponses)))));\n                         consumerClient.poll(time.timer(0));\n                     }\n                 }\n@@ -4532,7 +4584,7 @@ private FetchResponse fullFetchResponseWithAbortedTransactions(MemoryRecords rec\n                         .setLogStartOffset(0)\n                         .setAbortedTransactions(abortedTransactions)\n                         .setRecords(records));\n-        return FetchResponse.of(Errors.NONE, throttleTime, INVALID_SESSION_ID, new LinkedHashMap<>(partitions));\n+        return fetchResponse(Errors.NONE, throttleTime, INVALID_SESSION_ID, new LinkedHashMap<>(partitions));\n     }\n \n     private FetchResponse fullFetchResponse(TopicPartition tp, MemoryRecords records, Errors error, long hw, int throttleTime) {\n@@ -4549,7 +4601,7 @@ private FetchResponse fullFetchResponse(TopicPartition tp, MemoryRecords records\n                         .setLastStableOffset(lastStableOffset)\n                         .setLogStartOffset(0)\n                         .setRecords(records));\n-        return FetchResponse.of(Errors.NONE, throttleTime, INVALID_SESSION_ID, new LinkedHashMap<>(partitions));\n+        return fetchResponse(Errors.NONE, throttleTime, INVALID_SESSION_ID, new LinkedHashMap<>(partitions));\n     }\n \n     private FetchResponse fullFetchResponse(TopicPartition tp, MemoryRecords records, Errors error, long hw,\n@@ -4563,7 +4615,7 @@ private FetchResponse fullFetchResponse(TopicPartition tp, MemoryRecords records\n                         .setLogStartOffset(0)\n                         .setRecords(records)\n                         .setPreferredReadReplica(preferredReplicaId.orElse(FetchResponse.INVALID_PREFERRED_REPLICA_ID)));\n-        return FetchResponse.of(Errors.NONE, throttleTime, INVALID_SESSION_ID, new LinkedHashMap<>(partitions));\n+        return fetchResponse(Errors.NONE, throttleTime, INVALID_SESSION_ID, new LinkedHashMap<>(partitions));\n     }\n \n     private FetchResponse fetchResponse(TopicPartition tp, MemoryRecords records, Errors error, long hw,\n@@ -4576,7 +4628,7 @@ private FetchResponse fetchResponse(TopicPartition tp, MemoryRecords records, Er\n                         .setLastStableOffset(lastStableOffset)\n                         .setLogStartOffset(logStartOffset)\n                         .setRecords(records));\n-        return FetchResponse.of(Errors.NONE, throttleTime, INVALID_SESSION_ID, new LinkedHashMap<>(partitions));\n+        return fetchResponse(Errors.NONE, throttleTime, INVALID_SESSION_ID, new LinkedHashMap<>(partitions));\n     }\n \n     private MetadataResponse newMetadataResponse(String topic, Errors error) {\n@@ -4710,4 +4762,19 @@ private void buildDependencies(MetricConfig metricConfig,\n     private <T> List<Long> collectRecordOffsets(List<ConsumerRecord<T, T>> records) {\n         return records.stream().map(ConsumerRecord::offset).collect(Collectors.toList());\n     }\n+\n+    public static FetchResponse fetchResponse(Errors error,\n+                                              int throttleTimeMs,\n+                                              int sessionId,\n+                                              LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData) {\n+        return new FetchResponse(new FetchResponseData()\n+            .setThrottleTimeMs(throttleTimeMs)\n+            .setErrorCode(error.code())\n+            .setSessionId(sessionId)\n+            .setResponses(responseData.entrySet().stream()\n+                .map(entry -> new FetchResponseData.FetchableTopicResponse()\n+                    .setTopic(entry.getKey().topic())\n+                    .setPartitions(Collections.singletonList(entry.getValue())))\n+                .collect(Collectors.toList())));\n+    }\n }"
  },
  {
    "sha": "b281f02179be76d9de782dbd2f0b5c1094d1893d",
    "filename": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java",
    "status": "modified",
    "additions": 92,
    "deletions": 60,
    "changes": 152,
    "blob_url": "https://github.com/apache/kafka/blob/603da4d86b155321aa18cc2cda1b8659ea31bc2d/clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java",
    "raw_url": "https://github.com/apache/kafka/raw/603da4d86b155321aa18cc2cda1b8659ea31bc2d/clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java?ref=603da4d86b155321aa18cc2cda1b8659ea31bc2d",
    "patch": "@@ -806,55 +806,68 @@ public void produceRequestGetErrorResponseTest() {\n \n     @Test\n     public void fetchResponseVersionTest() {\n-        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n-\n         MemoryRecords records = MemoryRecords.readableRecords(ByteBuffer.allocate(10));\n-        responseData.put(new TopicPartition(\"test\", 0),\n-                new FetchResponseData.PartitionData()\n-                        .setPartitionIndex(0)\n-                        .setHighWatermark(1000000)\n-                        .setLogStartOffset(0)\n-                        .setRecords(records));\n-\n-        FetchResponse v0Response = FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, responseData);\n-        FetchResponse v1Response = FetchResponse.of(Errors.NONE, 10, INVALID_SESSION_ID, responseData);\n+        List<FetchResponseData.FetchableTopicResponse> topicResponses = new ArrayList<>();\n+        topicResponses.add(new FetchResponseData.FetchableTopicResponse()\n+            .setTopic(\"test\")\n+            .setPartitions(Collections.singletonList(new FetchResponseData.PartitionData()\n+                .setPartitionIndex(0)\n+                .setHighWatermark(1000000)\n+                .setLogStartOffset(0)\n+                .setRecords(records))));\n+\n+        FetchResponse v0Response = new FetchResponse(new FetchResponseData()\n+            .setThrottleTimeMs(0)\n+            .setErrorCode(Errors.NONE.code())\n+            .setSessionId(INVALID_SESSION_ID)\n+            .setResponses(topicResponses));\n+        FetchResponse v1Response = new FetchResponse(new FetchResponseData()\n+            .setThrottleTimeMs(10)\n+            .setErrorCode(Errors.NONE.code())\n+            .setSessionId(INVALID_SESSION_ID)\n+            .setResponses(topicResponses));\n         assertEquals(0, v0Response.throttleTimeMs(), \"Throttle time must be zero\");\n         assertEquals(10, v1Response.throttleTimeMs(), \"Throttle time must be 10\");\n-        assertEquals(responseData, v0Response.responseData(), \"Response data does not match\");\n-        assertEquals(responseData, v1Response.responseData(), \"Response data does not match\");\n+        assertEquals(v1Response.responseData(), v0Response.responseData(), \"Response data does not match\");\n     }\n \n     @Test\n     public void testFetchResponseV4() {\n-        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n+        List<FetchResponseData.FetchableTopicResponse> topicResponses = new ArrayList<>();\n         MemoryRecords records = MemoryRecords.readableRecords(ByteBuffer.allocate(10));\n \n         List<FetchResponseData.AbortedTransaction> abortedTransactions = asList(\n                 new FetchResponseData.AbortedTransaction().setProducerId(10).setFirstOffset(100),\n                 new FetchResponseData.AbortedTransaction().setProducerId(15).setFirstOffset(50)\n         );\n-        responseData.put(new TopicPartition(\"bar\", 0),\n-                new FetchResponseData.PartitionData()\n-                        .setPartitionIndex(0)\n-                        .setHighWatermark(1000000)\n-                        .setAbortedTransactions(abortedTransactions)\n-                        .setRecords(records));\n-        responseData.put(new TopicPartition(\"bar\", 1),\n-                new FetchResponseData.PartitionData()\n-                        .setPartitionIndex(1)\n-                        .setHighWatermark(900000)\n-                        .setLastStableOffset(5)\n-                        .setRecords(records));\n-        responseData.put(new TopicPartition(\"foo\", 0),\n-                new FetchResponseData.PartitionData()\n-                        .setPartitionIndex(0)\n-                        .setHighWatermark(70000)\n-                        .setLastStableOffset(6)\n-                        .setRecords(records));\n-\n-        FetchResponse response = FetchResponse.of(Errors.NONE, 10, INVALID_SESSION_ID, responseData);\n+        topicResponses.add(new FetchResponseData.FetchableTopicResponse()\n+            .setTopic(\"bar\")\n+            .setPartitions(Collections.singletonList(new FetchResponseData.PartitionData()\n+                .setPartitionIndex(0)\n+                .setHighWatermark(1000000)\n+                .setAbortedTransactions(abortedTransactions)\n+                .setRecords(records))));\n+        topicResponses.add(new FetchResponseData.FetchableTopicResponse()\n+            .setTopic(\"bar\")\n+            .setPartitions(Collections.singletonList(new FetchResponseData.PartitionData()\n+                .setPartitionIndex(1)\n+                .setHighWatermark(900000)\n+                .setLastStableOffset(5)\n+                .setRecords(records))));\n+        topicResponses.add(new FetchResponseData.FetchableTopicResponse()\n+            .setTopic(\"foo\")\n+            .setPartitions(Collections.singletonList(new FetchResponseData.PartitionData()\n+                .setPartitionIndex(0)\n+                .setHighWatermark(70000)\n+                .setLastStableOffset(6)\n+                .setRecords(records))));\n+        FetchResponse response = new FetchResponse(new FetchResponseData()\n+                .setThrottleTimeMs(10)\n+                .setErrorCode(Errors.NONE.code())\n+                .setSessionId(INVALID_SESSION_ID)\n+                .setResponses(topicResponses));\n         FetchResponse deserialized = FetchResponse.parse(response.serialize((short) 4), (short) 4);\n-        assertEquals(responseData, deserialized.responseData());\n+        assertEquals(response.data(), deserialized.data());\n     }\n \n     @Test\n@@ -1177,48 +1190,67 @@ private FetchRequest createFetchRequest(int version) {\n     }\n \n     private FetchResponse createFetchResponse(Errors error, int sessionId) {\n-        return FetchResponse.of(error, 25, sessionId, new LinkedHashMap<>());\n+        return new FetchResponse(new FetchResponseData()\n+                .setThrottleTimeMs(25)\n+                .setErrorCode(error.code())\n+                .setSessionId(sessionId));\n     }\n \n     private FetchResponse createFetchResponse(int sessionId) {\n-        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n+        List<FetchResponseData.FetchableTopicResponse> topicResponses = new ArrayList<>();\n         MemoryRecords records = MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"blah\".getBytes()));\n-        responseData.put(new TopicPartition(\"test\", 0), new FetchResponseData.PartitionData()\n-                        .setPartitionIndex(0)\n-                        .setHighWatermark(1000000)\n-                        .setLogStartOffset(0)\n-                        .setRecords(records));\n+        topicResponses.add(new FetchResponseData.FetchableTopicResponse()\n+            .setTopic(\"test\")\n+            .setPartitions(Collections.singletonList(new FetchResponseData.PartitionData()\n+                .setPartitionIndex(0)\n+                .setHighWatermark(1000000)\n+                .setLogStartOffset(0)\n+                .setRecords(records))));\n         List<FetchResponseData.AbortedTransaction> abortedTransactions = Collections.singletonList(\n             new FetchResponseData.AbortedTransaction().setProducerId(234L).setFirstOffset(999L));\n-        responseData.put(new TopicPartition(\"test\", 1), new FetchResponseData.PartitionData()\n-                        .setPartitionIndex(1)\n-                        .setHighWatermark(1000000)\n-                        .setLogStartOffset(0)\n-                        .setAbortedTransactions(abortedTransactions));\n-        return FetchResponse.of(Errors.NONE, 25, sessionId, responseData);\n+        topicResponses.add(new FetchResponseData.FetchableTopicResponse()\n+            .setTopic(\"test\")\n+            .setPartitions(Collections.singletonList(new FetchResponseData.PartitionData()\n+                .setPartitionIndex(1)\n+                .setHighWatermark(1000000)\n+                .setLogStartOffset(0)\n+                .setAbortedTransactions(abortedTransactions))));\n+        return new FetchResponse(new FetchResponseData()\n+                .setThrottleTimeMs(25)\n+                .setErrorCode(Errors.NONE.code())\n+                .setSessionId(sessionId)\n+                .setResponses(topicResponses));\n     }\n \n     private FetchResponse createFetchResponse(boolean includeAborted) {\n-        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData = new LinkedHashMap<>();\n+        List<FetchResponseData.FetchableTopicResponse> topicResponses = new ArrayList<>();\n         MemoryRecords records = MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"blah\".getBytes()));\n-        responseData.put(new TopicPartition(\"test\", 0), new FetchResponseData.PartitionData()\n-                        .setPartitionIndex(0)\n-                        .setHighWatermark(1000000)\n-                        .setLogStartOffset(0)\n-                        .setRecords(records));\n+        topicResponses.add(new FetchResponseData.FetchableTopicResponse()\n+            .setTopic(\"test\")\n+            .setPartitions(Collections.singletonList(new FetchResponseData.PartitionData()\n+                .setPartitionIndex(0)\n+                .setHighWatermark(1000000)\n+                .setLogStartOffset(0)\n+                .setRecords(records))));\n \n         List<FetchResponseData.AbortedTransaction> abortedTransactions = Collections.emptyList();\n         if (includeAborted) {\n             abortedTransactions = Collections.singletonList(\n                     new FetchResponseData.AbortedTransaction().setProducerId(234L).setFirstOffset(999L));\n         }\n-        responseData.put(new TopicPartition(\"test\", 1), new FetchResponseData.PartitionData()\n-                        .setPartitionIndex(1)\n-                        .setHighWatermark(1000000)\n-                        .setLogStartOffset(0)\n-                        .setAbortedTransactions(abortedTransactions));\n+        topicResponses.add(new FetchResponseData.FetchableTopicResponse()\n+            .setTopic(\"test\")\n+            .setPartitions(Collections.singletonList(new FetchResponseData.PartitionData()\n+                .setPartitionIndex(1)\n+                .setHighWatermark(1000000)\n+                .setLogStartOffset(0)\n+                .setAbortedTransactions(abortedTransactions))));\n \n-        return FetchResponse.of(Errors.NONE, 25, INVALID_SESSION_ID, responseData);\n+        return new FetchResponse(new FetchResponseData()\n+                .setThrottleTimeMs(25)\n+                .setErrorCode(Errors.NONE.code())\n+                .setSessionId(INVALID_SESSION_ID)\n+                .setResponses(topicResponses));\n     }\n \n     private HeartbeatRequest createHeartBeatRequest() {"
  },
  {
    "sha": "e41b4e05018112fca8e4bcf71a50bcf74bc68504",
    "filename": "core/src/main/scala/kafka/server/FetchSession.scala",
    "status": "modified",
    "additions": 99,
    "deletions": 65,
    "changes": 164,
    "blob_url": "https://github.com/apache/kafka/blob/603da4d86b155321aa18cc2cda1b8659ea31bc2d/core/src/main/scala/kafka/server/FetchSession.scala",
    "raw_url": "https://github.com/apache/kafka/raw/603da4d86b155321aa18cc2cda1b8659ea31bc2d/core/src/main/scala/kafka/server/FetchSession.scala",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/server/FetchSession.scala?ref=603da4d86b155321aa18cc2cda1b8659ea31bc2d",
    "patch": "@@ -34,9 +34,9 @@ import scala.math.Ordered.orderingToOrdered\n \n object FetchSession {\n   type REQ_MAP = util.Map[TopicPartition, FetchRequest.PartitionData]\n-  type RESP_MAP = util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n+  type RESP_MAP = util.List[FetchResponseData.FetchableTopicResponse]\n   type CACHE_MAP = ImplicitLinkedHashCollection[CachedPartition]\n-  type RESP_MAP_ITER = util.Iterator[util.Map.Entry[TopicPartition, FetchResponseData.PartitionData]]\n+  type RESP_MAP_ITER = util.Iterator[FetchResponseData.FetchableTopicResponse]\n \n   val NUM_INCREMENTAL_FETCH_SESSISONS = \"NumIncrementalFetchSessions\"\n   val NUM_INCREMENTAL_FETCH_PARTITIONS_CACHED = \"NumIncrementalFetchPartitionsCached\"\n@@ -289,19 +289,32 @@ trait FetchContext extends Logging {\n   def getResponseSize(updates: FetchSession.RESP_MAP, versionId: Short): Int\n \n   /**\n-    * Updates the fetch context with new partition information.  Generates response data.\n-    * The response data may require subsequent down-conversion.\n-    */\n+   * Updates the fetch context with new partition information.  Generates response data.\n+   * The response data may require subsequent down-conversion.\n+   *\n+   * @param updates this method may modify elements of it directly.\n+   * @return FetchResponse with modified updates\n+   */\n   def updateAndGenerateResponseData(updates: FetchSession.RESP_MAP): FetchResponse\n \n-  def partitionsToLogString(partitions: util.Collection[TopicPartition]): String =\n-    FetchSession.partitionsToLogString(partitions, isTraceEnabled)\n+  def partitionsToLogString(topics: FetchSession.RESP_MAP): String = {\n+    val topicPartitions = new util.ArrayList[TopicPartition]()\n+    topics.forEach { topicData =>\n+      topicData.partitions.forEach { partitionData =>\n+        topicPartitions.add(new TopicPartition(topicData.topic, partitionData.partitionIndex))\n+      }\n+    }\n+    FetchSession.partitionsToLogString(topicPartitions, isTraceEnabled)\n+  }\n \n   /**\n     * Return an empty throttled response due to quota violation.\n     */\n   def getThrottledResponse(throttleTimeMs: Int): FetchResponse =\n-    FetchResponse.of(Errors.NONE, throttleTimeMs, INVALID_SESSION_ID, new FetchSession.RESP_MAP)\n+    new FetchResponse(new FetchResponseData()\n+      .setErrorCode(Errors.NONE.code)\n+      .setThrottleTimeMs(throttleTimeMs)\n+      .setSessionId(INVALID_SESSION_ID))\n }\n \n /**\n@@ -314,13 +327,16 @@ class SessionErrorContext(val error: Errors,\n   override def foreachPartition(fun: (TopicPartition, FetchRequest.PartitionData) => Unit): Unit = {}\n \n   override def getResponseSize(updates: FetchSession.RESP_MAP, versionId: Short): Int = {\n-    FetchResponse.sizeOf(versionId, (new FetchSession.RESP_MAP).entrySet.iterator)\n+    FetchResponse.sizeOf(versionId, util.Collections.emptyList())\n   }\n \n   // Because of the fetch session error, we don't know what partitions were supposed to be in this request.\n   override def updateAndGenerateResponseData(updates: FetchSession.RESP_MAP): FetchResponse = {\n     debug(s\"Session error fetch context returning $error\")\n-    FetchResponse.of(error, 0, INVALID_SESSION_ID, new FetchSession.RESP_MAP)\n+    new FetchResponse(new FetchResponseData()\n+      .setErrorCode(error.code)\n+      .setThrottleTimeMs(0)\n+      .setSessionId(INVALID_SESSION_ID))\n   }\n }\n \n@@ -338,12 +354,16 @@ class SessionlessFetchContext(val fetchData: util.Map[TopicPartition, FetchReque\n   }\n \n   override def getResponseSize(updates: FetchSession.RESP_MAP, versionId: Short): Int = {\n-    FetchResponse.sizeOf(versionId, updates.entrySet.iterator)\n+    FetchResponse.sizeOf(versionId, updates)\n   }\n \n   override def updateAndGenerateResponseData(updates: FetchSession.RESP_MAP): FetchResponse = {\n-    debug(s\"Sessionless fetch context returning ${partitionsToLogString(updates.keySet)}\")\n-    FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, updates)\n+    debug(s\"Sessionless fetch context returning ${partitionsToLogString(updates)}\")\n+    new FetchResponse(new FetchResponseData()\n+      .setErrorCode(Errors.NONE.code)\n+      .setThrottleTimeMs(0)\n+      .setSessionId(INVALID_SESSION_ID)\n+      .setResponses(updates))\n   }\n }\n \n@@ -369,23 +389,31 @@ class FullFetchContext(private val time: Time,\n   }\n \n   override def getResponseSize(updates: FetchSession.RESP_MAP, versionId: Short): Int = {\n-    FetchResponse.sizeOf(versionId, updates.entrySet.iterator)\n+    FetchResponse.sizeOf(versionId, updates)\n   }\n \n   override def updateAndGenerateResponseData(updates: FetchSession.RESP_MAP): FetchResponse = {\n     def createNewSession: FetchSession.CACHE_MAP = {\n       val cachedPartitions = new FetchSession.CACHE_MAP(updates.size)\n-      updates.forEach { (part, respData) =>\n-        val reqData = fetchData.get(part)\n-        cachedPartitions.mustAdd(new CachedPartition(part, reqData, respData))\n+      updates.forEach { topicData =>\n+        topicData.partitions.forEach { partitionData =>\n+          val topicPartition = new TopicPartition(topicData.topic, partitionData.partitionIndex)\n+          val reqData = fetchData.get(topicPartition)\n+          cachedPartitions.mustAdd(new CachedPartition(topicPartition, reqData, partitionData))\n+        }\n       }\n       cachedPartitions\n     }\n     val responseSessionId = cache.maybeCreateSession(time.milliseconds(), isFromFollower,\n         updates.size, () => createNewSession)\n     debug(s\"Full fetch context with session id $responseSessionId returning \" +\n-      s\"${partitionsToLogString(updates.keySet)}\")\n-    FetchResponse.of(Errors.NONE, 0, responseSessionId, updates)\n+      s\"${partitionsToLogString(updates)}\")\n+    new FetchResponse(new FetchResponseData()\n+      .setErrorCode(Errors.NONE.code)\n+      .setThrottleTimeMs(0)\n+      .setSessionId(responseSessionId)\n+      .setResponses(updates)\n+    )\n   }\n }\n \n@@ -411,54 +439,51 @@ class IncrementalFetchContext(private val time: Time,\n     }\n   }\n \n-  // Iterator that goes over the given partition map and selects partitions that need to be included in the response.\n-  // If updateFetchContextAndRemoveUnselected is set to true, the fetch context will be updated for the selected\n-  // partitions and also remove unselected ones as they are encountered.\n-  private class PartitionIterator(val iter: FetchSession.RESP_MAP_ITER,\n-                                  val updateFetchContextAndRemoveUnselected: Boolean)\n-    extends FetchSession.RESP_MAP_ITER {\n-    var nextElement: util.Map.Entry[TopicPartition, FetchResponseData.PartitionData] = null\n-\n-    override def hasNext: Boolean = {\n-      while ((nextElement == null) && iter.hasNext) {\n-        val element = iter.next()\n-        val topicPart = element.getKey\n-        val respData = element.getValue\n-        val cachedPart = session.partitionMap.find(new CachedPartition(topicPart))\n-        val mustRespond = cachedPart.maybeUpdateResponseData(respData, updateFetchContextAndRemoveUnselected)\n-        if (mustRespond) {\n-          nextElement = element\n-          if (updateFetchContextAndRemoveUnselected) {\n-            session.partitionMap.remove(cachedPart)\n-            session.partitionMap.mustAdd(cachedPart)\n-          }\n-        } else {\n-          if (updateFetchContextAndRemoveUnselected) {\n-            iter.remove()\n-          }\n-        }\n+  /**\n+   * goes over the given partition map and remove partitions that need to be excluded in the response.\n+   * @param topicResponses topic data\n+   * @param updateFetchContextAndRemoveUnselected true, the fetch context will be updated for the selected partitions\n+   */\n+  private def keepRespondData(topicResponses: FetchSession.RESP_MAP, updateFetchContextAndRemoveUnselected: Boolean): Unit = {\n+    val topicIter = topicResponses.iterator\n+    while (topicIter.hasNext) {\n+      val topicResponse = topicIter.next\n+      val partitionIter = topicResponse.partitions().iterator\n+      while (partitionIter.hasNext) {\n+        val partitionResponse = partitionIter.next\n+        val cachedPart = cachedPartition(topicResponse.topic, partitionResponse.partitionIndex)\n+        val mustRespond = cachedPart.maybeUpdateResponseData(partitionResponse, updateFetchContextAndRemoveUnselected)\n+        if (mustRespond && updateFetchContextAndRemoveUnselected) update(cachedPart)\n+        if (!mustRespond) partitionIter.remove()\n       }\n-      nextElement != null\n+      if (topicResponse.partitions().isEmpty) topicIter.remove()\n     }\n+  }\n \n-    override def next(): util.Map.Entry[TopicPartition, FetchResponseData.PartitionData] = {\n-      if (!hasNext) throw new NoSuchElementException\n-      val element = nextElement\n-      nextElement = null\n-      element\n-    }\n+  def cachedPartition(topic: String, partition: Int): CachedPartition =\n+    session.partitionMap.find(new CachedPartition(topic, partition))\n \n-    override def remove() = throw new UnsupportedOperationException\n+  def update(cachedPartition: CachedPartition): Unit = {\n+    session.partitionMap.remove(cachedPartition)\n+    session.partitionMap.mustAdd(cachedPartition)\n   }\n \n   override def getResponseSize(updates: FetchSession.RESP_MAP, versionId: Short): Int = {\n     session.synchronized {\n       val expectedEpoch = JFetchMetadata.nextEpoch(reqMetadata.epoch)\n       if (session.epoch != expectedEpoch) {\n-        FetchResponse.sizeOf(versionId, (new FetchSession.RESP_MAP).entrySet.iterator)\n+        FetchResponse.sizeOf(versionId, util.Collections.emptyList())\n       } else {\n+        // we have to create a copy as `keepRespondData` remove the elements which can't be responded\n+        val copy = new util.LinkedList[FetchResponseData.FetchableTopicResponse]()\n+        updates.forEach { topicResponse =>\n+          copy.add(new FetchResponseData.FetchableTopicResponse()\n+            .setTopic(topicResponse.topic)\n+            .setPartitions(new util.LinkedList[FetchResponseData.PartitionData](topicResponse.partitions)))\n+        }\n         // Pass the partition iterator which updates neither the fetch context nor the partition map.\n-        FetchResponse.sizeOf(versionId, new PartitionIterator(updates.entrySet.iterator, false))\n+        keepRespondData(copy, false)\n+        FetchResponse.sizeOf(versionId, copy)\n       }\n     }\n   }\n@@ -471,16 +496,19 @@ class IncrementalFetchContext(private val time: Time,\n       if (session.epoch != expectedEpoch) {\n         info(s\"Incremental fetch session ${session.id} expected epoch $expectedEpoch, but \" +\n           s\"got ${session.epoch}.  Possible duplicate request.\")\n-        FetchResponse.of(Errors.INVALID_FETCH_SESSION_EPOCH, 0, session.id, new FetchSession.RESP_MAP)\n+        new FetchResponse(new FetchResponseData()\n+          .setErrorCode(Errors.INVALID_FETCH_SESSION_EPOCH.code)\n+          .setThrottleTimeMs(0)\n+          .setSessionId(session.id))\n       } else {\n-        // Iterate over the update list using PartitionIterator. This will prune updates which don't need to be sent\n-        val partitionIter = new PartitionIterator(updates.entrySet.iterator, true)\n-        while (partitionIter.hasNext) {\n-          partitionIter.next()\n-        }\n+        keepRespondData(updates, true)\n         debug(s\"Incremental fetch context with session id ${session.id} returning \" +\n-          s\"${partitionsToLogString(updates.keySet)}\")\n-        FetchResponse.of(Errors.NONE, 0, session.id, updates)\n+          s\"${partitionsToLogString(updates)}\")\n+        new FetchResponse(new FetchResponseData()\n+          .setErrorCode(Errors.NONE.code)\n+          .setThrottleTimeMs(0)\n+          .setSessionId(session.id)\n+          .setResponses(updates))\n       }\n     }\n   }\n@@ -493,9 +521,15 @@ class IncrementalFetchContext(private val time: Time,\n       if (session.epoch != expectedEpoch) {\n         info(s\"Incremental fetch session ${session.id} expected epoch $expectedEpoch, but \" +\n           s\"got ${session.epoch}.  Possible duplicate request.\")\n-        FetchResponse.of(Errors.INVALID_FETCH_SESSION_EPOCH, throttleTimeMs, session.id, new FetchSession.RESP_MAP)\n+        new FetchResponse(new FetchResponseData()\n+          .setErrorCode(Errors.INVALID_FETCH_SESSION_EPOCH.code)\n+          .setThrottleTimeMs(throttleTimeMs)\n+          .setSessionId(session.id))\n       } else {\n-        FetchResponse.of(Errors.NONE, throttleTimeMs, session.id, new FetchSession.RESP_MAP)\n+        new FetchResponse(new FetchResponseData()\n+          .setErrorCode(Errors.NONE.code)\n+          .setThrottleTimeMs(throttleTimeMs)\n+          .setSessionId(session.id))\n       }\n     }\n   }"
  },
  {
    "sha": "755c5f64debfd871e11053bf2a82e0f4a2df3b61",
    "filename": "core/src/main/scala/kafka/server/KafkaApis.scala",
    "status": "modified",
    "additions": 90,
    "deletions": 54,
    "changes": 144,
    "blob_url": "https://github.com/apache/kafka/blob/603da4d86b155321aa18cc2cda1b8659ea31bc2d/core/src/main/scala/kafka/server/KafkaApis.scala",
    "raw_url": "https://github.com/apache/kafka/raw/603da4d86b155321aa18cc2cda1b8659ea31bc2d/core/src/main/scala/kafka/server/KafkaApis.scala",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/server/KafkaApis.scala?ref=603da4d86b155321aa18cc2cda1b8659ea31bc2d",
    "patch": "@@ -724,8 +724,9 @@ class KafkaApis(val requestChannel: RequestChannel,\n       }\n     }\n \n-    def maybeConvertFetchedData(tp: TopicPartition,\n-                                partitionData: FetchResponseData.PartitionData): FetchResponseData.PartitionData = {\n+    def createConvertedPartition(topicName: String,\n+                                 partitionData: FetchResponseData.PartitionData): FetchResponseData.PartitionData = {\n+      val tp = new TopicPartition(topicName, partitionData.partitionIndex)\n       val logConfig = replicaManager.getLogConfig(tp)\n \n       if (logConfig.exists(_.compressionType == ZStdCompressionCodec.name) && versionId < 10) {\n@@ -793,69 +794,93 @@ class KafkaApis(val requestChannel: RequestChannel,\n       }\n     }\n \n-    // the callback for process a fetch response, invoked before throttling\n-    def processResponseCallback(responsePartitionData: Seq[(TopicPartition, FetchPartitionData)]): Unit = {\n-      val partitions = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n-      val reassigningPartitions = mutable.Set[TopicPartition]()\n+    def mergeErrors(responsePartitionData: Seq[(TopicPartition, FetchPartitionData)]): util.List[FetchResponseData.FetchableTopicResponse] = {\n+      // we use linked list rather than array list since the method `updateAndGenerateResponseData` directly update\n+      // input data to avoid collection copy. the performance of removing element from array list is not good.\n+      val topicResponses = new util.LinkedList[FetchResponseData.FetchableTopicResponse]()\n+\n+      def addPartition(topicName: String, partitionData: FetchResponseData.PartitionData): Unit = {\n+        var topicData = if (topicResponses.isEmpty) null else topicResponses.get(topicResponses.size - 1)\n+        if (topicData == null || topicData.topic != topicName) {\n+          topicData = new FetchResponseData.FetchableTopicResponse().setTopic(topicName)\n+            .setPartitions(new util.LinkedList[FetchResponseData.PartitionData]())\n+          topicResponses.add(topicData)\n+        }\n+        topicData.partitions.add(partitionData)\n+      }\n+\n       responsePartitionData.foreach { case (tp, data) =>\n-        val abortedTransactions = data.abortedTransactions.map(_.asJava).orNull\n-        val lastStableOffset = data.lastStableOffset.getOrElse(FetchResponse.INVALID_LAST_STABLE_OFFSET)\n-        if (data.isReassignmentFetch) reassigningPartitions.add(tp)\n         val partitionData = new FetchResponseData.PartitionData()\n           .setPartitionIndex(tp.partition)\n           .setErrorCode(maybeDownConvertStorageError(data.error).code)\n           .setHighWatermark(data.highWatermark)\n-          .setLastStableOffset(lastStableOffset)\n           .setLogStartOffset(data.logStartOffset)\n-          .setAbortedTransactions(abortedTransactions)\n           .setRecords(data.records)\n-          .setPreferredReadReplica(data.preferredReadReplica.getOrElse(FetchResponse.INVALID_PREFERRED_REPLICA_ID))\n+          // abortedTransactions is nullable so we set null to it.\n+          .setAbortedTransactions(data.abortedTransactions.map(_.asJava).orNull)\n+        data.lastStableOffset.foreach(partitionData.setLastStableOffset)\n+        data.preferredReadReplica.foreach(partitionData.setPreferredReadReplica)\n         data.divergingEpoch.foreach(partitionData.setDivergingEpoch)\n-        partitions.put(tp, partitionData)\n+        addPartition(tp.topic, partitionData)\n       }\n-      erroneous.foreach { case (tp, data) => partitions.put(tp, data) }\n+      erroneous.foreach(entry => addPartition(entry._1.topic, entry._2) )\n \n-      var unconvertedFetchResponse: FetchResponse = null\n+      topicResponses\n+    }\n \n-      def createResponse(throttleTimeMs: Int): FetchResponse = {\n-        // Down-convert messages for each partition if required\n-        val convertedData = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n-        unconvertedFetchResponse.responseData.forEach { (tp, unconvertedPartitionData) =>\n-          val error = Errors.forCode(unconvertedPartitionData.errorCode)\n-          if (error != Errors.NONE)\n-            debug(s\"Fetch request with correlation id ${request.header.correlationId} from client $clientId \" +\n-              s\"on partition $tp failed due to ${error.exceptionName}\")\n-          convertedData.put(tp, maybeConvertFetchedData(tp, unconvertedPartitionData))\n-        }\n+    def updateConversionStats(send: Send): Unit = {\n+      send match {\n+        case send: MultiRecordsSend if send.recordConversionStats != null =>\n+          send.recordConversionStats.asScala.toMap.forKeyValue((tp, stats) => updateRecordConversionStats(request, tp, stats))\n+        case _ =>\n+      }\n+    }\n \n-        // Prepare fetch response from converted data\n-        val response = FetchResponse.of(unconvertedFetchResponse.error, throttleTimeMs, unconvertedFetchResponse.sessionId, convertedData)\n-        // record the bytes out metrics only when the response is being sent\n-        response.responseData.forEach { (tp, data) =>\n-          brokerTopicStats.updateBytesOut(tp.topic, fetchRequest.isFromFollower,\n-            reassigningPartitions.contains(tp), FetchResponse.recordsSize(data))\n+    /**\n+     * update partition data of fetch response. We don't want to create copy of large collection so the converted\n+     * partition data is reassigned to inner array of fetch response.\n+     */\n+    def updateResponseWithConvertedPartition(fetchResponse: FetchResponse, throttleTimeMs: Int): Unit = {\n+      fetchResponse.data().setThrottleTimeMs(throttleTimeMs)\n+      fetchResponse.data.responses.forEach { topicResponse =>\n+        (0 until topicResponse.partitions().size()).foreach { index =>\n+          val partitionData = topicResponse.partitions().get(index)\n+          if (partitionData.errorCode != Errors.NONE.code)\n+            debug(s\"Fetch request with correlation id ${request.header.correlationId} from client $clientId \" +\n+              s\"on partition ${new TopicPartition(topicResponse.topic, partitionData.partitionIndex)} \" +\n+              s\"failed due to ${Errors.forCode(partitionData.errorCode).exceptionName}\")\n+          topicResponse.partitions().set(index, createConvertedPartition(topicResponse.topic, partitionData))\n         }\n-        response\n       }\n+    }\n \n-      def updateConversionStats(send: Send): Unit = {\n-        send match {\n-          case send: MultiRecordsSend if send.recordConversionStats != null =>\n-            send.recordConversionStats.asScala.toMap.foreach {\n-              case (tp, stats) => updateRecordConversionStats(request, tp, stats)\n-            }\n-          case _ =>\n+    /**\n+     * record the bytes out metrics of being sent response\n+     */\n+    def updateBytesOut(fetchResponse: FetchResponse,\n+                       responsePartitionData: Seq[(TopicPartition, FetchPartitionData)]): Unit = {\n+      val reassigningPartitions = responsePartitionData.filter(_._2.isReassignmentFetch).map(_._1).toSet\n+      fetchResponse.data.responses.forEach { topicData =>\n+        topicData.partitions.forEach { partitionData =>\n+          brokerTopicStats.updateBytesOut(topicData.topic, fetchRequest.isFromFollower,\n+            reassigningPartitions.contains(new TopicPartition(topicData.topic, partitionData.partitionIndex)),\n+            FetchResponse.recordsSize(partitionData))\n         }\n       }\n+    }\n+\n+    // the callback for process a fetch response, invoked before throttling\n+    def processResponseCallback(responsePartitionData: Seq[(TopicPartition, FetchPartitionData)]): Unit = {\n \n       if (fetchRequest.isFromFollower) {\n         // We've already evaluated against the quota and are good to go. Just need to record it now.\n-        unconvertedFetchResponse = fetchContext.updateAndGenerateResponseData(partitions)\n-        val responseSize = KafkaApis.sizeOfThrottledPartitions(versionId, unconvertedFetchResponse, quotas.leader)\n-        quotas.leader.record(responseSize)\n-        trace(s\"Sending Fetch response with partitions.size=${unconvertedFetchResponse.responseData.size}, \" +\n-          s\"metadata=${unconvertedFetchResponse.sessionId}\")\n-        requestHelper.sendResponseExemptThrottle(request, createResponse(0), Some(updateConversionStats))\n+        val fetchResponse = fetchContext.updateAndGenerateResponseData(mergeErrors(responsePartitionData))\n+        quotas.leader.record(KafkaApis.sizeOfThrottledPartitions(versionId, fetchResponse, quotas.leader))\n+        trace(s\"Sending Fetch response with partitions.size=${fetchResponse.responseData.size}, \" +\n+          s\"metadata=${fetchResponse.sessionId}\")\n+        updateResponseWithConvertedPartition(fetchResponse, 0)\n+        updateBytesOut(fetchResponse, responsePartitionData)\n+        requestHelper.sendResponseExemptThrottle(request, fetchResponse , Some(updateConversionStats))\n       } else {\n         // Fetch size used to determine throttle time is calculated before any down conversions.\n         // This may be slightly different from the actual response size. But since down conversions\n@@ -864,13 +889,14 @@ class KafkaApis(val requestChannel: RequestChannel,\n         // Record both bandwidth and request quota-specific values and throttle by muting the channel if any of the\n         // quotas have been violated. If both quotas have been violated, use the max throttle time between the two\n         // quotas. When throttled, we unrecord the recorded bandwidth quota value\n-        val responseSize = fetchContext.getResponseSize(partitions, versionId)\n+        val topicResponses = mergeErrors(responsePartitionData)\n+        val responseSize = fetchContext.getResponseSize(topicResponses, versionId)\n         val timeMs = time.milliseconds()\n         val requestThrottleTimeMs = quotas.request.maybeRecordAndGetThrottleTimeMs(request, timeMs)\n         val bandwidthThrottleTimeMs = quotas.fetch.maybeRecordAndGetThrottleTimeMs(request, responseSize, timeMs)\n \n         val maxThrottleTimeMs = math.max(bandwidthThrottleTimeMs, requestThrottleTimeMs)\n-        if (maxThrottleTimeMs > 0) {\n+        val fetchResponse = if (maxThrottleTimeMs > 0) {\n           request.apiThrottleTimeMs = maxThrottleTimeMs\n           // Even if we need to throttle for request quota violation, we should \"unrecord\" the already recorded value\n           // from the fetch quota because we are going to return an empty response.\n@@ -881,15 +907,17 @@ class KafkaApis(val requestChannel: RequestChannel,\n             requestHelper.throttle(quotas.request, request, requestThrottleTimeMs)\n           }\n           // If throttling is required, return an empty response.\n-          unconvertedFetchResponse = fetchContext.getThrottledResponse(maxThrottleTimeMs)\n+          fetchContext.getThrottledResponse(maxThrottleTimeMs)\n         } else {\n           // Get the actual response. This will update the fetch context.\n-          unconvertedFetchResponse = fetchContext.updateAndGenerateResponseData(partitions)\n-          trace(s\"Sending Fetch response with partitions.size=$responseSize, metadata=${unconvertedFetchResponse.sessionId}\")\n+          val response = fetchContext.updateAndGenerateResponseData(mergeErrors(responsePartitionData))\n+          trace(s\"Sending Fetch response with partitions.size=$responseSize, metadata=${response.sessionId}\")\n+          response\n         }\n-\n+        updateResponseWithConvertedPartition(fetchResponse, maxThrottleTimeMs)\n+        updateBytesOut(fetchResponse, responsePartitionData)\n         // Send the response immediately.\n-        requestChannel.sendResponse(request, createResponse(maxThrottleTimeMs), Some(updateConversionStats))\n+        requestChannel.sendResponse(request, fetchResponse , Some(updateConversionStats))\n       }\n     }\n \n@@ -3368,8 +3396,16 @@ object KafkaApis {\n   private[server] def sizeOfThrottledPartitions(versionId: Short,\n                                                 unconvertedResponse: FetchResponse,\n                                                 quota: ReplicationQuotaManager): Int = {\n-    FetchResponse.sizeOf(versionId, unconvertedResponse.responseData.entrySet\n-      .iterator.asScala.filter(element => quota.isThrottled(element.getKey)).asJava)\n+    val topicResponses = new util.ArrayList[FetchResponseData.FetchableTopicResponse]()\n+    unconvertedResponse.data.responses.forEach { topicData =>\n+      val topicResponse = new FetchResponseData.FetchableTopicResponse().setTopic(topicData.topic)\n+      topicData.partitions.forEach { partitionData =>\n+        if (quota.isThrottled(topicData.topic, partitionData.partitionIndex))\n+          topicResponse.partitions.add(partitionData)\n+      }\n+      if (!topicResponse.partitions.isEmpty) topicResponses.add(topicResponse)\n+    }\n+    FetchResponse.sizeOf(versionId, topicResponses)\n   }\n \n   // visible for testing"
  },
  {
    "sha": "08c985a108c75a0003704e2cc37eab0b4ae69e33",
    "filename": "core/src/main/scala/kafka/server/QuotaFactory.scala",
    "status": "modified",
    "additions": 1,
    "deletions": 2,
    "changes": 3,
    "blob_url": "https://github.com/apache/kafka/blob/603da4d86b155321aa18cc2cda1b8659ea31bc2d/core/src/main/scala/kafka/server/QuotaFactory.scala",
    "raw_url": "https://github.com/apache/kafka/raw/603da4d86b155321aa18cc2cda1b8659ea31bc2d/core/src/main/scala/kafka/server/QuotaFactory.scala",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/server/QuotaFactory.scala?ref=603da4d86b155321aa18cc2cda1b8659ea31bc2d",
    "patch": "@@ -18,7 +18,6 @@ package kafka.server\n \n import kafka.server.QuotaType._\n import kafka.utils.Logging\n-import org.apache.kafka.common.TopicPartition\n import org.apache.kafka.common.metrics.Metrics\n import org.apache.kafka.server.quota.ClientQuotaCallback\n import org.apache.kafka.common.utils.Time\n@@ -49,7 +48,7 @@ sealed trait QuotaType\n object QuotaFactory extends Logging {\n \n   object UnboundedQuota extends ReplicaQuota {\n-    override def isThrottled(topicPartition: TopicPartition): Boolean = false\n+    override def isThrottled(topicName: String, partition: Int): Boolean = false\n     override def isQuotaExceeded: Boolean = false\n     def record(value: Long): Unit = ()\n   }"
  },
  {
    "sha": "cd068b83d80955cd9cb3011ebd42a3a95943a9e0",
    "filename": "core/src/main/scala/kafka/server/ReplicationQuotaManager.scala",
    "status": "modified",
    "additions": 5,
    "deletions": 4,
    "changes": 9,
    "blob_url": "https://github.com/apache/kafka/blob/603da4d86b155321aa18cc2cda1b8659ea31bc2d/core/src/main/scala/kafka/server/ReplicationQuotaManager.scala",
    "raw_url": "https://github.com/apache/kafka/raw/603da4d86b155321aa18cc2cda1b8659ea31bc2d/core/src/main/scala/kafka/server/ReplicationQuotaManager.scala",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/server/ReplicationQuotaManager.scala?ref=603da4d86b155321aa18cc2cda1b8659ea31bc2d",
    "patch": "@@ -54,7 +54,8 @@ object ReplicationQuotaManagerConfig {\n \n trait ReplicaQuota {\n   def record(value: Long): Unit\n-  def isThrottled(topicPartition: TopicPartition): Boolean\n+  def isThrottled(topicName: String, partition: Int): Boolean\n+  def isThrottled(topicPartition: TopicPartition): Boolean = isThrottled(topicPartition.topic, topicPartition.partition)\n   def isQuotaExceeded: Boolean\n }\n \n@@ -120,10 +121,10 @@ class ReplicationQuotaManager(val config: ReplicationQuotaManagerConfig,\n     * @param topicPartition the partition to check\n     * @return\n     */\n-  override def isThrottled(topicPartition: TopicPartition): Boolean = {\n-    val partitions = throttledPartitions.get(topicPartition.topic)\n+  override def isThrottled(topicName: String, partition: Int): Boolean = {\n+    val partitions = throttledPartitions.get(topicName)\n     if (partitions != null)\n-      (partitions eq AllReplicas) || partitions.contains(topicPartition.partition)\n+      (partitions eq AllReplicas) || partitions.contains(partition)\n     else false\n   }\n "
  },
  {
    "sha": "7579941a4c1a63ce655b97d5efd0784ddfe6ff56",
    "filename": "core/src/test/scala/unit/kafka/server/FetchRequestDownConversionConfigTest.scala",
    "status": "modified",
    "additions": 4,
    "deletions": 1,
    "changes": 5,
    "blob_url": "https://github.com/apache/kafka/blob/603da4d86b155321aa18cc2cda1b8659ea31bc2d/core/src/test/scala/unit/kafka/server/FetchRequestDownConversionConfigTest.scala",
    "raw_url": "https://github.com/apache/kafka/raw/603da4d86b155321aa18cc2cda1b8659ea31bc2d/core/src/test/scala/unit/kafka/server/FetchRequestDownConversionConfigTest.scala",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/scala/unit/kafka/server/FetchRequestDownConversionConfigTest.scala?ref=603da4d86b155321aa18cc2cda1b8659ea31bc2d",
    "patch": "@@ -93,7 +93,10 @@ class FetchRequestDownConversionConfigTest extends BaseRequestTest {\n     val fetchRequest = FetchRequest.Builder.forConsumer(Int.MaxValue, 0, createPartitionMap(1024,\n       topicPartitions)).build(1)\n     val fetchResponse = sendFetchRequest(topicMap.head._2, fetchRequest)\n-    topicPartitions.foreach(tp => assertEquals(Errors.UNSUPPORTED_VERSION, Errors.forCode(fetchResponse.responseData.get(tp).errorCode)))\n+    topicPartitions.foreach { tp =>\n+      assertNotNull(fetchResponse.responseData.get(tp))\n+      assertEquals(Errors.UNSUPPORTED_VERSION, Errors.forCode(fetchResponse.responseData.get(tp).errorCode))\n+    }\n   }\n \n   /**"
  },
  {
    "sha": "8b32669f43bdf4cc47a8d09948225a30284fa9bd",
    "filename": "core/src/test/scala/unit/kafka/server/FetchSessionTest.scala",
    "status": "modified",
    "additions": 257,
    "deletions": 179,
    "changes": 436,
    "blob_url": "https://github.com/apache/kafka/blob/603da4d86b155321aa18cc2cda1b8659ea31bc2d/core/src/test/scala/unit/kafka/server/FetchSessionTest.scala",
    "raw_url": "https://github.com/apache/kafka/raw/603da4d86b155321aa18cc2cda1b8659ea31bc2d/core/src/test/scala/unit/kafka/server/FetchSessionTest.scala",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/scala/unit/kafka/server/FetchSessionTest.scala?ref=603da4d86b155321aa18cc2cda1b8659ea31bc2d",
    "patch": "@@ -155,22 +155,31 @@ class FetchSessionTest {\n     assertEquals(Optional.of(1), epochs1(tp1))\n     assertEquals(Optional.of(2), epochs1(tp2))\n \n-    val response = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n-    response.put(tp0, new FetchResponseData.PartitionData()\n-      .setPartitionIndex(tp0.partition)\n-      .setHighWatermark(100)\n-      .setLastStableOffset(100)\n-      .setLogStartOffset(100))\n-    response.put(tp1, new FetchResponseData.PartitionData()\n-      .setPartitionIndex(tp1.partition)\n-      .setHighWatermark(10)\n-      .setLastStableOffset(10)\n-      .setLogStartOffset(10))\n-    response.put(tp2, new FetchResponseData.PartitionData()\n-      .setPartitionIndex(tp2.partition)\n-      .setHighWatermark(5)\n-      .setLastStableOffset(5)\n-      .setLogStartOffset(5))\n+    val response = new util.LinkedList[FetchResponseData.FetchableTopicResponse]\n+    response.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(tp0.topic)\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+        new FetchResponseData.PartitionData()\n+          .setPartitionIndex(tp0.partition)\n+          .setHighWatermark(100)\n+          .setLastStableOffset(100)\n+          .setLogStartOffset(100)))))\n+    response.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(tp1.topic)\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+        new FetchResponseData.PartitionData()\n+          .setPartitionIndex(tp1.partition)\n+          .setHighWatermark(10)\n+          .setLastStableOffset(10)\n+          .setLogStartOffset(10)))))\n+    response.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(tp2.topic)\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+        new FetchResponseData.PartitionData()\n+          .setPartitionIndex(tp2.partition)\n+          .setHighWatermark(5)\n+          .setLastStableOffset(5)\n+          .setLogStartOffset(5)))))\n \n     val sessionId = context1.updateAndGenerateResponseData(response).sessionId()\n \n@@ -229,22 +238,31 @@ class FetchSessionTest {\n     assertEquals(Map(tp0 -> Optional.empty, tp1 -> Optional.empty, tp2 -> Optional.of(1)),\n       cachedLastFetchedEpochs(context1))\n \n-    val response = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n-    response.put(tp0, new FetchResponseData.PartitionData()\n-      .setPartitionIndex(tp0.partition)\n-      .setHighWatermark(100)\n-      .setLastStableOffset(100)\n-      .setLogStartOffset(100))\n-    response.put(tp1, new FetchResponseData.PartitionData()\n-      .setPartitionIndex(tp1.partition)\n-      .setHighWatermark(10)\n-      .setLastStableOffset(10)\n-      .setLogStartOffset(10))\n-    response.put(tp2, new FetchResponseData.PartitionData()\n-      .setPartitionIndex(tp2.partition)\n-      .setHighWatermark(5)\n-      .setLastStableOffset(5)\n-      .setLogStartOffset(5))\n+    val response = new util.ArrayList[FetchResponseData.FetchableTopicResponse]()\n+    response.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(tp0.topic)\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+        new FetchResponseData.PartitionData()\n+          .setPartitionIndex(tp0.partition)\n+          .setHighWatermark(100)\n+          .setLastStableOffset(100)\n+          .setLogStartOffset(100)))))\n+    response.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(tp1.topic)\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+        new FetchResponseData.PartitionData()\n+          .setPartitionIndex(tp1.partition)\n+          .setHighWatermark(10)\n+          .setLastStableOffset(10)\n+          .setLogStartOffset(10)))))\n+    response.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(tp2.topic)\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+        new FetchResponseData.PartitionData()\n+          .setPartitionIndex(tp2.partition)\n+          .setHighWatermark(5)\n+          .setLastStableOffset(5)\n+          .setLogStartOffset(5)))))\n \n     val sessionId = context1.updateAndGenerateResponseData(response).sessionId()\n \n@@ -296,23 +314,27 @@ class FetchSessionTest {\n     })\n     assertEquals(0, context2.getFetchOffset(new TopicPartition(\"foo\", 0)).get)\n     assertEquals(10, context2.getFetchOffset(new TopicPartition(\"foo\", 1)).get)\n-    val respData2 = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n-    respData2.put(new TopicPartition(\"foo\", 0),\n-      new FetchResponseData.PartitionData()\n-        .setPartitionIndex(0)\n-        .setHighWatermark(100)\n-        .setLastStableOffset(100)\n-        .setLogStartOffset(100))\n-    respData2.put(new TopicPartition(\"foo\", 1),\n-      new FetchResponseData.PartitionData()\n-        .setPartitionIndex(1)\n-        .setHighWatermark(10)\n-        .setLastStableOffset(10)\n-        .setLogStartOffset(10))\n+    val respData2 = new util.ArrayList[FetchResponseData.FetchableTopicResponse]()\n+    respData2.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(\"foo\")\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+        new FetchResponseData.PartitionData()\n+          .setPartitionIndex(0)\n+          .setHighWatermark(100)\n+          .setLastStableOffset(100)\n+          .setLogStartOffset(100)))))\n+    respData2.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(\"foo\")\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+        new FetchResponseData.PartitionData()\n+          .setPartitionIndex(1)\n+          .setHighWatermark(10)\n+          .setLastStableOffset(10)\n+          .setLogStartOffset(10)))))\n     val resp2 = context2.updateAndGenerateResponseData(respData2)\n     assertEquals(Errors.NONE, resp2.error())\n     assertTrue(resp2.sessionId() != INVALID_SESSION_ID)\n-    assertEquals(respData2, resp2.responseData)\n+    assertEquals(respData2, resp2.data().responses())\n \n     // Test trying to create a new session with an invalid epoch\n     val context3 = fetchManager.newContext(\n@@ -374,19 +396,23 @@ class FetchSessionTest {\n         new JFetchMetadata(prevSessionId, FINAL_EPOCH), reqData8, EMPTY_PART_LIST, false)\n       assertEquals(classOf[SessionlessFetchContext], context8.getClass)\n       assertEquals(0, cache.size)\n-      val respData8 = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n-      respData8.put(new TopicPartition(\"bar\", 0),\n-        new FetchResponseData.PartitionData()\n-          .setPartitionIndex(0)\n-          .setHighWatermark(100)\n-          .setLastStableOffset(100)\n-          .setLogStartOffset(100))\n-      respData8.put(new TopicPartition(\"bar\", 1),\n-        new FetchResponseData.PartitionData()\n-          .setPartitionIndex(1)\n-          .setHighWatermark(100)\n-          .setLastStableOffset(100)\n-          .setLogStartOffset(100))\n+      val respData8 = new util.ArrayList[FetchResponseData.FetchableTopicResponse]()\n+      respData8.add(new FetchResponseData.FetchableTopicResponse()\n+        .setTopic(\"bar\")\n+        .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+          new FetchResponseData.PartitionData()\n+            .setPartitionIndex(0)\n+            .setHighWatermark(100)\n+            .setLastStableOffset(100)\n+            .setLogStartOffset(100)))))\n+      respData8.add(new FetchResponseData.FetchableTopicResponse()\n+        .setTopic(\"bar\")\n+        .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+          new FetchResponseData.PartitionData()\n+            .setPartitionIndex(1)\n+            .setHighWatermark(100)\n+            .setLastStableOffset(100)\n+            .setLogStartOffset(100)))))\n       val resp8 = context8.updateAndGenerateResponseData(respData8)\n       assertEquals(Errors.NONE, resp8.error)\n       nextSessionId = resp8.sessionId\n@@ -407,17 +433,23 @@ class FetchSessionTest {\n       Optional.empty()))\n     val context1 = fetchManager.newContext(JFetchMetadata.INITIAL, reqData1, EMPTY_PART_LIST, false)\n     assertEquals(classOf[FullFetchContext], context1.getClass)\n-    val respData1 = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n-    respData1.put(new TopicPartition(\"foo\", 0), new FetchResponseData.PartitionData()\n-        .setPartitionIndex(0)\n-        .setHighWatermark(100)\n-        .setLastStableOffset(100)\n-        .setLogStartOffset(100))\n-    respData1.put(new TopicPartition(\"foo\", 1), new FetchResponseData.PartitionData()\n-        .setPartitionIndex(1)\n-        .setHighWatermark(10)\n-        .setLastStableOffset(10)\n-        .setLogStartOffset(10))\n+    val respData1 = new util.ArrayList[FetchResponseData.FetchableTopicResponse]()\n+    respData1.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(\"foo\")\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+        new FetchResponseData.PartitionData()\n+          .setPartitionIndex(0)\n+          .setHighWatermark(100)\n+          .setLastStableOffset(100)\n+          .setLogStartOffset(100)))))\n+    respData1.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(\"foo\")\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+        new FetchResponseData.PartitionData()\n+          .setPartitionIndex(1)\n+          .setHighWatermark(10)\n+          .setLastStableOffset(10)\n+          .setLogStartOffset(10)))))\n     val resp1 = context1.updateAndGenerateResponseData(respData1)\n     assertEquals(Errors.NONE, resp1.error())\n     assertTrue(resp1.sessionId() != INVALID_SESSION_ID)\n@@ -441,19 +473,23 @@ class FetchSessionTest {\n     assertEquals(10, context2.getFetchOffset(new TopicPartition(\"foo\", 1)).get)\n     assertEquals(15, context2.getFetchOffset(new TopicPartition(\"bar\", 0)).get)\n     assertEquals(None, context2.getFetchOffset(new TopicPartition(\"bar\", 2)))\n-    val respData2 = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n-    respData2.put(new TopicPartition(\"foo\", 1),\n-      new FetchResponseData.PartitionData()\n-        .setPartitionIndex(1)\n-        .setHighWatermark(10)\n-        .setLastStableOffset(10)\n-        .setLogStartOffset(10))\n-    respData2.put(new TopicPartition(\"bar\", 0),\n-      new FetchResponseData.PartitionData()\n-        .setPartitionIndex(0)\n-        .setHighWatermark(10)\n-        .setLastStableOffset(10)\n-        .setLogStartOffset(10))\n+    val respData2 = new util.ArrayList[FetchResponseData.FetchableTopicResponse]()\n+    respData2.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(\"foo\")\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+        new FetchResponseData.PartitionData()\n+          .setPartitionIndex(1)\n+          .setHighWatermark(10)\n+          .setLastStableOffset(10)\n+          .setLogStartOffset(10)))))\n+    respData2.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(\"bar\")\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+        new FetchResponseData.PartitionData()\n+          .setPartitionIndex(0)\n+          .setHighWatermark(10)\n+          .setLastStableOffset(10)\n+          .setLogStartOffset(10)))))\n     val resp2 = context2.updateAndGenerateResponseData(respData2)\n     assertEquals(Errors.NONE, resp2.error)\n     assertEquals(1, resp2.responseData.size)\n@@ -475,17 +511,23 @@ class FetchSessionTest {\n       Optional.empty()))\n     val session1context1 = fetchManager.newContext(JFetchMetadata.INITIAL, session1req, EMPTY_PART_LIST, false)\n     assertEquals(classOf[FullFetchContext], session1context1.getClass)\n-    val respData1 = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n-    respData1.put(new TopicPartition(\"foo\", 0), new FetchResponseData.PartitionData()\n-        .setPartitionIndex(0)\n-        .setHighWatermark(100)\n-        .setLastStableOffset(100)\n-        .setLogStartOffset(100))\n-    respData1.put(new TopicPartition(\"foo\", 1), new FetchResponseData.PartitionData()\n-        .setPartitionIndex(1)\n-        .setHighWatermark(10)\n-        .setLastStableOffset(10)\n-        .setLogStartOffset(10))\n+    val respData1 = new util.ArrayList[FetchResponseData.FetchableTopicResponse]()\n+    respData1.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(\"foo\")\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+        new FetchResponseData.PartitionData()\n+          .setPartitionIndex(0)\n+          .setHighWatermark(100)\n+          .setLastStableOffset(100)\n+          .setLogStartOffset(100)))))\n+    respData1.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(\"foo\")\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+        new FetchResponseData.PartitionData()\n+          .setPartitionIndex(1)\n+          .setHighWatermark(10)\n+          .setLastStableOffset(10)\n+          .setLogStartOffset(10)))))\n     val session1resp = session1context1.updateAndGenerateResponseData(respData1)\n     assertEquals(Errors.NONE, session1resp.error())\n     assertTrue(session1resp.sessionId() != INVALID_SESSION_ID)\n@@ -545,18 +587,23 @@ class FetchSessionTest {\n       Optional.empty()))\n     val session3context = fetchManager.newContext(JFetchMetadata.INITIAL, session3req, EMPTY_PART_LIST, false)\n     assertEquals(classOf[FullFetchContext], session3context.getClass)\n-    val respData3 = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n-    respData3.put(new TopicPartition(\"foo\", 0), new FetchResponseData.PartitionData()\n-      .setPartitionIndex(0)\n-      .setHighWatermark(100)\n-      .setLastStableOffset(100)\n-      .setLogStartOffset(100))\n-    respData3.put(new TopicPartition(\"foo\", 1),\n-      new FetchResponseData.PartitionData()\n-        .setPartitionIndex(1)\n-        .setHighWatermark(10)\n-        .setLastStableOffset(10)\n-        .setLogStartOffset(10))\n+    val respData3 = new util.ArrayList[FetchResponseData.FetchableTopicResponse]()\n+    respData3.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(\"foo\")\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+        new FetchResponseData.PartitionData()\n+          .setPartitionIndex(0)\n+          .setHighWatermark(100)\n+          .setLastStableOffset(100)\n+          .setLogStartOffset(100)))))\n+    respData3.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(\"foo\")\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+        new FetchResponseData.PartitionData()\n+          .setPartitionIndex(1)\n+          .setHighWatermark(10)\n+          .setLastStableOffset(10)\n+          .setLogStartOffset(10)))))\n     val session3resp = session3context.updateAndGenerateResponseData(respData3)\n     assertEquals(Errors.NONE, session3resp.error())\n     assertTrue(session3resp.sessionId() != INVALID_SESSION_ID)\n@@ -582,17 +629,23 @@ class FetchSessionTest {\n       Optional.empty()))\n     val session1context = fetchManager.newContext(JFetchMetadata.INITIAL, session1req, EMPTY_PART_LIST, true)\n     assertEquals(classOf[FullFetchContext], session1context.getClass)\n-    val respData1 = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n-    respData1.put(new TopicPartition(\"foo\", 0), new FetchResponseData.PartitionData()\n-        .setPartitionIndex(0)\n-        .setHighWatermark(100)\n-        .setLastStableOffset(100)\n-        .setLogStartOffset(100))\n-    respData1.put(new TopicPartition(\"foo\", 1), new FetchResponseData.PartitionData()\n-        .setPartitionIndex(1)\n-        .setHighWatermark(10)\n-        .setLastStableOffset(10)\n-        .setLogStartOffset(10))\n+    val respData1 = new util.ArrayList[FetchResponseData.FetchableTopicResponse]()\n+    respData1.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(\"foo\")\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+        new FetchResponseData.PartitionData()\n+          .setPartitionIndex(0)\n+          .setHighWatermark(100)\n+          .setLastStableOffset(100)\n+          .setLogStartOffset(100)))))\n+    respData1.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(\"foo\")\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+        new FetchResponseData.PartitionData()\n+          .setPartitionIndex(1)\n+          .setHighWatermark(10)\n+          .setLastStableOffset(10)\n+          .setLogStartOffset(10)))))\n     val session1resp = session1context.updateAndGenerateResponseData(respData1)\n     assertEquals(Errors.NONE, session1resp.error())\n     assertTrue(session1resp.sessionId() != INVALID_SESSION_ID)\n@@ -642,19 +695,23 @@ class FetchSessionTest {\n       Optional.empty()))\n     val session3context = fetchManager.newContext(JFetchMetadata.INITIAL, session3req, EMPTY_PART_LIST, true)\n     assertEquals(classOf[FullFetchContext], session3context.getClass)\n-    val respData3 = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n-    respData3.put(new TopicPartition(\"foo\", 0),\n-      new FetchResponseData.PartitionData()\n-        .setPartitionIndex(0)\n-        .setHighWatermark(100)\n-        .setLastStableOffset(100)\n-        .setLogStartOffset(100))\n-    respData3.put(new TopicPartition(\"foo\", 1),\n-      new FetchResponseData.PartitionData()\n-        .setPartitionIndex(1)\n-        .setHighWatermark(10)\n-        .setLastStableOffset(10)\n-        .setLogStartOffset(10))\n+    val respData3 = new util.ArrayList[FetchResponseData.FetchableTopicResponse]()\n+    respData3.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(\"foo\")\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+        new FetchResponseData.PartitionData()\n+          .setPartitionIndex(0)\n+          .setHighWatermark(100)\n+          .setLastStableOffset(100)\n+          .setLogStartOffset(100)))))\n+    respData3.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(\"foo\")\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+        new FetchResponseData.PartitionData()\n+          .setPartitionIndex(1)\n+          .setHighWatermark(10)\n+          .setLastStableOffset(10)\n+          .setLogStartOffset(10)))))\n     val session3resp = session3context.updateAndGenerateResponseData(respData3)\n     assertEquals(Errors.NONE, session3resp.error())\n     assertTrue(session3resp.sessionId() != INVALID_SESSION_ID)\n@@ -677,19 +734,23 @@ class FetchSessionTest {\n       Optional.empty()))\n     val session4context = fetchManager.newContext(JFetchMetadata.INITIAL, session4req, EMPTY_PART_LIST, true)\n     assertEquals(classOf[FullFetchContext], session4context.getClass)\n-    val respData4 = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n-    respData4.put(new TopicPartition(\"foo\", 0),\n-      new FetchResponseData.PartitionData()\n-        .setPartitionIndex(0)\n-        .setHighWatermark(100)\n-        .setLastStableOffset(100)\n-        .setLogStartOffset(100))\n-    respData4.put(new TopicPartition(\"foo\", 1),\n-      new FetchResponseData.PartitionData()\n-        .setPartitionIndex(1)\n-        .setHighWatermark(10)\n-        .setLastStableOffset(10)\n-        .setLogStartOffset(10))\n+    val respData4 = new util.ArrayList[FetchResponseData.FetchableTopicResponse]()\n+    respData4.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(\"foo\")\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+        new FetchResponseData.PartitionData()\n+          .setPartitionIndex(0)\n+          .setHighWatermark(100)\n+          .setLastStableOffset(100)\n+          .setLogStartOffset(100)))))\n+    respData4.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(\"foo\")\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+        new FetchResponseData.PartitionData()\n+          .setPartitionIndex(1)\n+          .setHighWatermark(10)\n+          .setLastStableOffset(10)\n+          .setLogStartOffset(10)))))\n     val session4resp = session3context.updateAndGenerateResponseData(respData4)\n     assertEquals(Errors.NONE, session4resp.error())\n     assertTrue(session4resp.sessionId() != INVALID_SESSION_ID)\n@@ -715,17 +776,23 @@ class FetchSessionTest {\n       Optional.empty()))\n     val context1 = fetchManager.newContext(JFetchMetadata.INITIAL, reqData1, EMPTY_PART_LIST, false)\n     assertEquals(classOf[FullFetchContext], context1.getClass)\n-    val respData1 = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n-    respData1.put(new TopicPartition(\"foo\", 0), new FetchResponseData.PartitionData()\n-        .setPartitionIndex(0)\n-        .setHighWatermark(100)\n-        .setLastStableOffset(100)\n-        .setLogStartOffset(100))\n-    respData1.put(new TopicPartition(\"foo\", 1), new FetchResponseData.PartitionData()\n-        .setPartitionIndex(1)\n-        .setHighWatermark(10)\n-        .setLastStableOffset(10)\n-        .setLogStartOffset(10))\n+    val respData1 = new util.ArrayList[FetchResponseData.FetchableTopicResponse]()\n+    respData1.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(\"foo\")\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+        new FetchResponseData.PartitionData()\n+          .setPartitionIndex(0)\n+          .setHighWatermark(100)\n+          .setLastStableOffset(100)\n+          .setLogStartOffset(100)))))\n+    respData1.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(\"foo\")\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+        new FetchResponseData.PartitionData()\n+          .setPartitionIndex(1)\n+          .setHighWatermark(10)\n+          .setLastStableOffset(10)\n+          .setLogStartOffset(10)))))\n     val resp1 = context1.updateAndGenerateResponseData(respData1)\n     assertEquals(Errors.NONE, resp1.error)\n     assertTrue(resp1.sessionId() != INVALID_SESSION_ID)\n@@ -740,8 +807,7 @@ class FetchSessionTest {\n     val context2 = fetchManager.newContext(\n       new JFetchMetadata(resp1.sessionId, 1), reqData2, removed2, false)\n     assertEquals(classOf[SessionlessFetchContext], context2.getClass)\n-    val respData2 = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n-    val resp2 = context2.updateAndGenerateResponseData(respData2)\n+    val resp2 = context2.updateAndGenerateResponseData(new util.ArrayList[FetchResponseData.FetchableTopicResponse]())\n     assertEquals(INVALID_SESSION_ID, resp2.sessionId)\n     assertTrue(resp2.responseData.isEmpty)\n     assertEquals(0, cache.size)\n@@ -762,19 +828,25 @@ class FetchSessionTest {\n     // Full fetch context returns all partitions in the response\n     val context1 = fetchManager.newContext(JFetchMetadata.INITIAL, reqData, EMPTY_PART_LIST, isFollower = false)\n     assertEquals(classOf[FullFetchContext], context1.getClass)\n-    val respData = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n-    respData.put(tp1, new FetchResponseData.PartitionData()\n-        .setPartitionIndex(tp1.partition)\n-        .setHighWatermark(105)\n-        .setLastStableOffset(105)\n-        .setLogStartOffset(0))\n+    val respData = new util.ArrayList[FetchResponseData.FetchableTopicResponse]()\n+    respData.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(tp1.topic)\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+          new FetchResponseData.PartitionData()\n+          .setPartitionIndex(tp1.partition)\n+          .setHighWatermark(105)\n+          .setLastStableOffset(105)\n+          .setLogStartOffset(0)))))\n     val divergingEpoch = new FetchResponseData.EpochEndOffset().setEpoch(3).setEndOffset(90)\n-    respData.put(tp2, new FetchResponseData.PartitionData()\n-        .setPartitionIndex(tp2.partition)\n-        .setHighWatermark(105)\n-        .setLastStableOffset(105)\n-        .setLogStartOffset(0)\n-        .setDivergingEpoch(divergingEpoch))\n+    respData.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(tp2.topic)\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+        new FetchResponseData.PartitionData()\n+          .setPartitionIndex(tp2.partition)\n+          .setHighWatermark(105)\n+          .setLastStableOffset(105)\n+          .setLogStartOffset(0)\n+          .setDivergingEpoch(divergingEpoch)))))\n     val resp1 = context1.updateAndGenerateResponseData(respData)\n     assertEquals(Errors.NONE, resp1.error)\n     assertNotEquals(INVALID_SESSION_ID, resp1.sessionId)\n@@ -790,24 +862,30 @@ class FetchSessionTest {\n     assertEquals(Collections.singleton(tp2), resp2.responseData.keySet)\n \n     // All partitions with divergent epoch should be returned.\n-    respData.put(tp1, new FetchResponseData.PartitionData()\n-      .setPartitionIndex(tp1.partition)\n-      .setHighWatermark(105)\n-      .setLastStableOffset(105)\n-      .setLogStartOffset(0)\n-      .setDivergingEpoch(divergingEpoch))\n+    respData.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(tp1.topic)\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+        new FetchResponseData.PartitionData()\n+          .setPartitionIndex(tp1.partition)\n+          .setHighWatermark(105)\n+          .setLastStableOffset(105)\n+          .setLogStartOffset(0)\n+          .setDivergingEpoch(divergingEpoch)))))\n     val resp3 = context2.updateAndGenerateResponseData(respData)\n     assertEquals(Errors.NONE, resp3.error)\n     assertEquals(resp1.sessionId, resp3.sessionId)\n     assertEquals(Utils.mkSet(tp1, tp2), resp3.responseData.keySet)\n \n     // Partitions that meet other conditions should be returned regardless of whether\n     // divergingEpoch is set or not.\n-    respData.put(tp1, new FetchResponseData.PartitionData()\n-      .setPartitionIndex(tp1.partition)\n-      .setHighWatermark(110)\n-      .setLastStableOffset(110)\n-      .setLogStartOffset(0))\n+    respData.add(new FetchResponseData.FetchableTopicResponse()\n+      .setTopic(tp1.topic)\n+      .setPartitions(new util.LinkedList(util.Collections.singletonList(\n+        new FetchResponseData.PartitionData()\n+          .setPartitionIndex(tp1.partition)\n+          .setHighWatermark(110)\n+          .setLastStableOffset(110)\n+          .setLogStartOffset(0)))))\n     val resp4 = context2.updateAndGenerateResponseData(respData)\n     assertEquals(Errors.NONE, resp4.error)\n     assertEquals(resp1.sessionId, resp4.sessionId)"
  },
  {
    "sha": "64bf4171f498802543c6952aea7cb1826f43d1eb",
    "filename": "core/src/test/scala/unit/kafka/server/KafkaApisTest.scala",
    "status": "modified",
    "additions": 24,
    "deletions": 17,
    "changes": 41,
    "blob_url": "https://github.com/apache/kafka/blob/603da4d86b155321aa18cc2cda1b8659ea31bc2d/core/src/test/scala/unit/kafka/server/KafkaApisTest.scala",
    "raw_url": "https://github.com/apache/kafka/raw/603da4d86b155321aa18cc2cda1b8659ea31bc2d/core/src/test/scala/unit/kafka/server/KafkaApisTest.scala",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/scala/unit/kafka/server/KafkaApisTest.scala?ref=603da4d86b155321aa18cc2cda1b8659ea31bc2d",
    "patch": "@@ -23,7 +23,6 @@ import java.util\n import java.util.Arrays.asList\n import java.util.concurrent.TimeUnit\n import java.util.{Collections, Optional, Properties, Random}\n-\n import kafka.api.{ApiVersion, KAFKA_0_10_2_IV0, KAFKA_2_2_IV1, LeaderAndIsr}\n import kafka.cluster.{Broker, Partition}\n import kafka.controller.{ControllerContext, KafkaController}\n@@ -3245,31 +3244,39 @@ class KafkaApisTest {\n \n   @Test\n   def testSizeOfThrottledPartitions(): Unit = {\n-\n     def fetchResponse(data: Map[TopicPartition, String]): FetchResponse = {\n-      val responseData = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData](\n-        data.map { case (tp, raw) =>\n-          tp -> new FetchResponseData.PartitionData()\n-            .setPartitionIndex(tp.partition)\n-            .setHighWatermark(105)\n-            .setLastStableOffset(105)\n-            .setLogStartOffset(0)\n-            .setRecords(MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(100, raw.getBytes(StandardCharsets.UTF_8))))\n-      }.toMap.asJava)\n-      FetchResponse.of(Errors.NONE, 100, 100, responseData)\n+      val topicResponses = data.map { case (tp, raw) =>\n+        tp -> new FetchResponseData.PartitionData()\n+          .setPartitionIndex(tp.partition)\n+          .setHighWatermark(105)\n+          .setLastStableOffset(105)\n+          .setLogStartOffset(0)\n+          .setRecords(MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(100, raw.getBytes(StandardCharsets.UTF_8))))\n+      }.groupBy[String](_._1.topic)\n+        .map(entry => new FetchResponseData.FetchableTopicResponse()\n+          .setTopic(entry._1)\n+          .setPartitions(entry._2.values.toSeq.asJava))\n+        .toSeq.asJava\n+\n+      new FetchResponse(new FetchResponseData()\n+        .setThrottleTimeMs(100)\n+        .setErrorCode(Errors.NONE.code)\n+        .setSessionId(100)\n+        .setResponses(topicResponses))\n     }\n \n     val throttledPartition = new TopicPartition(\"throttledData\", 0)\n     val throttledData = Map(throttledPartition -> \"throttledData\")\n-    val expectedSize = FetchResponse.sizeOf(FetchResponseData.HIGHEST_SUPPORTED_VERSION,\n-      fetchResponse(throttledData).responseData.entrySet.iterator)\n+    val expectedSize = FetchResponse.sizeOf(FetchResponseData.HIGHEST_SUPPORTED_VERSION, fetchResponse(throttledData).data.responses)\n \n     val response = fetchResponse(throttledData ++ Map(new TopicPartition(\"nonThrottledData\", 0) -> \"nonThrottledData\"))\n \n     val quota = Mockito.mock(classOf[ReplicationQuotaManager])\n-    Mockito.when(quota.isThrottled(ArgumentMatchers.any(classOf[TopicPartition])))\n-      .thenAnswer(invocation => throttledPartition == invocation.getArgument(0).asInstanceOf[TopicPartition])\n-\n+    Mockito.when(quota.isThrottled(ArgumentMatchers.any(classOf[String]), ArgumentMatchers.any(classOf[Int])))\n+      .thenAnswer { invocation =>\n+        throttledPartition.topic == invocation.getArgument(0).asInstanceOf[String] &&\n+          throttledPartition.partition == invocation.getArgument(1).asInstanceOf[Int]\n+      }\n     assertEquals(expectedSize, KafkaApis.sizeOfThrottledPartitions(FetchResponseData.HIGHEST_SUPPORTED_VERSION, response, quota))\n   }\n "
  },
  {
    "sha": "d21665af9b6a79fdfeedebc5cc30a680876f2b33",
    "filename": "core/src/test/scala/unit/kafka/server/epoch/util/ReplicaFetcherMockBlockingSend.scala",
    "status": "modified",
    "additions": 11,
    "deletions": 4,
    "changes": 15,
    "blob_url": "https://github.com/apache/kafka/blob/603da4d86b155321aa18cc2cda1b8659ea31bc2d/core/src/test/scala/unit/kafka/server/epoch/util/ReplicaFetcherMockBlockingSend.scala",
    "raw_url": "https://github.com/apache/kafka/raw/603da4d86b155321aa18cc2cda1b8659ea31bc2d/core/src/test/scala/unit/kafka/server/epoch/util/ReplicaFetcherMockBlockingSend.scala",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/scala/unit/kafka/server/epoch/util/ReplicaFetcherMockBlockingSend.scala?ref=603da4d86b155321aa18cc2cda1b8659ea31bc2d",
    "patch": "@@ -30,6 +30,7 @@ import org.apache.kafka.common.utils.{SystemTime, Time}\n import org.apache.kafka.common.{Node, TopicPartition}\n \n import scala.collection.Map\n+import scala.jdk.CollectionConverters._\n \n /**\n   * Stub network client used for testing the ReplicaFetcher, wraps the MockClient used for consumer testing\n@@ -95,11 +96,17 @@ class ReplicaFetcherMockBlockingSend(offsets: java.util.Map[TopicPartition, Epoc\n \n       case ApiKeys.FETCH =>\n         fetchCount += 1\n-        val partitionData = new util.LinkedHashMap[TopicPartition, FetchResponseData.PartitionData]\n-        fetchPartitionData.foreach { case (tp, data) => partitionData.put(tp, data) }\n+        val topicResponses = fetchPartitionData.groupBy[String](_._1.topic)\n+          .map(entry => new FetchResponseData.FetchableTopicResponse()\n+            .setTopic(entry._1)\n+            .setPartitions(entry._2.values.toSeq.asJava))\n+          .toSeq.asJava\n         fetchPartitionData = Map.empty\n-        FetchResponse.of(Errors.NONE, 0,\n-          if (partitionData.isEmpty) JFetchMetadata.INVALID_SESSION_ID else 1, partitionData)\n+        new FetchResponse(new FetchResponseData()\n+          .setThrottleTimeMs(0)\n+          .setErrorCode(Errors.NONE.code)\n+          .setSessionId(if (topicResponses.isEmpty) JFetchMetadata.INVALID_SESSION_ID else 1)\n+          .setResponses(topicResponses))\n \n       case _ =>\n         throw new UnsupportedOperationException"
  },
  {
    "sha": "476d8f01a951510baeb74004a0003a32d2e2eb0d",
    "filename": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/common/FetchResponseBenchmark.java",
    "status": "modified",
    "additions": 18,
    "deletions": 18,
    "changes": 36,
    "blob_url": "https://github.com/apache/kafka/blob/603da4d86b155321aa18cc2cda1b8659ea31bc2d/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/common/FetchResponseBenchmark.java",
    "raw_url": "https://github.com/apache/kafka/raw/603da4d86b155321aa18cc2cda1b8659ea31bc2d/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/common/FetchResponseBenchmark.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/common/FetchResponseBenchmark.java?ref=603da4d86b155321aa18cc2cda1b8659ea31bc2d",
    "patch": "@@ -17,7 +17,6 @@\n \n package org.apache.kafka.jmh.common;\n \n-import org.apache.kafka.common.TopicPartition;\n import org.apache.kafka.common.message.FetchResponseData;\n import org.apache.kafka.common.network.Send;\n import org.apache.kafka.common.protocol.ApiKeys;\n@@ -43,7 +42,8 @@\n \n import java.io.IOException;\n import java.nio.charset.StandardCharsets;\n-import java.util.LinkedHashMap;\n+import java.util.ArrayList;\n+import java.util.List;\n import java.util.UUID;\n import java.util.concurrent.TimeUnit;\n \n@@ -60,7 +60,7 @@\n     @Param({\"3\", \"10\", \"20\"})\n     private int partitionCount;\n \n-    LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseData;\n+    private List<FetchResponseData.FetchableTopicResponse> topicResponses = new ArrayList<>();\n \n     ResponseHeader header;\n \n@@ -73,27 +73,27 @@ public void setup() {\n                 new SimpleRecord(1001, \"key2\".getBytes(StandardCharsets.UTF_8), \"value2\".getBytes(StandardCharsets.UTF_8)),\n                 new SimpleRecord(1002, \"key3\".getBytes(StandardCharsets.UTF_8), \"value3\".getBytes(StandardCharsets.UTF_8)));\n \n-        this.responseData = new LinkedHashMap<>();\n         for (int topicIdx = 0; topicIdx < topicCount; topicIdx++) {\n-            String topic = UUID.randomUUID().toString();\n+            List<FetchResponseData.PartitionData> partitionResponses = new ArrayList<>();\n             for (int partitionId = 0; partitionId < partitionCount; partitionId++) {\n-                FetchResponseData.PartitionData partitionData = new FetchResponseData.PartitionData()\n-                                .setPartitionIndex(partitionId)\n-                                .setLastStableOffset(0)\n-                                .setLogStartOffset(0)\n-                                .setRecords(records);\n-                responseData.put(new TopicPartition(topic, partitionId), partitionData);\n+                partitionResponses.add(new FetchResponseData.PartitionData()\n+                        .setPartitionIndex(partitionId)\n+                        .setLastStableOffset(0)\n+                        .setLogStartOffset(0)\n+                        .setRecords(records));\n             }\n+            String topic = UUID.randomUUID().toString();\n+            topicResponses.add(new FetchResponseData.FetchableTopicResponse()\n+                    .setTopic(topic)\n+                    .setPartitions(partitionResponses));\n         }\n \n         this.header = new ResponseHeader(100, ApiKeys.FETCH.responseHeaderVersion(ApiKeys.FETCH.latestVersion()));\n-        this.fetchResponse = FetchResponse.of(Errors.NONE, 0, 0, responseData);\n-    }\n-\n-    @Benchmark\n-    public int testConstructFetchResponse() {\n-        FetchResponse fetchResponse = FetchResponse.of(Errors.NONE, 0, 0, responseData);\n-        return fetchResponse.responseData().size();\n+        this.fetchResponse = new FetchResponse(new FetchResponseData()\n+            .setErrorCode(Errors.NONE.code())\n+            .setThrottleTimeMs(0)\n+            .setSessionId(0)\n+            .setResponses(topicResponses));\n     }\n \n     @Benchmark"
  },
  {
    "sha": "d569111d4cd4da5b488d1b539e6f2e66a5fc82fc",
    "filename": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetcher/ReplicaFetcherThreadBenchmark.java",
    "status": "modified",
    "additions": 11,
    "deletions": 7,
    "changes": 18,
    "blob_url": "https://github.com/apache/kafka/blob/603da4d86b155321aa18cc2cda1b8659ea31bc2d/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetcher/ReplicaFetcherThreadBenchmark.java",
    "raw_url": "https://github.com/apache/kafka/raw/603da4d86b155321aa18cc2cda1b8659ea31bc2d/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetcher/ReplicaFetcherThreadBenchmark.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetcher/ReplicaFetcherThreadBenchmark.java?ref=603da4d86b155321aa18cc2cda1b8659ea31bc2d",
    "patch": "@@ -81,7 +81,6 @@\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.Collections;\n-import java.util.LinkedHashMap;\n import java.util.List;\n import java.util.Optional;\n import java.util.Properties;\n@@ -136,11 +135,16 @@ public void setup() throws IOException {\n                 Time.SYSTEM,\n                 true);\n \n-        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> initialFetched = new LinkedHashMap<>();\n+        FetchResponseData.FetchableTopicResponse topicResponse = new FetchResponseData.FetchableTopicResponse()\n+                .setTopic(\"topic\");\n+        FetchResponseData fetchedData = new FetchResponseData()\n+                .setErrorCode(Errors.NONE.code())\n+                .setThrottleTimeMs(0)\n+                .setSessionId(999)\n+                .setResponses(Collections.singletonList(topicResponse));\n         scala.collection.mutable.Map<TopicPartition, InitialFetchState> initialFetchStates = new scala.collection.mutable.HashMap<>();\n         for (int i = 0; i < partitionCount; i++) {\n-            TopicPartition tp = new TopicPartition(\"topic\", i);\n-\n+            TopicPartition tp = new TopicPartition(topicResponse.topic(), i);\n             List<Integer> replicas = Arrays.asList(0, 1, 2);\n             LeaderAndIsrRequestData.LeaderAndIsrPartitionState partitionState = new LeaderAndIsrRequestData.LeaderAndIsrPartitionState()\n                     .setControllerEpoch(0)\n@@ -173,7 +177,7 @@ public int sizeInBytes() {\n                     return null;\n                 }\n             };\n-            initialFetched.put(tp, new FetchResponseData.PartitionData()\n+            topicResponse.partitions().add(new FetchResponseData.PartitionData()\n                     .setPartitionIndex(tp.partition())\n                     .setLastStableOffset(0)\n                     .setLogStartOffset(0)\n@@ -188,7 +192,7 @@ public int sizeInBytes() {\n         // so that we do not measure this time as part of the steady state work\n         fetcher.doWork();\n         // handle response to engage the incremental fetch session handler\n-        fetcher.fetchSessionHandler().handleResponse(FetchResponse.of(Errors.NONE, 0, 999, initialFetched));\n+        fetcher.fetchSessionHandler().handleResponse(new FetchResponse(fetchedData));\n     }\n \n     @TearDown(Level.Trial)\n@@ -259,7 +263,7 @@ public void record(long value) {\n                         }\n \n                         @Override\n-                        public boolean isThrottled(TopicPartition topicPartition) {\n+                        public boolean isThrottled(String topicName, int partition) {\n                             return false;\n                         }\n                     },"
  },
  {
    "sha": "bc89bd75daa0f2a0b964bc7a1235da01a26a9b75",
    "filename": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetchsession/FetchSessionBenchmark.java",
    "status": "modified",
    "additions": 14,
    "deletions": 7,
    "changes": 21,
    "blob_url": "https://github.com/apache/kafka/blob/603da4d86b155321aa18cc2cda1b8659ea31bc2d/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetchsession/FetchSessionBenchmark.java",
    "raw_url": "https://github.com/apache/kafka/raw/603da4d86b155321aa18cc2cda1b8659ea31bc2d/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetchsession/FetchSessionBenchmark.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetchsession/FetchSessionBenchmark.java?ref=603da4d86b155321aa18cc2cda1b8659ea31bc2d",
    "patch": "@@ -38,6 +38,7 @@\n import org.openjdk.jmh.annotations.Warmup;\n \n import java.util.ArrayList;\n+import java.util.Collections;\n import java.util.LinkedHashMap;\n import java.util.Map;\n import java.util.Optional;\n@@ -70,21 +71,27 @@ public void setUp() {\n         handler = new FetchSessionHandler(LOG_CONTEXT, 1);\n         FetchSessionHandler.Builder builder = handler.newBuilder();\n \n-        LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> respMap = new LinkedHashMap<>();\n+        FetchResponseData.FetchableTopicResponse topicResponses = new FetchResponseData.FetchableTopicResponse()\n+                .setTopic(\"foo\");\n+        FetchResponseData fetchedData = new FetchResponseData()\n+                .setErrorCode(Errors.NONE.code())\n+                .setThrottleTimeMs(0)\n+                .setSessionId(1)\n+                .setResponses(Collections.singletonList(topicResponses));\n         for (int i = 0; i < partitionCount; i++) {\n-            TopicPartition tp = new TopicPartition(\"foo\", i);\n+            TopicPartition tp = new TopicPartition(topicResponses.topic(), i);\n             FetchRequest.PartitionData partitionData = new FetchRequest.PartitionData(0, 0, 200,\n                     Optional.empty());\n             fetches.put(tp, partitionData);\n             builder.add(tp, partitionData);\n-            respMap.put(tp, new FetchResponseData.PartitionData()\n-                            .setPartitionIndex(tp.partition())\n-                            .setLastStableOffset(0)\n-                            .setLogStartOffset(0));\n+            topicResponses.partitions().add(new FetchResponseData.PartitionData()\n+                    .setPartitionIndex(tp.partition())\n+                    .setLastStableOffset(0)\n+                    .setLogStartOffset(0));\n         }\n         builder.build();\n         // build and handle an initial response so that the next fetch will be incremental\n-        handler.handleResponse(FetchResponse.of(Errors.NONE, 0, 1, respMap));\n+        handler.handleResponse(new FetchResponse(fetchedData));\n \n         int counter = 0;\n         for (TopicPartition topicPartition: new ArrayList<>(fetches.keySet())) {"
  },
  {
    "sha": "facacbd677cbd5151f14244da0b20cb566855f4a",
    "filename": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetchsession/IncrementalFetchContextBenchmark.java",
    "status": "added",
    "additions": 122,
    "deletions": 0,
    "changes": 122,
    "blob_url": "https://github.com/apache/kafka/blob/603da4d86b155321aa18cc2cda1b8659ea31bc2d/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetchsession/IncrementalFetchContextBenchmark.java",
    "raw_url": "https://github.com/apache/kafka/raw/603da4d86b155321aa18cc2cda1b8659ea31bc2d/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetchsession/IncrementalFetchContextBenchmark.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/jmh-benchmarks/src/main/java/org/apache/kafka/jmh/fetchsession/IncrementalFetchContextBenchmark.java?ref=603da4d86b155321aa18cc2cda1b8659ea31bc2d",
    "patch": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.jmh.fetchsession;\n+\n+import kafka.server.CachedPartition;\n+import kafka.server.FetchSession;\n+import kafka.server.IncrementalFetchContext;\n+import org.apache.kafka.common.message.FetchResponseData;\n+import org.apache.kafka.common.requests.FetchMetadata;\n+import org.apache.kafka.common.requests.FetchResponse;\n+import org.apache.kafka.common.utils.Time;\n+import org.mockito.Mockito;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Level;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+\n+import java.util.Collections;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+@State(Scope.Benchmark)\n+@Fork(value = 1)\n+@Warmup(iterations = 5)\n+@Measurement(iterations = 10)\n+@BenchmarkMode(Mode.AverageTime)\n+@OutputTimeUnit(TimeUnit.MILLISECONDS)\n+public class IncrementalFetchContextBenchmark {\n+\n+    private final List<FetchResponseData.PartitionData> partitionData = partitionData();\n+    private final IncrementalFetchContext fetchContext = incrementalFetchContext();\n+\n+    private List<FetchResponseData.FetchableTopicResponse> topicResponses;\n+\n+    // updateAndGenerateResponseData remove fields from topicResponses so we have to initialize topicResponses for each invocation\n+    @Setup(Level.Invocation)\n+    public void setup() {\n+        topicResponses = topicResponses();\n+    }\n+\n+    private List<FetchResponseData.FetchableTopicResponse> topicResponses() {\n+        List<FetchResponseData.FetchableTopicResponse> topicResponses = new LinkedList<>();\n+        topicResponses.add(new FetchResponseData.FetchableTopicResponse()\n+            .setTopic(\"foo\")\n+            .setPartitions(new LinkedList<>(partitionData)));\n+        return topicResponses;\n+    }\n+\n+    private static List<FetchResponseData.PartitionData> partitionData() {\n+        return Collections.unmodifiableList(IntStream.range(0, 2000)\n+            .mapToObj(i -> new FetchResponseData.PartitionData()\n+                .setPartitionIndex(i)\n+                .setLastStableOffset(0)\n+                .setLogStartOffset(0))\n+                .collect(Collectors.toList()));\n+    }\n+\n+    private static IncrementalFetchContext incrementalFetchContext() {\n+        FetchMetadata metadata = Mockito.mock(FetchMetadata.class);\n+        Mockito.when(metadata.epoch()).thenReturn(100);\n+\n+        CachedPartition cp = Mockito.mock(CachedPartition.class);\n+        Mockito.when(cp.maybeUpdateResponseData(\n+                Mockito.any(FetchResponseData.PartitionData.class),\n+                Mockito.anyBoolean())).thenAnswer(invocation -> {\n+            FetchResponseData.PartitionData partitionData = invocation.getArgument(0);\n+            return partitionData.partitionIndex() % 2 == 0;\n+        });\n+\n+        FetchSession session = Mockito.mock(FetchSession.class);\n+        Mockito.when(session.epoch()).thenReturn(101);\n+\n+        return new IncrementalFetchContext(Time.SYSTEM, metadata, session) {\n+            @Override\n+            public CachedPartition cachedPartition(String topic, int partition) {\n+                return cp;\n+            }\n+\n+            @Override\n+            public void update(CachedPartition cp) {\n+\n+            }\n+        };\n+    }\n+\n+    @Benchmark\n+    @OutputTimeUnit(TimeUnit.MILLISECONDS)\n+    public FetchResponse updateAndGenerateResponseData() {\n+        return fetchContext.updateAndGenerateResponseData(topicResponses);\n+    }\n+\n+    @Benchmark\n+    @OutputTimeUnit(TimeUnit.MILLISECONDS)\n+    public int getResponseSize() {\n+        return fetchContext.getResponseSize(topicResponses, FetchResponseData.HIGHEST_SUPPORTED_VERSION);\n+    }\n+}"
  }
]
