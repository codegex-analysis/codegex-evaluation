[
  {
    "sha": "ca5cd52a8bc22f960171d6476761cd56765b7808",
    "filename": "checkstyle/suppressions.xml",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/apache/kafka/blob/3ab71a2d792f1b30196d9d75bb6a9a06671f907d/checkstyle/suppressions.xml",
    "raw_url": "https://github.com/apache/kafka/raw/3ab71a2d792f1b30196d9d75bb6a9a06671f907d/checkstyle/suppressions.xml",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/checkstyle/suppressions.xml?ref=3ab71a2d792f1b30196d9d75bb6a9a06671f907d",
    "patch": "@@ -271,7 +271,7 @@\n     <suppress checks=\"CyclomaticComplexity\"\n               files=\"(ReplicationControlManager).java\"/>\n     <suppress checks=\"NPathComplexity\"\n-              files=\"KafkaEventQueue.java\"/>\n+              files=\"(KafkaEventQueue|ReplicationControlManager).java\"/>\n     <suppress checks=\"(NPathComplexity|ClassFanOutComplexity|CyclomaticComplexity|ClassDataAbstractionCoupling|LocalVariableName|MemberName|ParameterName|MethodLength|JavaNCSS|AvoidStarImport)\"\n             files=\"metadata[\\\\/]src[\\\\/](generated|generated-test)[\\\\/].+.java$\"/>\n </suppressions>"
  },
  {
    "sha": "f88fc62a348044b787b11659756626514db5a3dd",
    "filename": "core/src/main/scala/kafka/server/ControllerApis.scala",
    "status": "modified",
    "additions": 57,
    "deletions": 1,
    "changes": 58,
    "blob_url": "https://github.com/apache/kafka/blob/3ab71a2d792f1b30196d9d75bb6a9a06671f907d/core/src/main/scala/kafka/server/ControllerApis.scala",
    "raw_url": "https://github.com/apache/kafka/raw/3ab71a2d792f1b30196d9d75bb6a9a06671f907d/core/src/main/scala/kafka/server/ControllerApis.scala",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/server/ControllerApis.scala?ref=3ab71a2d792f1b30196d9d75bb6a9a06671f907d",
    "patch": "@@ -32,11 +32,13 @@ import org.apache.kafka.common.acl.AclOperation.{ALTER, ALTER_CONFIGS, CLUSTER_A\n import org.apache.kafka.common.config.ConfigResource\n import org.apache.kafka.common.errors.{ApiException, ClusterAuthorizationException, InvalidRequestException, TopicDeletionDisabledException}\n import org.apache.kafka.common.internals.FatalExitError\n+import org.apache.kafka.common.message.CreatePartitionsRequestData.CreatePartitionsTopic\n+import org.apache.kafka.common.message.CreatePartitionsResponseData.CreatePartitionsTopicResult\n import org.apache.kafka.common.message.CreateTopicsRequestData.CreatableTopicCollection\n import org.apache.kafka.common.message.CreateTopicsResponseData.CreatableTopicResult\n import org.apache.kafka.common.message.DeleteTopicsResponseData.{DeletableTopicResult, DeletableTopicResultCollection}\n import org.apache.kafka.common.message.MetadataResponseData.MetadataResponseBroker\n-import org.apache.kafka.common.message.{BeginQuorumEpochResponseData, BrokerHeartbeatResponseData, BrokerRegistrationResponseData, CreateTopicsResponseData, DeleteTopicsRequestData, DeleteTopicsResponseData, DescribeQuorumResponseData, EndQuorumEpochResponseData, FetchResponseData, MetadataResponseData, SaslAuthenticateResponseData, SaslHandshakeResponseData, UnregisterBrokerResponseData, VoteResponseData}\n+import org.apache.kafka.common.message.{BeginQuorumEpochResponseData, BrokerHeartbeatResponseData, BrokerRegistrationResponseData, CreatePartitionsRequestData, CreatePartitionsResponseData, CreateTopicsResponseData, DeleteTopicsRequestData, DeleteTopicsResponseData, DescribeQuorumResponseData, EndQuorumEpochResponseData, FetchResponseData, MetadataResponseData, SaslAuthenticateResponseData, SaslHandshakeResponseData, UnregisterBrokerResponseData, VoteResponseData}\n import org.apache.kafka.common.protocol.Errors.{INVALID_REQUEST, TOPIC_AUTHORIZATION_FAILED}\n import org.apache.kafka.common.protocol.{ApiKeys, ApiMessage, Errors}\n import org.apache.kafka.common.requests._\n@@ -91,6 +93,7 @@ class ControllerApis(val requestChannel: RequestChannel,\n         case ApiKeys.ENVELOPE => handleEnvelopeRequest(request)\n         case ApiKeys.SASL_HANDSHAKE => handleSaslHandshakeRequest(request)\n         case ApiKeys.SASL_AUTHENTICATE => handleSaslAuthenticateRequest(request)\n+        case ApiKeys.CREATE_PARTITIONS => handleCreatePartitions(request)\n         case _ => throw new ApiException(s\"Unsupported ApiKey ${request.context.header.apiKey}\")\n       }\n     } catch {\n@@ -535,4 +538,57 @@ class ControllerApis(val requestChannel: RequestChannel,\n         }\n       })\n   }\n+\n+  def handleCreatePartitions(request: RequestChannel.Request): Unit = {\n+    val responses = createPartitions(request.body[CreatePartitionsRequest].data,\n+      authHelper.authorize(request.context, CREATE, CLUSTER, CLUSTER_NAME),\n+      names => authHelper.filterByAuthorized(request.context, CREATE, TOPIC, names)(n => n))\n+    requestHelper.sendResponseMaybeThrottle(request, throttleTimeMs => {\n+      val responseData = new CreatePartitionsResponseData().\n+        setResults(responses).\n+        setThrottleTimeMs(throttleTimeMs)\n+      new CreatePartitionsResponse(responseData)\n+    })\n+  }\n+\n+  def createPartitions(request: CreatePartitionsRequestData,\n+                       hasClusterAuth: Boolean,\n+                       getCreatableTopics: Iterable[String] => Set[String]): util.List[CreatePartitionsTopicResult] = {\n+    val responses = new util.ArrayList[CreatePartitionsTopicResult]()\n+    val duplicateTopicNames = new util.HashSet[String]()\n+    val topicNames = new util.HashSet[String]()\n+    request.topics().forEach {\n+      topic =>\n+        if (!topicNames.add(topic.name())) {\n+          duplicateTopicNames.add(topic.name())\n+        }\n+    }\n+    duplicateTopicNames.forEach {\n+      case topicName => responses.add(new CreatePartitionsTopicResult().\n+        setName(topicName).\n+        setErrorCode(INVALID_REQUEST.code()).\n+        setErrorMessage(\"Duplicate topic name.\"))\n+        topicNames.remove(topicName)\n+    }\n+    val authorizedTopicNames = {\n+      if (hasClusterAuth) {\n+        topicNames.asScala\n+      } else {\n+        getCreatableTopics(topicNames.asScala)\n+      }\n+    }\n+    val topics = new util.ArrayList[CreatePartitionsTopic]\n+    topicNames.forEach {\n+      case topicName => if (authorizedTopicNames.contains(topicName)) {\n+        topics.add(request.topics().find(topicName))\n+      } else {\n+        responses.add(new CreatePartitionsTopicResult().\n+          setName(topicName).\n+          setErrorCode(TOPIC_AUTHORIZATION_FAILED.code()))\n+      }\n+    }\n+    val results = controller.createPartitions(topics).get()\n+    results.forEach(response => responses.add(response))\n+    responses\n+  }\n }"
  },
  {
    "sha": "2c2ee3ddb2b9477a344c2360227e0cc73e4e47ee",
    "filename": "core/src/test/java/kafka/test/MockController.java",
    "status": "modified",
    "additions": 27,
    "deletions": 0,
    "changes": 27,
    "blob_url": "https://github.com/apache/kafka/blob/3ab71a2d792f1b30196d9d75bb6a9a06671f907d/core/src/test/java/kafka/test/MockController.java",
    "raw_url": "https://github.com/apache/kafka/raw/3ab71a2d792f1b30196d9d75bb6a9a06671f907d/core/src/test/java/kafka/test/MockController.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/java/kafka/test/MockController.java?ref=3ab71a2d792f1b30196d9d75bb6a9a06671f907d",
    "patch": "@@ -25,6 +25,8 @@\n import org.apache.kafka.common.message.AlterIsrResponseData;\n import org.apache.kafka.common.message.BrokerHeartbeatRequestData;\n import org.apache.kafka.common.message.BrokerRegistrationRequestData;\n+import org.apache.kafka.common.message.CreatePartitionsRequestData.CreatePartitionsTopic;\n+import org.apache.kafka.common.message.CreatePartitionsResponseData.CreatePartitionsTopicResult;\n import org.apache.kafka.common.message.CreateTopicsRequestData;\n import org.apache.kafka.common.message.CreateTopicsResponseData;\n import org.apache.kafka.common.message.ElectLeadersRequestData;\n@@ -39,8 +41,10 @@\n import org.apache.kafka.metadata.BrokerRegistrationReply;\n import org.apache.kafka.metadata.FeatureMapAndEpoch;\n \n+import java.util.ArrayList;\n import java.util.Collection;\n import java.util.HashMap;\n+import java.util.List;\n import java.util.Map;\n import java.util.concurrent.CompletableFuture;\n \n@@ -201,6 +205,29 @@ private MockController(Collection<MockTopic> initialTopics) {\n         throw new UnsupportedOperationException();\n     }\n \n+    @Override\n+    synchronized public CompletableFuture<List<CreatePartitionsTopicResult>>\n+            createPartitions(List<CreatePartitionsTopic> topicList) {\n+        if (!active) {\n+            CompletableFuture<List<CreatePartitionsTopicResult>> future = new CompletableFuture<>();\n+            future.completeExceptionally(NOT_CONTROLLER_EXCEPTION);\n+            return future;\n+        }\n+        List<CreatePartitionsTopicResult> results = new ArrayList<>();\n+        for (CreatePartitionsTopic topic : topicList) {\n+            if (topicNameToId.containsKey(topic.name())) {\n+                results.add(new CreatePartitionsTopicResult().setName(topic.name()).\n+                    setErrorCode(Errors.NONE.code()).\n+                    setErrorMessage(null));\n+            } else {\n+                results.add(new CreatePartitionsTopicResult().setName(topic.name()).\n+                    setErrorCode(Errors.UNKNOWN_TOPIC_OR_PARTITION.code()).\n+                    setErrorMessage(\"No such topic as \" + topic.name()));\n+            }\n+        }\n+        return CompletableFuture.completedFuture(results);\n+    }\n+\n     @Override\n     public void beginShutdown() {\n         this.active = false;"
  },
  {
    "sha": "105541ce4d424cc792e53a7c21ea0ffd7a233843",
    "filename": "core/src/test/scala/unit/kafka/server/ControllerApisTest.scala",
    "status": "modified",
    "additions": 27,
    "deletions": 1,
    "changes": 28,
    "blob_url": "https://github.com/apache/kafka/blob/3ab71a2d792f1b30196d9d75bb6a9a06671f907d/core/src/test/scala/unit/kafka/server/ControllerApisTest.scala",
    "raw_url": "https://github.com/apache/kafka/raw/3ab71a2d792f1b30196d9d75bb6a9a06671f907d/core/src/test/scala/unit/kafka/server/ControllerApisTest.scala",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/scala/unit/kafka/server/ControllerApisTest.scala?ref=3ab71a2d792f1b30196d9d75bb6a9a06671f907d",
    "patch": "@@ -32,9 +32,11 @@ import org.apache.kafka.common.Uuid.ZERO_UUID\n import org.apache.kafka.common.errors.{InvalidRequestException, NotControllerException, TopicDeletionDisabledException}\n import org.apache.kafka.common.memory.MemoryPool\n import org.apache.kafka.common.message.ApiMessageType.ListenerType\n+import org.apache.kafka.common.message.CreatePartitionsRequestData.CreatePartitionsTopic\n+import org.apache.kafka.common.message.CreatePartitionsResponseData.CreatePartitionsTopicResult\n import org.apache.kafka.common.message.DeleteTopicsRequestData.DeleteTopicState\n import org.apache.kafka.common.message.DeleteTopicsResponseData.DeletableTopicResult\n-import org.apache.kafka.common.message.{BrokerRegistrationRequestData, DeleteTopicsRequestData}\n+import org.apache.kafka.common.message.{BrokerRegistrationRequestData, CreatePartitionsRequestData, DeleteTopicsRequestData}\n import org.apache.kafka.common.network.{ClientInformation, ListenerName}\n import org.apache.kafka.common.protocol.{ApiKeys, Errors}\n import org.apache.kafka.common.requests.{AbstractRequest, AbstractResponse, BrokerRegistrationRequest, BrokerRegistrationResponse, RequestContext, RequestHeader, RequestTestUtils}\n@@ -339,6 +341,30 @@ class ControllerApisTest {\n         _ => Set(\"foo\", \"bar\")))\n   }\n \n+  @Test\n+  def testCreatePartitionsRequest(): Unit = {\n+    val controller = new MockController.Builder().\n+      newInitialTopic(\"foo\", Uuid.fromString(\"vZKYST0pSA2HO5x_6hoO2Q\")).\n+      newInitialTopic(\"bar\", Uuid.fromString(\"VlFu5c51ToiNx64wtwkhQw\")).build()\n+    val controllerApis = createControllerApis(None, controller)\n+    val request = new CreatePartitionsRequestData()\n+    request.topics().add(new CreatePartitionsTopic().setName(\"foo\").setAssignments(null).setCount(5))\n+    request.topics().add(new CreatePartitionsTopic().setName(\"bar\").setAssignments(null).setCount(5))\n+    request.topics().add(new CreatePartitionsTopic().setName(\"bar\").setAssignments(null).setCount(5))\n+    request.topics().add(new CreatePartitionsTopic().setName(\"bar\").setAssignments(null).setCount(5))\n+    request.topics().add(new CreatePartitionsTopic().setName(\"baz\").setAssignments(null).setCount(5))\n+    assertEquals(Set(new CreatePartitionsTopicResult().setName(\"foo\").\n+        setErrorCode(Errors.NONE.code()).\n+        setErrorMessage(null),\n+      new CreatePartitionsTopicResult().setName(\"bar\").\n+        setErrorCode(Errors.INVALID_REQUEST.code()).\n+        setErrorMessage(\"Duplicate topic name.\"),\n+      new CreatePartitionsTopicResult().setName(\"baz\").\n+        setErrorCode(Errors.TOPIC_AUTHORIZATION_FAILED.code()).\n+        setErrorMessage(null)),\n+      controllerApis.createPartitions(request, false, _ => Set(\"foo\", \"bar\")).asScala.toSet)\n+  }\n+\n   @AfterEach\n   def tearDown(): Unit = {\n     quotas.shutdown()"
  },
  {
    "sha": "d78ac8833cad862ef7c00d2ba5e372b5ea31a130",
    "filename": "metadata/src/main/java/org/apache/kafka/controller/BrokerHeartbeatManager.java",
    "status": "modified",
    "additions": 5,
    "deletions": 2,
    "changes": 7,
    "blob_url": "https://github.com/apache/kafka/blob/3ab71a2d792f1b30196d9d75bb6a9a06671f907d/metadata/src/main/java/org/apache/kafka/controller/BrokerHeartbeatManager.java",
    "raw_url": "https://github.com/apache/kafka/raw/3ab71a2d792f1b30196d9d75bb6a9a06671f907d/metadata/src/main/java/org/apache/kafka/controller/BrokerHeartbeatManager.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/metadata/src/main/java/org/apache/kafka/controller/BrokerHeartbeatManager.java?ref=3ab71a2d792f1b30196d9d75bb6a9a06671f907d",
    "patch": "@@ -435,6 +435,7 @@ long nextCheckTimeNs() {\n     /**\n      * Place replicas on unfenced brokers.\n      *\n+     * @param startPartition    The partition ID to start with.\n      * @param numPartitions     The number of partitions to place.\n      * @param numReplicas       The number of replicas for each partition.\n      * @param idToRack          A function mapping broker id to broker rack.\n@@ -444,14 +445,16 @@ long nextCheckTimeNs() {\n      *\n      * @throws InvalidReplicationFactorException    If too many replicas were requested.\n      */\n-    List<List<Integer>> placeReplicas(int numPartitions, short numReplicas,\n+    List<List<Integer>> placeReplicas(int startPartition,\n+                                      int numPartitions,\n+                                      short numReplicas,\n                                       Function<Integer, Optional<String>> idToRack,\n                                       ReplicaPlacementPolicy policy) {\n         // TODO: support using fenced brokers here if necessary to get to the desired\n         // number of replicas. We probably need to add a fenced boolean in UsableBroker.\n         Iterator<UsableBroker> iterator = new UsableBrokerIterator(\n             unfenced.iterator(), idToRack);\n-        return policy.createPlacement(numPartitions, numReplicas, iterator);\n+        return policy.createPlacement(startPartition, numPartitions, numReplicas, iterator);\n     }\n \n     static class UsableBrokerIterator implements Iterator<UsableBroker> {"
  },
  {
    "sha": "5cdfc9c63a8f38996c0d667f4d8f5375db436dea",
    "filename": "metadata/src/main/java/org/apache/kafka/controller/ClusterControlManager.java",
    "status": "modified",
    "additions": 4,
    "deletions": 2,
    "changes": 6,
    "blob_url": "https://github.com/apache/kafka/blob/3ab71a2d792f1b30196d9d75bb6a9a06671f907d/metadata/src/main/java/org/apache/kafka/controller/ClusterControlManager.java",
    "raw_url": "https://github.com/apache/kafka/raw/3ab71a2d792f1b30196d9d75bb6a9a06671f907d/metadata/src/main/java/org/apache/kafka/controller/ClusterControlManager.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/metadata/src/main/java/org/apache/kafka/controller/ClusterControlManager.java?ref=3ab71a2d792f1b30196d9d75bb6a9a06671f907d",
    "patch": "@@ -303,11 +303,13 @@ public void replay(UnfenceBrokerRecord record) {\n         }\n     }\n \n-    public List<List<Integer>> placeReplicas(int numPartitions, short numReplicas) {\n+    public List<List<Integer>> placeReplicas(int startPartition,\n+                                             int numPartitions,\n+                                             short numReplicas) {\n         if (heartbeatManager == null) {\n             throw new RuntimeException(\"ClusterControlManager is not active.\");\n         }\n-        return heartbeatManager.placeReplicas(numPartitions, numReplicas,\n+        return heartbeatManager.placeReplicas(startPartition, numPartitions, numReplicas,\n             id -> brokerRegistrations.get(id).rack(), placementPolicy);\n     }\n "
  },
  {
    "sha": "4f844cb092dbec04d853a7d81ea0aff01864800c",
    "filename": "metadata/src/main/java/org/apache/kafka/controller/Controller.java",
    "status": "modified",
    "additions": 12,
    "deletions": 0,
    "changes": 12,
    "blob_url": "https://github.com/apache/kafka/blob/3ab71a2d792f1b30196d9d75bb6a9a06671f907d/metadata/src/main/java/org/apache/kafka/controller/Controller.java",
    "raw_url": "https://github.com/apache/kafka/raw/3ab71a2d792f1b30196d9d75bb6a9a06671f907d/metadata/src/main/java/org/apache/kafka/controller/Controller.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/metadata/src/main/java/org/apache/kafka/controller/Controller.java?ref=3ab71a2d792f1b30196d9d75bb6a9a06671f907d",
    "patch": "@@ -24,6 +24,8 @@\n import org.apache.kafka.common.message.AlterIsrResponseData;\n import org.apache.kafka.common.message.BrokerHeartbeatRequestData;\n import org.apache.kafka.common.message.BrokerRegistrationRequestData;\n+import org.apache.kafka.common.message.CreatePartitionsRequestData.CreatePartitionsTopic;\n+import org.apache.kafka.common.message.CreatePartitionsResponseData.CreatePartitionsTopicResult;\n import org.apache.kafka.common.message.CreateTopicsRequestData;\n import org.apache.kafka.common.message.CreateTopicsResponseData;\n import org.apache.kafka.common.message.ElectLeadersRequestData;\n@@ -36,6 +38,7 @@\n import org.apache.kafka.metadata.FeatureMapAndEpoch;\n \n import java.util.Collection;\n+import java.util.List;\n import java.util.Map;\n import java.util.concurrent.CompletableFuture;\n \n@@ -187,6 +190,15 @@\n         Collection<ClientQuotaAlteration> quotaAlterations, boolean validateOnly\n     );\n \n+    /**\n+     * Create partitions on certain topics.\n+     *\n+     * @param topics            The list of topics to create partitions for.\n+     * @return                  A future yielding per-topic results.\n+     */\n+    CompletableFuture<List<CreatePartitionsTopicResult>>\n+            createPartitions(List<CreatePartitionsTopic> topics);\n+\n     /**\n      * Begin shutting down, but don't block.  You must still call close to clean up all\n      * resources."
  },
  {
    "sha": "23015f64548b4e296bfdffd7dae7ae4f58937df4",
    "filename": "metadata/src/main/java/org/apache/kafka/controller/QuorumController.java",
    "status": "modified",
    "additions": 12,
    "deletions": 0,
    "changes": 12,
    "blob_url": "https://github.com/apache/kafka/blob/3ab71a2d792f1b30196d9d75bb6a9a06671f907d/metadata/src/main/java/org/apache/kafka/controller/QuorumController.java",
    "raw_url": "https://github.com/apache/kafka/raw/3ab71a2d792f1b30196d9d75bb6a9a06671f907d/metadata/src/main/java/org/apache/kafka/controller/QuorumController.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/metadata/src/main/java/org/apache/kafka/controller/QuorumController.java?ref=3ab71a2d792f1b30196d9d75bb6a9a06671f907d",
    "patch": "@@ -40,6 +40,8 @@\n import org.apache.kafka.common.message.AlterIsrResponseData;\n import org.apache.kafka.common.message.BrokerHeartbeatRequestData;\n import org.apache.kafka.common.message.BrokerRegistrationRequestData;\n+import org.apache.kafka.common.message.CreatePartitionsRequestData.CreatePartitionsTopic;\n+import org.apache.kafka.common.message.CreatePartitionsResponseData.CreatePartitionsTopicResult;\n import org.apache.kafka.common.message.CreateTopicsRequestData;\n import org.apache.kafka.common.message.CreateTopicsResponseData;\n import org.apache.kafka.common.message.ElectLeadersRequestData;\n@@ -954,6 +956,16 @@ public void processBatchEndOffset(long offset) {\n         });\n     }\n \n+    @Override\n+    public CompletableFuture<List<CreatePartitionsTopicResult>>\n+            createPartitions(List<CreatePartitionsTopic> topics) {\n+        if (topics.isEmpty()) {\n+            return CompletableFuture.completedFuture(Collections.emptyList());\n+        }\n+        return appendWriteEvent(\"createPartitions\", () ->\n+            replicationControl.createPartitions(topics));\n+    }\n+\n     @Override\n     public CompletableFuture<Void> waitForReadyBrokers(int minBrokers) {\n         final CompletableFuture<Void> future = new CompletableFuture<>();"
  },
  {
    "sha": "097463f7e76da4b0b9ac987be82b5b05a71c14ff",
    "filename": "metadata/src/main/java/org/apache/kafka/controller/ReplicaPlacementPolicy.java",
    "status": "modified",
    "additions": 4,
    "deletions": 1,
    "changes": 5,
    "blob_url": "https://github.com/apache/kafka/blob/3ab71a2d792f1b30196d9d75bb6a9a06671f907d/metadata/src/main/java/org/apache/kafka/controller/ReplicaPlacementPolicy.java",
    "raw_url": "https://github.com/apache/kafka/raw/3ab71a2d792f1b30196d9d75bb6a9a06671f907d/metadata/src/main/java/org/apache/kafka/controller/ReplicaPlacementPolicy.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/metadata/src/main/java/org/apache/kafka/controller/ReplicaPlacementPolicy.java?ref=3ab71a2d792f1b30196d9d75bb6a9a06671f907d",
    "patch": "@@ -32,6 +32,7 @@\n     /**\n      * Create a new replica placement.\n      *\n+     * @param startPartition        The partition ID to start with.\n      * @param numPartitions         The number of partitions to create placements for.\n      * @param numReplicas           The number of replicas to create for each partitions.\n      *                              Must be positive.\n@@ -41,7 +42,9 @@\n      *\n      * @throws InvalidReplicationFactorException    If too many replicas were requested.\n      */\n-    List<List<Integer>> createPlacement(int numPartitions, short numReplicas,\n+    List<List<Integer>> createPlacement(int startPartition,\n+                                        int numPartitions,\n+                                        short numReplicas,\n                                         Iterator<UsableBroker> iterator)\n         throws InvalidReplicationFactorException;\n }"
  },
  {
    "sha": "14852dcec0d5786835045d2d536052fb915d55a9",
    "filename": "metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java",
    "status": "modified",
    "additions": 114,
    "deletions": 1,
    "changes": 115,
    "blob_url": "https://github.com/apache/kafka/blob/3ab71a2d792f1b30196d9d75bb6a9a06671f907d/metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java",
    "raw_url": "https://github.com/apache/kafka/raw/3ab71a2d792f1b30196d9d75bb6a9a06671f907d/metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java?ref=3ab71a2d792f1b30196d9d75bb6a9a06671f907d",
    "patch": "@@ -23,14 +23,21 @@\n import org.apache.kafka.common.config.ConfigResource;\n import org.apache.kafka.common.errors.ApiException;\n import org.apache.kafka.common.errors.BrokerIdNotRegisteredException;\n+import org.apache.kafka.common.errors.InvalidPartitionsException;\n+import org.apache.kafka.common.errors.InvalidReplicaAssignmentException;\n import org.apache.kafka.common.errors.InvalidReplicationFactorException;\n import org.apache.kafka.common.errors.InvalidRequestException;\n import org.apache.kafka.common.errors.InvalidTopicException;\n+import org.apache.kafka.common.errors.UnknownServerException;\n import org.apache.kafka.common.errors.UnknownTopicIdException;\n+import org.apache.kafka.common.errors.UnknownTopicOrPartitionException;\n import org.apache.kafka.common.internals.Topic;\n import org.apache.kafka.common.message.AlterIsrRequestData;\n import org.apache.kafka.common.message.AlterIsrResponseData;\n import org.apache.kafka.common.message.BrokerHeartbeatRequestData;\n+import org.apache.kafka.common.message.CreatePartitionsRequestData.CreatePartitionsAssignment;\n+import org.apache.kafka.common.message.CreatePartitionsRequestData.CreatePartitionsTopic;\n+import org.apache.kafka.common.message.CreatePartitionsResponseData.CreatePartitionsTopicResult;\n import org.apache.kafka.common.message.CreateTopicsRequestData;\n import org.apache.kafka.common.message.CreateTopicsRequestData.CreatableReplicaAssignment;\n import org.apache.kafka.common.message.CreateTopicsRequestData.CreatableTopic;\n@@ -496,7 +503,7 @@ private ApiError createTopic(CreatableTopic topic,\n                 defaultReplicationFactor : topic.replicationFactor();\n             try {\n                 List<List<Integer>> replicas = clusterControl.\n-                    placeReplicas(numPartitions, replicationFactor);\n+                    placeReplicas(0, numPartitions, replicationFactor);\n                 for (int partitionId = 0; partitionId < replicas.size(); partitionId++) {\n                     int[] r = Replicas.toArray(replicas.get(partitionId));\n                     newParts.put(partitionId,\n@@ -1005,4 +1012,110 @@ int bestLeader(int[] replicas, int[] isr, boolean unclean) {\n         }\n         return ControllerResult.of(records, null);\n     }\n+\n+    ControllerResult<List<CreatePartitionsTopicResult>>\n+            createPartitions(List<CreatePartitionsTopic> topics) {\n+        List<ApiMessageAndVersion> records = new ArrayList<>();\n+        List<CreatePartitionsTopicResult> results = new ArrayList<>();\n+        for (CreatePartitionsTopic topic : topics) {\n+            ApiError apiError = ApiError.NONE;\n+            try {\n+                createPartitions(topic, records);\n+            } catch (ApiException e) {\n+                apiError = ApiError.fromThrowable(e);\n+            } catch (Exception e) {\n+                log.error(\"Unexpected createPartitions error for {}\", topic, e);\n+                apiError = ApiError.fromThrowable(e);\n+            }\n+            results.add(new CreatePartitionsTopicResult().\n+                setName(topic.name()).\n+                setErrorCode(apiError.error().code()).\n+                setErrorMessage(apiError.message()));\n+        }\n+        return new ControllerResult<>(records, results, true);\n+    }\n+\n+    void createPartitions(CreatePartitionsTopic topic,\n+                          List<ApiMessageAndVersion> records) {\n+        Uuid topicId = topicsByName.get(topic.name());\n+        if (topicId == null) {\n+            throw new UnknownTopicOrPartitionException();\n+        }\n+        TopicControlInfo topicInfo = topics.get(topicId);\n+        if (topicInfo == null) {\n+            throw new UnknownTopicOrPartitionException();\n+        }\n+        if (topic.count() == topicInfo.parts.size()) {\n+            throw new InvalidPartitionsException(\"Topic already has \" +\n+                topicInfo.parts.size() + \" partition(s).\");\n+        } else if (topic.count() < topicInfo.parts.size()) {\n+            throw new InvalidPartitionsException(\"The topic \" + topic.name() + \" currently \" +\n+                \"has \" + topicInfo.parts.size() + \" partition(s); \" + topic.count() +\n+                \" would not be an increase.\");\n+        }\n+        int additional = topic.count() - topicInfo.parts.size();\n+        if (topic.assignments() != null) {\n+            if (topic.assignments().size() != additional) {\n+                throw new InvalidReplicaAssignmentException(\"Attempted to add \" + additional +\n+                    \" additional partition(s), but only \" + topic.assignments().size() +\n+                    \" assignment(s) were specified.\");\n+            }\n+        }\n+        Iterator<PartitionControlInfo> iterator = topicInfo.parts.values().iterator();\n+        if (!iterator.hasNext()) {\n+            throw new UnknownServerException(\"Invalid state: topic \" + topic.name() +\n+                \" appears to have no partitions.\");\n+        }\n+        PartitionControlInfo partitionInfo = iterator.next();\n+        if (partitionInfo.replicas.length > Short.MAX_VALUE) {\n+            throw new UnknownServerException(\"Invalid replication factor \" +\n+                partitionInfo.replicas.length + \": expected a number less than 65536.\");\n+        }\n+        short replicationFactor = (short) partitionInfo.replicas.length;\n+        int startPartitionId = topicInfo.parts.size();\n+\n+        List<List<Integer>> placements = null;\n+        if (topic.assignments() != null) {\n+            placements = new ArrayList<>();\n+            for (CreatePartitionsAssignment assignment : topic.assignments()) {\n+                List<Integer> sortedBrokerIds = new ArrayList<>(assignment.brokerIds());\n+                sortedBrokerIds.sort(Integer::compare);\n+                Integer prevBrokerId = null;\n+                for (Integer brokerId : sortedBrokerIds) {\n+                    if (!clusterControl.unfenced(brokerId)) {\n+                        throw new InvalidReplicaAssignmentException(\"Unable to place new \" +\n+                            \"partition on broker \" + brokerId + \".\");\n+                    }\n+                    if (brokerId.equals(prevBrokerId)) {\n+                        throw new InvalidReplicaAssignmentException(\"Duplicate broker ID \" +\n+                            prevBrokerId + \" given in replica assignment.\");\n+                    }\n+                    prevBrokerId = brokerId;\n+                }\n+                if (sortedBrokerIds.size() != replicationFactor) {\n+                    throw new InvalidReplicaAssignmentException(\"Needed \" +\n+                        replicationFactor + \" replica(s), but only found \" +\n+                        sortedBrokerIds.size() + \" in replica assignment.\");\n+                }\n+                placements.add(assignment.brokerIds());\n+            }\n+        } else {\n+            placements = clusterControl.placeReplicas(startPartitionId, additional,\n+                replicationFactor);\n+        }\n+        int partitionId = startPartitionId;\n+        for (List<Integer> placement : placements) {\n+            records.add(new ApiMessageAndVersion(new PartitionRecord().\n+                setPartitionId(partitionId).\n+                setTopicId(topicId).\n+                setReplicas(placement).\n+                setIsr(placement).\n+                setRemovingReplicas(null).\n+                setAddingReplicas(null).\n+                setLeader(placement.get(0)).\n+                setLeaderEpoch(0).\n+                setPartitionEpoch(0), (short) 0));\n+            partitionId++;\n+        }\n+    }\n }"
  },
  {
    "sha": "a2f7c892e45dc80f7680f7bfaac0364f18110078",
    "filename": "metadata/src/main/java/org/apache/kafka/controller/SimpleReplicaPlacementPolicy.java",
    "status": "modified",
    "additions": 2,
    "deletions": 1,
    "changes": 3,
    "blob_url": "https://github.com/apache/kafka/blob/3ab71a2d792f1b30196d9d75bb6a9a06671f907d/metadata/src/main/java/org/apache/kafka/controller/SimpleReplicaPlacementPolicy.java",
    "raw_url": "https://github.com/apache/kafka/raw/3ab71a2d792f1b30196d9d75bb6a9a06671f907d/metadata/src/main/java/org/apache/kafka/controller/SimpleReplicaPlacementPolicy.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/metadata/src/main/java/org/apache/kafka/controller/SimpleReplicaPlacementPolicy.java?ref=3ab71a2d792f1b30196d9d75bb6a9a06671f907d",
    "patch": "@@ -41,7 +41,8 @@ public SimpleReplicaPlacementPolicy(Random random) {\n     }\n \n     @Override\n-    public List<List<Integer>> createPlacement(int numPartitions,\n+    public List<List<Integer>> createPlacement(int startPartition,\n+                                               int numPartitions,\n                                                short numReplicas,\n                                                Iterator<UsableBroker> iterator) {\n         List<UsableBroker> usable = new ArrayList<>();"
  },
  {
    "sha": "e9757ebdb43ac9f9851d482ee43a691ea556419c",
    "filename": "metadata/src/test/java/org/apache/kafka/controller/ClusterControlManagerTest.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/apache/kafka/blob/3ab71a2d792f1b30196d9d75bb6a9a06671f907d/metadata/src/test/java/org/apache/kafka/controller/ClusterControlManagerTest.java",
    "raw_url": "https://github.com/apache/kafka/raw/3ab71a2d792f1b30196d9d75bb6a9a06671f907d/metadata/src/test/java/org/apache/kafka/controller/ClusterControlManagerTest.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/metadata/src/test/java/org/apache/kafka/controller/ClusterControlManagerTest.java?ref=3ab71a2d792f1b30196d9d75bb6a9a06671f907d",
    "patch": "@@ -138,7 +138,7 @@ public void testPlaceReplicas(int numUsableBrokers) throws Exception {\n                 String.format(\"broker %d was not unfenced.\", i));\n         }\n         for (int i = 0; i < 100; i++) {\n-            List<List<Integer>> results = clusterControl.placeReplicas(1, (short) 3);\n+            List<List<Integer>> results = clusterControl.placeReplicas(0, 1, (short) 3);\n             HashSet<Integer> seen = new HashSet<>();\n             for (Integer result : results.get(0)) {\n                 assertTrue(result >= 0);"
  },
  {
    "sha": "f1f702d3432b6ca98c78134a3a7e1506ab9f2112",
    "filename": "metadata/src/test/java/org/apache/kafka/controller/ReplicationControlManagerTest.java",
    "status": "modified",
    "additions": 92,
    "deletions": 0,
    "changes": 92,
    "blob_url": "https://github.com/apache/kafka/blob/3ab71a2d792f1b30196d9d75bb6a9a06671f907d/metadata/src/test/java/org/apache/kafka/controller/ReplicationControlManagerTest.java",
    "raw_url": "https://github.com/apache/kafka/raw/3ab71a2d792f1b30196d9d75bb6a9a06671f907d/metadata/src/test/java/org/apache/kafka/controller/ReplicationControlManagerTest.java",
    "contents_url": "https://api.github.com/repos/apache/kafka/contents/metadata/src/test/java/org/apache/kafka/controller/ReplicationControlManagerTest.java?ref=3ab71a2d792f1b30196d9d75bb6a9a06671f907d",
    "patch": "@@ -24,6 +24,11 @@\n import org.apache.kafka.common.message.AlterIsrRequestData;\n import org.apache.kafka.common.message.AlterIsrResponseData;\n import org.apache.kafka.common.message.BrokerHeartbeatRequestData;\n+import org.apache.kafka.common.message.CreatePartitionsRequestData;\n+import org.apache.kafka.common.message.CreatePartitionsRequestData.CreatePartitionsAssignment;\n+import org.apache.kafka.common.message.CreatePartitionsRequestData.CreatePartitionsTopic;\n+import org.apache.kafka.common.message.CreatePartitionsResponseData;\n+import org.apache.kafka.common.message.CreatePartitionsResponseData.CreatePartitionsTopicResult;\n import org.apache.kafka.common.message.CreateTopicsRequestData;\n import org.apache.kafka.common.message.CreateTopicsRequestData.CreatableReplicaAssignment;\n import org.apache.kafka.common.message.CreateTopicsRequestData.CreatableTopic;\n@@ -56,6 +61,8 @@\n import java.util.stream.Collectors;\n import java.util.stream.IntStream;\n \n+import static org.apache.kafka.common.protocol.Errors.INVALID_PARTITIONS;\n+import static org.apache.kafka.common.protocol.Errors.INVALID_REPLICA_ASSIGNMENT;\n import static org.apache.kafka.common.protocol.Errors.INVALID_TOPIC_EXCEPTION;\n import static org.apache.kafka.common.protocol.Errors.NONE;\n import static org.apache.kafka.common.protocol.Errors.UNKNOWN_TOPIC_ID;\n@@ -507,4 +514,89 @@ public void testDeleteTopics() throws Exception {\n                 Long.MAX_VALUE, Collections.singleton(\"foo\")));\n         assertEmptyTopicConfigs(ctx, \"foo\");\n     }\n+\n+\n+    @Test\n+    public void testCreatePartitions() throws Exception {\n+        ReplicationControlTestContext ctx = new ReplicationControlTestContext();\n+        ReplicationControlManager replicationControl = ctx.replicationControl;\n+        CreateTopicsRequestData request = new CreateTopicsRequestData();\n+        request.topics().add(new CreatableTopic().setName(\"foo\").\n+            setNumPartitions(3).setReplicationFactor((short) 2));\n+        request.topics().add(new CreatableTopic().setName(\"bar\").\n+            setNumPartitions(4).setReplicationFactor((short) 2));\n+        request.topics().add(new CreatableTopic().setName(\"quux\").\n+            setNumPartitions(2).setReplicationFactor((short) 2));\n+        request.topics().add(new CreatableTopic().setName(\"foo2\").\n+            setNumPartitions(2).setReplicationFactor((short) 2));\n+        registerBroker(0, ctx);\n+        unfenceBroker(0, ctx);\n+        registerBroker(1, ctx);\n+        unfenceBroker(1, ctx);\n+        ControllerResult<CreateTopicsResponseData> createTopicResult =\n+            replicationControl.createTopics(request);\n+        ctx.replay(createTopicResult.records());\n+        List<CreatePartitionsTopic> topics = new ArrayList<>();\n+        topics.add(new CreatePartitionsTopic().\n+            setName(\"foo\").setCount(5).setAssignments(null));\n+        topics.add(new CreatePartitionsTopic().\n+            setName(\"bar\").setCount(3).setAssignments(null));\n+        topics.add(new CreatePartitionsTopic().\n+            setName(\"baz\").setCount(3).setAssignments(null));\n+        topics.add(new CreatePartitionsTopic().\n+            setName(\"quux\").setCount(2).setAssignments(null));\n+        ControllerResult<List<CreatePartitionsTopicResult>> createPartitionsResult =\n+            replicationControl.createPartitions(topics);\n+        assertEquals(Arrays.asList(new CreatePartitionsTopicResult().\n+                setName(\"foo\").\n+                setErrorCode(NONE.code()).\n+                setErrorMessage(null),\n+            new CreatePartitionsTopicResult().\n+                setName(\"bar\").\n+                setErrorCode(INVALID_PARTITIONS.code()).\n+                setErrorMessage(\"The topic bar currently has 4 partition(s); 3 would not be an increase.\"),\n+            new CreatePartitionsTopicResult().\n+                setName(\"baz\").\n+                setErrorCode(UNKNOWN_TOPIC_OR_PARTITION.code()).\n+                setErrorMessage(null),\n+            new CreatePartitionsTopicResult().\n+                setName(\"quux\").\n+                setErrorCode(INVALID_PARTITIONS.code()).\n+                setErrorMessage(\"Topic already has 2 partition(s).\")),\n+            createPartitionsResult.response());\n+        ctx.replay(createPartitionsResult.records());\n+        List<CreatePartitionsTopic> topics2 = new ArrayList<>();\n+        topics2.add(new CreatePartitionsTopic().\n+            setName(\"foo\").setCount(6).setAssignments(Arrays.asList(\n+                new CreatePartitionsAssignment().setBrokerIds(Arrays.asList(1, 0)))));\n+        topics2.add(new CreatePartitionsTopic().\n+            setName(\"bar\").setCount(5).setAssignments(Arrays.asList(\n+            new CreatePartitionsAssignment().setBrokerIds(Arrays.asList(1)))));\n+        topics2.add(new CreatePartitionsTopic().\n+            setName(\"quux\").setCount(4).setAssignments(Arrays.asList(\n+            new CreatePartitionsAssignment().setBrokerIds(Arrays.asList(1, 0)))));\n+        topics2.add(new CreatePartitionsTopic().\n+            setName(\"foo2\").setCount(3).setAssignments(Arrays.asList(\n+            new CreatePartitionsAssignment().setBrokerIds(Arrays.asList(2, 0)))));\n+        ControllerResult<List<CreatePartitionsTopicResult>> createPartitionsResult2 =\n+            replicationControl.createPartitions(topics2);\n+        assertEquals(Arrays.asList(new CreatePartitionsTopicResult().\n+                setName(\"foo\").\n+                setErrorCode(NONE.code()).\n+                setErrorMessage(null),\n+            new CreatePartitionsTopicResult().\n+                setName(\"bar\").\n+                setErrorCode(INVALID_REPLICA_ASSIGNMENT.code()).\n+                setErrorMessage(\"Needed 2 replica(s), but only found 1 in replica assignment.\"),\n+            new CreatePartitionsTopicResult().\n+                setName(\"quux\").\n+                setErrorCode(INVALID_REPLICA_ASSIGNMENT.code()).\n+                setErrorMessage(\"Attempted to add 2 additional partition(s), but only 1 assignment(s) were specified.\"),\n+            new CreatePartitionsTopicResult().\n+                setName(\"foo2\").\n+                setErrorCode(INVALID_REPLICA_ASSIGNMENT.code()).\n+                setErrorMessage(\"Unable to place new partition on broker 2.\")),\n+            createPartitionsResult2.response());\n+        ctx.replay(createPartitionsResult2.records());\n+    }\n }"
  }
]
