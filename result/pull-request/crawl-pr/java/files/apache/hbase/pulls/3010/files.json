[
  {
    "sha": "a08694934d90b8b983a04d4097e0e216d52cecb8",
    "filename": "hbase-protocol-shaded/src/main/protobuf/server/master/ReplicationServerStatus.proto",
    "status": "modified",
    "additions": 17,
    "deletions": 4,
    "changes": 21,
    "blob_url": "https://github.com/apache/hbase/blob/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-protocol-shaded/src/main/protobuf/server/master/ReplicationServerStatus.proto",
    "raw_url": "https://github.com/apache/hbase/raw/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-protocol-shaded/src/main/protobuf/server/master/ReplicationServerStatus.proto",
    "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-protocol-shaded/src/main/protobuf/server/master/ReplicationServerStatus.proto?ref=2135ff77c7727efd0ab12754283474dd49f42ba4",
    "patch": "@@ -25,10 +25,23 @@ option java_generic_services = true;\n option java_generate_equals_and_hash = true;\n option optimize_for = SPEED;\n \n-import \"server/master/RegionServerStatus.proto\";\n+import \"HBase.proto\";\n+import \"server/ClusterStatus.proto\";\n \n-service ReplicationServerStatusService {\n+message ReplicationServerReportRequest {\n+  required ServerName server = 1;\n+\n+  /** load the server is under */\n+  optional ServerLoad load = 2;\n+\n+  /** The replication queues which this replication server is responsible for. */\n+  repeated string queue_node = 3;\n+}\n \n-  rpc ReplicationServerReport(RegionServerReportRequest)\n-      returns(RegionServerReportResponse);\n+message ReplicationServerReportResponse {\n+}\n+\n+service ReplicationServerStatusService {\n+  rpc ReplicationServerReport(ReplicationServerReportRequest)\n+    returns(ReplicationServerReportResponse);\n }"
  },
  {
    "sha": "42a56c0c3812544f9dbe6db68d87ef2285ed15e6",
    "filename": "hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueueInfo.java",
    "status": "modified",
    "additions": 27,
    "deletions": 1,
    "changes": 28,
    "blob_url": "https://github.com/apache/hbase/blob/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueueInfo.java",
    "raw_url": "https://github.com/apache/hbase/raw/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueueInfo.java",
    "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ReplicationQueueInfo.java?ref=2135ff77c7727efd0ab12754283474dd49f42ba4",
    "patch": "@@ -31,11 +31,16 @@\n  * This class is responsible for the parsing logic for a queue id representing a queue.\n  * It will extract the peerId if it's recovered as well as the dead region servers\n  * that were part of the queue's history.\n+ * One replication queue has only one owner. And the owner must be one region server. When enable\n+ * replication offload feature, region server will not start replication source thread to replicate\n+ * data. The replication queue will be used by replication server which is responsible for\n+ * replicating data.\n  */\n @InterfaceAudience.Private\n public class ReplicationQueueInfo {\n   private static final Logger LOG = LoggerFactory.getLogger(ReplicationQueueInfo.class);\n \n+  private final ServerName owner;\n   private final String peerId;\n   private final String queueId;\n   private boolean queueRecovered;\n@@ -46,7 +51,8 @@\n    * The passed queueId will be either the id of the peer or the handling story of that queue\n    * in the form of id-servername-*\n    */\n-  public ReplicationQueueInfo(String queueId) {\n+  public ReplicationQueueInfo(ServerName owner, String queueId) {\n+    this.owner = owner;\n     this.queueId = queueId;\n     String[] parts = queueId.split(\"-\", 2);\n     this.queueRecovered = parts.length != 1;\n@@ -57,6 +63,22 @@ public ReplicationQueueInfo(String queueId) {\n     }\n   }\n \n+  /**\n+   * A util method to parse the peerId from queueId.\n+   */\n+  public static String parsePeerId(String queueId) {\n+    String[] parts = queueId.split(\"-\", 2);\n+    return parts.length != 1 ? parts[0] : queueId;\n+  }\n+\n+  /**\n+   * A util method to check whether a queue is recovered.\n+   */\n+  public static boolean isQueueRecovered(String queueId) {\n+    String[] parts = queueId.split(\"-\", 2);\n+    return parts.length != 1;\n+  }\n+\n   /**\n    * Parse dead server names from queue id. servername can contain \"-\" such as\n    * \"ip-10-46-221-101.ec2.internal\", so we need skip some \"-\" during parsing for the following\n@@ -114,6 +136,10 @@ public ReplicationQueueInfo(String queueId) {\n     return Collections.unmodifiableList(this.deadRegionServers);\n   }\n \n+  public ServerName getOwner() {\n+    return this.owner;\n+  }\n+\n   public String getPeerId() {\n     return this.peerId;\n   }"
  },
  {
    "sha": "7dbfe41ae0db22e528ce373bd0413a4d036e92fd",
    "filename": "hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ReplicationUtils.java",
    "status": "modified",
    "additions": 1,
    "deletions": 2,
    "changes": 3,
    "blob_url": "https://github.com/apache/hbase/blob/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ReplicationUtils.java",
    "raw_url": "https://github.com/apache/hbase/raw/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ReplicationUtils.java",
    "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ReplicationUtils.java?ref=2135ff77c7727efd0ab12754283474dd49f42ba4",
    "patch": "@@ -86,8 +86,7 @@ public static void removeAllQueues(ReplicationQueueStorage queueStorage, String\n     for (ServerName replicator : queueStorage.getListOfReplicators()) {\n       List<String> queueIds = queueStorage.getAllQueues(replicator);\n       for (String queueId : queueIds) {\n-        ReplicationQueueInfo queueInfo = new ReplicationQueueInfo(queueId);\n-        if (queueInfo.getPeerId().equals(peerId)) {\n+        if (ReplicationQueueInfo.parsePeerId(queueId).equals(peerId)) {\n           queueStorage.removeQueue(replicator, queueId);\n         }\n       }"
  },
  {
    "sha": "141e8900ba1788899fef173d959d0f218cc1b40f",
    "filename": "hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/apache/hbase/blob/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java",
    "raw_url": "https://github.com/apache/hbase/raw/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java",
    "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java?ref=2135ff77c7727efd0ab12754283474dd49f42ba4",
    "patch": "@@ -205,7 +205,7 @@ public void removeWAL(ServerName serverName, String queueId, String fileName)\n \n   private void addLastSeqIdsToOps(String queueId, Map<String, Long> lastSeqIds,\n       List<ZKUtilOp> listOfOps) throws KeeperException, ReplicationException {\n-    String peerId = new ReplicationQueueInfo(queueId).getPeerId();\n+    String peerId = ReplicationQueueInfo.parsePeerId(queueId);\n     for (Entry<String, Long> lastSeqEntry : lastSeqIds.entrySet()) {\n       String path = getSerialReplicationRegionPeerNode(lastSeqEntry.getKey(), peerId);\n       Pair<Long, Integer> p = getLastSequenceIdWithVersion(lastSeqEntry.getKey(), peerId);"
  },
  {
    "sha": "cfe2510f53a3b4f7d9323e485ee1bde327c70560",
    "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/client/AsyncReplicationServerAdmin.java",
    "status": "modified",
    "additions": 14,
    "deletions": 0,
    "changes": 14,
    "blob_url": "https://github.com/apache/hbase/blob/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/client/AsyncReplicationServerAdmin.java",
    "raw_url": "https://github.com/apache/hbase/raw/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/client/AsyncReplicationServerAdmin.java",
    "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/client/AsyncReplicationServerAdmin.java?ref=2135ff77c7727efd0ab12754283474dd49f42ba4",
    "patch": "@@ -29,6 +29,8 @@\n \n import org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos;\n import org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationServerProtos.ReplicationServerService;\n+import org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationServerProtos.StartReplicationSourceRequest;\n+import org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationServerProtos.StartReplicationSourceResponse;\n \n /**\n  * A simple wrapper of the {@link ReplicationServerService} for a replication server.\n@@ -70,11 +72,23 @@ void call(ReplicationServerService.Interface stub, HBaseRpcController controller\n     return future;\n   }\n \n+  private <RESP> CompletableFuture<RESP> call(RpcCall<RESP> rpcCall) {\n+    return call(rpcCall, null);\n+  }\n+\n   public CompletableFuture<AdminProtos.ReplicateWALEntryResponse> replicateWALEntry(\n       AdminProtos.ReplicateWALEntryRequest request, CellScanner cellScanner, int timeout) {\n     return call((stub, controller, done) -> {\n       controller.setCallTimeout(timeout);\n       stub.replicateWALEntry(controller, request, done);\n     }, cellScanner);\n   }\n+\n+  public CompletableFuture<StartReplicationSourceResponse> startReplicationSource(\n+    StartReplicationSourceRequest request, int timeout) {\n+    return call((stub, controller, done) -> {\n+      controller.setCallTimeout(timeout);\n+      stub.startReplicationSource(controller, request, done);\n+    });\n+  }\n }"
  },
  {
    "sha": "257074154f3d116aca0fd56cf17e2cab33ff34b0",
    "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/apache/hbase/blob/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
    "raw_url": "https://github.com/apache/hbase/raw/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java",
    "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java?ref=2135ff77c7727efd0ab12754283474dd49f42ba4",
    "patch": "@@ -3795,6 +3795,6 @@ public MetaLocationSyncer getMetaLocationSyncer() {\n \n   @Override\n   public List<ServerName> listReplicationSinkServers() throws IOException {\n-    return this.serverManager.getOnlineServersList();\n+    return this.replicationServerManager.getOnlineServersList();\n   }\n }"
  },
  {
    "sha": "a57b05cc5f35e8019ffdf095cccb4c5afd63b1a9",
    "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java",
    "status": "modified",
    "additions": 11,
    "deletions": 8,
    "changes": 19,
    "blob_url": "https://github.com/apache/hbase/blob/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java",
    "raw_url": "https://github.com/apache/hbase/raw/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java",
    "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java?ref=2135ff77c7727efd0ab12754283474dd49f42ba4",
    "patch": "@@ -75,7 +75,6 @@\n import org.apache.hadoop.hbase.ipc.ServerNotRunningYetException;\n import org.apache.hadoop.hbase.ipc.ServerRpcController;\n import org.apache.hadoop.hbase.master.assignment.RegionStates;\n-import org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure;\n import org.apache.hadoop.hbase.master.janitor.MetaFixer;\n import org.apache.hadoop.hbase.master.locking.LockProcedure;\n import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;\n@@ -401,6 +400,8 @@\n import org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationProtos.TransitReplicationPeerSyncReplicationStateResponse;\n import org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationProtos.UpdateReplicationPeerConfigRequest;\n import org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationProtos.UpdateReplicationPeerConfigResponse;\n+import org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationServerStatusProtos.ReplicationServerReportRequest;\n+import org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationServerStatusProtos.ReplicationServerReportResponse;\n import org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationServerStatusProtos.ReplicationServerStatusService;\n import org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos.SnapshotDescription;\n import org.apache.hadoop.hbase.shaded.protobuf.generated.VisibilityLabelsProtos.VisibilityLabelsService;\n@@ -3399,6 +3400,7 @@ public UpdateRSGroupConfigResponse updateRSGroupConfig(RpcController controller,\n       .addAllBalancerDecision(balancerDecisions).build();\n   }\n \n+  @Override\n   public ListReplicationSinkServersResponse listReplicationSinkServers(\n     RpcController controller, ListReplicationSinkServersRequest request)\n     throws ServiceException {\n@@ -3408,7 +3410,7 @@ public ListReplicationSinkServersResponse listReplicationSinkServers(\n       if (master.getMasterCoprocessorHost() != null) {\n         master.getMasterCoprocessorHost().preListReplicationSinkServers();\n       }\n-      builder.addAllServerName(master.getReplicationServerManager().getOnlineServersList().stream()\n+      builder.addAllServerName(master.listReplicationSinkServers().stream()\n         .map(ProtobufUtil::toServerName).collect(Collectors.toList()));\n       if (master.getMasterCoprocessorHost() != null) {\n         master.getMasterCoprocessorHost().postListReplicationSinkServers();\n@@ -3420,8 +3422,8 @@ public ListReplicationSinkServersResponse listReplicationSinkServers(\n   }\n \n   @Override\n-  public RegionServerReportResponse replicationServerReport(RpcController controller,\n-      RegionServerReportRequest request) throws ServiceException {\n+  public ReplicationServerReportResponse replicationServerReport(RpcController controller,\n+    ReplicationServerReportRequest request) throws ServiceException {\n     try {\n       master.checkServiceStarted();\n       int versionNumber = 0;\n@@ -3435,16 +3437,17 @@ public RegionServerReportResponse replicationServerReport(RpcController controll\n       ServerName serverName = ProtobufUtil.toServerName(request.getServer());\n       ServerMetrics oldMetrics = master.getReplicationServerManager().getServerMetrics(serverName);\n       ServerMetrics newMetrics =\n-          ServerMetricsBuilder.toServerMetrics(serverName, versionNumber, version, sl);\n-      master.getReplicationServerManager().serverReport(serverName, newMetrics);\n+        ServerMetricsBuilder.toServerMetrics(serverName, versionNumber, version, sl);\n+      Set<String> queueNodes = request.getQueueNodeList().stream().collect(Collectors.toSet());\n+      master.getReplicationServerManager().serverReport(serverName, newMetrics, queueNodes);\n       if (sl != null && master.metricsMaster != null) {\n         // Up our metrics.\n         master.metricsMaster.incrementRequests(sl.getTotalNumberOfRequests()\n-            - (oldMetrics != null ? oldMetrics.getRequestCount() : 0));\n+          - (oldMetrics != null ? oldMetrics.getRequestCount() : 0));\n       }\n     } catch (IOException ioe) {\n       throw new ServiceException(ioe);\n     }\n-    return RegionServerReportResponse.newBuilder().build();\n+    return ReplicationServerReportResponse.newBuilder().build();\n   }\n }"
  },
  {
    "sha": "c058414ed775aee55aebd2eb6f4259f9f87a05de",
    "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/ReplicationServerManager.java",
    "status": "modified",
    "additions": 104,
    "deletions": 55,
    "changes": 159,
    "blob_url": "https://github.com/apache/hbase/blob/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ReplicationServerManager.java",
    "raw_url": "https://github.com/apache/hbase/raw/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ReplicationServerManager.java",
    "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ReplicationServerManager.java?ref=2135ff77c7727efd0ab12754283474dd49f42ba4",
    "patch": "@@ -17,17 +17,29 @@\n  */\n package org.apache.hadoop.hbase.master;\n \n+import static org.apache.hadoop.hbase.protobuf.ReplicationProtobufUtil.buildStartReplicationSourceRequest;\n+\n+import java.io.IOException;\n import java.util.ArrayList;\n-import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n import java.util.List;\n import java.util.Map;\n+import java.util.Set;\n import java.util.concurrent.ConcurrentNavigableMap;\n import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.ThreadLocalRandom;\n+import java.util.stream.Collectors;\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.ScheduledChore;\n import org.apache.hadoop.hbase.ServerMetrics;\n import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.replication.ReplicationException;\n+import org.apache.hadoop.hbase.replication.ReplicationStorageFactory;\n+import org.apache.hadoop.hbase.replication.ZKReplicationQueueStorage;\n+import org.apache.hadoop.hbase.util.FutureUtils;\n+import org.apache.hadoop.hbase.util.Pair;\n import org.apache.yetus.audience.InterfaceAudience;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n@@ -49,19 +61,23 @@\n \n   private static final Logger LOG = LoggerFactory.getLogger(ReplicationServerManager.class);\n \n-  public static final String ONLINE_SERVER_REFRESH_INTERVAL =\n-      \"hbase.master.replication.server.refresh.interval\";\n-  public static final int ONLINE_SERVER_REFRESH_INTERVAL_DEFAULT = 60 * 1000; // 1 mins\n+  public static final String REPLICATION_SERVER_REFRESH_PERIOD =\n+      \"hbase.master.replication.server.refresh.period\";\n+  public static final int REPLICATION_SERVER_REFRESH_PERIOD_DEFAULT = 60 * 1000; // 1 mins\n \n   private final MasterServices master;\n \n-  /** Map of registered servers to their current load */\n-  private final ConcurrentNavigableMap<ServerName, ServerMetrics> onlineServers =\n+  /**\n+   * Map of registered servers to their current load\n+   */\n+  private final ConcurrentNavigableMap<ServerName, Pair<ServerMetrics, Set<String>>> onlineServers =\n     new ConcurrentSkipListMap<>();\n \n-  private OnlineServerRefresher onlineServerRefresher;\n+  private ReplicationServerRefresher refresher;\n   private int refreshPeriod;\n \n+  private ZKReplicationQueueStorage zkQueueStorage;\n+\n   /**\n    * Constructor.\n    */\n@@ -74,55 +90,54 @@ public ReplicationServerManager(final MasterServices master) {\n    */\n   public void startChore() {\n     Configuration conf = master.getConfiguration();\n-    refreshPeriod = conf.getInt(ONLINE_SERVER_REFRESH_INTERVAL,\n-        ONLINE_SERVER_REFRESH_INTERVAL_DEFAULT);\n-    onlineServerRefresher = new OnlineServerRefresher(\"ReplicationServerRefresher\", refreshPeriod);\n-    master.getChoreService().scheduleChore(onlineServerRefresher);\n+    this.zkQueueStorage = (ZKReplicationQueueStorage) ReplicationStorageFactory\n+      .getReplicationQueueStorage(master.getZooKeeper(), conf);\n+    refreshPeriod = conf.getInt(REPLICATION_SERVER_REFRESH_PERIOD,\n+      REPLICATION_SERVER_REFRESH_PERIOD_DEFAULT);\n+    refresher = new ReplicationServerRefresher(\"ReplicationServerRefresher\", refreshPeriod);\n+    master.getChoreService().scheduleChore(refresher);\n   }\n \n   /**\n    * Stop the ServerManager.\n    */\n   public void stop() {\n-    if (onlineServerRefresher != null) {\n-      onlineServerRefresher.cancel();\n+    if (refresher != null) {\n+      refresher.cancel();\n     }\n   }\n \n-  public void serverReport(ServerName sn, ServerMetrics sl) {\n-    if (null == this.onlineServers.replace(sn, sl)) {\n-      if (!checkAndRecordNewServer(sn, sl)) {\n-        LOG.info(\"ReplicationServerReport ignored, could not record the server: {}\", sn);\n-      }\n+  public void serverReport(ServerName sn, ServerMetrics sm, Set<String> queueNodes) {\n+    if (!onlineServers.containsKey(sn)) {\n+      tryRecordNewServer(sn, sm, queueNodes);\n+    } else {\n+      onlineServers.put(sn, new Pair<>(sm, queueNodes));\n     }\n   }\n \n   /**\n    * Check is a server of same host and port already exists,\n    * if not, or the existed one got a smaller start code, record it.\n-   *\n-   * @param serverName the server to check and record\n-   * @param sl the server load on the server\n-   * @return true if the server is recorded, otherwise, false\n    */\n-  private boolean checkAndRecordNewServer(final ServerName serverName, final ServerMetrics sl) {\n+  private void tryRecordNewServer(ServerName sn, ServerMetrics sm, Set<String> queueNodes) {\n     ServerName existingServer = null;\n     synchronized (this.onlineServers) {\n-      existingServer = findServerWithSameHostnamePort(serverName);\n-      if (existingServer != null && (existingServer.getStartcode() > serverName.getStartcode())) {\n+      existingServer = findServerWithSameHostnamePort(sn);\n+      if (existingServer != null && (existingServer.getStartcode() > sn.getStartcode())) {\n         LOG.info(\"ReplicationServer serverName={} rejected; we already have {} registered with \"\n-          + \"same hostname and port\", serverName, existingServer);\n-        return false;\n+          + \"same hostname and port\", sn, existingServer);\n+        return;\n       }\n-      recordNewServer(serverName, sl);\n+      LOG.info(\"Registering ReplicationServer={} assigned replication queues: {}\", sn,\n+        String.join(\",\", queueNodes));\n+      this.onlineServers.put(sn, new Pair<>(sm, queueNodes));\n       // Note that we assume that same ts means same server, and don't expire in that case.\n-      if (existingServer != null && (existingServer.getStartcode() < serverName.getStartcode())) {\n+      if (existingServer != null && (existingServer.getStartcode() < sn.getStartcode())) {\n         LOG.info(\"Triggering server recovery; existingServer {} looks stale, new server: {}\",\n-            existingServer, serverName);\n+            existingServer, sn);\n         expireServer(existingServer);\n       }\n     }\n-    return true;\n   }\n \n   /**\n@@ -132,22 +147,13 @@ private boolean checkAndRecordNewServer(final ServerName serverName, final Serve\n   private ServerName findServerWithSameHostnamePort(final ServerName serverName) {\n     ServerName end = ServerName.valueOf(serverName.getHostname(), serverName.getPort(),\n       Long.MAX_VALUE);\n-\n     ServerName r = onlineServers.lowerKey(end);\n     if (r != null && ServerName.isSameAddress(r, serverName)) {\n       return r;\n     }\n     return null;\n   }\n \n-  /**\n-   * Assumes onlineServers is locked.\n-   */\n-  private void recordNewServer(final ServerName serverName, final ServerMetrics sl) {\n-    LOG.info(\"Registering ReplicationServer={}\", serverName);\n-    this.onlineServers.put(serverName, sl);\n-  }\n-\n   /**\n    * Assumes onlineServers is locked.\n    * Expire the passed server. Remove it from list of online servers\n@@ -157,16 +163,6 @@ public void expireServer(final ServerName serverName) {\n     onlineServers.remove(serverName);\n   }\n \n-  /**\n-   * @return Read-only map of servers to serverinfo\n-   */\n-  public Map<ServerName, ServerMetrics> getOnlineServers() {\n-    // Presumption is that iterating the returned Map is OK.\n-    synchronized (this.onlineServers) {\n-      return Collections.unmodifiableMap(this.onlineServers);\n-    }\n-  }\n-\n   /**\n    * @return A copy of the internal list of online servers.\n    */\n@@ -179,26 +175,79 @@ public void expireServer(final ServerName serverName) {\n    * @return ServerMetrics if serverName is known else null\n    */\n   public ServerMetrics getServerMetrics(final ServerName serverName) {\n-    return this.onlineServers.get(serverName);\n+    if (!this.onlineServers.containsKey(serverName)) {\n+      return null;\n+    }\n+    return this.onlineServers.get(serverName).getFirst();\n   }\n \n-  private class OnlineServerRefresher extends ScheduledChore {\n+  /**\n+   * This chore is responsible for 3 things:\n+   * 1. Find all alive replication servers.\n+   * 2. Find all replication queues.\n+   * 3. Assign different queue to different replication server.\n+   */\n+  private class ReplicationServerRefresher extends ScheduledChore {\n \n-    public OnlineServerRefresher(String name, int p) {\n-      super(name, master, p, 60 * 1000); // delay one minute before first execute\n+    public ReplicationServerRefresher(String name, int p) {\n+      super(name, master, p, p);\n     }\n \n     @Override\n     protected void chore() {\n       synchronized (onlineServers) {\n         List<ServerName> servers = getOnlineServersList();\n         servers.forEach(s -> {\n-          ServerMetrics metrics = onlineServers.get(s);\n+          ServerMetrics metrics = onlineServers.get(s).getFirst();\n           if (metrics.getReportTimestamp() + refreshPeriod < System.currentTimeMillis()) {\n             expireServer(s);\n           }\n         });\n       }\n+      Set<String> assignedQueueNodes =\n+        onlineServers.values().stream().map(Pair::getSecond).flatMap(Set::stream)\n+          .collect(Collectors.toSet());\n+      Map<ServerName, Set<String>> unassigned = new HashMap<>();\n+      // Because all replication queues is owned by region servers. List all region servers and get\n+      // their replication queues.\n+      try {\n+        for (ServerName producer : zkQueueStorage.getListOfReplicators()) {\n+          List<String> queues = zkQueueStorage.getAllQueues(producer);\n+          for (String queue : queues) {\n+            String queueNode = zkQueueStorage.getQueueNode(producer, queue);\n+            LOG.debug(\"Found one replication queue {}\", queueNode);\n+            if (!assignedQueueNodes.contains(queueNode)) {\n+              unassigned.computeIfAbsent(producer, p -> new HashSet<>()).add(queue);\n+            }\n+          }\n+        }\n+      } catch(ReplicationException e){\n+        LOG.warn(\"Failed to get all replication queues\", e);\n+      }\n+\n+      ServerName[] consumers = getOnlineServersList().toArray(new ServerName[0]);\n+      if (consumers.length == 0) {\n+        LOG.warn(\"No replication server available!\");\n+        return;\n+      }\n+      // Assign different queue to different replication server\n+      for (Map.Entry<ServerName, Set<String>> entry : unassigned.entrySet()) {\n+        ServerName producer = entry.getKey();\n+        for (String queueId : entry.getValue()) {\n+          ServerName consumer = consumers[ThreadLocalRandom.current().nextInt(consumers.length)];\n+          try {\n+            FutureUtils.get(master.getAsyncClusterConnection().getReplicationServerAdmin(consumer)\n+              .startReplicationSource(buildStartReplicationSourceRequest(producer, queueId),\n+                10000));\n+            LOG.info(\"Started replication source on replication server {},\"\n+              + \" replication queue: producer={}, queueId={}\", consumer, producer, queueId);\n+          } catch (IOException e) {\n+            // Just log the exception and the replication queue will be reassigned in next chore\n+            LOG.warn(\"Failed to start replication source on replication server {},\"\n+              + \" replication queue: producer={}, queueId={}\", consumer, producer, queueId, e);\n+          }\n+        }\n+      }\n     }\n   }\n }"
  },
  {
    "sha": "c2a21a58b99d4e11b89b1fb294824168ce237bb9",
    "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/ReplicationPeerManager.java",
    "status": "modified",
    "additions": 1,
    "deletions": 2,
    "changes": 3,
    "blob_url": "https://github.com/apache/hbase/blob/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/ReplicationPeerManager.java",
    "raw_url": "https://github.com/apache/hbase/raw/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/ReplicationPeerManager.java",
    "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/master/replication/ReplicationPeerManager.java?ref=2135ff77c7727efd0ab12754283474dd49f42ba4",
    "patch": "@@ -103,8 +103,7 @@ private void checkQueuesDeleted(String peerId)\n     for (ServerName replicator : queueStorage.getListOfReplicators()) {\n       List<String> queueIds = queueStorage.getAllQueues(replicator);\n       for (String queueId : queueIds) {\n-        ReplicationQueueInfo queueInfo = new ReplicationQueueInfo(queueId);\n-        if (queueInfo.getPeerId().equals(peerId)) {\n+        if (ReplicationQueueInfo.parsePeerId(queueId).equals(peerId)) {\n           throw new DoNotRetryIOException(\"undeleted queue for peerId: \" + peerId +\n             \", replicator: \" + replicator + \", queueId: \" + queueId);\n         }"
  },
  {
    "sha": "c779e2ba9281574993a048e32d6a1b46cff1bdc3",
    "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/ReplicationProtobufUtil.java",
    "status": "modified",
    "additions": 10,
    "deletions": 0,
    "changes": 10,
    "blob_url": "https://github.com/apache/hbase/blob/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/ReplicationProtobufUtil.java",
    "raw_url": "https://github.com/apache/hbase/raw/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/ReplicationProtobufUtil.java",
    "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/ReplicationProtobufUtil.java?ref=2135ff77c7727efd0ab12754283474dd49f42ba4",
    "patch": "@@ -26,6 +26,7 @@\n import org.apache.hadoop.hbase.Cell;\n import org.apache.hadoop.hbase.CellScanner;\n import org.apache.hadoop.hbase.PrivateCellUtil;\n+import org.apache.hadoop.hbase.ServerName;\n import org.apache.hadoop.hbase.client.AsyncRegionServerAdmin;\n import org.apache.hadoop.hbase.client.AsyncReplicationServerAdmin;\n import org.apache.hadoop.hbase.io.SizedCellScanner;\n@@ -38,8 +39,11 @@\n \n import org.apache.hbase.thirdparty.com.google.protobuf.UnsafeByteOperations;\n \n+import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;\n import org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos.ReplicateWALEntryRequest;\n import org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos.WALEntry;\n+import org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationServerProtos;\n+import org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationServerProtos.StartReplicationSourceRequest;\n import org.apache.hadoop.hbase.shaded.protobuf.generated.WALProtos;\n \n @InterfaceAudience.Private\n@@ -185,4 +189,10 @@ public long heapSize() {\n       }\n     };\n   }\n+\n+  public static StartReplicationSourceRequest buildStartReplicationSourceRequest(\n+    ServerName producer, String queueId) {\n+    return ReplicationServerProtos.StartReplicationSourceRequest.newBuilder()\n+      .setServerName(ProtobufUtil.toServerName(producer)).setQueueId(queueId).build();\n+  }\n }"
  },
  {
    "sha": "231eed07682ea000826ea1a40e1c006823d6d1f9",
    "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/HBaseReplicationEndpoint.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/apache/hbase/blob/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/HBaseReplicationEndpoint.java",
    "raw_url": "https://github.com/apache/hbase/raw/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/HBaseReplicationEndpoint.java",
    "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/HBaseReplicationEndpoint.java?ref=2135ff77c7727efd0ab12754283474dd49f42ba4",
    "patch": "@@ -367,7 +367,7 @@ protected SinkPeer getReplicationSink() throws IOException {\n     return createSinkPeer(serverName);\n   }\n \n-  private SinkPeer createSinkPeer(ServerName serverName) throws IOException {\n+  private SinkPeer createSinkPeer(ServerName serverName) {\n     if (fetchServersUseZk) {\n       return new RegionServerSinkPeer(serverName, conn.getRegionServerAdmin(serverName));\n     } else {"
  },
  {
    "sha": "ccc31c86a29efff9ceafcba9421689d77e2438cd",
    "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/HReplicationServer.java",
    "status": "modified",
    "additions": 134,
    "deletions": 137,
    "changes": 271,
    "blob_url": "https://github.com/apache/hbase/blob/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/HReplicationServer.java",
    "raw_url": "https://github.com/apache/hbase/raw/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/HReplicationServer.java",
    "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/HReplicationServer.java?ref=2135ff77c7727efd0ab12754283474dd49f42ba4",
    "patch": "@@ -17,6 +17,9 @@\n  */\n package org.apache.hadoop.hbase.replication;\n \n+import static org.apache.hadoop.hbase.master.ReplicationServerManager.REPLICATION_SERVER_REFRESH_PERIOD;\n+import static org.apache.hadoop.hbase.master.ReplicationServerManager.REPLICATION_SERVER_REFRESH_PERIOD_DEFAULT;\n+\n import java.io.IOException;\n import java.lang.management.MemoryUsage;\n import java.net.InetSocketAddress;\n@@ -26,6 +29,7 @@\n import java.util.UUID;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ConcurrentMap;\n+import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.concurrent.atomic.AtomicLong;\n \n@@ -36,8 +40,10 @@\n import org.apache.hadoop.hbase.CoordinatedStateManager;\n import org.apache.hadoop.hbase.DoNotRetryIOException;\n import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.ScheduledChore;\n import org.apache.hadoop.hbase.Server;\n import org.apache.hadoop.hbase.ServerName;\n+import org.apache.hadoop.hbase.Stoppable;\n import org.apache.hadoop.hbase.YouAreDeadException;\n import org.apache.hadoop.hbase.client.AsyncClusterConnection;\n import org.apache.hadoop.hbase.client.ClusterConnectionFactory;\n@@ -79,7 +85,7 @@\n \n import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;\n import org.apache.hadoop.hbase.shaded.protobuf.generated.ClusterStatusProtos;\n-import org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos.RegionServerReportRequest;\n+import org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationServerStatusProtos.ReplicationServerReportRequest;\n import org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationServerStatusProtos.ReplicationServerStatusService;\n \n /**\n@@ -140,6 +146,10 @@\n   // master address tracker\n   private final MasterAddressTracker masterAddressTracker;\n \n+  private ServerName masterServerName;\n+  // Stub to do region server status calls against the master.\n+  private volatile ReplicationServerStatusService.BlockingInterface rssStub;\n+\n   /**\n    * The asynchronous cluster connection to be shared by services.\n    */\n@@ -158,17 +168,16 @@\n   private final ConcurrentMap<String, ReplicationSourceInterface> sources =\n     new ConcurrentHashMap<>();\n \n-  private final ReplicationQueueStorage queueStorage;\n+  private final ZKReplicationQueueStorage zkQueueStorage;\n   private final ReplicationPeers replicationPeers;\n \n-  // Stub to do region server status calls against the master.\n-  private volatile ReplicationServerStatusService.BlockingInterface rssStub;\n-\n   // RPC client. Used to make the stub above that does region server status checking.\n   private RpcClient rpcClient;\n \n   private ReplicationSinkService replicationSinkService;\n \n+  private final int statsPeriodInSecond;\n+\n   public HReplicationServer(final Configuration conf) throws Exception {\n     TraceUtil.initTracer(conf);\n     try {\n@@ -193,33 +202,30 @@ public HReplicationServer(final Configuration conf) throws Exception {\n       // or process owner as default super user.\n       Superusers.initialize(conf);\n \n-      this.msgInterval = conf.getInt(\"hbase.replicationserver.msginterval\", 3 * 1000);\n+      this.msgInterval = conf.getInt(\"hbase.replication.server.msginterval\", 3 * 1000);\n       this.sleeper = new Sleeper(this.msgInterval, this);\n \n       this.shortOperationTimeout = conf.getInt(HConstants.HBASE_RPC_SHORTOPERATION_TIMEOUT_KEY,\n           HConstants.DEFAULT_HBASE_RPC_SHORTOPERATION_TIMEOUT);\n       this.totalBufferLimit = conf.getLong(HConstants.REPLICATION_SOURCE_TOTAL_BUFFER_KEY,\n         HConstants.REPLICATION_SOURCE_TOTAL_BUFFER_DFAULT);\n+      this.statsPeriodInSecond = conf.getInt(\"replication.stats.thread.period.seconds\", 5 * 60);\n       this.globalMetrics =\n         CompatibilitySingletonFactory.getInstance(MetricsReplicationSourceFactory.class)\n           .getGlobalSource();\n \n       initializeFileSystem();\n       this.choreService = new ChoreService(getName(), true);\n \n-      // Some unit tests don't need a cluster, so no zookeeper at all\n-      if (!conf.getBoolean(\"hbase.testing.nocluster\", false)) {\n-        // Open connection to zookeeper and set primary watcher\n-        zooKeeper = new ZKWatcher(conf, getProcessName() + \":\" +\n-            rpcServices.isa.getPort(), this, false);\n-        masterAddressTracker = new MasterAddressTracker(getZooKeeper(), this);\n-        masterAddressTracker.start();\n-      } else {\n-        zooKeeper = null;\n-        masterAddressTracker = null;\n-      }\n+      // Open connection to zookeeper and set primary watcher\n+      zooKeeper =\n+        new ZKWatcher(conf, getProcessName() + \":\" + rpcServices.isa.getPort(), this, false);\n+      masterAddressTracker = new MasterAddressTracker(getZooKeeper(), this);\n+      masterAddressTracker.start();\n+\n+      this.zkQueueStorage = (ZKReplicationQueueStorage) ReplicationStorageFactory\n+        .getReplicationQueueStorage(zooKeeper, conf);\n \n-      this.queueStorage = ReplicationStorageFactory.getReplicationQueueStorage(zooKeeper, conf);\n       this.replicationPeers =\n         ReplicationFactory.getReplicationPeers(zooKeeper, this.conf);\n       this.replicationPeers.init();\n@@ -266,13 +272,19 @@ public void run() {\n \n       online.set(true);\n \n-      long lastMsg = System.currentTimeMillis();\n+      int refreshPeriod = conf.getInt(REPLICATION_SERVER_REFRESH_PERIOD,\n+        REPLICATION_SERVER_REFRESH_PERIOD_DEFAULT);\n+      long lastReportedTime = System.currentTimeMillis();\n       // The main run loop.\n       while (!isStopped()) {\n         long now = System.currentTimeMillis();\n-        if ((now - lastMsg) >= msgInterval) {\n-          tryReplicationServerReport(lastMsg, now);\n-          lastMsg = System.currentTimeMillis();\n+        if ((now - lastReportedTime) >= msgInterval) {\n+          if (tryReplicationServerReport(lastReportedTime, now)) {\n+            lastReportedTime = now;\n+          }\n+        }\n+        if ((now - lastReportedTime) >= refreshPeriod) {\n+          stop(\"Connection loss with master\");\n         }\n         if (!isStopped() && !isAborted()) {\n           this.sleeper.sleep();\n@@ -307,7 +319,7 @@ public void run() {\n     if (this.zooKeeper != null) {\n       this.zooKeeper.close();\n     }\n-    LOG.info(\"Exiting; stopping=\" + this.serverName + \"; zookeeper connection closed.\");\n+    LOG.info(\"Exiting; stopping={}; zookeeper connection closed.\", this.serverName);\n   }\n \n   private Configuration cleanupConfiguration() {\n@@ -501,6 +513,9 @@ private void startReplicationService() throws IOException {\n     if (this.replicationSinkService != null) {\n       this.replicationSinkService.startReplicationService();\n     }\n+    this.choreService.scheduleChore(\n+      new ReplicationStatisticsChore(\"ReplicationSourceStatistics\", this,\n+        (int) TimeUnit.SECONDS.toMillis(statsPeriodInSecond)));\n   }\n \n   /**\n@@ -534,39 +549,63 @@ protected boolean setAbortRequested() {\n     return abortRequested.compareAndSet(false, true);\n   }\n \n-  private void tryReplicationServerReport(long reportStartTime, long reportEndTime)\n-      throws IOException {\n-    ReplicationServerStatusService.BlockingInterface rss = rssStub;\n-    if (rss == null) {\n-      ServerName masterServerName = createReplicationServerStatusStub(true);\n-      rss = rssStub;\n-      if (masterServerName == null || rss == null) {\n-        return;\n+  /**\n+   * @return True if report to master successfully.\n+   */\n+  private boolean tryReplicationServerReport(long reportStartTime, long reportEndTime)\n+      throws YouAreDeadException {\n+    ServerName newMasterServerName = masterAddressTracker.getMasterAddress(true);\n+    if (newMasterServerName == null) {\n+      LOG.warn(\"No master found. Will retry report next time.\");\n+      return false;\n+    }\n+    if (masterServerName == null || !ServerName\n+      .isSameAddress(masterServerName, newMasterServerName)) {\n+      ReplicationServerStatusService.BlockingInterface intRssStub =\n+        createReplicationServerStatusStub(newMasterServerName);\n+      if (intRssStub == null) {\n+        return false;\n       }\n+      masterServerName = newMasterServerName;\n+      rssStub = intRssStub;\n     }\n-    ClusterStatusProtos.ServerLoad sl = buildServerLoad(reportStartTime, reportEndTime);\n+\n     try {\n-      RegionServerReportRequest.Builder request = RegionServerReportRequest\n-          .newBuilder();\n-      request.setServer(ProtobufUtil.toServerName(this.serverName));\n-      request.setLoad(sl);\n-      rss.replicationServerReport(null, request.build());\n+      rssStub.replicationServerReport(null, buildReportRequest(reportStartTime, reportEndTime));\n     } catch (ServiceException se) {\n       IOException ioe = ProtobufUtil.getRemoteException(se);\n       if (ioe instanceof YouAreDeadException) {\n-        // This will be caught and handled as a fatal error in run()\n-        throw ioe;\n+        throw (YouAreDeadException) ioe;\n       }\n-      if (rssStub == rss) {\n-        rssStub = null;\n+      LOG.warn(\"Failed to report to master {}\", masterServerName, ioe);\n+      return false;\n+    }\n+    return true;\n+  }\n+\n+  private ReplicationServerStatusService.BlockingInterface createReplicationServerStatusStub(\n+    ServerName sm) {\n+    try {\n+      BlockingRpcChannel channel = this.rpcClient\n+        .createBlockingRpcChannel(sm, userProvider.getCurrent(), shortOperationTimeout);\n+      return ReplicationServerStatusService.newBlockingStub(channel);\n+    } catch (IOException e) {\n+      e = e instanceof RemoteException ? ((RemoteException) e).unwrapRemoteException() : e;\n+      if (e instanceof ServerNotRunningYetException) {\n+        LOG.info(\"Master {} isn't available yet. Will retry report next time.\", sm);\n+      } else {\n+        LOG.warn(\"Unable to connect to master {}. Will retry report next time.\", sm, e);\n       }\n-      // Couldn't connect to the master, get location from zk and reconnect\n-      // Method blocks until new master is found or we are stopped\n-      createReplicationServerStatusStub(true);\n     }\n+    return null;\n   }\n \n-  private ClusterStatusProtos.ServerLoad buildServerLoad(long reportStartTime, long reportEndTime) {\n+  private ReplicationServerReportRequest buildReportRequest(long reportStartTime,\n+    long reportEndTime) {\n+    ReplicationServerReportRequest.Builder request = ReplicationServerReportRequest\n+      .newBuilder();\n+    request.setServer(ProtobufUtil.toServerName(this.serverName));\n+\n     long usedMemory = -1L;\n     long maxMemory = -1L;\n     final MemoryUsage usage = MemorySizeUtil.safeGetHeapMemoryUsage();\n@@ -594,92 +633,12 @@ private void tryReplicationServerReport(long reportStartTime, long reportEndTime\n         serverLoad.setReplLoadSink(rLoad.getReplicationLoadSink());\n       }\n     }\n-    return serverLoad.build();\n-  }\n-\n-  /**\n-   * Get the current master from ZooKeeper and open the RPC connection to it. To get a fresh\n-   * connection, the current rssStub must be null. Method will block until a master is available.\n-   * You can break from this block by requesting the server stop.\n-   * @param refresh If true then master address will be read from ZK, otherwise use cached data\n-   * @return master + port, or null if server has been stopped\n-   */\n-  private synchronized ServerName createReplicationServerStatusStub(boolean refresh) {\n-    if (rssStub != null) {\n-      return masterAddressTracker.getMasterAddress();\n-    }\n-    ServerName sn = null;\n-    long previousLogTime = 0;\n-    ReplicationServerStatusService.BlockingInterface intRssStub = null;\n-    boolean interrupted = false;\n-    try {\n-      while (keepLooping()) {\n-        sn = this.masterAddressTracker.getMasterAddress(refresh);\n-        if (sn == null) {\n-          if (!keepLooping()) {\n-            // give up with no connection.\n-            LOG.debug(\"No master found and cluster is stopped; bailing out\");\n-            return null;\n-          }\n-          if (System.currentTimeMillis() > (previousLogTime + 1000)) {\n-            LOG.debug(\"No master found; retry\");\n-            previousLogTime = System.currentTimeMillis();\n-          }\n-          refresh = true; // let's try pull it from ZK directly\n-          if (sleepInterrupted(200)) {\n-            interrupted = true;\n-          }\n-          continue;\n-        }\n-\n-        try {\n-          BlockingRpcChannel channel =\n-              this.rpcClient.createBlockingRpcChannel(sn, userProvider.getCurrent(),\n-                  shortOperationTimeout);\n-          intRssStub = ReplicationServerStatusService.newBlockingStub(channel);\n-          break;\n-        } catch (IOException e) {\n-          if (System.currentTimeMillis() > (previousLogTime + 1000)) {\n-            e = e instanceof RemoteException ?\n-                ((RemoteException)e).unwrapRemoteException() : e;\n-            if (e instanceof ServerNotRunningYetException) {\n-              LOG.info(\"Master isn't available yet, retrying\");\n-            } else {\n-              LOG.warn(\"Unable to connect to master. Retrying. Error was:\", e);\n-            }\n-            previousLogTime = System.currentTimeMillis();\n-          }\n-          if (sleepInterrupted(200)) {\n-            interrupted = true;\n-          }\n-        }\n-      }\n-    } finally {\n-      if (interrupted) {\n-        Thread.currentThread().interrupt();\n-      }\n-    }\n-    this.rssStub = intRssStub;\n-    return sn;\n-  }\n-\n-  /**\n-   * @return True if we should break loop because cluster is going down or\n-   *   this server has been stopped or hdfs has gone bad.\n-   */\n-  private boolean keepLooping() {\n-    return !this.stopped;\n-  }\n-\n-  private static boolean sleepInterrupted(long millis) {\n-    boolean interrupted = false;\n-    try {\n-      Thread.sleep(millis);\n-    } catch (InterruptedException e) {\n-      LOG.warn(\"Interrupted while sleeping\");\n-      interrupted = true;\n+    request.setLoad(serverLoad.build());\n+    for (ReplicationSourceInterface source : this.sources.values()) {\n+      request.addQueueNode(\n+        zkQueueStorage.getQueueNode(source.getServerWALsBelongTo(), source.getQueueId()));\n     }\n-    return interrupted;\n+    return request.build();\n   }\n \n   @Override\n@@ -706,20 +665,20 @@ public void finishRecoveredSource(RecoveredReplicationSource src) {\n       src.getStats());\n   }\n \n-  public void startReplicationSource(ServerName producer, String queueId)\n+  public void startReplicationSource(ServerName owner, String queueId)\n     throws IOException, ReplicationException {\n-    ReplicationQueueInfo replicationQueueInfo = new ReplicationQueueInfo(queueId);\n+    ReplicationQueueInfo replicationQueueInfo = new ReplicationQueueInfo(owner, queueId);\n     String peerId = replicationQueueInfo.getPeerId();\n     this.replicationPeers.addPeer(peerId);\n     Path walDir =\n-      new Path(walRootDir, AbstractFSWALProvider.getWALDirectoryName(producer.toString()));\n+      new Path(walRootDir, AbstractFSWALProvider.getWALDirectoryName(owner.toString()));\n     MetricsSource metrics = new MetricsSource(queueId);\n \n     ReplicationSourceInterface src = ReplicationSourceFactory.create(conf, queueId);\n     // init replication source\n-    src.init(conf, walFs, walDir, this, queueStorage, replicationPeers.getPeer(peerId), this,\n-      producer, queueId, clusterId, p -> OptionalLong.empty(), metrics);\n-    queueStorage.getWALsInQueue(producer, queueId)\n+    src.init(conf, walFs, walDir, this, zkQueueStorage, replicationPeers.getPeer(peerId), this,\n+      replicationQueueInfo, clusterId, p -> OptionalLong.empty(), metrics);\n+    zkQueueStorage.getWALsInQueue(owner, queueId)\n       .forEach(walName -> src.enqueueLog(new Path(walDir, walName)));\n     src.startup();\n     sources.put(queueId, src);\n@@ -731,7 +690,7 @@ public void startReplicationSource(ServerName producer, String queueId)\n    * @param queueId the id of replication queue to delete\n    */\n   private void deleteQueue(String queueId) {\n-    abortWhenFail(() -> this.queueStorage.removeQueue(getServerName(), queueId));\n+    abortWhenFail(() -> this.zkQueueStorage.removeQueue(getServerName(), queueId));\n   }\n \n   @FunctionalInterface\n@@ -746,4 +705,42 @@ private void abortWhenFail(ReplicationQueueOperation op) {\n       abort(\"Failed to operate on replication queue\", e);\n     }\n   }\n+\n+  /**\n+   * Get a string representation of all the sources' metrics\n+   */\n+  private String getStats() {\n+    StringBuilder stats = new StringBuilder();\n+    // Print stats that apply across all Replication Sources\n+    stats.append(\"Global stats: \");\n+    stats.append(\"WAL Edits Buffer Used=\").append(getTotalBufferUsed().get()).append(\"B, Limit=\")\n+      .append(getTotalBufferLimit()).append(\"B\\n\");\n+    for (ReplicationSourceInterface source : this.sources.values()) {\n+      if (source.isRecovered()) {\n+        stats.append(\"Recovered source for cluster/machine(s) \");\n+      } else {\n+        stats.append(\"Normal source for cluster \");\n+      }\n+      stats.append(source.getQueueId()).append(\": \").append(source.getStats()).append(\"\\n\");\n+    }\n+    return stats.toString();\n+  }\n+\n+  private final class ReplicationStatisticsChore extends ScheduledChore {\n+\n+    ReplicationStatisticsChore(String name, Stoppable stopper, int period) {\n+      super(name, stopper, period);\n+    }\n+\n+    @Override\n+    protected void chore() {\n+      printStats(getStats());\n+    }\n+\n+    private void printStats(String stats) {\n+      if (!stats.isEmpty()) {\n+        LOG.info(stats);\n+      }\n+    }\n+  }\n }"
  },
  {
    "sha": "29eaaf0e723fae2e6d0f585bfa13eee6fd84de91",
    "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/apache/hbase/blob/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java",
    "raw_url": "https://github.com/apache/hbase/raw/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java",
    "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.java?ref=2135ff77c7727efd0ab12754283474dd49f42ba4",
    "patch": "@@ -321,7 +321,7 @@ public String dumpQueues(ZKWatcher zkw, Set<String> peerIds,\n         deadRegionServers.add(regionserver.getServerName());\n       }\n       for (String queueId : queueIds) {\n-        ReplicationQueueInfo queueInfo = new ReplicationQueueInfo(queueId);\n+        ReplicationQueueInfo queueInfo = new ReplicationQueueInfo(regionserver, queueId);\n         List<String> wals = queueStorage.getWALsInQueue(regionserver, queueId);\n         Collections.sort(wals);\n         if (!peerIds.contains(queueInfo.getPeerId())) {"
  },
  {
    "sha": "b8286d8e9a2f10e297d9e7062b3187af9b75f4fa",
    "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RecoveredReplicationSource.java",
    "status": "modified",
    "additions": 5,
    "deletions": 7,
    "changes": 12,
    "blob_url": "https://github.com/apache/hbase/blob/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RecoveredReplicationSource.java",
    "raw_url": "https://github.com/apache/hbase/raw/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RecoveredReplicationSource.java",
    "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RecoveredReplicationSource.java?ref=2135ff77c7727efd0ab12754283474dd49f42ba4",
    "patch": "@@ -29,6 +29,7 @@\n import org.apache.hadoop.hbase.Server;\n import org.apache.hadoop.hbase.ServerName;\n import org.apache.hadoop.hbase.replication.ReplicationPeer;\n+import org.apache.hadoop.hbase.replication.ReplicationQueueInfo;\n import org.apache.hadoop.hbase.replication.ReplicationQueueStorage;\n import org.apache.hadoop.hbase.replication.ReplicationSourceController;\n import org.apache.hadoop.hbase.util.CommonFSUtils;\n@@ -46,17 +47,14 @@\n \n   private static final Logger LOG = LoggerFactory.getLogger(RecoveredReplicationSource.class);\n \n-  private String actualPeerId;\n-\n   @Override\n   public void init(Configuration conf, FileSystem fs, Path walDir,\n     ReplicationSourceController overallController, ReplicationQueueStorage queueStorage,\n-    ReplicationPeer replicationPeer, Server server, ServerName producer, String queueId,\n+    ReplicationPeer replicationPeer, Server server, ReplicationQueueInfo queueInfo,\n     UUID clusterId, WALFileLengthProvider walFileLengthProvider, MetricsSource metrics)\n     throws IOException {\n-    super.init(conf, fs, walDir, overallController, queueStorage, replicationPeer, server, producer,\n-      queueId, clusterId, walFileLengthProvider, metrics);\n-    this.actualPeerId = this.replicationQueueInfo.getPeerId();\n+    super.init(conf, fs, walDir, overallController, queueStorage, replicationPeer, server,\n+      queueInfo, clusterId, walFileLengthProvider, metrics);\n   }\n \n   @Override\n@@ -154,7 +152,7 @@ void tryFinish() {\n \n   @Override\n   public String getPeerId() {\n-    return this.actualPeerId;\n+    return this.replicationQueueInfo.getPeerId();\n   }\n \n   @Override"
  },
  {
    "sha": "974759d57e9c1cfbcae90e2b641e8e48d9f41426",
    "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
    "status": "modified",
    "additions": 8,
    "deletions": 2,
    "changes": 10,
    "blob_url": "https://github.com/apache/hbase/blob/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
    "raw_url": "https://github.com/apache/hbase/raw/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java",
    "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java?ref=2135ff77c7727efd0ab12754283474dd49f42ba4",
    "patch": "@@ -17,6 +17,9 @@\n  */\n package org.apache.hadoop.hbase.replication.regionserver;\n \n+import static org.apache.hadoop.hbase.HConstants.REPLICATION_OFFLOAD_ENABLE_DEFAULT;\n+import static org.apache.hadoop.hbase.HConstants.REPLICATION_OFFLOAD_ENABLE_KEY;\n+\n import java.io.IOException;\n import java.util.ArrayList;\n import java.util.List;\n@@ -170,9 +173,12 @@ public void stopReplicationService() {\n   @Override\n   public void startReplicationService() throws IOException {\n     this.replicationManager.init();\n-    this.server.getChoreService().scheduleChore(\n-      new ReplicationStatisticsChore(\"ReplicationSourceStatistics\", server,\n+    // No need to start ReplicationStatisticsChore if replication offload enabled\n+    if (!conf.getBoolean(REPLICATION_OFFLOAD_ENABLE_KEY, REPLICATION_OFFLOAD_ENABLE_DEFAULT)) {\n+      this.server.getChoreService().scheduleChore(\n+        new ReplicationStatisticsChore(\"ReplicationSourceStatistics\", server,\n           (int) TimeUnit.SECONDS.toMillis(statsPeriodInSecond)));\n+    }\n     LOG.info(\"{} started\", this.server.toString());\n   }\n "
  },
  {
    "sha": "5e6e4996f5c82b8eb04795de5937285f1d6eeaab",
    "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
    "status": "modified",
    "additions": 13,
    "deletions": 22,
    "changes": 35,
    "blob_url": "https://github.com/apache/hbase/blob/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
    "raw_url": "https://github.com/apache/hbase/raw/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java",
    "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java?ref=2135ff77c7727efd0ab12754283474dd49f42ba4",
    "patch": "@@ -111,8 +111,6 @@\n   private UUID clusterId;\n   // total number of edits we replicated\n   private AtomicLong totalReplicatedEdits = new AtomicLong(0);\n-  // The znode we currently play with\n-  protected String queueId;\n   // Maximum number of retries before taking bold actions\n   private int maxRetriesMultiplier;\n   // Indicates if this particular source is running\n@@ -151,7 +149,6 @@\n   private int waitOnEndpointSeconds = -1;\n \n   private Thread initThread;\n-  private Thread fetchWALsThread;\n \n   /**\n    * WALs to replicate.\n@@ -189,7 +186,7 @@\n   @Override\n   public void init(Configuration conf, FileSystem fs, Path walDir,\n     ReplicationSourceController overallController, ReplicationQueueStorage queueStorage,\n-    ReplicationPeer replicationPeer, Server server, ServerName producer, String queueId,\n+    ReplicationPeer replicationPeer, Server server, ReplicationQueueInfo queueInfo,\n     UUID clusterId, WALFileLengthProvider walFileLengthProvider, MetricsSource metrics)\n     throws IOException {\n     this.server = server;\n@@ -211,8 +208,7 @@ public void init(Configuration conf, FileSystem fs, Path walDir,\n     this.metrics = metrics;\n     this.clusterId = clusterId;\n \n-    this.queueId = queueId;\n-    this.replicationQueueInfo = new ReplicationQueueInfo(queueId);\n+    this.replicationQueueInfo = queueInfo;\n \n     // A defaultBandwidth of '0' means no bandwidth; i.e. no throttling.\n     defaultBandwidth = this.conf.getLong(\"replication.source.per.peer.node.bandwidth\", 0);\n@@ -227,16 +223,16 @@ public void init(Configuration conf, FileSystem fs, Path walDir,\n       HConstants.REPLICATION_OFFLOAD_ENABLE_DEFAULT)) {\n       if (queueStorage instanceof ZKReplicationQueueStorage) {\n         ZKReplicationQueueStorage zkQueueStorage = (ZKReplicationQueueStorage) queueStorage;\n-        zkQueueStorage.getZookeeper().registerListener(\n-          new ReplicationQueueListener(this, zkQueueStorage, producer, queueId, walDir));\n+        zkQueueStorage.getZookeeper().registerListener(new ReplicationQueueListener(this,\n+          zkQueueStorage, replicationQueueInfo.getOwner(), getQueueId(), walDir));\n         LOG.info(\"Register a ZKListener to track the WALs from {}'s replication queue, queueId={}\",\n-          producer, queueId);\n+          replicationQueueInfo.getOwner(), getQueueId());\n       } else {\n         throw new UnsupportedOperationException(\n           \"hbase.replication.offload.enabled=true only support ZKReplicationQueueStorage\");\n       }\n     }\n-    LOG.info(\"queueId={}, ReplicationSource: {}, currentBandwidth={}\", queueId,\n+    LOG.info(\"queueId={}, ReplicationSource: {}, currentBandwidth={}\", getQueueId(),\n       replicationPeer.getId(), this.currentBandwidth);\n   }\n \n@@ -266,8 +262,7 @@ public void enqueueLog(Path wal) {\n       }\n     }\n     if (LOG.isTraceEnabled()) {\n-      LOG.trace(\"{} Added wal {} to queue of source {}.\", logPeerId(), walPrefix,\n-        this.replicationQueueInfo.getQueueId());\n+      LOG.trace(\"{} Added wal {} to queue of source {}.\", logPeerId(), walPrefix, getQueueId());\n     }\n   }\n \n@@ -340,7 +335,7 @@ private void tryStartNewShipper(String walGroupId) {\n             createNewWALReader(walGroupId, worker.getStartPosition());\n         Threads.setDaemonThreadRunning(\n             walReader, Thread.currentThread().getName()\n-            + \".replicationSource.wal-reader.\" + walGroupId + \",\" + queueId,\n+            + \".replicationSource.wal-reader.\" + walGroupId + \",\" + getQueueId(),\n           (t,e) -> this.uncaughtException(t, e, null, this.getPeerId()));\n         worker.setWALReader(walReader);\n         worker.startup((t,e) -> this.uncaughtException(t, e, null, this.getPeerId()));\n@@ -579,7 +574,7 @@ public ReplicationSourceInterface startup() {\n     startupOngoing.set(true);\n     initThread = new Thread(this::initialize);\n     Threads.setDaemonThreadRunning(initThread,\n-      Thread.currentThread().getName() + \".replicationSource,\" + this.queueId,\n+      Thread.currentThread().getName() + \".replicationSource,\" + getQueueId(),\n       (t,e) -> {\n         //if first initialization attempt failed, and abortOnError is false, we will\n         //keep looping in this thread until initialize eventually succeeds,\n@@ -623,10 +618,10 @@ public void terminate(String reason, Exception cause, boolean clearMetrics) {\n   public void terminate(String reason, Exception cause, boolean clearMetrics,\n       boolean join) {\n     if (cause == null) {\n-      LOG.info(\"{} Closing source {} because: {}\", logPeerId(), this.queueId, reason);\n+      LOG.info(\"{} Closing source {} because: {}\", logPeerId(), getQueueId(), reason);\n     } else {\n       LOG.error(\"{} Closing source {} because an error occurred: {}\",\n-        logPeerId(), this.queueId, reason, cause);\n+        logPeerId(), getQueueId(), reason, cause);\n     }\n     this.sourceRunning = false;\n     if (initThread != null && Thread.currentThread() != initThread) {\n@@ -683,7 +678,7 @@ public void terminate(String reason, Exception cause, boolean clearMetrics,\n             TimeUnit.MILLISECONDS);\n         } catch (TimeoutException te) {\n           LOG.warn(\"{} Got exception while waiting for endpoint to shutdown \"\n-            + \"for replication source : {}\", logPeerId(), this.queueId, te);\n+            + \"for replication source : {}\", logPeerId(), getQueueId(), te);\n         }\n       }\n     }\n@@ -695,11 +690,6 @@ public void terminate(String reason, Exception cause, boolean clearMetrics,\n     }\n   }\n \n-  @Override\n-  public String getQueueId() {\n-    return this.queueId;\n-  }\n-\n   @Override\n   public Path getCurrentPath() {\n     // only for testing\n@@ -716,6 +706,7 @@ public boolean isSourceActive() {\n     return !this.server.isStopped() && this.sourceRunning;\n   }\n \n+  @Override\n   public ReplicationQueueInfo getReplicationQueueInfo() {\n     return replicationQueueInfo;\n   }"
  },
  {
    "sha": "6254af5e24ce59d7bdce7846e832a3f623d72a36",
    "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceFactory.java",
    "status": "modified",
    "additions": 1,
    "deletions": 2,
    "changes": 3,
    "blob_url": "https://github.com/apache/hbase/blob/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceFactory.java",
    "raw_url": "https://github.com/apache/hbase/raw/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceFactory.java",
    "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceFactory.java?ref=2135ff77c7727efd0ab12754283474dd49f42ba4",
    "patch": "@@ -36,8 +36,7 @@\n   private ReplicationSourceFactory() {}\n \n   public static ReplicationSourceInterface create(Configuration conf, String queueId) {\n-    ReplicationQueueInfo replicationQueueInfo = new ReplicationQueueInfo(queueId);\n-    boolean isQueueRecovered = replicationQueueInfo.isQueueRecovered();\n+    boolean isQueueRecovered = ReplicationQueueInfo.isQueueRecovered(queueId);\n     ReplicationSourceInterface src;\n     try {\n       String defaultReplicationSourceImpl ="
  },
  {
    "sha": "1122e5e1c39af40a6d233881e46a92a0b3893763",
    "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceInterface.java",
    "status": "modified",
    "additions": 13,
    "deletions": 4,
    "changes": 17,
    "blob_url": "https://github.com/apache/hbase/blob/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceInterface.java",
    "raw_url": "https://github.com/apache/hbase/raw/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceInterface.java",
    "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceInterface.java?ref=2135ff77c7727efd0ab12754283474dd49f42ba4",
    "patch": "@@ -30,6 +30,7 @@\n import org.apache.hadoop.hbase.ServerName;\n import org.apache.hadoop.hbase.replication.ReplicationEndpoint;\n import org.apache.hadoop.hbase.replication.ReplicationPeer;\n+import org.apache.hadoop.hbase.replication.ReplicationQueueInfo;\n import org.apache.hadoop.hbase.replication.ReplicationQueueStorage;\n import org.apache.hadoop.hbase.replication.ReplicationSourceController;\n import org.apache.hadoop.hbase.wal.WAL.Entry;\n@@ -50,15 +51,14 @@\n    * @param queueStorage the replication queue storage\n    * @param replicationPeer the replication peer\n    * @param server the server which start and run this replication source\n-   * @param producer the name of region server which produce WAL to the replication queue\n-   * @param queueId the id of our replication queue\n+   * @param queueInfo the replication queue\n    * @param clusterId unique UUID for the cluster\n    * @param walFileLengthProvider used to get the WAL length\n    * @param metrics metrics for this replication source\n    */\n   void init(Configuration conf, FileSystem fs, Path walDir,\n     ReplicationSourceController overallController, ReplicationQueueStorage queueStorage,\n-    ReplicationPeer replicationPeer, Server server, ServerName producer, String queueId,\n+    ReplicationPeer replicationPeer, Server server, ReplicationQueueInfo queueInfo,\n     UUID clusterId, WALFileLengthProvider walFileLengthProvider, MetricsSource metrics)\n     throws IOException;\n \n@@ -105,7 +105,16 @@ void init(Configuration conf, FileSystem fs, Path walDir,\n    *\n    * @return queue id\n    */\n-  String getQueueId();\n+  default String getQueueId() {\n+    return getReplicationQueueInfo().getQueueId();\n+  }\n+\n+  /**\n+   * Get the replication queue info\n+   *\n+   * @return the replication queue info\n+   */\n+  ReplicationQueueInfo getReplicationQueueInfo();\n \n   /**\n    * Get the id that the source is replicating to."
  },
  {
    "sha": "d7fdb34d34df19c45ab5df450da3fcf27492abec",
    "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java",
    "status": "modified",
    "additions": 28,
    "deletions": 41,
    "changes": 69,
    "blob_url": "https://github.com/apache/hbase/blob/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java",
    "raw_url": "https://github.com/apache/hbase/raw/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java",
    "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java?ref=2135ff77c7727efd0ab12754283474dd49f42ba4",
    "patch": "@@ -147,8 +147,6 @@\n    */\n   AtomicReference<ReplicationSourceInterface> catalogReplicationSource = new AtomicReference<>();\n \n-  private final Map<String, MetricsSource> sourceMetrics = new HashMap<>();\n-\n   /**\n    * When enable replication offload, will not create replication source and only write WAL to\n    * replication queue storage. The replication source will be started by ReplicationServer.\n@@ -357,14 +355,15 @@ public void removePeer(String peerId) {\n   private ReplicationSourceInterface createSource(String queueId, ReplicationPeer replicationPeer)\n       throws IOException {\n     ReplicationSourceInterface src = ReplicationSourceFactory.create(conf, queueId);\n+    ReplicationQueueInfo queueInfo = new ReplicationQueueInfo(server.getServerName(), queueId);\n     // Init the just created replication source. Pass the default walProvider's wal file length\n     // provider. Presumption is we replicate user-space Tables only. For hbase:meta region replica\n     // replication, see #createCatalogReplicationSource().\n     WALFileLengthProvider walFileLengthProvider =\n-      this.walFactory.getWALProvider() != null?\n+      this.walFactory.getWALProvider() != null ?\n         this.walFactory.getWALProvider().getWALFileLengthProvider() : p -> OptionalLong.empty();\n-    src.init(conf, fs, logDir, this, queueStorage, replicationPeer, server, server.getServerName(),\n-      queueId, clusterId, walFileLengthProvider, new MetricsSource(queueId));\n+    src.init(conf, fs, logDir, this, queueStorage, replicationPeer, server, queueInfo,\n+      clusterId, walFileLengthProvider, new MetricsSource(queueId));\n     return src;\n   }\n \n@@ -716,8 +715,7 @@ public void run() {\n         Set<String> walsSet = entry.getValue();\n         try {\n           // there is not an actual peer defined corresponding to peerId for the failover.\n-          ReplicationQueueInfo replicationQueueInfo = new ReplicationQueueInfo(queueId);\n-          String actualPeerId = replicationQueueInfo.getPeerId();\n+          String actualPeerId = ReplicationQueueInfo.parsePeerId(queueId);\n \n           ReplicationPeerImpl peer = replicationPeers.getPeer(actualPeerId);\n           if (peer == null || !isOldPeer(actualPeerId, peer)) {\n@@ -784,6 +782,9 @@ public void join() {\n     for (ReplicationSourceInterface source : this.sources.values()) {\n       source.terminate(\"Region server is closing\");\n     }\n+    for (ReplicationSourceInterface source : this.oldsources) {\n+      source.terminate(\"Region server is closing\");\n+    }\n   }\n \n   /**\n@@ -795,12 +796,7 @@ public void join() {\n     Map<String, Map<String, NavigableSet<String>>> walsById = new HashMap<>();\n     for (ReplicationSourceInterface source : sources.values()) {\n       String queueId = source.getQueueId();\n-      Map<String, NavigableSet<String>> walsByGroup = new HashMap<>();\n-      walsById.put(queueId, walsByGroup);\n-      for (String wal : this.queueStorage.getWALsInQueue(this.server.getServerName(), queueId)) {\n-        String walPrefix = AbstractFSWALProvider.getWALPrefixFromWALName(wal);\n-        walsByGroup.computeIfAbsent(walPrefix, p -> new TreeSet<>()).add(wal);\n-      }\n+      walsById.put(queueId, getWALsByQueueId(queueId));\n     }\n     return Collections.unmodifiableMap(walsById);\n   }\n@@ -814,16 +810,21 @@ public void join() {\n     Map<String, Map<String, NavigableSet<String>>> walsByIdRecoveredQueues = new HashMap<>();\n     for (ReplicationSourceInterface source : oldsources) {\n       String queueId = source.getQueueId();\n-      Map<String, NavigableSet<String>> walsByGroup = new HashMap<>();\n-      walsByIdRecoveredQueues.put(queueId, walsByGroup);\n-      for (String wal : this.queueStorage.getWALsInQueue(this.server.getServerName(), queueId)) {\n-        String walPrefix = AbstractFSWALProvider.getWALPrefixFromWALName(wal);\n-        walsByGroup.computeIfAbsent(walPrefix, p -> new TreeSet<>()).add(wal);\n-      }\n+      walsByIdRecoveredQueues.put(queueId, getWALsByQueueId(queueId));\n     }\n     return Collections.unmodifiableMap(walsByIdRecoveredQueues);\n   }\n \n+  private Map<String, NavigableSet<String>> getWALsByQueueId(String queueId)\n+    throws ReplicationException {\n+    Map<String, NavigableSet<String>> walsByGroup = new HashMap<>();\n+    for (String wal : this.queueStorage.getWALsInQueue(this.server.getServerName(), queueId)) {\n+      String walPrefix = AbstractFSWALProvider.getWALPrefixFromWALName(wal);\n+      walsByGroup.computeIfAbsent(walPrefix, p -> new TreeSet<>()).add(wal);\n+    }\n+    return walsByGroup;\n+  }\n+\n   /**\n    * Get a list of all the normal sources of this rs\n    * @return list of all normal sources\n@@ -939,23 +940,24 @@ public String getStats() {\n   public void addHFileRefs(TableName tableName, byte[] family, List<Pair<Path, Path>> pairs)\n       throws IOException {\n     for (ReplicationSourceInterface source : this.sources.values()) {\n-      throwIOExceptionWhenFail(() -> addHFileRefs(source.getPeerId(), tableName, family, pairs));\n+      throwIOExceptionWhenFail(() -> addHFileRefs(source, tableName, family, pairs));\n     }\n   }\n \n   /**\n    * Add hfile names to the queue to be replicated.\n-   * @param peerId the replication peer id\n+   * @param source the replication source\n    * @param tableName Name of the table these files belongs to\n    * @param family Name of the family these files belong to\n    * @param pairs list of pairs of { HFile location in staging dir, HFile path in region dir which\n    *          will be added in the queue for replication}\n    * @throws ReplicationException If failed to add hfile references\n    */\n-  private void addHFileRefs(String peerId, TableName tableName, byte[] family,\n+  private void addHFileRefs(ReplicationSourceInterface source, TableName tableName, byte[] family,\n     List<Pair<Path, Path>> pairs) throws ReplicationException {\n+    String peerId = source.getPeerId();\n     // Only the normal replication source update here, its peerId is equals to queueId.\n-    MetricsSource metrics = sourceMetrics.get(peerId);\n+    MetricsSource metrics = source.getSourceMetrics();\n     ReplicationPeer replicationPeer = replicationPeers.getPeer(peerId);\n     Set<String> namespaces = replicationPeer.getNamespaces();\n     Map<TableName, List<String>> tableCFMap = replicationPeer.getTableCFs();\n@@ -1050,8 +1052,10 @@ private ReplicationSourceInterface createCatalogReplicationSource(RegionInfo reg\n     CatalogReplicationSourcePeer peer = new CatalogReplicationSourcePeer(this.conf,\n       this.clusterId.toString());\n     final ReplicationSourceInterface crs = new CatalogReplicationSource();\n+    ReplicationQueueInfo queueInfo =\n+      new ReplicationQueueInfo(server.getServerName(), peer.getId());\n     crs.init(conf, fs, logDir, this, new NoopReplicationQueueStorage(), peer, server,\n-      server.getServerName(), peer.getId(), clusterId, walProvider.getWALFileLengthProvider(),\n+      queueInfo, clusterId, walProvider.getWALFileLengthProvider(),\n       new MetricsSource(peer.getId()));\n     // Add listener on the provider so we can pick up the WAL to replicate on roll.\n     WALActionsListener listener = new WALActionsListener() {\n@@ -1071,21 +1075,4 @@ private ReplicationSourceInterface createCatalogReplicationSource(RegionInfo reg\n     }\n     return crs.startup();\n   }\n-\n-  private NavigableSet<String> getWalsToRemove(String queueId, String log, boolean inclusive) {\n-    NavigableSet<String> walsToRemove = new TreeSet<>();\n-    String logPrefix = AbstractFSWALProvider.getWALPrefixFromWALName(log);\n-    try {\n-      this.queueStorage.getWALsInQueue(this.server.getServerName(), queueId).forEach(wal -> {\n-        String walPrefix = AbstractFSWALProvider.getWALPrefixFromWALName(wal);\n-        if (walPrefix.equals(logPrefix)) {\n-          walsToRemove.add(wal);\n-        }\n-      });\n-    } catch (ReplicationException e) {\n-      // Just log the exception here, as the recovered replication source will try to cleanup again.\n-      LOG.warn(\"Failed to read wals in queue {}\", queueId, e);\n-    }\n-    return walsToRemove.headSet(log, inclusive);\n-  }\n }"
  },
  {
    "sha": "9a535eaaa0e467ad70a8c82b9790080ff01d4477",
    "filename": "hbase-server/src/main/java/org/apache/hadoop/hbase/util/hbck/ReplicationChecker.java",
    "status": "modified",
    "additions": 5,
    "deletions": 7,
    "changes": 12,
    "blob_url": "https://github.com/apache/hbase/blob/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/util/hbck/ReplicationChecker.java",
    "raw_url": "https://github.com/apache/hbase/raw/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/main/java/org/apache/hadoop/hbase/util/hbck/ReplicationChecker.java",
    "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/main/java/org/apache/hadoop/hbase/util/hbck/ReplicationChecker.java?ref=2135ff77c7727efd0ab12754283474dd49f42ba4",
    "patch": "@@ -69,13 +69,12 @@ public boolean hasUnDeletedQueues() {\n     Set<String> peerIds = new HashSet<>(peerStorage.listPeerIds());\n     for (ServerName replicator : queueStorage.getListOfReplicators()) {\n       for (String queueId : queueStorage.getAllQueues(replicator)) {\n-        ReplicationQueueInfo queueInfo = new ReplicationQueueInfo(queueId);\n-        if (!peerIds.contains(queueInfo.getPeerId())) {\n+        String peerId = ReplicationQueueInfo.parsePeerId(queueId);\n+        if (!peerIds.contains(peerId)) {\n           undeletedQueues.computeIfAbsent(replicator, key -> new ArrayList<>()).add(queueId);\n           LOG.debug(\n             \"Undeleted replication queue for removed peer found: \" +\n-              \"[removedPeerId={}, replicator={}, queueId={}]\",\n-            queueInfo.getPeerId(), replicator, queueId);\n+              \"[removedPeerId={}, replicator={}, queueId={}]\", peerId, replicator, queueId);\n         }\n       }\n     }\n@@ -99,10 +98,9 @@ public void checkUnDeletedQueues() throws ReplicationException {\n     undeletedQueueIds = getUnDeletedQueues();\n     undeletedQueueIds.forEach((replicator, queueIds) -> {\n       queueIds.forEach(queueId -> {\n-        ReplicationQueueInfo queueInfo = new ReplicationQueueInfo(queueId);\n         String msg = \"Undeleted replication queue for removed peer found: \" +\n-          String.format(\"[removedPeerId=%s, replicator=%s, queueId=%s]\", queueInfo.getPeerId(),\n-            replicator, queueId);\n+          String.format(\"[removedPeerId=%s, replicator=%s, queueId=%s]\",\n+            ReplicationQueueInfo.parsePeerId(queueId), replicator, queueId);\n         errorReporter.reportError(HbckErrorReporter.ERROR_CODE.UNDELETED_REPLICATION_QUEUE, msg);\n       });\n     });"
  },
  {
    "sha": "185d3cd56d13aefe47d09c2b28eb10d996e51b17",
    "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/ReplicationSourceDummy.java",
    "status": "modified",
    "additions": 5,
    "deletions": 11,
    "changes": 16,
    "blob_url": "https://github.com/apache/hbase/blob/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/ReplicationSourceDummy.java",
    "raw_url": "https://github.com/apache/hbase/raw/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/ReplicationSourceDummy.java",
    "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/ReplicationSourceDummy.java?ref=2135ff77c7727efd0ab12754283474dd49f42ba4",
    "patch": "@@ -39,6 +39,7 @@\n public class ReplicationSourceDummy implements ReplicationSourceInterface {\n \n   private ReplicationPeer replicationPeer;\n+  private ReplicationQueueInfo replicationQueueInfo;\n   private String queueId;\n   private Path currentPath;\n   private MetricsSource metrics;\n@@ -48,10 +49,10 @@\n   @Override\n   public void init(Configuration conf, FileSystem fs, Path walDir,\n     ReplicationSourceController overallController, ReplicationQueueStorage queueStorage,\n-    ReplicationPeer replicationPeer, Server server, ServerName producer, String queueId,\n+    ReplicationPeer replicationPeer, Server server, ReplicationQueueInfo queueInfo,\n     UUID clusterId, WALFileLengthProvider walFileLengthProvider, MetricsSource metrics)\n     throws IOException {\n-    this.queueId = queueId;\n+    this.replicationQueueInfo = queueInfo;\n     this.metrics = metrics;\n     this.walFileLengthProvider = walFileLengthProvider;\n     this.replicationPeer = replicationPeer;\n@@ -96,15 +97,8 @@ public void terminate(String reason, Exception e, boolean clearMetrics) {\n   }\n \n   @Override\n-  public String getQueueId() {\n-    return queueId;\n-  }\n-\n-  @Override\n-  public String getPeerId() {\n-    String[] parts = queueId.split(\"-\", 2);\n-    return parts.length != 1 ?\n-        parts[0] : queueId;\n+  public ReplicationQueueInfo getReplicationQueueInfo() {\n+    return replicationQueueInfo;\n   }\n \n   @Override"
  },
  {
    "sha": "5d5d5694bf6b90baa9dadf175a0b915d19dc359e",
    "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestMultiReplicationServers.java",
    "status": "added",
    "additions": 73,
    "deletions": 0,
    "changes": 73,
    "blob_url": "https://github.com/apache/hbase/blob/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestMultiReplicationServers.java",
    "raw_url": "https://github.com/apache/hbase/raw/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestMultiReplicationServers.java",
    "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestMultiReplicationServers.java?ref=2135ff77c7727efd0ab12754283474dd49f42ba4",
    "patch": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hbase.replication;\n+\n+import static org.apache.hadoop.hbase.master.ReplicationServerManager.REPLICATION_SERVER_REFRESH_PERIOD;\n+import static org.apache.hadoop.hbase.replication.ReplicationServerRpcServices.REPLICATION_SERVER_PORT;\n+\n+import org.apache.hadoop.hbase.HBaseClassTestRule;\n+import org.apache.hadoop.hbase.HConstants;\n+import org.apache.hadoop.hbase.testclassification.LargeTests;\n+import org.apache.hadoop.hbase.testclassification.ReplicationTests;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@Category({ ReplicationTests.class, LargeTests.class })\n+public class TestMultiReplicationServers extends TestReplicationBase {\n+\n+  @ClassRule\n+  public static final HBaseClassTestRule CLASS_RULE =\n+    HBaseClassTestRule.forClass(TestMultiReplicationServers.class);\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(TestMultiReplicationServers.class);\n+\n+  private static HReplicationServer[] replicationServers;\n+\n+  @BeforeClass\n+  public static void setUpBeforeClass() throws Exception {\n+    // 3 RegionServers and 3 ReplicationServers\n+    NUM_SLAVES1 = 3;\n+    UTIL1.getConfiguration().setInt(REPLICATION_SERVER_PORT, 0);\n+    UTIL1.getConfiguration().setBoolean(HConstants.REPLICATION_OFFLOAD_ENABLE_KEY, true);\n+    UTIL1.getConfiguration().setLong(REPLICATION_SERVER_REFRESH_PERIOD, 10000);\n+    TestReplicationBase.setUpBeforeClass();\n+    replicationServers = new HReplicationServer[NUM_SLAVES1];\n+    for (int i = 0; i < NUM_SLAVES1; i++) {\n+      replicationServers[i] = new HReplicationServer(UTIL1.getConfiguration());\n+      replicationServers[i].start();\n+    }\n+  }\n+\n+  @AfterClass\n+  public static void tearDownAfterClass() throws Exception {\n+    for (int i = 0; i < NUM_SLAVES1; i++) {\n+      replicationServers[i].stop(\"Tear down after test\");\n+    }\n+    TestReplicationBase.tearDownAfterClass();\n+  }\n+\n+  @Test\n+  public void test() throws Exception {\n+    runSmallBatchTest();\n+  }\n+}\n\\ No newline at end of file"
  },
  {
    "sha": "9e66917d36cbe047f1d7f4c55cd387b5e7e29aa3",
    "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationServerSink.java",
    "status": "modified",
    "additions": 10,
    "deletions": 5,
    "changes": 15,
    "blob_url": "https://github.com/apache/hbase/blob/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationServerSink.java",
    "raw_url": "https://github.com/apache/hbase/raw/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationServerSink.java",
    "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationServerSink.java?ref=2135ff77c7727efd0ab12754283474dd49f42ba4",
    "patch": "@@ -18,7 +18,8 @@\n package org.apache.hadoop.hbase.replication;\n \n import static org.apache.hadoop.hbase.HConstants.HBASE_CLIENT_OPERATION_TIMEOUT;\n-import static org.apache.hadoop.hbase.master.ReplicationServerManager.ONLINE_SERVER_REFRESH_INTERVAL;\n+import static org.apache.hadoop.hbase.master.ReplicationServerManager.REPLICATION_SERVER_REFRESH_PERIOD;\n+import static org.apache.hadoop.hbase.replication.ReplicationServerRpcServices.REPLICATION_SERVER_PORT;\n import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n@@ -92,8 +93,9 @@\n \n   @BeforeClass\n   public static void beforeClass() throws Exception {\n+    CONF.setInt(REPLICATION_SERVER_PORT, 0);\n     CONF.setLong(HBASE_CLIENT_OPERATION_TIMEOUT, 1000);\n-    CONF.setLong(ONLINE_SERVER_REFRESH_INTERVAL, 10000);\n+    CONF.setLong(REPLICATION_SERVER_REFRESH_PERIOD, 10000);\n     CONF.setBoolean(HConstants.REPLICATION_OFFLOAD_ENABLE_KEY, true);\n     TEST_UTIL.startMiniCluster(StartMiniClusterOption.builder().numReplicationServers(1).build());\n     MASTER = TEST_UTIL.getMiniHBaseCluster().getMaster();\n@@ -110,6 +112,9 @@ public static void beforeClass() throws Exception {\n \n   @AfterClass\n   public static void afterClass() throws IOException {\n+    if (!REPLICATION_SERVER.isStopped()) {\n+      REPLICATION_SERVER.stop(\"afterClass\");\n+    }\n     TEST_UTIL.shutdownMiniCluster();\n   }\n \n@@ -171,7 +176,7 @@ private void replicateWALEntryAndVerify(ReplicationServerSinkPeer sinkPeer) thro\n   public void testReplicationServerReport() throws Exception {\n     ReplicationServerManager replicationServerManager = MASTER.getReplicationServerManager();\n     assertNotNull(replicationServerManager);\n-    TEST_UTIL.waitFor(60000, () -> !replicationServerManager.getOnlineServers().isEmpty()\n+    TEST_UTIL.waitFor(60000, () -> !replicationServerManager.getOnlineServersList().isEmpty()\n         && null != replicationServerManager.getServerMetrics(REPLICATION_SERVER_NAME));\n     // put data via replication server\n     testReplicateWAL();\n@@ -189,13 +194,13 @@ public void testReplicationServerExpire() throws Exception {\n \n     ReplicationServerManager replicationServerManager = MASTER.getReplicationServerManager();\n     TEST_UTIL.waitFor(60000, () ->\n-        initialNum + 1 == replicationServerManager.getOnlineServers().size()\n+        initialNum + 1 == replicationServerManager.getOnlineServersList().size()\n         && null != replicationServerManager.getServerMetrics(replicationServerName));\n \n     replicationServer.stop(\"test\");\n \n     TEST_UTIL.waitFor(180000, 1000, () ->\n-        initialNum == replicationServerManager.getOnlineServers().size());\n+        initialNum == replicationServerManager.getOnlineServersList().size());\n     assertNull(replicationServerManager.getServerMetrics(replicationServerName));\n   }\n }"
  },
  {
    "sha": "81f9db02325f1cc91712ba2201a095c3e1e1a03e",
    "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationServerSource.java",
    "status": "modified",
    "additions": 3,
    "deletions": 0,
    "changes": 3,
    "blob_url": "https://github.com/apache/hbase/blob/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationServerSource.java",
    "raw_url": "https://github.com/apache/hbase/raw/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationServerSource.java",
    "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationServerSource.java?ref=2135ff77c7727efd0ab12754283474dd49f42ba4",
    "patch": "@@ -17,6 +17,8 @@\n  */\n package org.apache.hadoop.hbase.replication;\n \n+import static org.apache.hadoop.hbase.replication.ReplicationServerRpcServices.REPLICATION_SERVER_PORT;\n+\n import org.apache.hadoop.hbase.HBaseClassTestRule;\n import org.apache.hadoop.hbase.HConstants;\n import org.apache.hadoop.hbase.ServerName;\n@@ -42,6 +44,7 @@\n \n   @BeforeClass\n   public static void setUpBeforeClass() throws Exception {\n+    UTIL1.getConfiguration().setInt(REPLICATION_SERVER_PORT, 0);\n     UTIL1.getConfiguration().setBoolean(HConstants.REPLICATION_OFFLOAD_ENABLE_KEY, true);\n     TestReplicationBase.setUpBeforeClass();\n     replicationServer = new HReplicationServer(UTIL1.getConfiguration());"
  },
  {
    "sha": "094a5251098e9aada64472655e7eaff1db33a2b7",
    "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSource.java",
    "status": "modified",
    "additions": 18,
    "deletions": 12,
    "changes": 30,
    "blob_url": "https://github.com/apache/hbase/blob/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSource.java",
    "raw_url": "https://github.com/apache/hbase/raw/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSource.java",
    "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSource.java?ref=2135ff77c7727efd0ab12754283474dd49f42ba4",
    "patch": "@@ -56,6 +56,7 @@\n import org.apache.hadoop.hbase.replication.ReplicationEndpoint;\n import org.apache.hadoop.hbase.replication.ReplicationPeer;\n import org.apache.hadoop.hbase.replication.ReplicationPeerConfig;\n+import org.apache.hadoop.hbase.replication.ReplicationQueueInfo;\n import org.apache.hadoop.hbase.replication.ReplicationQueueStorage;\n import org.apache.hadoop.hbase.replication.WALEntryFilter;\n import org.apache.hadoop.hbase.testclassification.MediumTests;\n@@ -139,8 +140,9 @@ public void testDefaultSkipsMetaWAL() throws IOException {\n     String queueId = \"qid\";\n     RegionServerServices rss =\n       TEST_UTIL.createMockRegionServerService(ServerName.parseServerName(\"a.b.c,1,1\"));\n-    rs.init(conf, null, null, manager, null, mockPeer, rss, rss.getServerName(), queueId, null,\n-      p -> OptionalLong.empty(), new MetricsSource(queueId));\n+    rs.init(conf, null, null, manager, null, mockPeer, rss,\n+      new ReplicationQueueInfo(rss.getServerName(), queueId), null, p -> OptionalLong.empty(),\n+      new MetricsSource(queueId));\n     try {\n       rs.startup();\n       assertTrue(rs.isSourceActive());\n@@ -177,8 +179,9 @@ public void testWALEntryFilter() throws IOException {\n     String queueId = \"qid\";\n     RegionServerServices rss =\n       TEST_UTIL.createMockRegionServerService(ServerName.parseServerName(\"a.b.c,1,1\"));\n-    rs.init(conf, null, null, manager, null, mockPeer, rss, rss.getServerName(), queueId, uuid,\n-      p -> OptionalLong.empty(), new MetricsSource(queueId));\n+    rs.init(conf, null, null, manager, null, mockPeer, rss,\n+      new ReplicationQueueInfo(rss.getServerName(), queueId), uuid, p -> OptionalLong.empty(),\n+      new MetricsSource(queueId));\n     try {\n       rs.startup();\n       TEST_UTIL.waitFor(30000, () -> rs.getWalEntryFilter() != null);\n@@ -265,8 +268,8 @@ public void testTerminateTimeout() throws Exception {\n       testConf.setInt(\"replication.source.maxretriesmultiplier\", 1);\n       ReplicationSourceManager manager = Mockito.mock(ReplicationSourceManager.class);\n       Mockito.when(manager.getTotalBufferUsed()).thenReturn(new AtomicLong(0));\n-      source.init(testConf, null, null, manager, null, mockPeer, null, null, \"testPeer\", null,\n-        p -> OptionalLong.empty(), null);\n+      source.init(testConf, null, null, manager, null, mockPeer, null,\n+        new ReplicationQueueInfo(null, \"testPeer\"), null, p -> OptionalLong.empty(), null);\n       ExecutorService executor = Executors.newSingleThreadExecutor();\n       Future<?> future = executor.submit(\n         () -> source.terminate(\"testing source termination\"));\n@@ -289,8 +292,9 @@ public void testTerminateClearsBuffer() throws Exception {\n     ReplicationPeer mockPeer = mock(ReplicationPeer.class);\n     Mockito.when(mockPeer.getPeerBandwidth()).thenReturn(0L);\n     Configuration testConf = HBaseConfiguration.create();\n-    source.init(testConf, null, null, mockManager, null, mockPeer, null, null,\n-      \"testPeer\", null, p -> OptionalLong.empty(), mock(MetricsSource.class));\n+    source.init(testConf, null, null, mockManager, null, mockPeer, null,\n+      new ReplicationQueueInfo(null, \"testPeer\"), null, p -> OptionalLong.empty(),\n+      mock(MetricsSource.class));\n     ReplicationSourceWALReader reader = new ReplicationSourceWALReader(null,\n       conf, null, 0, null, source, null);\n     ReplicationSourceShipper shipper =\n@@ -524,8 +528,9 @@ private RegionServerServices setupForAbortTests(ReplicationSource rs, Configurat\n     String queueId = \"qid\";\n     RegionServerServices rss =\n       TEST_UTIL.createMockRegionServerService(ServerName.parseServerName(\"a.b.c,1,1\"));\n-    rs.init(conf, null, null, manager, null, mockPeer, rss, rss.getServerName(), queueId, null,\n-      p -> OptionalLong.empty(), new MetricsSource(queueId));\n+    rs.init(conf, null, null, manager, null, mockPeer, rss,\n+      new ReplicationQueueInfo(rss.getServerName(), queueId), null, p -> OptionalLong.empty(),\n+      new MetricsSource(queueId));\n     return rss;\n   }\n \n@@ -624,8 +629,9 @@ public void testAgeOfOldestWal() throws Exception {\n         TEST_UTIL.createMockRegionServerService(ServerName.parseServerName(\"a.b.c,1,1\"));\n \n       ReplicationSource source = new ReplicationSource();\n-      source.init(conf, null, null, manager, null, mockPeer, rss, rss.getServerName(), id, null,\n-        p -> OptionalLong.empty(), metrics);\n+      source.init(conf, null, null, manager, null, mockPeer, rss,\n+        new ReplicationQueueInfo(rss.getServerName(), id), null, p -> OptionalLong.empty(),\n+        metrics);\n \n       final Path log1 = new Path(logDir, \"log-walgroup-a.8\");\n       manualEdge.setValue(10);"
  },
  {
    "sha": "f20edac784afcb3c0cca3d9dc566fa6012d10432",
    "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSourceManager.java",
    "status": "modified",
    "additions": 8,
    "deletions": 5,
    "changes": 13,
    "blob_url": "https://github.com/apache/hbase/blob/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSourceManager.java",
    "raw_url": "https://github.com/apache/hbase/raw/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSourceManager.java",
    "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSourceManager.java?ref=2135ff77c7727efd0ab12754283474dd49f42ba4",
    "patch": "@@ -72,6 +72,7 @@\n import org.apache.hadoop.hbase.replication.ReplicationPeer;\n import org.apache.hadoop.hbase.replication.ReplicationPeerConfig;\n import org.apache.hadoop.hbase.replication.ReplicationPeers;\n+import org.apache.hadoop.hbase.replication.ReplicationQueueInfo;\n import org.apache.hadoop.hbase.replication.ReplicationQueueStorage;\n import org.apache.hadoop.hbase.replication.ReplicationSourceController;\n import org.apache.hadoop.hbase.replication.ReplicationSourceDummy;\n@@ -415,8 +416,8 @@ public void testCleanupFailoverQueues() throws Exception {\n     assertEquals(files, manager.getWalsByIdRecoveredQueues().get(id).get(group));\n     ReplicationSourceInterface source = new ReplicationSource();\n     source.init(conf, fs, null, manager, manager.getQueueStorage(), rp1.getPeer(\"1\"),\n-      manager.getServer(), manager.getServer().getServerName(), id, null, p -> OptionalLong.empty(),\n-      null);\n+      manager.getServer(), new ReplicationQueueInfo(manager.getServer().getServerName(), id), null,\n+      p -> OptionalLong.empty(), null);\n     source.cleanOldWALs(file2, false);\n     // log1 should be deleted\n     assertEquals(Sets.newHashSet(file2), manager.getWalsByIdRecoveredQueues().get(id).get(group));\n@@ -634,15 +635,17 @@ public void testRemoveRemoteWALs() throws Exception {\n       ReplicationSourceInterface source = new ReplicationSource();\n       source.init(conf, fs, null, manager, manager.getQueueStorage(),\n         mockReplicationPeerForSyncReplication(peerId2), manager.getServer(),\n-        manager.getServer().getServerName(), peerId2, null, p -> OptionalLong.empty(), null);\n+        new ReplicationQueueInfo(manager.getServer().getServerName(), peerId2), null,\n+        p -> OptionalLong.empty(), null);\n       source.cleanOldWALs(walName, true);\n       // still there if peer id does not match\n       assertTrue(fs.exists(remoteWAL));\n \n       source = new ReplicationSource();\n       source.init(conf, fs, null, manager, manager.getQueueStorage(),\n         mockReplicationPeerForSyncReplication(slaveId), manager.getServer(),\n-        manager.getServer().getServerName(), slaveId, null, p -> OptionalLong.empty(), null);\n+        new ReplicationQueueInfo(manager.getServer().getServerName(), slaveId), null,\n+        p -> OptionalLong.empty(), null);\n       source.cleanOldWALs(walName, true);\n       assertFalse(fs.exists(remoteWAL));\n     } finally {\n@@ -823,7 +826,7 @@ private int isLogZnodesMapPopulated() {\n     @Override\n     public void init(Configuration conf, FileSystem fs, Path walDir,\n       ReplicationSourceController overallController, ReplicationQueueStorage queueStorage,\n-      ReplicationPeer replicationPeer, Server server, ServerName producer, String queueId,\n+      ReplicationPeer replicationPeer, Server server, ReplicationQueueInfo queueInfo,\n       UUID clusterId, WALFileLengthProvider walFileLengthProvider, MetricsSource metrics)\n       throws IOException {\n       throw new IOException(\"Failing deliberately\");"
  },
  {
    "sha": "86c141b3a6368b912caf40025fcc09efa5b528bb",
    "filename": "hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSourceManagerZkImpl.java",
    "status": "modified",
    "additions": 2,
    "deletions": 1,
    "changes": 3,
    "blob_url": "https://github.com/apache/hbase/blob/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSourceManagerZkImpl.java",
    "raw_url": "https://github.com/apache/hbase/raw/2135ff77c7727efd0ab12754283474dd49f42ba4/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSourceManagerZkImpl.java",
    "contents_url": "https://api.github.com/repos/apache/hbase/contents/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSourceManagerZkImpl.java?ref=2135ff77c7727efd0ab12754283474dd49f42ba4",
    "patch": "@@ -95,7 +95,8 @@ public void testNodeFailoverDeadServerParsing() throws Exception {\n         queueStorage.claimQueue(serverName, unclaimed.get(0), s3.getServerName()).getFirst();\n     queueStorage.removeReplicatorIfQueueIsEmpty(serverName);\n \n-    ReplicationQueueInfo replicationQueueInfo = new ReplicationQueueInfo(queue3);\n+    ReplicationQueueInfo replicationQueueInfo =\n+      new ReplicationQueueInfo(s3.getServerName(), queue3);\n     List<ServerName> result = replicationQueueInfo.getDeadRegionServers();\n     // verify\n     assertTrue(result.contains(server.getServerName()));"
  }
]
