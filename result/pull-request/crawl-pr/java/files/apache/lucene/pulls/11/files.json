[
  {
    "sha": "2148261c95c173cbfb48a2d8a8c7f4e65739e49c",
    "filename": "lucene/CHANGES.txt",
    "status": "modified",
    "additions": 6,
    "deletions": 0,
    "changes": 6,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/CHANGES.txt",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/CHANGES.txt",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/CHANGES.txt?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -92,6 +92,12 @@ API Changes\n * LUCENE-9480: Make DataInput's skipBytes(long) abstract as the implementation was not performant.\n   IndexInput's api is unaffected: skipBytes() is implemented via seek(). (Greg Miller)\n \n+* LUCENE-9334:  Require consistency between data-structures on a per-field basis.\n+  A field across all documents within an index must be indexed with the same index\n+  options and data-structures. As a consequence of this, doc values updates are\n+  only applicable for fields that are indexed with doc values only\n+\n+\n Improvements\n \n * LUCENE-9687: Hunspell support improvements: add API for spell-checking and suggestions, support compound words,"
  },
  {
    "sha": "2db9b61fc60c290c41bb8886501fbfbcc6bf6d24",
    "filename": "lucene/MIGRATE.md",
    "status": "modified",
    "additions": 13,
    "deletions": 4,
    "changes": 17,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/MIGRATE.md",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/MIGRATE.md",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/MIGRATE.md?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -358,8 +358,17 @@ Rather, an IllegalArgumentException shall be thrown. This is introduced for bett\n defence and to ensure that there is no bubbling up of errors when Lucene is\n used in multi level applications\n \n-## Assumption of data consistency between different data-structures sharing the same field name\n+### Require consistency between data-structures on a per-field basis\n \n-Sorting on a numeric field that is indexed with both doc values and points may use an\n-optimization to skip non-competitive documents. This optimization relies on the assumption\n-that the same data is stored in these points and doc values.\n+A field must be indexed with the same index options and data-structures across \n+all documents within an index. Thus, for example, it is not allowed to have \n+one document in a index where a certain field is indexed with doc values \n+and points, and another document where the same field is indexed only with \n+points.\n+\n+### Doc values updates are allowed only for doc values only fields\n+\n+Previously IndexWriter could update doc values for a binary or numeric docValue \n+field that was also indexed with other data structures (e.g. postings, vectors \n+etc). This is not allowed anymore. A field must be indexed with only doc values \n+to be allowed for doc values updates in IndexWriter."
  },
  {
    "sha": "26a0cd213713ce49490f09fdc16e4f8e3a920847",
    "filename": "lucene/core/src/java/org/apache/lucene/index/FieldInfo.java",
    "status": "modified",
    "additions": 228,
    "deletions": 79,
    "changes": 307,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/java/org/apache/lucene/index/FieldInfo.java",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/java/org/apache/lucene/index/FieldInfo.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/core/src/java/org/apache/lucene/index/FieldInfo.java?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -111,6 +111,43 @@ public FieldInfo(\n \n   /** Performs internal consistency checks. Always returns true (or throws IllegalStateException) */\n   public boolean checkConsistency() {\n+    return checkConsistency(\n+        name,\n+        storeTermVector,\n+        omitNorms,\n+        storePayloads,\n+        indexOptions,\n+        docValuesType,\n+        dvGen,\n+        pointDimensionCount,\n+        pointIndexDimensionCount,\n+        pointNumBytes,\n+        vectorDimension,\n+        vectorSearchStrategy);\n+  }\n+\n+  /**\n+   * Check correctness of FieldInfo options\n+   *\n+   * @throws IllegalStateException if some options are incorrect\n+   * @return {@code true} if all options are correct\n+   */\n+  public static boolean checkConsistency(\n+      String name,\n+      boolean storeTermVector,\n+      boolean omitNorms,\n+      boolean storePayloads,\n+      IndexOptions indexOptions,\n+      DocValuesType docValuesType,\n+      long dvGen,\n+      int pointDimensionCount,\n+      int pointIndexDimensionCount,\n+      int pointNumBytes,\n+      int vectorDimension,\n+      VectorValues.SearchStrategy vectorSearchStrategy) {\n+    if (indexOptions == null) {\n+      throw new IllegalStateException(\"IndexOptions must not be null (field: '\" + name + \"')\");\n+    }\n     if (indexOptions != IndexOptions.NONE) {\n       // Cannot store payloads unless positions are indexed:\n       if (indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0 && storePayloads) {\n@@ -130,127 +167,239 @@ public boolean checkConsistency() {\n       }\n     }\n \n-    if (pointDimensionCount < 0) {\n+    if (docValuesType == null) {\n+      throw new IllegalStateException(\"DocValuesType must not be null (field: '\" + name + \"')\");\n+    }\n+    if (dvGen != -1 && docValuesType == DocValuesType.NONE) {\n       throw new IllegalStateException(\n-          \"pointDimensionCount must be >= 0; got \" + pointDimensionCount);\n+          \"field '\"\n+              + name\n+              + \"' cannot have a docvalues update generation without having docvalues\");\n     }\n \n+    if (pointDimensionCount < 0) {\n+      throw new IllegalStateException(\n+          \"pointDimensionCount must be >= 0; got \"\n+              + pointDimensionCount\n+              + \" (field: '\"\n+              + name\n+              + \"')\");\n+    }\n     if (pointIndexDimensionCount < 0) {\n       throw new IllegalStateException(\n-          \"pointIndexDimensionCount must be >= 0; got \" + pointIndexDimensionCount);\n+          \"pointIndexDimensionCount must be >= 0; got \"\n+              + pointIndexDimensionCount\n+              + \" (field: '\"\n+              + name\n+              + \"')\");\n     }\n-\n     if (pointNumBytes < 0) {\n-      throw new IllegalStateException(\"pointNumBytes must be >= 0; got \" + pointNumBytes);\n+      throw new IllegalStateException(\n+          \"pointNumBytes must be >= 0; got \" + pointNumBytes + \" (field: '\" + name + \"')\");\n     }\n \n     if (pointDimensionCount != 0 && pointNumBytes == 0) {\n       throw new IllegalStateException(\n-          \"pointNumBytes must be > 0 when pointDimensionCount=\" + pointDimensionCount);\n+          \"pointNumBytes must be > 0 when pointDimensionCount=\"\n+              + pointDimensionCount\n+              + \" (field: '\"\n+              + name\n+              + \"')\");\n     }\n-\n     if (pointIndexDimensionCount != 0 && pointDimensionCount == 0) {\n       throw new IllegalStateException(\n-          \"pointIndexDimensionCount must be 0 when pointDimensionCount=0\");\n+          \"pointIndexDimensionCount must be 0 when pointDimensionCount=0\"\n+              + \" (field: '\"\n+              + name\n+              + \"')\");\n     }\n-\n     if (pointNumBytes != 0 && pointDimensionCount == 0) {\n       throw new IllegalStateException(\n-          \"pointDimensionCount must be > 0 when pointNumBytes=\" + pointNumBytes);\n+          \"pointDimensionCount must be > 0 when pointNumBytes=\"\n+              + pointNumBytes\n+              + \" (field: '\"\n+              + name\n+              + \"')\");\n     }\n \n-    if (dvGen != -1 && docValuesType == DocValuesType.NONE) {\n+    if (vectorSearchStrategy == null) {\n       throw new IllegalStateException(\n-          \"field '\"\n-              + name\n-              + \"' cannot have a docvalues update generation without having docvalues\");\n+          \"Vector search strategy must not be null (field: '\" + name + \"')\");\n     }\n-\n     if (vectorDimension < 0) {\n-      throw new IllegalStateException(\"vectorDimension must be >=0; got \" + vectorDimension);\n+      throw new IllegalStateException(\n+          \"vectorDimension must be >=0; got \" + vectorDimension + \" (field: '\" + name + \"')\");\n     }\n-\n     if (vectorDimension == 0 && vectorSearchStrategy != VectorValues.SearchStrategy.NONE) {\n       throw new IllegalStateException(\n-          \"vector search strategy must be NONE when dimension = 0; got \" + vectorSearchStrategy);\n+          \"vector search strategy must be NONE when dimension = 0; got \"\n+              + vectorSearchStrategy\n+              + \" (field: '\"\n+              + name\n+              + \"')\");\n     }\n-\n     return true;\n   }\n \n-  // should only be called by FieldInfos#addOrUpdate\n-  void update(\n-      boolean storeTermVector,\n-      boolean omitNorms,\n-      boolean storePayloads,\n-      IndexOptions indexOptions,\n-      Map<String, String> attributes,\n-      int dimensionCount,\n-      int indexDimensionCount,\n-      int dimensionNumBytes) {\n-    if (indexOptions == null) {\n-      throw new NullPointerException(\"IndexOptions must not be null (field: \\\"\" + name + \"\\\")\");\n-    }\n-    // System.out.println(\"FI.update field=\" + name + \" indexed=\" + indexed + \" omitNorms=\" +\n-    // omitNorms + \" this.omitNorms=\" + this.omitNorms);\n-    if (this.indexOptions != indexOptions) {\n-      if (this.indexOptions == IndexOptions.NONE) {\n-        this.indexOptions = indexOptions;\n-      } else if (indexOptions != IndexOptions.NONE) {\n-        throw new IllegalArgumentException(\n-            \"cannot change field \\\"\"\n-                + name\n-                + \"\\\" from index options=\"\n-                + this.indexOptions\n-                + \" to inconsistent index options=\"\n-                + indexOptions);\n-      }\n+  /**\n+   * Verify that the provided FieldInfo has the same schema as this FieldInfo\n+   *\n+   * @param o – other FieldInfo whose schema is verified against this FieldInfo's schema\n+   * @throws IllegalArgumentException if the field schemas are not the same\n+   */\n+  void verifySameSchema(FieldInfo o) {\n+    String fieldName = this.name;\n+    verifySameIndexOptions(fieldName, this.indexOptions, o.getIndexOptions());\n+    if (this.indexOptions != IndexOptions.NONE) {\n+      verifySameOmitNorms(fieldName, this.omitNorms, o.omitNorms);\n+      verifySameStoreTermVectors(fieldName, this.storeTermVector, o.storeTermVector);\n+    }\n+    verifySameDocValuesType(fieldName, this.docValuesType, o.docValuesType);\n+    verifySamePointsOptions(\n+        fieldName,\n+        this.pointDimensionCount,\n+        this.pointIndexDimensionCount,\n+        this.pointNumBytes,\n+        o.pointDimensionCount,\n+        o.pointIndexDimensionCount,\n+        o.pointNumBytes);\n+    verifySameVectorOptions(\n+        fieldName,\n+        this.vectorDimension,\n+        this.vectorSearchStrategy,\n+        o.vectorDimension,\n+        o.vectorSearchStrategy);\n+  }\n+\n+  /**\n+   * Verify that the provided index options are the same\n+   *\n+   * @throws IllegalArgumentException if they are not the same\n+   */\n+  static void verifySameIndexOptions(\n+      String fieldName, IndexOptions indexOptions1, IndexOptions indexOptions2) {\n+    if (indexOptions1 != indexOptions2) {\n+      throw new IllegalArgumentException(\n+          \"cannot change field \\\"\"\n+              + fieldName\n+              + \"\\\" from index options=\"\n+              + indexOptions1\n+              + \" to inconsistent index options=\"\n+              + indexOptions2);\n     }\n+  }\n \n-    if (this.pointDimensionCount == 0 && dimensionCount != 0) {\n-      this.pointDimensionCount = dimensionCount;\n-      this.pointIndexDimensionCount = indexDimensionCount;\n-      this.pointNumBytes = dimensionNumBytes;\n-    } else if (dimensionCount != 0\n-        && (this.pointDimensionCount != dimensionCount\n-            || this.pointIndexDimensionCount != indexDimensionCount\n-            || this.pointNumBytes != dimensionNumBytes)) {\n+  /**\n+   * Verify that the provided docValues type are the same\n+   *\n+   * @throws IllegalArgumentException if they are not the same\n+   */\n+  static void verifySameDocValuesType(\n+      String fieldName, DocValuesType docValuesType1, DocValuesType docValuesType2) {\n+    if (docValuesType1 != docValuesType2) {\n       throw new IllegalArgumentException(\n           \"cannot change field \\\"\"\n-              + name\n+              + fieldName\n+              + \"\\\" from doc values type=\"\n+              + docValuesType1\n+              + \" to inconsistent doc values type=\"\n+              + docValuesType2);\n+    }\n+  }\n+\n+  /**\n+   * Verify that the provided store term vectors options are the same\n+   *\n+   * @throws IllegalArgumentException if they are not the same\n+   */\n+  static void verifySameStoreTermVectors(\n+      String fieldName, boolean storeTermVector1, boolean storeTermVector2) {\n+    if (storeTermVector1 != storeTermVector2) {\n+      throw new IllegalArgumentException(\n+          \"cannot change field \\\"\"\n+              + fieldName\n+              + \"\\\" from storeTermVector=\"\n+              + storeTermVector1\n+              + \" to inconsistent storeTermVector=\"\n+              + storeTermVector2);\n+    }\n+  }\n+\n+  /**\n+   * Verify that the provided omitNorms are the same\n+   *\n+   * @throws IllegalArgumentException if they are not the same\n+   */\n+  static void verifySameOmitNorms(String fieldName, boolean omitNorms1, boolean omitNorms2) {\n+    if (omitNorms1 != omitNorms2) {\n+      throw new IllegalArgumentException(\n+          \"cannot change field \\\"\"\n+              + fieldName\n+              + \"\\\" from omitNorms=\"\n+              + omitNorms1\n+              + \" to inconsistent omitNorms=\"\n+              + omitNorms2);\n+    }\n+  }\n+\n+  /**\n+   * Verify that the provided points indexing options are the same\n+   *\n+   * @throws IllegalArgumentException if they are not the same\n+   */\n+  static void verifySamePointsOptions(\n+      String fieldName,\n+      int pointDimensionCount1,\n+      int indexDimensionCount1,\n+      int numBytes1,\n+      int pointDimensionCount2,\n+      int indexDimensionCount2,\n+      int numBytes2) {\n+    if (pointDimensionCount1 != pointDimensionCount2\n+        || indexDimensionCount1 != indexDimensionCount2\n+        || numBytes1 != numBytes2) {\n+      throw new IllegalArgumentException(\n+          \"cannot change field \\\"\"\n+              + fieldName\n               + \"\\\" from points dimensionCount=\"\n-              + this.pointDimensionCount\n+              + pointDimensionCount1\n               + \", indexDimensionCount=\"\n-              + this.pointIndexDimensionCount\n+              + indexDimensionCount1\n               + \", numBytes=\"\n-              + this.pointNumBytes\n+              + numBytes1\n               + \" to inconsistent dimensionCount=\"\n-              + dimensionCount\n+              + pointDimensionCount2\n               + \", indexDimensionCount=\"\n-              + indexDimensionCount\n+              + indexDimensionCount2\n               + \", numBytes=\"\n-              + dimensionNumBytes);\n+              + numBytes2);\n     }\n+  }\n \n-    // if updated field data is not for indexing, leave the updates out\n-    if (this.indexOptions != IndexOptions.NONE) {\n-      this.storeTermVector |= storeTermVector; // once vector, always vector\n-      this.storePayloads |= storePayloads;\n-\n-      // Awkward: only drop norms if incoming update is indexed:\n-      if (indexOptions != IndexOptions.NONE && this.omitNorms != omitNorms) {\n-        this.omitNorms = true; // if one require omitNorms at least once, it remains off for life\n-      }\n-    }\n-    if (this.indexOptions == IndexOptions.NONE\n-        || this.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {\n-      // cannot store payloads if we don't store positions:\n-      this.storePayloads = false;\n-    }\n-    if (attributes != null) {\n-      this.attributes.putAll(attributes);\n+  /**\n+   * Verify that the provided vector indexing options are the same\n+   *\n+   * @throws IllegalArgumentException if they are not the same\n+   */\n+  static void verifySameVectorOptions(\n+      String fieldName,\n+      int vd1,\n+      VectorValues.SearchStrategy vst1,\n+      int vd2,\n+      VectorValues.SearchStrategy vst2) {\n+    if (vd1 != vd2 || vst1 != vst2) {\n+      throw new IllegalArgumentException(\n+          \"cannot change field \\\"\"\n+              + fieldName\n+              + \"\\\" from vector dimension=\"\n+              + vd1\n+              + \", vector search strategy=\"\n+              + vst1\n+              + \" to inconsistent vector dimension=\"\n+              + vd2\n+              + \", vector search strategy=\"\n+              + vst2);\n     }\n-    this.checkConsistency();\n   }\n \n   /**"
  },
  {
    "sha": "fc938c34f149609ee14290a4075b9e6d7eba761a",
    "filename": "lucene/core/src/java/org/apache/lucene/index/FieldInfos.java",
    "status": "modified",
    "additions": 400,
    "deletions": 498,
    "changes": 898,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/java/org/apache/lucene/index/FieldInfos.java",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/java/org/apache/lucene/index/FieldInfos.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/core/src/java/org/apache/lucene/index/FieldInfos.java?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -16,6 +16,13 @@\n  */\n package org.apache.lucene.index;\n \n+import static org.apache.lucene.index.FieldInfo.verifySameDocValuesType;\n+import static org.apache.lucene.index.FieldInfo.verifySameIndexOptions;\n+import static org.apache.lucene.index.FieldInfo.verifySameOmitNorms;\n+import static org.apache.lucene.index.FieldInfo.verifySamePointsOptions;\n+import static org.apache.lucene.index.FieldInfo.verifySameStoreTermVectors;\n+import static org.apache.lucene.index.FieldInfo.verifySameVectorOptions;\n+\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.Collection;\n@@ -311,6 +318,8 @@ public FieldDimensions(int dimensionCount, int indexDimensionCount, int dimensio\n     private final Map<String, FieldDimensions> dimensions;\n \n     private final Map<String, FieldVectorProperties> vectorProps;\n+    private final Map<String, Boolean> omitNorms;\n+    private final Map<String, Boolean> storeTermVectors;\n \n     // TODO: we should similarly catch an attempt to turn\n     // norms back on after they were already committed; today\n@@ -327,6 +336,8 @@ public FieldDimensions(int dimensionCount, int indexDimensionCount, int dimensio\n       this.docValuesType = new HashMap<>();\n       this.dimensions = new HashMap<>();\n       this.vectorProps = new HashMap<>();\n+      this.omitNorms = new HashMap<>();\n+      this.storeTermVectors = new HashMap<>();\n       this.softDeletesFieldName = softDeletesFieldName;\n     }\n \n@@ -339,110 +350,17 @@ synchronized int addOrGet(\n         String fieldName,\n         int preferredFieldNumber,\n         IndexOptions indexOptions,\n+        boolean storeTermVector,\n+        boolean omitNorms,\n         DocValuesType dvType,\n         int dimensionCount,\n         int indexDimensionCount,\n         int dimensionNumBytes,\n         int vectorDimension,\n         VectorValues.SearchStrategy searchStrategy,\n         boolean isSoftDeletesField) {\n-      if (indexOptions != IndexOptions.NONE) {\n-        IndexOptions currentOpts = this.indexOptions.get(fieldName);\n-        if (currentOpts == null) {\n-          this.indexOptions.put(fieldName, indexOptions);\n-        } else if (currentOpts != IndexOptions.NONE && currentOpts != indexOptions) {\n-          throw new IllegalArgumentException(\n-              \"cannot change field \\\"\"\n-                  + fieldName\n-                  + \"\\\" from index options=\"\n-                  + currentOpts\n-                  + \" to inconsistent index options=\"\n-                  + indexOptions);\n-        }\n-      }\n-      if (dvType != DocValuesType.NONE) {\n-        DocValuesType currentDVType = docValuesType.get(fieldName);\n-        if (currentDVType == null) {\n-          docValuesType.put(fieldName, dvType);\n-        } else if (currentDVType != DocValuesType.NONE && currentDVType != dvType) {\n-          throw new IllegalArgumentException(\n-              \"cannot change DocValues type from \"\n-                  + currentDVType\n-                  + \" to \"\n-                  + dvType\n-                  + \" for field \\\"\"\n-                  + fieldName\n-                  + \"\\\"\");\n-        }\n-      }\n-      if (dimensionCount != 0) {\n-        FieldDimensions dims = dimensions.get(fieldName);\n-        if (dims != null) {\n-          if (dims.dimensionCount != dimensionCount) {\n-            throw new IllegalArgumentException(\n-                \"cannot change point dimension count from \"\n-                    + dims.dimensionCount\n-                    + \" to \"\n-                    + dimensionCount\n-                    + \" for field=\\\"\"\n-                    + fieldName\n-                    + \"\\\"\");\n-          }\n-          if (dims.indexDimensionCount != indexDimensionCount) {\n-            throw new IllegalArgumentException(\n-                \"cannot change point index dimension count from \"\n-                    + dims.indexDimensionCount\n-                    + \" to \"\n-                    + indexDimensionCount\n-                    + \" for field=\\\"\"\n-                    + fieldName\n-                    + \"\\\"\");\n-          }\n-          if (dims.dimensionNumBytes != dimensionNumBytes) {\n-            throw new IllegalArgumentException(\n-                \"cannot change point numBytes from \"\n-                    + dims.dimensionNumBytes\n-                    + \" to \"\n-                    + dimensionNumBytes\n-                    + \" for field=\\\"\"\n-                    + fieldName\n-                    + \"\\\"\");\n-          }\n-        } else {\n-          dimensions.put(\n-              fieldName,\n-              new FieldDimensions(dimensionCount, indexDimensionCount, dimensionNumBytes));\n-        }\n-      }\n-      if (vectorDimension != 0) {\n-        FieldVectorProperties props = vectorProps.get(fieldName);\n-        if (props != null) {\n-          if (props.numDimensions != vectorDimension) {\n-            throw new IllegalArgumentException(\n-                \"cannot change vector dimension from \"\n-                    + props.numDimensions\n-                    + \" to \"\n-                    + vectorDimension\n-                    + \" for field=\\\"\"\n-                    + fieldName\n-                    + \"\\\"\");\n-          }\n-          if (props.searchStrategy != searchStrategy) {\n-            throw new IllegalArgumentException(\n-                \"cannot change vector search strategy from \"\n-                    + props.searchStrategy\n-                    + \" to \"\n-                    + searchStrategy\n-                    + \" for field=\\\"\"\n-                    + fieldName\n-                    + \"\\\"\");\n-          }\n-        } else {\n-          vectorProps.put(fieldName, new FieldVectorProperties(vectorDimension, searchStrategy));\n-        }\n-      }\n       Integer fieldNumber = nameToNumber.get(fieldName);\n-      if (fieldNumber == null) {\n+      if (fieldNumber == null) { // first time we see this field in this index\n         final Integer preferredBoxed = Integer.valueOf(preferredFieldNumber);\n         if (preferredFieldNumber != -1 && !numberToName.containsKey(preferredBoxed)) {\n           // cool - we can use this number globally\n@@ -455,8 +373,42 @@ synchronized int addOrGet(\n           fieldNumber = lowestUnassignedFieldNumber;\n         }\n         assert fieldNumber >= 0;\n+        FieldInfo.checkConsistency(\n+            fieldName,\n+            storeTermVector,\n+            omitNorms,\n+            false,\n+            indexOptions,\n+            dvType,\n+            -1,\n+            dimensionCount,\n+            indexDimensionCount,\n+            dimensionNumBytes,\n+            vectorDimension,\n+            searchStrategy);\n         numberToName.put(fieldNumber, fieldName);\n         nameToNumber.put(fieldName, fieldNumber);\n+        this.indexOptions.put(fieldName, indexOptions);\n+        if (indexOptions != IndexOptions.NONE) {\n+          this.storeTermVectors.put(fieldName, storeTermVector);\n+          this.omitNorms.put(fieldName, omitNorms);\n+        }\n+        docValuesType.put(fieldName, dvType);\n+        dimensions.put(\n+            fieldName, new FieldDimensions(dimensionCount, indexDimensionCount, dimensionNumBytes));\n+        vectorProps.put(fieldName, new FieldVectorProperties(vectorDimension, searchStrategy));\n+      } else {\n+        verifySameSchema(\n+            fieldName,\n+            indexOptions,\n+            storeTermVector,\n+            omitNorms,\n+            dvType,\n+            dimensionCount,\n+            indexDimensionCount,\n+            dimensionNumBytes,\n+            vectorDimension,\n+            searchStrategy);\n       }\n \n       if (isSoftDeletesField) {\n@@ -481,205 +433,166 @@ synchronized int addOrGet(\n                 + fieldName\n                 + \"] as non-soft-deletes already\");\n       }\n-\n       return fieldNumber.intValue();\n     }\n \n-    synchronized void verifyConsistent(Integer number, String name, IndexOptions indexOptions) {\n-      if (name.equals(numberToName.get(number)) == false) {\n-        throw new IllegalArgumentException(\n-            \"field number \"\n-                + number\n-                + \" is already mapped to field name \\\"\"\n-                + numberToName.get(number)\n-                + \"\\\", not \\\"\"\n-                + name\n-                + \"\\\"\");\n-      }\n-      if (number.equals(nameToNumber.get(name)) == false) {\n-        throw new IllegalArgumentException(\n-            \"field name \\\"\"\n-                + name\n-                + \"\\\" is already mapped to field number \\\"\"\n-                + nameToNumber.get(name)\n-                + \"\\\", not \\\"\"\n-                + number\n-                + \"\\\"\");\n-      }\n-      IndexOptions currentIndexOptions = this.indexOptions.get(name);\n-      if (indexOptions != IndexOptions.NONE\n-          && currentIndexOptions != null\n-          && currentIndexOptions != IndexOptions.NONE\n-          && indexOptions != currentIndexOptions) {\n-        throw new IllegalArgumentException(\n-            \"cannot change field \\\"\"\n-                + name\n-                + \"\\\" from index options=\"\n-                + currentIndexOptions\n-                + \" to inconsistent index options=\"\n-                + indexOptions);\n-      }\n-    }\n+    private void verifySameSchema(\n+        String fieldName,\n+        IndexOptions indexOptions,\n+        boolean storeTermVector,\n+        boolean omitNorms,\n+        DocValuesType dvType,\n+        int dimensionCount,\n+        int indexDimensionCount,\n+        int dimensionNumBytes,\n+        int vectorDimension,\n+        VectorValues.SearchStrategy searchStrategy) {\n \n-    synchronized void verifyConsistent(Integer number, String name, DocValuesType dvType) {\n-      if (name.equals(numberToName.get(number)) == false) {\n-        throw new IllegalArgumentException(\n-            \"field number \"\n-                + number\n-                + \" is already mapped to field name \\\"\"\n-                + numberToName.get(number)\n-                + \"\\\", not \\\"\"\n-                + name\n-                + \"\\\"\");\n-      }\n-      if (number.equals(nameToNumber.get(name)) == false) {\n-        throw new IllegalArgumentException(\n-            \"field name \\\"\"\n-                + name\n-                + \"\\\" is already mapped to field number \\\"\"\n-                + nameToNumber.get(name)\n-                + \"\\\", not \\\"\"\n-                + number\n-                + \"\\\"\");\n-      }\n-      DocValuesType currentDVType = docValuesType.get(name);\n-      if (dvType != DocValuesType.NONE\n-          && currentDVType != null\n-          && currentDVType != DocValuesType.NONE\n-          && dvType != currentDVType) {\n-        throw new IllegalArgumentException(\n-            \"cannot change DocValues type from \"\n-                + currentDVType\n-                + \" to \"\n-                + dvType\n-                + \" for field \\\"\"\n-                + name\n-                + \"\\\"\");\n-      }\n+      IndexOptions currentOpts = this.indexOptions.get(fieldName);\n+      verifySameIndexOptions(fieldName, currentOpts, indexOptions);\n+      if (currentOpts != IndexOptions.NONE) {\n+        boolean curStoreTermVector = this.storeTermVectors.get(fieldName);\n+        verifySameStoreTermVectors(fieldName, curStoreTermVector, storeTermVector);\n+        boolean curOmitNorms = this.omitNorms.get(fieldName);\n+        verifySameOmitNorms(fieldName, curOmitNorms, omitNorms);\n+      }\n+\n+      DocValuesType currentDVType = docValuesType.get(fieldName);\n+      verifySameDocValuesType(fieldName, currentDVType, dvType);\n+\n+      FieldDimensions dims = dimensions.get(fieldName);\n+      verifySamePointsOptions(\n+          fieldName,\n+          dims.dimensionCount,\n+          dims.indexDimensionCount,\n+          dims.dimensionNumBytes,\n+          dimensionCount,\n+          indexDimensionCount,\n+          dimensionNumBytes);\n+\n+      FieldVectorProperties props = vectorProps.get(fieldName);\n+      verifySameVectorOptions(\n+          fieldName, props.numDimensions, props.searchStrategy, vectorDimension, searchStrategy);\n     }\n \n-    synchronized void verifyConsistentDimensions(\n-        Integer number,\n-        String name,\n-        int dataDimensionCount,\n-        int indexDimensionCount,\n-        int dimensionNumBytes) {\n-      if (name.equals(numberToName.get(number)) == false) {\n-        throw new IllegalArgumentException(\n-            \"field number \"\n-                + number\n-                + \" is already mapped to field name \\\"\"\n-                + numberToName.get(number)\n-                + \"\\\", not \\\"\"\n-                + name\n-                + \"\\\"\");\n-      }\n-      if (number.equals(nameToNumber.get(name)) == false) {\n-        throw new IllegalArgumentException(\n-            \"field name \\\"\"\n-                + name\n-                + \"\\\" is already mapped to field number \\\"\"\n-                + nameToNumber.get(name)\n-                + \"\\\", not \\\"\"\n-                + number\n-                + \"\\\"\");\n-      }\n-      FieldDimensions dim = dimensions.get(name);\n-      if (dim != null) {\n-        if (dim.dimensionCount != dataDimensionCount) {\n+    /**\n+     * This function is called from {@code IndexWriter} to verify if doc values of the field can be\n+     * updated\n+     *\n+     * @param fieldName - name of the field\n+     * @param dvType - expected doc values type\n+     * @param fieldMustExist – if the field must exist.\n+     * @throws IllegalArgumentException if the field must exist, but it doesn't, or if the field\n+     *     exists, but it is not doc values only field with the provided doc values type.\n+     */\n+    synchronized void verifyDvOnlyField(\n+        String fieldName, DocValuesType dvType, boolean fieldMustExist) {\n+      if (nameToNumber.containsKey(fieldName) == false) {\n+        if (fieldMustExist) {\n           throw new IllegalArgumentException(\n-              \"cannot change point dimension count from \"\n-                  + dim.dimensionCount\n-                  + \" to \"\n-                  + dataDimensionCount\n-                  + \" for field=\\\"\"\n-                  + name\n-                  + \"\\\"\");\n+              \"Can't update [\"\n+                  + dvType\n+                  + \"] doc values; the field [\"\n+                  + fieldName\n+                  + \"] doesn't exist.\");\n+        } else {\n+          // create dv only field\n+          addOrGet(\n+              fieldName,\n+              -1,\n+              IndexOptions.NONE,\n+              false,\n+              false,\n+              dvType,\n+              0,\n+              0,\n+              0,\n+              0,\n+              VectorValues.SearchStrategy.NONE,\n+              (softDeletesFieldName != null && softDeletesFieldName.equals(fieldName)));\n         }\n-        if (dim.indexDimensionCount != indexDimensionCount) {\n+      } else {\n+        // verify that field is doc values only field with the give doc values type\n+        DocValuesType fieldDvType = docValuesType.get(fieldName);\n+        if (dvType != docValuesType.get(fieldName)) {\n           throw new IllegalArgumentException(\n-              \"cannot change point index dimension count from \"\n-                  + dim.indexDimensionCount\n-                  + \" to \"\n-                  + indexDimensionCount\n-                  + \" for field=\\\"\"\n-                  + name\n-                  + \"\\\"\");\n+              \"Can't update [\"\n+                  + dvType\n+                  + \"] doc values; the field [\"\n+                  + fieldName\n+                  + \"] has inconsistent doc values' type of [\"\n+                  + fieldDvType\n+                  + \"].\");\n         }\n-        if (dim.dimensionNumBytes != dimensionNumBytes) {\n+\n+        FieldDimensions fdimensions = dimensions.get(fieldName);\n+        if (fdimensions != null && fdimensions.dimensionCount != 0) {\n           throw new IllegalArgumentException(\n-              \"cannot change point numBytes from \"\n-                  + dim.dimensionNumBytes\n-                  + \" to \"\n-                  + dimensionNumBytes\n-                  + \" for field=\\\"\"\n-                  + name\n-                  + \"\\\"\");\n+              \"Can't update [\"\n+                  + dvType\n+                  + \"] doc values; the field [\"\n+                  + fieldName\n+                  + \"] must be doc values only field, but is also indexed with points.\");\n         }\n-      }\n-    }\n \n-    synchronized void verifyConsistentVectorProperties(\n-        Integer number,\n-        String name,\n-        int numDimensions,\n-        VectorValues.SearchStrategy searchStrategy) {\n-      if (name.equals(numberToName.get(number)) == false) {\n-        throw new IllegalArgumentException(\n-            \"field number \"\n-                + number\n-                + \" is already mapped to field name \\\"\"\n-                + numberToName.get(number)\n-                + \"\\\", not \\\"\"\n-                + name\n-                + \"\\\"\");\n-      }\n-      if (number.equals(nameToNumber.get(name)) == false) {\n-        throw new IllegalArgumentException(\n-            \"field name \\\"\"\n-                + name\n-                + \"\\\" is already mapped to field number \\\"\"\n-                + nameToNumber.get(name)\n-                + \"\\\", not \\\"\"\n-                + number\n-                + \"\\\"\");\n-      }\n-      FieldVectorProperties props = vectorProps.get(name);\n-      if (props != null) {\n-        if (props.numDimensions != numDimensions) {\n+        IndexOptions ioptions = indexOptions.get(fieldName);\n+        if (ioptions != null && ioptions != IndexOptions.NONE) {\n           throw new IllegalArgumentException(\n-              \"cannot change vector dimension from \"\n-                  + props.numDimensions\n-                  + \" to \"\n-                  + numDimensions\n-                  + \" for field=\\\"\"\n-                  + name\n-                  + \"\\\"\");\n+              \"Can't update [\"\n+                  + dvType\n+                  + \"] doc values; the field [\"\n+                  + fieldName\n+                  + \"] must be doc values only field, but is also indexed with postings.\");\n         }\n-        if (props.searchStrategy != searchStrategy) {\n+\n+        FieldVectorProperties fvp = vectorProps.get(fieldName);\n+        if (fvp != null && fvp.numDimensions != 0) {\n           throw new IllegalArgumentException(\n-              \"cannot change vector search strategy from \"\n-                  + props.searchStrategy\n-                  + \" to \"\n-                  + searchStrategy\n-                  + \" for field=\\\"\"\n-                  + name\n-                  + \"\\\"\");\n+              \"Can't update [\"\n+                  + dvType\n+                  + \"] doc values; the field [\"\n+                  + fieldName\n+                  + \"] must be doc values only field, but is also indexed with vectors.\");\n         }\n       }\n     }\n \n     /**\n-     * Returns true if the {@code fieldName} exists in the map and is of the same {@code dvType}.\n+     * Construct a new FieldInfo based on the options in global field numbers. This method is not\n+     * synchronized as all the options it uses are not modifiable.\n+     *\n+     * @param fieldName name of the field\n+     * @param dvType doc values type\n+     * @param newFieldNumber a new field number\n+     * @return {@code null} if {@code fieldName} doesn't exist in the map or is not of the same\n+     *     {@code dvType} returns a new FieldInfo based based on the options in global field numbers\n      */\n-    synchronized boolean contains(String fieldName, DocValuesType dvType) {\n-      // used by IndexWriter.updateNumericDocValue\n-      if (!nameToNumber.containsKey(fieldName)) {\n-        return false;\n-      } else {\n-        // only return true if the field has the same dvType as the requested one\n-        return dvType == docValuesType.get(fieldName);\n-      }\n+    FieldInfo constructFieldInfo(String fieldName, DocValuesType dvType, int newFieldNumber) {\n+      Integer fieldNumber;\n+      synchronized (this) {\n+        fieldNumber = nameToNumber.get(fieldName);\n+      }\n+      if (fieldNumber == null) return null;\n+      DocValuesType dvType0 = docValuesType.get(fieldName);\n+      if (dvType != dvType0) return null;\n+\n+      boolean isSoftDeletesField = fieldName.equals(softDeletesFieldName);\n+      return new FieldInfo(\n+          fieldName,\n+          newFieldNumber,\n+          false,\n+          false,\n+          false,\n+          IndexOptions.NONE,\n+          dvType,\n+          -1,\n+          new HashMap<>(),\n+          0,\n+          0,\n+          0,\n+          0,\n+          VectorValues.SearchStrategy.NONE,\n+          isSoftDeletesField);\n     }\n \n     synchronized Set<String> getFieldNames() {\n@@ -694,85 +607,6 @@ synchronized void clear() {\n       dimensions.clear();\n       lowestUnassignedFieldNumber = -1;\n     }\n-\n-    synchronized void setIndexOptions(int number, String name, IndexOptions indexOptions) {\n-      verifyConsistent(number, name, indexOptions);\n-      this.indexOptions.put(name, indexOptions);\n-    }\n-\n-    synchronized void setDocValuesType(int number, String name, DocValuesType dvType) {\n-      verifyConsistent(number, name, dvType);\n-      docValuesType.put(name, dvType);\n-    }\n-\n-    synchronized void setDimensions(\n-        int number,\n-        String name,\n-        int dimensionCount,\n-        int indexDimensionCount,\n-        int dimensionNumBytes) {\n-      if (dimensionCount > PointValues.MAX_DIMENSIONS) {\n-        throw new IllegalArgumentException(\n-            \"dimensionCount must be <= PointValues.MAX_DIMENSIONS (= \"\n-                + PointValues.MAX_DIMENSIONS\n-                + \"); got \"\n-                + dimensionCount\n-                + \" for field=\\\"\"\n-                + name\n-                + \"\\\"\");\n-      }\n-      if (dimensionNumBytes > PointValues.MAX_NUM_BYTES) {\n-        throw new IllegalArgumentException(\n-            \"dimension numBytes must be <= PointValues.MAX_NUM_BYTES (= \"\n-                + PointValues.MAX_NUM_BYTES\n-                + \"); got \"\n-                + dimensionNumBytes\n-                + \" for field=\\\"\"\n-                + name\n-                + \"\\\"\");\n-      }\n-      if (indexDimensionCount > dimensionCount) {\n-        throw new IllegalArgumentException(\n-            \"indexDimensionCount must be <= dimensionCount (= \"\n-                + dimensionCount\n-                + \"); got \"\n-                + indexDimensionCount\n-                + \" for field=\\\"\"\n-                + name\n-                + \"\\\"\");\n-      }\n-      if (indexDimensionCount > PointValues.MAX_INDEX_DIMENSIONS) {\n-        throw new IllegalArgumentException(\n-            \"indexDimensionCount must be <= PointValues.MAX_INDEX_DIMENSIONS (= \"\n-                + PointValues.MAX_INDEX_DIMENSIONS\n-                + \"); got \"\n-                + indexDimensionCount\n-                + \" for field=\\\"\"\n-                + name\n-                + \"\\\"\");\n-      }\n-      verifyConsistentDimensions(\n-          number, name, dimensionCount, indexDimensionCount, dimensionNumBytes);\n-      dimensions.put(\n-          name, new FieldDimensions(dimensionCount, indexDimensionCount, dimensionNumBytes));\n-    }\n-\n-    synchronized void setVectorDimensionsAndSearchStrategy(\n-        int number, String name, int numDimensions, VectorValues.SearchStrategy searchStrategy) {\n-      if (numDimensions <= 0) {\n-        throw new IllegalArgumentException(\n-            \"vector numDimensions must be > 0; got \" + numDimensions);\n-      }\n-      if (numDimensions > VectorValues.MAX_DIMENSIONS) {\n-        throw new IllegalArgumentException(\n-            \"vector numDimensions must be <= VectorValues.MAX_DIMENSIONS (=\"\n-                + VectorValues.MAX_DIMENSIONS\n-                + \"); got \"\n-                + numDimensions);\n-      }\n-      verifyConsistentVectorProperties(number, name, numDimensions, searchStrategy);\n-      vectorProps.put(name, new FieldVectorProperties(numDimensions, searchStrategy));\n-    }\n   }\n \n   static final class Builder {\n@@ -786,8 +620,24 @@ synchronized void setVectorDimensionsAndSearchStrategy(\n       this.globalFieldNumbers = globalFieldNumbers;\n     }\n \n+    /**\n+     * Adds the given FieldInfos to this Builder, if these fields don't exist in this Builder. Also\n+     * adds new fields with theirs schema options to the global FieldNumbers if these fields don't\n+     * exist globally in the index.\n+     *\n+     * <p>If any field already exists: 1) the provided FieldInfo's schema is checked against the\n+     * existing field and 2) the provided FieldInfo's attributes are added to the existing\n+     * FieldInfo's attributes.\n+     *\n+     * @param other – FieldInfos to add\n+     * @throws IllegalArgumentException if there already exists field with this name in Builder but\n+     *     with a different schema\n+     * @throws IllegalArgumentException if there already exists field with this name globally but\n+     *     with a different schema.\n+     * @throws IllegalStateException if the Builder is already finished building and doesn't accept\n+     *     new fields.\n+     */\n     public void add(FieldInfos other) {\n-      assert assertNotFinished();\n       for (FieldInfo fieldInfo : other) {\n         add(fieldInfo);\n       }\n@@ -796,167 +646,161 @@ public void add(FieldInfos other) {\n     /** Create a new field, or return existing one. */\n     public FieldInfo getOrAdd(String name) {\n       FieldInfo fi = fieldInfo(name);\n-      if (fi == null) {\n-        assert assertNotFinished();\n-        // This field wasn't yet added to this in-RAM\n-        // segment's FieldInfo, so now we get a global\n-        // number for this field.  If the field was seen\n-        // before then we'll get the same name and number,\n-        // else we'll allocate a new one:\n-        final boolean isSoftDeletesField = name.equals(globalFieldNumbers.softDeletesFieldName);\n-        final int fieldNumber =\n-            globalFieldNumbers.addOrGet(\n-                name,\n-                -1,\n-                IndexOptions.NONE,\n-                DocValuesType.NONE,\n-                0,\n-                0,\n-                0,\n-                0,\n-                VectorValues.SearchStrategy.NONE,\n-                isSoftDeletesField);\n-        fi =\n-            new FieldInfo(\n-                name,\n-                fieldNumber,\n-                false,\n-                false,\n-                false,\n-                IndexOptions.NONE,\n-                DocValuesType.NONE,\n-                -1,\n-                new HashMap<>(),\n-                0,\n-                0,\n-                0,\n-                0,\n-                VectorValues.SearchStrategy.NONE,\n-                isSoftDeletesField);\n-        assert !byName.containsKey(fi.name);\n-        globalFieldNumbers.verifyConsistent(\n-            Integer.valueOf(fi.number), fi.name, DocValuesType.NONE);\n-        byName.put(fi.name, fi);\n+      if (fi != null) {\n+        return fi;\n+      } else {\n+        return add(\n+            name,\n+            -1,\n+            false,\n+            false,\n+            false,\n+            IndexOptions.NONE,\n+            DocValuesType.NONE,\n+            -1,\n+            new HashMap<>(),\n+            0,\n+            0,\n+            0,\n+            0,\n+            VectorValues.SearchStrategy.NONE,\n+            name.equals(globalFieldNumbers.softDeletesFieldName));\n       }\n+    }\n \n-      return fi;\n+    /**\n+     * Adds the given FieldInfo to this Builder if this field doesn't exist in this Builder. Also\n+     * adds a new field with its schema options to the global FieldNumbers if the field doesn't\n+     * exist globally in the index.\n+     *\n+     * <p>If the field already exists: 1) the provided FieldInfo's schema is checked against the\n+     * existing field and 2) the provided FieldInfo's attributes are added to the existing\n+     * FieldInfo's attributes.\n+     *\n+     * @param fi – FieldInfo to add\n+     * @throws IllegalArgumentException if there already exists field with this name in Builder but\n+     *     with a different schema\n+     * @throws IllegalArgumentException if there already exists field with this name globally but\n+     *     with a different schema.\n+     * @throws IllegalStateException if the Builder is already finished building and doesn't accept\n+     *     new fields.\n+     */\n+    public void add(FieldInfo fi) {\n+      add(fi, -1);\n     }\n \n-    private FieldInfo addOrUpdateInternal(\n+    /**\n+     * Adds the given FieldInfo with the given doc values generation to this Builder if this field\n+     * doesn't exist in this Builder. Also adds a new field with its schema options to the global\n+     * FieldNumbers if the field doesn't exist globally in the index.\n+     *\n+     * <p>If the field already exists: 1) the provided FieldInfo's schema is checked against the\n+     * existing field and 2) the provided FieldInfo's attributes are added to the existing\n+     * FieldInfo's attributes.\n+     *\n+     * @param fi – FieldInfo to add\n+     * @param dvGen – doc values generation\n+     * @throws IllegalArgumentException if there already exists field with this name in Builder but\n+     *     with a different schema\n+     * @throws IllegalArgumentException if there already exists field with this name globally but\n+     *     with a different schema.\n+     * @throws IllegalStateException if the Builder is already finished building and doesn't accept\n+     *     new fields.\n+     */\n+    public void add(FieldInfo fi, long dvGen) {\n+      // IMPORTANT - reuse the field number if possible for consistent field numbers across segments\n+      if (fi.getDocValuesType() == null) {\n+        throw new NullPointerException(\"DocValuesType must not be null\");\n+      }\n+      final FieldInfo curFi = fieldInfo(fi.name);\n+      if (curFi == null) {\n+        // original attributes is UnmodifiableMap\n+        Map<String, String> attributes =\n+            fi.attributes() == null ? null : new HashMap<>(fi.attributes());\n+        add(\n+            fi.name,\n+            fi.number,\n+            fi.hasVectors(),\n+            fi.omitsNorms(),\n+            fi.hasPayloads(),\n+            fi.getIndexOptions(),\n+            fi.getDocValuesType(),\n+            dvGen,\n+            attributes,\n+            fi.getPointDimensionCount(),\n+            fi.getPointIndexDimensionCount(),\n+            fi.getPointNumBytes(),\n+            fi.getVectorDimension(),\n+            fi.getVectorSearchStrategy(),\n+            fi.isSoftDeletesField());\n+      } else {\n+        curFi.verifySameSchema(fi);\n+        if (fi.attributes() != null) {\n+          fi.attributes().forEach((k, v) -> curFi.putAttribute(k, v));\n+        }\n+      }\n+    }\n+\n+    /**\n+     * Adds a new FieldInfo with the provided schema options to this Builder if this field doesn't\n+     * exist in this Builder. Also adds a new field with its schema options to the global\n+     * FieldNumbers if the field doesn't exist globally in the index.\n+     *\n+     * <p>If the field already exists: 1) the provided FieldInfo's schema is checked against the\n+     * existing field and 2) the provided FieldInfo's attributes are added to the existing\n+     * FieldInfo's attributes.\n+     *\n+     * @param name – name of the field\n+     * @param storeTermVector – if term vectors are stored\n+     * @param omitNorms – if norms are not stored\n+     * @param storePayloads – if payloads are stored\n+     * @param indexOptions – index options\n+     * @param docValuesType – type of doc values\n+     * @param dvGen - doc values generation\n+     * @param attributes – attributes\n+     * @param dataDimensionCount – number of point data dimensions\n+     * @param indexDimensionCount – number of point index dimensions\n+     * @param dimensionNumBytes – number of bytes in a dimension\n+     * @param vectorDimension – number of vector dimensions\n+     * @param vectorSearchStrategy – vector strategy\n+     * @return a created FieldInfo based on the provided schema options\n+     * @throws IllegalArgumentException if there already exists field with this name in Builder but\n+     *     with a different schema\n+     * @throws IllegalArgumentException if there already exists field with this name globally but\n+     *     with a different schema.\n+     * @throws IllegalStateException if the Builder is already finished building and doesn't accept\n+     *     new fields.\n+     */\n+    public FieldInfo add(\n         String name,\n-        int preferredFieldNumber,\n         boolean storeTermVector,\n         boolean omitNorms,\n         boolean storePayloads,\n         IndexOptions indexOptions,\n-        DocValuesType docValues,\n+        DocValuesType docValuesType,\n         long dvGen,\n         Map<String, String> attributes,\n         int dataDimensionCount,\n         int indexDimensionCount,\n         int dimensionNumBytes,\n         int vectorDimension,\n-        VectorValues.SearchStrategy vectorSearchStrategy,\n-        boolean isSoftDeletesField) {\n-      assert assertNotFinished();\n-      if (docValues == null) {\n-        throw new NullPointerException(\"DocValuesType must not be null\");\n-      }\n-      if (attributes != null) {\n-        // original attributes is UnmodifiableMap\n-        attributes = new HashMap<>(attributes);\n-      }\n-\n-      FieldInfo fi = fieldInfo(name);\n-      if (fi == null) {\n-        // This field wasn't yet added to this in-RAM\n-        // segment's FieldInfo, so now we get a global\n-        // number for this field.  If the field was seen\n-        // before then we'll get the same name and number,\n-        // else we'll allocate a new one:\n-        final int fieldNumber =\n-            globalFieldNumbers.addOrGet(\n-                name,\n-                preferredFieldNumber,\n-                indexOptions,\n-                docValues,\n-                dataDimensionCount,\n-                indexDimensionCount,\n-                dimensionNumBytes,\n-                vectorDimension,\n-                vectorSearchStrategy,\n-                isSoftDeletesField);\n-        fi =\n-            new FieldInfo(\n-                name,\n-                fieldNumber,\n-                storeTermVector,\n-                omitNorms,\n-                storePayloads,\n-                indexOptions,\n-                docValues,\n-                dvGen,\n-                attributes,\n-                dataDimensionCount,\n-                indexDimensionCount,\n-                dimensionNumBytes,\n-                vectorDimension,\n-                vectorSearchStrategy,\n-                isSoftDeletesField);\n-        assert !byName.containsKey(fi.name);\n-        globalFieldNumbers.verifyConsistent(\n-            Integer.valueOf(fi.number), fi.name, fi.getDocValuesType());\n-        byName.put(fi.name, fi);\n-      } else {\n-        fi.update(\n-            storeTermVector,\n-            omitNorms,\n-            storePayloads,\n-            indexOptions,\n-            attributes,\n-            dataDimensionCount,\n-            indexDimensionCount,\n-            dimensionNumBytes);\n-\n-        if (docValues != DocValuesType.NONE) {\n-          // Only pay the synchronization cost if fi does not already have a DVType\n-          boolean updateGlobal = fi.getDocValuesType() == DocValuesType.NONE;\n-          if (updateGlobal) {\n-            // Must also update docValuesType map so it's\n-            // aware of this field's DocValuesType.  This will throw IllegalArgumentException if\n-            // an illegal type change was attempted.\n-            globalFieldNumbers.setDocValuesType(fi.number, name, docValues);\n-          }\n-\n-          fi.setDocValuesType(docValues); // this will also perform the consistency check.\n-          fi.setDocValuesGen(dvGen);\n-        }\n-      }\n-      return fi;\n-    }\n-\n-    public FieldInfo add(FieldInfo fi) {\n-      return add(fi, -1);\n-    }\n-\n-    public FieldInfo add(FieldInfo fi, long dvGen) {\n-      // IMPORTANT - reuse the field number if possible for consistent field numbers across segments\n-      return addOrUpdateInternal(\n-          fi.name,\n-          fi.number,\n-          fi.hasVectors(),\n-          fi.omitsNorms(),\n-          fi.hasPayloads(),\n-          fi.getIndexOptions(),\n-          fi.getDocValuesType(),\n+        VectorValues.SearchStrategy vectorSearchStrategy) {\n+      return add(\n+          name,\n+          -1,\n+          storeTermVector,\n+          omitNorms,\n+          storePayloads,\n+          indexOptions,\n+          docValuesType,\n           dvGen,\n-          fi.attributes(),\n-          fi.getPointDimensionCount(),\n-          fi.getPointIndexDimensionCount(),\n-          fi.getPointNumBytes(),\n-          fi.getVectorDimension(),\n-          fi.getVectorSearchStrategy(),\n-          fi.isSoftDeletesField());\n+          attributes,\n+          dataDimensionCount,\n+          indexDimensionCount,\n+          dimensionNumBytes,\n+          vectorDimension,\n+          vectorSearchStrategy,\n+          name.equals(globalFieldNumbers.softDeletesFieldName));\n     }\n \n     public FieldInfo fieldInfo(String fieldName) {\n@@ -972,6 +816,64 @@ private boolean assertNotFinished() {\n       return true;\n     }\n \n+    private FieldInfo add(\n+        String name,\n+        int preferredFieldNumber,\n+        boolean storeTermVector,\n+        boolean omitNorms,\n+        boolean storePayloads,\n+        IndexOptions indexOptions,\n+        DocValuesType docValues,\n+        long dvGen,\n+        Map<String, String> attributes,\n+        int dataDimensionCount,\n+        int indexDimensionCount,\n+        int dimensionNumBytes,\n+        int vectorDimension,\n+        VectorValues.SearchStrategy vectorSearchStrategy,\n+        boolean isSoftDeletesField) {\n+      // This field wasn't yet added to this in-RAM\n+      // segment's FieldInfo, so now we get a global\n+      // number for this field.  If the field was seen\n+      // before then we'll get the same name and number,\n+      // else we'll allocate a new one:\n+      assert assertNotFinished();\n+      final int fieldNumber =\n+          globalFieldNumbers.addOrGet(\n+              name,\n+              preferredFieldNumber,\n+              indexOptions,\n+              storeTermVector,\n+              omitNorms,\n+              docValues,\n+              dataDimensionCount,\n+              indexDimensionCount,\n+              dimensionNumBytes,\n+              vectorDimension,\n+              vectorSearchStrategy,\n+              isSoftDeletesField);\n+      FieldInfo fi =\n+          new FieldInfo(\n+              name,\n+              fieldNumber,\n+              storeTermVector,\n+              omitNorms,\n+              storePayloads,\n+              indexOptions,\n+              docValues,\n+              dvGen,\n+              attributes,\n+              dataDimensionCount,\n+              indexDimensionCount,\n+              dimensionNumBytes,\n+              vectorDimension,\n+              vectorSearchStrategy,\n+              isSoftDeletesField);\n+      assert byName.containsKey(fi.name) == false;\n+      byName.put(fi.name, fi);\n+      return fi;\n+    }\n+\n     FieldInfos finish() {\n       finished = true;\n       return new FieldInfos(byName.values().toArray(new FieldInfo[byName.size()]));"
  },
  {
    "sha": "15c3e244180a1e6c0ff55e8e2de2fd35c94409aa",
    "filename": "lucene/core/src/java/org/apache/lucene/index/IndexWriter.java",
    "status": "modified",
    "additions": 12,
    "deletions": 25,
    "changes": 37,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -1262,6 +1262,8 @@ private FieldNumbers getFieldNumberMap() throws IOException {\n             fi.name,\n             fi.number,\n             fi.getIndexOptions(),\n+            fi.hasVectors(),\n+            fi.omitsNorms(),\n             fi.getDocValuesType(),\n             fi.getPointDimensionCount(),\n             fi.getPointIndexDimensionCount(),\n@@ -1852,7 +1854,7 @@ public long softUpdateDocument(\n   /**\n    * Updates a document's {@link NumericDocValues} for <code>field</code> to the given <code>value\n    * </code>. You can only update fields that already exist in the index, not add new fields through\n-   * this method.\n+   * this method. You can only update fields that were indexed with doc values only.\n    *\n    * @param term the term to identify the document(s) to be updated\n    * @param field field name of the {@link NumericDocValues} field\n@@ -1863,9 +1865,7 @@ public long softUpdateDocument(\n    */\n   public long updateNumericDocValue(Term term, String field, long value) throws IOException {\n     ensureOpen();\n-    if (!globalFieldNumberMap.contains(field, DocValuesType.NUMERIC)) {\n-      throw new IllegalArgumentException(\"can only update existing numeric-docvalues fields!\");\n-    }\n+    globalFieldNumberMap.verifyDvOnlyField(field, DocValuesType.NUMERIC, true);\n     if (config.getIndexSortFields().contains(field)) {\n       throw new IllegalArgumentException(\n           \"cannot update docvalues field involved in the index sort, field=\"\n@@ -1885,7 +1885,7 @@ public long updateNumericDocValue(Term term, String field, long value) throws IO\n   /**\n    * Updates a document's {@link BinaryDocValues} for <code>field</code> to the given <code>value\n    * </code>. You can only update fields that already exist in the index, not add new fields through\n-   * this method.\n+   * this method. You can only update fields that were indexed only with doc values.\n    *\n    * <p><b>NOTE:</b> this method currently replaces the existing value of all affected documents\n    * with the new value.\n@@ -1902,9 +1902,7 @@ public long updateBinaryDocValue(Term term, String field, BytesRef value) throws\n     if (value == null) {\n       throw new IllegalArgumentException(\"cannot update a field to a null value: \" + field);\n     }\n-    if (!globalFieldNumberMap.contains(field, DocValuesType.BINARY)) {\n-      throw new IllegalArgumentException(\"can only update existing binary-docvalues fields!\");\n-    }\n+    globalFieldNumberMap.verifyDvOnlyField(field, DocValuesType.BINARY, true);\n     try {\n       return maybeProcessEvents(\n           docWriter.updateDocValues(new BinaryDocValuesUpdate(term, field, value)));\n@@ -1949,23 +1947,10 @@ public long updateDocValues(Term term, Field... updates) throws IOException {\n         throw new IllegalArgumentException(\n             \"can only update NUMERIC or BINARY fields! field=\" + f.name());\n       }\n-      if (globalFieldNumberMap.contains(f.name(), dvType) == false) {\n-        // if this field doesn't exists we try to add it. if it exists and the DV type doesn't match\n-        // we\n-        // get a consistent error message as if you try to do that during an indexing operation.\n-        globalFieldNumberMap.addOrGet(\n-            f.name(),\n-            -1,\n-            IndexOptions.NONE,\n-            dvType,\n-            0,\n-            0,\n-            0,\n-            0,\n-            VectorValues.SearchStrategy.NONE,\n-            f.name().equals(config.softDeletesField));\n-        assert globalFieldNumberMap.contains(f.name(), dvType);\n-      }\n+      // if this field doesn't exists we try to add it.\n+      // if it exists and the DV type doesn't match or it is not DV only field,\n+      // we will get an error.\n+      globalFieldNumberMap.verifyDvOnlyField(f.name(), dvType, false);\n       if (config.getIndexSortFields().contains(f.name())) {\n         throw new IllegalArgumentException(\n             \"cannot update docvalues field involved in the index sort, field=\"\n@@ -3027,6 +3012,8 @@ public long addIndexes(Directory... dirs) throws IOException {\n                   fi.name,\n                   fi.number,\n                   fi.getIndexOptions(),\n+                  fi.hasVectors(),\n+                  fi.omitsNorms(),\n                   fi.getDocValuesType(),\n                   fi.getPointDimensionCount(),\n                   fi.getPointIndexDimensionCount(),"
  },
  {
    "sha": "129a0f93b940c19a39795d2d2e0d1e6200ca98f8",
    "filename": "lucene/core/src/java/org/apache/lucene/index/IndexingChain.java",
    "status": "modified",
    "additions": 345,
    "deletions": 231,
    "changes": 576,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/java/org/apache/lucene/index/IndexingChain.java",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/java/org/apache/lucene/index/IndexingChain.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/core/src/java/org/apache/lucene/index/IndexingChain.java?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -79,6 +79,7 @@\n \n   // Holds fields seen in each document\n   private PerField[] fields = new PerField[1];\n+  private PerField[] docFields = new PerField[2];\n   private final InfoStream infoStream;\n   private final ByteBlockPool.Allocator byteBlockAllocator;\n   private final LiveIndexWriterConfig indexWriterConfig;\n@@ -349,7 +350,8 @@ private void writePoints(SegmentWriteState state, Sorter.DocMap sortMap) throws\n \n             perField.pointValuesWriter.flush(state, sortMap, pointsWriter);\n             perField.pointValuesWriter = null;\n-          } else if (perField.fieldInfo.getPointDimensionCount() != 0) {\n+          } else if (perField.fieldInfo != null\n+              && perField.fieldInfo.getPointDimensionCount() != 0) {\n             // BUG\n             throw new AssertionError(\n                 \"segment=\"\n@@ -399,7 +401,8 @@ private void writeDocValues(SegmentWriteState state, Sorter.DocMap sortMap) thro\n             }\n             perField.docValuesWriter.flush(state, sortMap, dvConsumer);\n             perField.docValuesWriter = null;\n-          } else if (perField.fieldInfo.getDocValuesType() != DocValuesType.NONE) {\n+          } else if (perField.fieldInfo != null\n+              && perField.fieldInfo.getDocValuesType() != DocValuesType.NONE) {\n             // BUG\n             throw new AssertionError(\n                 \"segment=\"\n@@ -470,7 +473,7 @@ private void writeVectors(SegmentWriteState state, Sorter.DocMap sortMap) throws\n \n             perField.vectorValuesWriter.flush(sortMap, vectorWriter);\n             perField.vectorValuesWriter = null;\n-          } else if (perField.fieldInfo.getVectorDimension() != 0) {\n+          } else if (perField.fieldInfo != null && perField.fieldInfo.getVectorDimension() != 0) {\n             // BUG\n             throw new AssertionError(\n                 \"segment=\"\n@@ -548,7 +551,7 @@ private void rehash() {\n     for (int j = 0; j < fieldHash.length; j++) {\n       PerField fp0 = fieldHash[j];\n       while (fp0 != null) {\n-        final int hashPos2 = fp0.fieldInfo.name.hashCode() & newHashMask;\n+        final int hashPos2 = fp0.fieldName.hashCode() & newHashMask;\n         PerField nextFP0 = fp0.next;\n         fp0.next = newHashArray[hashPos2];\n         newHashArray[hashPos2] = fp0;\n@@ -581,78 +584,166 @@ private void finishStoredFields() throws IOException {\n   }\n \n   void processDocument(int docID, Iterable<? extends IndexableField> document) throws IOException {\n-\n-    // How many indexed field names we've seen (collapses\n-    // multiple field instances by the same name):\n+    // number of unique fields by names (collapses multiple field instances by the same name)\n     int fieldCount = 0;\n-\n+    int indexedFieldCount = 0; // number of unique fields indexed with postings\n     long fieldGen = nextFieldGen++;\n+    int docFieldIdx = 0;\n \n     // NOTE: we need two passes here, in case there are\n     // multi-valued fields, because we must process all\n     // instances of a given field at once, since the\n     // analyzer is free to reuse TokenStream across fields\n     // (i.e., we cannot have more than one TokenStream\n     // running \"at once\"):\n-\n     termsHash.startDocument();\n-\n     startStoredFields(docID);\n     try {\n+      // 1st pass over doc fields – verify that doc schema matches the index schema\n+      // build schema for each unique doc field\n+      for (IndexableField field : document) {\n+        IndexableFieldType fieldType = field.fieldType();\n+        PerField pf = getOrAddPerField(field.name(), fieldType);\n+        if (pf.fieldGen != fieldGen) { // first time we see this field in this document\n+          fields[fieldCount++] = pf;\n+          pf.fieldGen = fieldGen;\n+          pf.reset(docID);\n+        }\n+        if (docFieldIdx >= docFields.length) oversizeDocFields();\n+        docFields[docFieldIdx++] = pf;\n+        updateDocFieldSchema(field.name(), pf.schema, fieldType);\n+      }\n+      // for each field verify that its schema within the current doc matches its schema in the\n+      // index\n+      for (int i = 0; i < fieldCount; i++) {\n+        PerField pf = fields[i];\n+        if (pf.fieldInfo == null) { // the first time we see this field in this segment\n+          initializeFieldInfo(pf);\n+        } else {\n+          pf.schema.assertSameSchema(pf.fieldInfo);\n+        }\n+      }\n+\n+      // 2nd pass over doc fields – index each field\n+      // also count the number of unique fields indexed with postings\n+      docFieldIdx = 0;\n       for (IndexableField field : document) {\n-        fieldCount = processField(docID, field, fieldGen, fieldCount);\n+        if (processField(docID, field, docFields[docFieldIdx])) {\n+          fields[indexedFieldCount] = docFields[docFieldIdx];\n+          indexedFieldCount++;\n+        }\n+        docFieldIdx++;\n       }\n     } finally {\n       if (hasHitAbortingException == false) {\n         // Finish each indexed field name seen in the document:\n-        for (int i = 0; i < fieldCount; i++) {\n+        for (int i = 0; i < indexedFieldCount; i++) {\n           fields[i].finish(docID);\n         }\n         finishStoredFields();\n+        // TODO: for broken docs, optimize termsHash.finishDocument\n+        try {\n+          termsHash.finishDocument(docID);\n+        } catch (Throwable th) {\n+          // Must abort, on the possibility that on-disk term\n+          // vectors are now corrupt:\n+          abortingExceptionConsumer.accept(th);\n+          throw th;\n+        }\n       }\n     }\n-\n-    try {\n-      termsHash.finishDocument(docID);\n-    } catch (Throwable th) {\n-      // Must abort, on the possibility that on-disk term\n-      // vectors are now corrupt:\n-      abortingExceptionConsumer.accept(th);\n-      throw th;\n-    }\n   }\n \n-  private int processField(int docID, IndexableField field, long fieldGen, int fieldCount)\n-      throws IOException {\n-    String fieldName = field.name();\n-    IndexableFieldType fieldType = field.fieldType();\n+  private void oversizeDocFields() {\n+    PerField[] newDocFields =\n+        new PerField\n+            [ArrayUtil.oversize(docFields.length + 1, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];\n+    System.arraycopy(docFields, 0, newDocFields, 0, docFields.length);\n+    docFields = newDocFields;\n+  }\n \n-    PerField fp = null;\n+  private void initializeFieldInfo(PerField pf) throws IOException {\n+    // Create and add a new fieldInfo to fieldInfos for this segment.\n+    // During the creation of FieldInfo there is also verification of the correctness of all its\n+    // parameters.\n+\n+    // If the fieldInfo doesn't exist in globalFieldNumbers for the whole index, it will be added\n+    // there.\n+    // If the field already exists in globalFieldNumbers (i.e. field present in other segments),\n+    // we check consistency of its schema with schema for the whole index.\n+    FieldSchema s = pf.schema;\n+    if (indexWriterConfig.getIndexSort() != null && s.docValuesType != DocValuesType.NONE) {\n+      final Sort indexSort = indexWriterConfig.getIndexSort();\n+      validateIndexSortDVType(indexSort, pf.fieldName, s.docValuesType);\n+    }\n \n-    if (fieldType.indexOptions() == null) {\n-      throw new NullPointerException(\n-          \"IndexOptions must not be null (field: \\\"\" + field.name() + \"\\\")\");\n+    FieldInfo fi =\n+        fieldInfos.add(\n+            pf.fieldName,\n+            s.storeTermVector,\n+            s.omitNorms,\n+            false,\n+            s.indexOptions,\n+            s.docValuesType,\n+            s.dvGen,\n+            s.attributes,\n+            s.pointDimensionCount,\n+            s.pointIndexDimensionCount,\n+            s.pointNumBytes,\n+            s.vectorDimension,\n+            s.vectorSearchStrategy);\n+    pf.setFieldInfo(fi);\n+    if (fi.getIndexOptions() != IndexOptions.NONE) {\n+      pf.setInvertState();\n     }\n+    DocValuesType dvType = fi.getDocValuesType();\n+    if (dvType != DocValuesType.NONE) {\n+      switch (dvType) {\n+        case NUMERIC:\n+          pf.docValuesWriter = new NumericDocValuesWriter(fi, bytesUsed);\n+          break;\n+        case BINARY:\n+          pf.docValuesWriter = new BinaryDocValuesWriter(fi, bytesUsed);\n+          break;\n+        case SORTED:\n+          pf.docValuesWriter = new SortedDocValuesWriter(fi, bytesUsed);\n+          break;\n+        case SORTED_NUMERIC:\n+          pf.docValuesWriter = new SortedNumericDocValuesWriter(fi, bytesUsed);\n+          break;\n+        case SORTED_SET:\n+          pf.docValuesWriter = new SortedSetDocValuesWriter(fi, bytesUsed);\n+          break;\n+        default:\n+          throw new AssertionError(\"unrecognized DocValues.Type: \" + dvType);\n+      }\n+    }\n+    if (fi.getPointDimensionCount() != 0) {\n+      pf.pointValuesWriter = new PointValuesWriter(byteBlockAllocator, bytesUsed, fi);\n+    }\n+    if (fi.getVectorDimension() != 0) {\n+      pf.vectorValuesWriter = new VectorValuesWriter(fi, bytesUsed);\n+    }\n+  }\n \n-    // Invert indexed fields:\n-    if (fieldType.indexOptions() != IndexOptions.NONE) {\n-      fp = getOrAddField(fieldName, fieldType, true);\n-      boolean first = fp.fieldGen != fieldGen;\n-      fp.invert(docID, field, first);\n+  /** Index each field Returns {@code true}, if we are indexing a unique field with postings */\n+  private boolean processField(int docID, IndexableField field, PerField pf) throws IOException {\n+    IndexableFieldType fieldType = field.fieldType();\n+    boolean indexedField = false;\n \n-      if (first) {\n-        fields[fieldCount++] = fp;\n-        fp.fieldGen = fieldGen;\n+    // Invert indexed fields\n+    if (fieldType.indexOptions() != IndexOptions.NONE) {\n+      if (pf.first) { // first time we see this field in this doc\n+        pf.invert(docID, field, true);\n+        pf.first = false;\n+        indexedField = true;\n+      } else {\n+        pf.invert(docID, field, false);\n       }\n-    } else {\n-      verifyUnIndexedFieldType(fieldName, fieldType);\n     }\n \n-    // Add stored fields:\n+    // Add stored fields\n     if (fieldType.stored()) {\n-      if (fp == null) {\n-        fp = getOrAddField(fieldName, fieldType, false);\n-      }\n       String value = field.stringValue();\n       if (value != null && value.length() > IndexWriter.MAX_STORED_STRING_LENGTH) {\n         throw new IllegalArgumentException(\n@@ -663,38 +754,91 @@ private int processField(int docID, IndexableField field, long fieldGen, int fie\n                 + \" characters) to store\");\n       }\n       try {\n-        storedFieldsConsumer.writeField(fp.fieldInfo, field);\n+        storedFieldsConsumer.writeField(pf.fieldInfo, field);\n       } catch (Throwable th) {\n         onAbortingException(th);\n         throw th;\n       }\n     }\n \n     DocValuesType dvType = fieldType.docValuesType();\n-    if (dvType == null) {\n-      throw new NullPointerException(\n-          \"docValuesType must not be null (field: \\\"\" + fieldName + \"\\\")\");\n-    }\n     if (dvType != DocValuesType.NONE) {\n-      if (fp == null) {\n-        fp = getOrAddField(fieldName, fieldType, false);\n-      }\n-      indexDocValue(docID, fp, dvType, field);\n+      indexDocValue(docID, pf, dvType, field);\n     }\n     if (fieldType.pointDimensionCount() != 0) {\n-      if (fp == null) {\n-        fp = getOrAddField(fieldName, fieldType, false);\n-      }\n-      indexPoint(docID, fp, field);\n+      pf.pointValuesWriter.addPackedValue(docID, field.binaryValue());\n     }\n     if (fieldType.vectorDimension() != 0) {\n-      if (fp == null) {\n-        fp = getOrAddField(fieldName, fieldType, false);\n+      pf.vectorValuesWriter.addValue(docID, ((VectorField) field).vectorValue());\n+    }\n+    return indexedField;\n+  }\n+\n+  /**\n+   * Returns a previously created {@link PerField}, absorbing the type information from {@link\n+   * FieldType}, and creates a new {@link PerField} if this field name wasn't seen yet.\n+   */\n+  private PerField getOrAddPerField(String fieldName, IndexableFieldType fieldType) {\n+    final int hashPos = fieldName.hashCode() & hashMask;\n+    PerField pf = fieldHash[hashPos];\n+    while (pf != null && pf.fieldName.equals(fieldName) == false) {\n+      pf = pf.next;\n+    }\n+    if (pf == null) {\n+      // first time we encounter field with this name in this segment\n+      FieldSchema schema = new FieldSchema(fieldName);\n+      Map<String, String> attributes = fieldType.getAttributes();\n+      if (attributes != null) {\n+        attributes.forEach((k, v) -> schema.putAttribute(k, v));\n+      }\n+      pf =\n+          new PerField(\n+              fieldName,\n+              indexCreatedVersionMajor,\n+              schema,\n+              indexWriterConfig.getSimilarity(),\n+              indexWriterConfig.getInfoStream(),\n+              indexWriterConfig.getAnalyzer());\n+      pf.next = fieldHash[hashPos];\n+      fieldHash[hashPos] = pf;\n+      totalFieldCount++;\n+      // At most 50% load factor:\n+      if (totalFieldCount >= fieldHash.length / 2) {\n+        rehash();\n+      }\n+      if (totalFieldCount > fields.length) {\n+        PerField[] newFields =\n+            new PerField\n+                [ArrayUtil.oversize(totalFieldCount, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];\n+        System.arraycopy(fields, 0, newFields, 0, fields.length);\n+        fields = newFields;\n       }\n-      indexVector(docID, fp, field);\n     }\n+    return pf;\n+  }\n \n-    return fieldCount;\n+  // update schema for field as seen in a particular document\n+  private static void updateDocFieldSchema(\n+      String fieldName, FieldSchema schema, IndexableFieldType fieldType) {\n+    if (fieldType.indexOptions() != IndexOptions.NONE) {\n+      schema.setIndexOptions(\n+          fieldType.indexOptions(), fieldType.omitNorms(), fieldType.storeTermVectors());\n+    } else {\n+      // TODO: should this be checked when a fieldType is created?\n+      verifyUnIndexedFieldType(fieldName, fieldType);\n+    }\n+    if (fieldType.docValuesType() != DocValuesType.NONE) {\n+      schema.setDocValues(fieldType.docValuesType(), -1);\n+    }\n+    if (fieldType.pointDimensionCount() != 0) {\n+      schema.setPoints(\n+          fieldType.pointDimensionCount(),\n+          fieldType.pointIndexDimensionCount(),\n+          fieldType.pointNumBytes());\n+    }\n+    if (fieldType.vectorDimension() != 0) {\n+      schema.setVectors(fieldType.vectorSearchStrategy(), fieldType.vectorDimension());\n+    }\n   }\n \n   private static void verifyUnIndexedFieldType(String name, IndexableFieldType ft) {\n@@ -728,33 +872,6 @@ private static void verifyUnIndexedFieldType(String name, IndexableFieldType ft)\n     }\n   }\n \n-  /** Called from processDocument to index one field's point */\n-  private void indexPoint(int docID, PerField fp, IndexableField field) {\n-    int pointDimensionCount = field.fieldType().pointDimensionCount();\n-    int pointIndexDimensionCount = field.fieldType().pointIndexDimensionCount();\n-\n-    int dimensionNumBytes = field.fieldType().pointNumBytes();\n-\n-    // Record dimensions for this field; this setter will throw IllegalArgExc if\n-    // the dimensions were already set to something different:\n-    if (fp.fieldInfo.getPointDimensionCount() == 0) {\n-      fieldInfos.globalFieldNumbers.setDimensions(\n-          fp.fieldInfo.number,\n-          fp.fieldInfo.name,\n-          pointDimensionCount,\n-          pointIndexDimensionCount,\n-          dimensionNumBytes);\n-    }\n-\n-    fp.fieldInfo.setPointDimensions(\n-        pointDimensionCount, pointIndexDimensionCount, dimensionNumBytes);\n-\n-    if (fp.pointValuesWriter == null) {\n-      fp.pointValuesWriter = new PointValuesWriter(byteBlockAllocator, bytesUsed, fp.fieldInfo);\n-    }\n-    fp.pointValuesWriter.addPackedValue(docID, field.binaryValue());\n-  }\n-\n   private void validateIndexSortDVType(Sort indexSort, String fieldToValidate, DocValuesType dvType)\n       throws IOException {\n     for (SortField sortField : indexSort.getSort()) {\n@@ -850,28 +967,9 @@ public FieldInfos getFieldInfos() {\n   }\n \n   /** Called from processDocument to index one field's doc value */\n-  private void indexDocValue(int docID, PerField fp, DocValuesType dvType, IndexableField field)\n-      throws IOException {\n-\n-    if (fp.fieldInfo.getDocValuesType() == DocValuesType.NONE) {\n-      // This is the first time we are seeing this field indexed with doc values, so we\n-      // now record the DV type so that any future attempt to (illegally) change\n-      // the DV type of this field, will throw an IllegalArgExc:\n-      if (indexWriterConfig.getIndexSort() != null) {\n-        final Sort indexSort = indexWriterConfig.getIndexSort();\n-        validateIndexSortDVType(indexSort, fp.fieldInfo.name, dvType);\n-      }\n-      fieldInfos.globalFieldNumbers.setDocValuesType(\n-          fp.fieldInfo.number, fp.fieldInfo.name, dvType);\n-    }\n-\n-    fp.fieldInfo.setDocValuesType(dvType);\n-\n+  private void indexDocValue(int docID, PerField fp, DocValuesType dvType, IndexableField field) {\n     switch (dvType) {\n       case NUMERIC:\n-        if (fp.docValuesWriter == null) {\n-          fp.docValuesWriter = new NumericDocValuesWriter(fp.fieldInfo, bytesUsed);\n-        }\n         if (field.numericValue() == null) {\n           throw new IllegalArgumentException(\n               \"field=\\\"\" + fp.fieldInfo.name + \"\\\": null value not allowed\");\n@@ -881,31 +979,19 @@ private void indexDocValue(int docID, PerField fp, DocValuesType dvType, Indexab\n         break;\n \n       case BINARY:\n-        if (fp.docValuesWriter == null) {\n-          fp.docValuesWriter = new BinaryDocValuesWriter(fp.fieldInfo, bytesUsed);\n-        }\n         ((BinaryDocValuesWriter) fp.docValuesWriter).addValue(docID, field.binaryValue());\n         break;\n \n       case SORTED:\n-        if (fp.docValuesWriter == null) {\n-          fp.docValuesWriter = new SortedDocValuesWriter(fp.fieldInfo, bytesUsed);\n-        }\n         ((SortedDocValuesWriter) fp.docValuesWriter).addValue(docID, field.binaryValue());\n         break;\n \n       case SORTED_NUMERIC:\n-        if (fp.docValuesWriter == null) {\n-          fp.docValuesWriter = new SortedNumericDocValuesWriter(fp.fieldInfo, bytesUsed);\n-        }\n         ((SortedNumericDocValuesWriter) fp.docValuesWriter)\n             .addValue(docID, field.numericValue().longValue());\n         break;\n \n       case SORTED_SET:\n-        if (fp.docValuesWriter == null) {\n-          fp.docValuesWriter = new SortedSetDocValuesWriter(fp.fieldInfo, bytesUsed);\n-        }\n         ((SortedSetDocValuesWriter) fp.docValuesWriter).addValue(docID, field.binaryValue());\n         break;\n \n@@ -914,103 +1000,16 @@ private void indexDocValue(int docID, PerField fp, DocValuesType dvType, Indexab\n     }\n   }\n \n-  /** Called from processDocument to index one field's vector value */\n-  private void indexVector(int docID, PerField fp, IndexableField field) {\n-    int dimension = field.fieldType().vectorDimension();\n-    VectorValues.SearchStrategy searchStrategy = field.fieldType().vectorSearchStrategy();\n-\n-    // Record dimensions and distance function for this field; this setter will throw IllegalArgExc\n-    // if\n-    // the dimensions or distance function were already set to something different:\n-    if (fp.fieldInfo.getVectorDimension() == 0) {\n-      fieldInfos.globalFieldNumbers.setVectorDimensionsAndSearchStrategy(\n-          fp.fieldInfo.number, fp.fieldInfo.name, dimension, searchStrategy);\n-    }\n-    fp.fieldInfo.setVectorDimensionAndSearchStrategy(dimension, searchStrategy);\n-\n-    if (fp.vectorValuesWriter == null) {\n-      fp.vectorValuesWriter = new VectorValuesWriter(fp.fieldInfo, bytesUsed);\n-    }\n-    fp.vectorValuesWriter.addValue(docID, ((VectorField) field).vectorValue());\n-  }\n-\n   /** Returns a previously created {@link PerField}, or null if this field name wasn't seen yet. */\n   private PerField getPerField(String name) {\n     final int hashPos = name.hashCode() & hashMask;\n     PerField fp = fieldHash[hashPos];\n-    while (fp != null && !fp.fieldInfo.name.equals(name)) {\n-      fp = fp.next;\n-    }\n-    return fp;\n-  }\n-\n-  /**\n-   * Returns a previously created {@link PerField}, absorbing the type information from {@link\n-   * FieldType}, and creates a new {@link PerField} if this field name wasn't seen yet.\n-   */\n-  private PerField getOrAddField(String name, IndexableFieldType fieldType, boolean invert) {\n-\n-    // Make sure we have a PerField allocated\n-    final int hashPos = name.hashCode() & hashMask;\n-    PerField fp = fieldHash[hashPos];\n-    while (fp != null && !fp.fieldInfo.name.equals(name)) {\n+    while (fp != null && !fp.fieldName.equals(name)) {\n       fp = fp.next;\n     }\n-\n-    if (fp == null) {\n-      // First time we are seeing this field in this segment\n-\n-      FieldInfo fi = fieldInfos.getOrAdd(name);\n-      initIndexOptions(fi, fieldType.indexOptions());\n-      Map<String, String> attributes = fieldType.getAttributes();\n-      if (attributes != null) {\n-        attributes.forEach((k, v) -> fi.putAttribute(k, v));\n-      }\n-\n-      fp =\n-          new PerField(\n-              indexCreatedVersionMajor,\n-              fi,\n-              invert,\n-              indexWriterConfig.getSimilarity(),\n-              indexWriterConfig.getInfoStream(),\n-              indexWriterConfig.getAnalyzer());\n-      fp.next = fieldHash[hashPos];\n-      fieldHash[hashPos] = fp;\n-      totalFieldCount++;\n-\n-      // At most 50% load factor:\n-      if (totalFieldCount >= fieldHash.length / 2) {\n-        rehash();\n-      }\n-\n-      if (totalFieldCount > fields.length) {\n-        PerField[] newFields =\n-            new PerField\n-                [ArrayUtil.oversize(totalFieldCount, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];\n-        System.arraycopy(fields, 0, newFields, 0, fields.length);\n-        fields = newFields;\n-      }\n-\n-    } else if (invert && fp.invertState == null) {\n-      initIndexOptions(fp.fieldInfo, fieldType.indexOptions());\n-      fp.setInvertState();\n-    }\n-\n     return fp;\n   }\n \n-  private void initIndexOptions(FieldInfo info, IndexOptions indexOptions) {\n-    // Messy: must set this here because e.g. FreqProxTermsWriterPerField looks at the initial\n-    // IndexOptions to decide what arrays it must create).\n-    assert info.getIndexOptions() == IndexOptions.NONE;\n-    // This is the first time we are seeing this field indexed, so we now\n-    // record the index options so that any future attempt to (illegally)\n-    // change the index options of this field, will throw an IllegalArgExc:\n-    fieldInfos.globalFieldNumbers.setIndexOptions(info.number, info.name, indexOptions);\n-    info.setIndexOptions(indexOptions);\n-  }\n-\n   @Override\n   public long ramBytesUsed() {\n     return bytesUsed.get()\n@@ -1025,9 +1024,10 @@ public long ramBytesUsed() {\n \n   /** NOTE: not static: accesses at least docState, termsHash. */\n   private final class PerField implements Comparable<PerField> {\n-\n+    final String fieldName;\n     final int indexCreatedVersionMajor;\n-    final FieldInfo fieldInfo;\n+    final FieldSchema schema;\n+    FieldInfo fieldInfo;\n     final Similarity similarity;\n \n     FieldInvertState invertState;\n@@ -1056,22 +1056,31 @@ public long ramBytesUsed() {\n     TokenStream tokenStream;\n     private final InfoStream infoStream;\n     private final Analyzer analyzer;\n+    private boolean first; // first in a document\n \n     PerField(\n+        String fieldName,\n         int indexCreatedVersionMajor,\n-        FieldInfo fieldInfo,\n-        boolean invert,\n+        FieldSchema schema,\n         Similarity similarity,\n         InfoStream infoStream,\n         Analyzer analyzer) {\n+      this.fieldName = fieldName;\n       this.indexCreatedVersionMajor = indexCreatedVersionMajor;\n-      this.fieldInfo = fieldInfo;\n+      this.schema = schema;\n       this.similarity = similarity;\n       this.infoStream = infoStream;\n       this.analyzer = analyzer;\n-      if (invert) {\n-        setInvertState();\n-      }\n+    }\n+\n+    void reset(int docId) {\n+      first = true;\n+      schema.reset(docId);\n+    }\n+\n+    void setFieldInfo(FieldInfo fieldInfo) {\n+      assert this.fieldInfo == null;\n+      this.fieldInfo = fieldInfo;\n     }\n \n     void setInvertState() {\n@@ -1082,14 +1091,17 @@ void setInvertState() {\n       if (fieldInfo.omitsNorms() == false) {\n         assert norms == null;\n         // Even if no documents actually succeed in setting a norm, we still write norms for this\n-        // segment:\n+        // segment\n         norms = new NormValuesWriter(fieldInfo, bytesUsed);\n       }\n+      if (fieldInfo.hasVectors()) {\n+        termVectorsWriter.setHasVectors();\n+      }\n     }\n \n     @Override\n     public int compareTo(PerField other) {\n-      return this.fieldInfo.name.compareTo(other.fieldInfo.name);\n+      return this.fieldName.compareTo(other.fieldName);\n     }\n \n     public void finish(int docID) throws IOException {\n@@ -1109,7 +1121,6 @@ public void finish(int docID) throws IOException {\n         }\n         norms.addValue(docID, normValue);\n       }\n-\n       termsHashPerField.finish();\n     }\n \n@@ -1119,22 +1130,11 @@ public void finish(int docID) throws IOException {\n      */\n     public void invert(int docID, IndexableField field, boolean first) throws IOException {\n       if (first) {\n-        // First time we're seeing this field (indexed) in\n-        // this document:\n+        // First time we're seeing this field (indexed) in this document\n         invertState.reset();\n       }\n \n-      IndexableFieldType fieldType = field.fieldType();\n-\n-      IndexOptions indexOptions = fieldType.indexOptions();\n-      fieldInfo.setIndexOptions(indexOptions);\n-\n-      if (fieldType.omitNorms()) {\n-        fieldInfo.setOmitsNorms();\n-      }\n-\n-      final boolean analyzed = fieldType.tokenized() && analyzer != null;\n-\n+      final boolean analyzed = field.fieldType().tokenized() && analyzer != null;\n       /*\n        * To assist people in tracking down problems in analysis components, we wish to write the field name to the infostream\n        * when we fail. We expect some caller to eventually deal with the real exception, so we don't want any 'catch' clauses,\n@@ -1313,4 +1313,118 @@ public void recycleIntBlocks(int[][] blocks, int offset, int length) {\n       bytesUsed.addAndGet(-(length * (IntBlockPool.INT_BLOCK_SIZE * Integer.BYTES)));\n     }\n   }\n+\n+  /**\n+   * A schema of the field in the current document. With every new document this schema is reset. As\n+   * the document fields are processed, we update the schema with options encountered in this\n+   * document. Once the processing for the document is done, we compare the built schema of the\n+   * current document with the corresponding FieldInfo (FieldInfo is built on a first document in\n+   * the segment where we encounter this field). If there is inconsistency, we raise an error. This\n+   * ensures that a field has the same data structures across all documents.\n+   */\n+  private static final class FieldSchema {\n+    private final String name;\n+    private int docID = 0;\n+    private final Map<String, String> attributes = new HashMap<>();\n+    private boolean omitNorms = false;\n+    private boolean storeTermVector = false;\n+    private IndexOptions indexOptions = IndexOptions.NONE;\n+    private long dvGen = -1;\n+    private DocValuesType docValuesType = DocValuesType.NONE;\n+    private int pointDimensionCount = 0;\n+    private int pointIndexDimensionCount = 0;\n+    private int pointNumBytes = 0;\n+    private int vectorDimension = 0;\n+    private VectorValues.SearchStrategy vectorSearchStrategy = VectorValues.SearchStrategy.NONE;\n+\n+    private static String errMsg =\n+        \"Inconsistency of field data structures across documents for field \";\n+\n+    FieldSchema(String name) {\n+      this.name = name;\n+    }\n+\n+    String putAttribute(String key, String value) {\n+      return attributes.put(key, value);\n+    }\n+\n+    private void assertSame(boolean same) {\n+      if (same == false) {\n+        throw new IllegalArgumentException(errMsg + \"[\" + name + \"] of doc [\" + docID + \"].\");\n+      }\n+    }\n+\n+    void setIndexOptions(\n+        IndexOptions newIndexOptions, boolean newOmitNorms, boolean newStoreTermVector) {\n+      if (indexOptions == IndexOptions.NONE) {\n+        indexOptions = newIndexOptions;\n+        omitNorms = newOmitNorms;\n+        storeTermVector = newStoreTermVector;\n+      } else {\n+        assertSame(\n+            indexOptions == newIndexOptions\n+                && omitNorms == newOmitNorms\n+                && storeTermVector == newStoreTermVector);\n+      }\n+    }\n+\n+    void setDocValues(DocValuesType newDocValuesType, long newDvGen) {\n+      if (docValuesType == DocValuesType.NONE) {\n+        this.docValuesType = newDocValuesType;\n+        this.dvGen = newDvGen;\n+      } else {\n+        assertSame(docValuesType == newDocValuesType && dvGen == newDvGen);\n+      }\n+    }\n+\n+    void setPoints(int dimensionCount, int indexDimensionCount, int numBytes) {\n+      if (pointIndexDimensionCount == 0) {\n+        pointDimensionCount = dimensionCount;\n+        pointIndexDimensionCount = indexDimensionCount;\n+        pointNumBytes = numBytes;\n+      } else {\n+        assertSame(\n+            pointDimensionCount == dimensionCount\n+                && pointIndexDimensionCount == indexDimensionCount\n+                && pointNumBytes == numBytes);\n+      }\n+    }\n+\n+    void setVectors(VectorValues.SearchStrategy searchStrategy, int dimension) {\n+      if (vectorSearchStrategy == VectorValues.SearchStrategy.NONE) {\n+        this.vectorDimension = dimension;\n+        this.vectorSearchStrategy = searchStrategy;\n+      } else {\n+        assertSame(vectorSearchStrategy == searchStrategy && vectorDimension == dimension);\n+      }\n+    }\n+\n+    void reset(int doc) {\n+      docID = doc;\n+      omitNorms = false;\n+      storeTermVector = false;\n+      indexOptions = IndexOptions.NONE;\n+      dvGen = -1;\n+      docValuesType = DocValuesType.NONE;\n+      pointDimensionCount = 0;\n+      pointIndexDimensionCount = 0;\n+      pointNumBytes = 0;\n+      vectorDimension = 0;\n+      vectorSearchStrategy = VectorValues.SearchStrategy.NONE;\n+    }\n+\n+    void assertSameSchema(FieldInfo fi) {\n+      assertSame(\n+          indexOptions == fi.getIndexOptions()\n+              && omitNorms == fi.omitsNorms()\n+              && storeTermVector == fi.hasVectors()\n+              && docValuesType == fi.getDocValuesType()\n+              && dvGen == fi.getDocValuesGen()\n+              && pointDimensionCount == fi.getPointDimensionCount()\n+              && pointIndexDimensionCount == fi.getPointIndexDimensionCount()\n+              && pointNumBytes == fi.getPointNumBytes()\n+              && vectorDimension == fi.getVectorDimension()\n+              && vectorSearchStrategy == fi.getVectorSearchStrategy());\n+    }\n+  }\n }"
  },
  {
    "sha": "8a581070d7461f3b5087810b0ef68b353898fccc",
    "filename": "lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates.java",
    "status": "modified",
    "additions": 4,
    "deletions": 5,
    "changes": 9,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates.java",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/core/src/java/org/apache/lucene/index/ReadersAndUpdates.java?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -602,20 +602,19 @@ public synchronized boolean writeFieldUpdates(\n         }\n \n         // create new fields with the right DV type\n-        FieldInfos.Builder builder = new FieldInfos.Builder(fieldNumbers);\n         for (List<DocValuesFieldUpdates> updates : pendingDVUpdates.values()) {\n           DocValuesFieldUpdates update = updates.get(0);\n-\n           if (byName.containsKey(update.field)) {\n             // the field already exists in this segment\n             FieldInfo fi = byName.get(update.field);\n             fi.setDocValuesType(update.type);\n           } else {\n             // the field is not present in this segment so we clone the global field\n             // (which is guaranteed to exist) and remaps its field number locally.\n-            assert fieldNumbers.contains(update.field, update.type);\n-            FieldInfo fi = cloneFieldInfo(builder.getOrAdd(update.field), ++maxFieldNumber);\n-            fi.setDocValuesType(update.type);\n+            FieldInfo fi =\n+                fieldNumbers.constructFieldInfo(update.field, update.type, maxFieldNumber + 1);\n+            assert fi != null;\n+            maxFieldNumber++;\n             byName.put(fi.name, fi);\n           }\n         }"
  },
  {
    "sha": "c6ffab8545f3986a38ece9f977d6b4876c32ca15",
    "filename": "lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumer.java",
    "status": "modified",
    "additions": 5,
    "deletions": 1,
    "changes": 6,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumer.java",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumer.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumer.java?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -48,7 +48,7 @@\n \n   final ByteSliceReader vectorSliceReaderOff = new ByteSliceReader();\n \n-  boolean hasVectors;\n+  private boolean hasVectors;\n   private int numVectorFields;\n   int lastDocID;\n   private TermVectorsConsumerPerField[] perFields = new TermVectorsConsumerPerField[1];\n@@ -109,6 +109,10 @@ void initTermVectorsWriter() throws IOException {\n     }\n   }\n \n+  void setHasVectors() {\n+    hasVectors = true;\n+  }\n+\n   @Override\n   void finishDocument(int docID) throws IOException {\n "
  },
  {
    "sha": "a4c24482201bf3da61c8f984d7c1db2348f59128",
    "filename": "lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java",
    "status": "modified",
    "additions": 0,
    "deletions": 3,
    "changes": 3,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -149,9 +149,6 @@ boolean start(IndexableField field, boolean first) {\n       doVectors = field.fieldType().storeTermVectors();\n \n       if (doVectors) {\n-\n-        termsWriter.hasVectors = true;\n-\n         doVectorPositions = field.fieldType().storeTermVectorPositions();\n \n         // Somewhat confusingly, unlike postings, you are"
  },
  {
    "sha": "15657a9d2b10c4de7a2233e733e7a42e00ac2ea5",
    "filename": "lucene/core/src/test/org/apache/lucene/document/TestPerFieldConsistency.java",
    "status": "added",
    "additions": 150,
    "deletions": 0,
    "changes": 150,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/document/TestPerFieldConsistency.java",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/document/TestPerFieldConsistency.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/core/src/test/org/apache/lucene/document/TestPerFieldConsistency.java?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.lucene.document;\n+\n+import static com.carrotsearch.randomizedtesting.RandomizedTest.randomIntBetween;\n+\n+import java.io.IOException;\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexWriterConfig;\n+import org.apache.lucene.index.LeafReader;\n+import org.apache.lucene.index.VectorValues;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.LuceneTestCase;\n+\n+public class TestPerFieldConsistency extends LuceneTestCase {\n+\n+  public void testDocWithMissingSchemaOptionsThrowsError() throws IOException {\n+    try (Directory dir = newDirectory();\n+        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig()); ) {\n+      final String FIELD_NAME = \"myfield\";\n+      final Field textField = new Field(FIELD_NAME, \"myvalue\", TextField.TYPE_NOT_STORED);\n+      final Field docValuesField = new BinaryDocValuesField(FIELD_NAME, new BytesRef(\"myvalue\"));\n+      final Field pointField = new LongPoint(FIELD_NAME, 1);\n+      final Field vectorField =\n+          new VectorField(\n+              FIELD_NAME, new float[] {.1f, .2f, .3f}, VectorValues.SearchStrategy.EUCLIDEAN_HNSW);\n+      final Field[] fields = new Field[] {textField, docValuesField, pointField, vectorField};\n+      final String[] errorMsgs = new String[] {\"index options\", \"doc values\", \"points\", \"vector\"};\n+\n+      final Document doc0 = new Document();\n+      for (Field field : fields) {\n+        doc0.add(field);\n+      }\n+      writer.addDocument(doc0);\n+\n+      // the same segment: indexing a doc with a missing field throws error\n+      int missingFieldIdx = randomIntBetween(0, fields.length - 1);\n+      final Document doc1 = new Document();\n+      for (int i = 0; i < fields.length; i++) {\n+        if (i != missingFieldIdx) {\n+          doc1.add(fields[i]);\n+        }\n+      }\n+      IllegalArgumentException exception =\n+          expectThrows(IllegalArgumentException.class, () -> writer.addDocument(doc1));\n+      String expectedErrMsg =\n+          \"Inconsistency of field data structures across documents for field [myfield] of doc [1].\";\n+      assertEquals(expectedErrMsg, exception.getMessage());\n+\n+      writer.flush();\n+      try (IndexReader reader = DirectoryReader.open(writer)) {\n+        LeafReader lr1 = reader.leaves().get(0).reader();\n+        assertEquals(1, lr1.numDocs());\n+        assertEquals(1, lr1.numDeletedDocs());\n+      }\n+\n+      // diff segment, same index: indexing a doc with a missing field throws error\n+      exception = expectThrows(IllegalArgumentException.class, () -> writer.addDocument(doc1));\n+      assertTrue(\n+          exception\n+              .getMessage()\n+              .contains(\"cannot change field \\\"myfield\\\" from \" + errorMsgs[missingFieldIdx]));\n+\n+      writer.addDocument(doc0); // add document with correct data structures\n+\n+      writer.flush();\n+      try (IndexReader reader = DirectoryReader.open(writer)) {\n+        LeafReader lr2 = reader.leaves().get(1).reader();\n+        assertEquals(1, lr2.numDocs());\n+        assertEquals(1, lr2.numDeletedDocs());\n+      }\n+    }\n+  }\n+\n+  public void testDocWithExtraIndexingOptionsThrowsError() throws IOException {\n+    try (Directory dir = newDirectory();\n+        IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig()); ) {\n+      final String FIELD_NAME = \"myfield\";\n+      final Field textField = new Field(FIELD_NAME, \"myvalue\", TextField.TYPE_NOT_STORED);\n+      final Field docValuesField = new BinaryDocValuesField(FIELD_NAME, new BytesRef(\"myvalue\"));\n+      final Field pointField = new LongPoint(FIELD_NAME, 1);\n+      final Field vectorField =\n+          new VectorField(\n+              FIELD_NAME, new float[] {.1f, .2f, .3f}, VectorValues.SearchStrategy.EUCLIDEAN_HNSW);\n+      final Field[] fields = new Field[] {textField, docValuesField, pointField, vectorField};\n+      final String[] errorMsgs = new String[] {\"index options\", \"doc values\", \"points\", \"vector\"};\n+\n+      final Document doc0 = new Document();\n+      int existingFieldIdx = randomIntBetween(0, fields.length - 1);\n+      doc0.add(fields[existingFieldIdx]);\n+      writer.addDocument(doc0);\n+\n+      // the same segment: indexing a field with extra field indexing options returns error\n+      int extraFieldIndex = randomIntBetween(0, fields.length - 1);\n+      while (extraFieldIndex == existingFieldIdx) {\n+        extraFieldIndex = randomIntBetween(0, fields.length - 1);\n+      }\n+      final Document doc1 = new Document();\n+      doc1.add(fields[existingFieldIdx]);\n+      doc1.add(fields[extraFieldIndex]);\n+\n+      IllegalArgumentException exception =\n+          expectThrows(IllegalArgumentException.class, () -> writer.addDocument(doc1));\n+      String expectedErrMsg =\n+          \"Inconsistency of field data structures across documents for field [myfield] of doc [1].\";\n+      assertEquals(expectedErrMsg, exception.getMessage());\n+\n+      writer.flush();\n+      try (IndexReader reader = DirectoryReader.open(writer)) {\n+        LeafReader lr1 = reader.leaves().get(0).reader();\n+        assertEquals(1, lr1.numDocs());\n+        assertEquals(1, lr1.numDeletedDocs());\n+      }\n+\n+      // diff segment, same index: indexing a field with extra field indexing options returns error\n+      exception = expectThrows(IllegalArgumentException.class, () -> writer.addDocument(doc1));\n+      assertTrue(\n+          exception\n+              .getMessage()\n+              .contains(\"cannot change field \\\"myfield\\\" from \" + errorMsgs[extraFieldIndex]));\n+\n+      writer.addDocument(doc0); // add document with correct data structures\n+\n+      writer.flush();\n+      try (IndexReader reader = DirectoryReader.open(writer)) {\n+        LeafReader lr2 = reader.leaves().get(1).reader();\n+        assertEquals(1, lr2.numDocs());\n+        assertEquals(1, lr2.numDeletedDocs());\n+      }\n+    }\n+  }\n+}"
  },
  {
    "sha": "f4e3a9313aa2def732610e37db788c3f740a15ab",
    "filename": "lucene/core/src/test/org/apache/lucene/index/TestAtomicUpdate.java",
    "status": "modified",
    "additions": 2,
    "deletions": 4,
    "changes": 6,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestAtomicUpdate.java",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestAtomicUpdate.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/core/src/test/org/apache/lucene/index/TestAtomicUpdate.java?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -24,8 +24,6 @@\n import org.apache.lucene.document.Document;\n import org.apache.lucene.document.Field;\n import org.apache.lucene.document.IntPoint;\n-import org.apache.lucene.document.StringField;\n-import org.apache.lucene.document.TextField;\n import org.apache.lucene.store.ByteBuffersDirectory;\n import org.apache.lucene.store.Directory;\n import org.apache.lucene.store.MockDirectoryWrapper;\n@@ -71,9 +69,9 @@ public void doWork(int currentIteration) throws IOException {\n       // Update all 100 docs...\n       for (int i = 0; i < 100; i++) {\n         Document d = new Document();\n-        d.add(new StringField(\"id\", Integer.toString(i), Field.Store.YES));\n+        d.add(newStringField(\"id\", Integer.toString(i), Field.Store.YES));\n         d.add(\n-            new TextField(\n+            newTextField(\n                 \"contents\", English.intToEnglish(i + 10 * currentIteration), Field.Store.NO));\n         d.add(new IntPoint(\"doc\", i));\n         d.add(new IntPoint(\"doc2d\", i, i));"
  },
  {
    "sha": "3d6bc59d3a57f00d4130dbccfe2fb846cb6de8fc",
    "filename": "lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates.java",
    "status": "modified",
    "additions": 39,
    "deletions": 18,
    "changes": 57,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates.java",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates.java?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -953,27 +953,41 @@ public void testUpdateSegmentWithPostingButNoDocValues() throws Exception {\n     writer.commit();\n \n     // second segment with no BDV\n-    doc = new Document();\n-    doc.add(new StringField(\"id\", \"doc1\", Store.NO));\n-    doc.add(new StringField(\"bdv\", \"mock-value\", Store.NO));\n-    writer.addDocument(doc);\n-    writer.commit();\n+    Document doc2 = new Document();\n+    doc2.add(new StringField(\"id\", \"doc1\", Store.NO));\n+    doc2.add(new StringField(\"bdv\", \"mock-value\", Store.NO));\n+    IllegalArgumentException exception =\n+        expectThrows(IllegalArgumentException.class, () -> writer.addDocument(doc2));\n+    String expectedErrMsg =\n+        \"cannot change field \\\"bdv\\\" from doc values type=BINARY to inconsistent doc values type=NONE\";\n+    assertEquals(expectedErrMsg, exception.getMessage());\n+\n+    doc2.add(new BinaryDocValuesField(\"bdv\", toBytes(10L)));\n+    writer.addDocument(doc2);\n+\n+    // update doc values of bdv field in the second segment\n+    exception =\n+        expectThrows(\n+            IllegalArgumentException.class,\n+            () -> writer.updateBinaryDocValue(new Term(\"id\", \"doc1\"), \"bdv\", toBytes(5L)));\n+    expectedErrMsg =\n+        \"Can't update [BINARY] doc values; the field [bdv] must be doc values only field, but is also indexed with postings.\";\n+    assertEquals(expectedErrMsg, exception.getMessage());\n \n-    // update document in the second segment\n-    writer.updateBinaryDocValue(new Term(\"id\", \"doc1\"), \"bdv\", toBytes(5L));\n+    writer.commit();\n     writer.close();\n \n     DirectoryReader reader = DirectoryReader.open(dir);\n-    for (LeafReaderContext context : reader.leaves()) {\n-      LeafReader r = context.reader();\n-      BinaryDocValues bdv = r.getBinaryDocValues(\"bdv\");\n-      for (int i = 0; i < r.maxDoc(); i++) {\n-        assertEquals(i, bdv.nextDoc());\n-        assertEquals(5L, getValue(bdv));\n-      }\n-    }\n-    reader.close();\n+    LeafReader r1 = reader.leaves().get(0).reader();\n+    BinaryDocValues bdv1 = r1.getBinaryDocValues(\"bdv\");\n+    assertEquals(0, bdv1.nextDoc());\n+    assertEquals(5L, getValue(bdv1));\n+    LeafReader r2 = reader.leaves().get(1).reader();\n+    BinaryDocValues bdv2 = r2.getBinaryDocValues(\"bdv\");\n+    assertEquals(1, bdv2.nextDoc());\n+    assertEquals(10L, getValue(bdv2));\n \n+    reader.close();\n     dir.close();\n   }\n \n@@ -989,13 +1003,20 @@ public void testUpdateBinaryDVFieldWithSameNameAsPostingField() throws Exception\n     doc.add(new BinaryDocValuesField(\"f\", toBytes(5L)));\n     writer.addDocument(doc);\n     writer.commit();\n-    writer.updateBinaryDocValue(new Term(\"f\", \"mock-value\"), \"f\", toBytes(17L));\n+\n+    IllegalArgumentException exception =\n+        expectThrows(\n+            IllegalArgumentException.class,\n+            () -> writer.updateBinaryDocValue(new Term(\"f\", \"mock-value\"), \"f\", toBytes(17L)));\n+    String expectedErrMsg =\n+        \"Can't update [BINARY] doc values; the field [f] must be doc values only field, but is also indexed with postings.\";\n+    assertEquals(expectedErrMsg, exception.getMessage());\n     writer.close();\n \n     DirectoryReader r = DirectoryReader.open(dir);\n     BinaryDocValues bdv = r.leaves().get(0).reader().getBinaryDocValues(\"f\");\n     assertEquals(0, bdv.nextDoc());\n-    assertEquals(17, getValue(bdv));\n+    assertEquals(5, getValue(bdv));\n     r.close();\n \n     dir.close();"
  },
  {
    "sha": "8d52ed76b86ed9e8ece554c21b0fbb0221e1e168",
    "filename": "lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers.java",
    "status": "modified",
    "additions": 2,
    "deletions": 6,
    "changes": 8,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers.java",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/core/src/test/org/apache/lucene/index/TestConsistentFieldNumbers.java?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -57,10 +57,8 @@ public void testSameFieldNumbersAcrossSegments() throws Exception {\n       }\n \n       Document d2 = new Document();\n-      FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n-      customType2.setStoreTermVectors(true);\n       d2.add(new TextField(\"f2\", \"second field\", Field.Store.NO));\n-      d2.add(new Field(\"f1\", \"first field\", customType2));\n+      d2.add(new TextField(\"f1\", \"first field\", Field.Store.YES));\n       d2.add(new TextField(\"f3\", \"third field\", Field.Store.NO));\n       d2.add(new TextField(\"f4\", \"fourth field\", Field.Store.NO));\n       writer.addDocument(d2);\n@@ -121,10 +119,8 @@ public void testAddIndexes() throws Exception {\n                 .setMergePolicy(NoMergePolicy.INSTANCE));\n \n     Document d2 = new Document();\n-    FieldType customType2 = new FieldType(TextField.TYPE_STORED);\n-    customType2.setStoreTermVectors(true);\n     d2.add(new TextField(\"f2\", \"second field\", Field.Store.YES));\n-    d2.add(new Field(\"f1\", \"first field\", customType2));\n+    d2.add(new TextField(\"f1\", \"first field\", Field.Store.YES));\n     d2.add(new TextField(\"f3\", \"third field\", Field.Store.YES));\n     d2.add(new TextField(\"f4\", \"fourth field\", Field.Store.YES));\n     writer.addDocument(d2);"
  },
  {
    "sha": "05ba2f935afe071f81b2db396d061789b3b9aca7",
    "filename": "lucene/core/src/test/org/apache/lucene/index/TestDocValuesIndexing.java",
    "status": "modified",
    "additions": 0,
    "deletions": 23,
    "changes": 23,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestDocValuesIndexing.java",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestDocValuesIndexing.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/core/src/test/org/apache/lucene/index/TestDocValuesIndexing.java?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -844,29 +844,6 @@ public void testTypeChangeViaAddIndexesIR2() throws Exception {\n     dir.close();\n   }\n \n-  public void testDocsWithField() throws Exception {\n-    Directory dir = newDirectory();\n-    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n-    IndexWriter writer = new IndexWriter(dir, conf);\n-    Document doc = new Document();\n-    doc.add(new NumericDocValuesField(\"dv\", 0L));\n-    writer.addDocument(doc);\n-\n-    doc = new Document();\n-    doc.add(new TextField(\"dv\", \"some text\", Field.Store.NO));\n-    doc.add(new NumericDocValuesField(\"dv\", 0L));\n-    writer.addDocument(doc);\n-\n-    DirectoryReader r = writer.getReader();\n-    writer.close();\n-\n-    LeafReader subR = r.leaves().get(0).reader();\n-    assertEquals(2, subR.numDocs());\n-\n-    r.close();\n-    dir.close();\n-  }\n-\n   public void testSameFieldNameForPostingAndDocValue() throws Exception {\n     // LUCENE-5192: FieldInfos.Builder neglected to update\n     // globalFieldNumbers.docValuesType map if the field existed, resulting in"
  },
  {
    "sha": "531166985d6620367e27fd4d24c4ecf73754b8e3",
    "filename": "lucene/core/src/test/org/apache/lucene/index/TestFieldInfos.java",
    "status": "modified",
    "additions": 6,
    "deletions": 0,
    "changes": 6,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestFieldInfos.java",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestFieldInfos.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/core/src/test/org/apache/lucene/index/TestFieldInfos.java?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -199,6 +199,8 @@ public void testFieldNumbersAutoIncrement() {\n           \"field\" + i,\n           -1,\n           IndexOptions.NONE,\n+          false,\n+          false,\n           DocValuesType.NONE,\n           0,\n           0,\n@@ -212,6 +214,8 @@ public void testFieldNumbersAutoIncrement() {\n             \"EleventhField\",\n             -1,\n             IndexOptions.NONE,\n+            false,\n+            false,\n             DocValuesType.NONE,\n             0,\n             0,\n@@ -227,6 +231,8 @@ public void testFieldNumbersAutoIncrement() {\n             \"PostClearField\",\n             -1,\n             IndexOptions.NONE,\n+            false,\n+            false,\n             DocValuesType.NONE,\n             0,\n             0,"
  },
  {
    "sha": "36c5f4b2c7d12b851d8dcefa45126bd5075dfff8",
    "filename": "lucene/core/src/test/org/apache/lucene/index/TestIndexOptions.java",
    "status": "modified",
    "additions": 6,
    "deletions": 29,
    "changes": 35,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestIndexOptions.java",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestIndexOptions.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/core/src/test/org/apache/lucene/index/TestIndexOptions.java?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -20,7 +20,6 @@\n import java.util.Collections;\n import org.apache.lucene.document.Field;\n import org.apache.lucene.document.FieldType;\n-import org.apache.lucene.document.IntPoint;\n import org.apache.lucene.document.TextField;\n import org.apache.lucene.store.Directory;\n import org.apache.lucene.util.IOUtils;\n@@ -31,51 +30,29 @@\n   public void testChangeIndexOptionsViaAddDocument() throws IOException {\n     for (IndexOptions from : IndexOptions.values()) {\n       for (IndexOptions to : IndexOptions.values()) {\n-        for (boolean preExisting : new boolean[] {false, true}) {\n-          for (boolean onNewSegment : new boolean[] {false, true}) {\n-            doTestChangeIndexOptionsViaAddDocument(preExisting, onNewSegment, from, to);\n-          }\n-        }\n+        doTestChangeIndexOptionsViaAddDocument(from, to);\n       }\n     }\n   }\n \n-  private void doTestChangeIndexOptionsViaAddDocument(\n-      boolean preExistingField, boolean onNewSegment, IndexOptions from, IndexOptions to)\n+  private void doTestChangeIndexOptionsViaAddDocument(IndexOptions from, IndexOptions to)\n       throws IOException {\n     Directory dir = newDirectory();\n     IndexWriter w = new IndexWriter(dir, newIndexWriterConfig());\n-    if (preExistingField) {\n-      w.addDocument(Collections.singleton(new IntPoint(\"foo\", 1)));\n-      if (onNewSegment) {\n-        DirectoryReader.open(w).close();\n-      }\n-    }\n     FieldType ft1 = new FieldType(TextField.TYPE_STORED);\n     ft1.setIndexOptions(from);\n     w.addDocument(Collections.singleton(new Field(\"foo\", \"bar\", ft1)));\n-    if (onNewSegment) {\n-      DirectoryReader.open(w).close();\n-    }\n     FieldType ft2 = new FieldType(TextField.TYPE_STORED);\n     ft2.setIndexOptions(to);\n-    if (from == IndexOptions.NONE || to == IndexOptions.NONE || from == to) {\n+    if (from == to) {\n       w.addDocument(Collections.singleton(new Field(\"foo\", \"bar\", ft2))); // no exception\n-      w.forceMerge(1);\n-      try (LeafReader r = getOnlyLeafReader(DirectoryReader.open(w))) {\n-        IndexOptions expected = from == IndexOptions.NONE ? to : from;\n-        assertEquals(expected, r.getFieldInfos().fieldInfo(\"foo\").getIndexOptions());\n-      }\n     } else {\n       IllegalArgumentException e =\n           expectThrows(\n               IllegalArgumentException.class,\n               () -> w.addDocument(Collections.singleton(new Field(\"foo\", \"bar\", ft2))));\n       assertEquals(\n-          \"cannot change field \\\"foo\\\" from index options=\"\n-              + from\n-              + \" to inconsistent index options=\"\n-              + to,\n+          \"Inconsistency of field data structures across documents for field [foo] of doc [1].\",\n           e.getMessage());\n     }\n     w.close();\n@@ -105,7 +82,7 @@ private void doTestChangeIndexOptionsAddIndexesCodecReader(IndexOptions from, In\n     w2.addDocument(Collections.singleton(new Field(\"foo\", \"bar\", ft2)));\n \n     try (CodecReader cr = (CodecReader) getOnlyLeafReader(DirectoryReader.open(w2))) {\n-      if (from == IndexOptions.NONE || to == IndexOptions.NONE || from == to) {\n+      if (from == to) {\n         w1.addIndexes(cr); // no exception\n         w1.forceMerge(1);\n         try (LeafReader r = getOnlyLeafReader(DirectoryReader.open(w1))) {\n@@ -150,7 +127,7 @@ private void doTestChangeIndexOptionsAddIndexesDirectory(IndexOptions from, Inde\n     w2.addDocument(Collections.singleton(new Field(\"foo\", \"bar\", ft2)));\n     w2.close();\n \n-    if (from == IndexOptions.NONE || to == IndexOptions.NONE || from == to) {\n+    if (from == to) {\n       w1.addIndexes(dir2); // no exception\n       w1.forceMerge(1);\n       try (LeafReader r = getOnlyLeafReader(DirectoryReader.open(w1))) {"
  },
  {
    "sha": "6a57bf99f629d2003e61166d303617b3746a6a12",
    "filename": "lucene/core/src/test/org/apache/lucene/index/TestIndexSorting.java",
    "status": "modified",
    "additions": 3,
    "deletions": 1,
    "changes": 4,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestIndexSorting.java",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestIndexSorting.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/core/src/test/org/apache/lucene/index/TestIndexSorting.java?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -2828,7 +2828,9 @@ public void testWrongSortFieldType() throws Exception {\n         w.addDocument(doc);\n         doc.add(dvs.get(j));\n         exc = expectThrows(IllegalArgumentException.class, () -> w.addDocument(doc));\n-        assertThat(exc.getMessage(), containsString(\"cannot change DocValues type\"));\n+        assertEquals(\n+            \"Inconsistency of field data structures across documents for field [field] of doc [2].\",\n+            exc.getMessage());\n         w.rollback();\n         IOUtils.close(w);\n       }"
  },
  {
    "sha": "8a46af4ad989abd735d71dcaed3e8a52539b1293",
    "filename": "lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java",
    "status": "modified",
    "additions": 8,
    "deletions": 6,
    "changes": 14,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -16,6 +16,8 @@\n  */\n package org.apache.lucene.index;\n \n+import static org.apache.lucene.index.DocHelper.TEXT_TYPE_STORED_WITH_TVS;\n+\n import com.carrotsearch.randomizedtesting.generators.RandomPicks;\n import java.io.ByteArrayOutputStream;\n import java.io.Closeable;\n@@ -2006,7 +2008,7 @@ public void testNullAnalyzer() throws IOException {\n         NullPointerException.class,\n         () -> {\n           Document broke = new Document();\n-          broke.add(newTextField(\"test\", \"broken\", Field.Store.NO));\n+          broke.add(new Field(\"test\", \"broken\", TEXT_TYPE_STORED_WITH_TVS));\n           iw.addDocument(broke);\n         });\n \n@@ -2816,16 +2818,16 @@ public void testPendingDeleteDVGeneration() throws IOException {\n       IndexWriter w = new IndexWriter(dir, iwc);\n       Document d = new Document();\n       d.add(new StringField(\"id\", \"1\", Field.Store.YES));\n-      d.add(new NumericDocValuesField(\"id\", 1));\n+      d.add(new NumericDocValuesField(\"nvd\", 1));\n       w.addDocument(d);\n       d = new Document();\n       d.add(new StringField(\"id\", \"2\", Field.Store.YES));\n-      d.add(new NumericDocValuesField(\"id\", 2));\n+      d.add(new NumericDocValuesField(\"nvd\", 2));\n       w.addDocument(d);\n       w.flush();\n       d = new Document();\n       d.add(new StringField(\"id\", \"1\", Field.Store.YES));\n-      d.add(new NumericDocValuesField(\"id\", 1));\n+      d.add(new NumericDocValuesField(\"nvd\", 1));\n       w.updateDocument(new Term(\"id\", \"1\"), d);\n       w.commit();\n       Set<String> files = new HashSet<>(Arrays.asList(dir.listAll()));\n@@ -2834,12 +2836,12 @@ public void testPendingDeleteDVGeneration() throws IOException {\n         if (random().nextBoolean()) {\n           d = new Document();\n           d.add(new StringField(\"id\", \"1\", Field.Store.YES));\n-          d.add(new NumericDocValuesField(\"id\", 1));\n+          d.add(new NumericDocValuesField(\"nvd\", 1));\n           w.updateDocument(new Term(\"id\", \"1\"), d);\n         } else if (random().nextBoolean()) {\n           w.deleteDocuments(new Term(\"id\", \"2\"));\n         } else {\n-          w.updateNumericDocValue(new Term(\"id\", \"1\"), \"id\", 2);\n+          w.updateNumericDocValue(new Term(\"id\", \"1\"), \"nvd\", 2);\n         }\n         w.prepareCommit();\n         List<String> newFiles = new ArrayList<>(Arrays.asList(dir.listAll()));"
  },
  {
    "sha": "d65d8128ccfd9a989df9053b8fea2fb62a139216",
    "filename": "lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDeleteByQuery.java",
    "status": "removed",
    "additions": 0,
    "deletions": 70,
    "changes": 70,
    "blob_url": "https://github.com/apache/lucene/blob/dcb52acd7d0af630486a62dc568d0ea52ec1ea52/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDeleteByQuery.java",
    "raw_url": "https://github.com/apache/lucene/raw/dcb52acd7d0af630486a62dc568d0ea52ec1ea52/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDeleteByQuery.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDeleteByQuery.java?ref=dcb52acd7d0af630486a62dc568d0ea52ec1ea52",
    "patch": "@@ -1,70 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements.  See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.lucene.index;\n-\n-import org.apache.lucene.document.Document;\n-import org.apache.lucene.document.Field;\n-import org.apache.lucene.search.MatchAllDocsQuery;\n-import org.apache.lucene.store.Directory;\n-import org.apache.lucene.util.LuceneTestCase;\n-\n-public class TestIndexWriterDeleteByQuery extends LuceneTestCase {\n-\n-  // LUCENE-6379\n-  public void testDeleteMatchAllDocsQuery() throws Exception {\n-    Directory dir = newMaybeVirusCheckingDirectory();\n-    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig());\n-    Document doc = new Document();\n-    // Norms are disabled:\n-    doc.add(newStringField(\"field\", \"foo\", Field.Store.NO));\n-    w.addDocument(doc);\n-    DirectoryReader r = DirectoryReader.open(w);\n-    FieldInfo fi = FieldInfos.getMergedFieldInfos(r).fieldInfo(\"field\");\n-    assertNotNull(fi);\n-    assertFalse(fi.hasNorms());\n-    assertEquals(1, r.numDocs());\n-    assertEquals(1, r.maxDoc());\n-\n-    w.deleteDocuments(new MatchAllDocsQuery());\n-    DirectoryReader r2 = DirectoryReader.openIfChanged(r);\n-    r.close();\n-\n-    assertNotNull(r2);\n-    assertEquals(0, r2.numDocs());\n-    assertEquals(0, r2.maxDoc());\n-\n-    // Confirm the omitNorms bit is in fact no longer set:\n-    doc = new Document();\n-    // Norms are disabled:\n-    doc.add(newTextField(\"field\", \"foo\", Field.Store.NO));\n-    w.addDocument(doc);\n-\n-    DirectoryReader r3 = DirectoryReader.openIfChanged(r2);\n-    r2.close();\n-    assertNotNull(r3);\n-    assertEquals(1, r3.numDocs());\n-    assertEquals(1, r3.maxDoc());\n-\n-    // Make sure norms can come back to life for a field after deleting by MatchAllDocsQuery:\n-    fi = FieldInfos.getMergedFieldInfos(r3).fieldInfo(\"field\");\n-    assertNotNull(fi);\n-    assertTrue(fi.hasNorms());\n-    r3.close();\n-    w.close();\n-    dir.close();\n-  }\n-}"
  },
  {
    "sha": "fc93c604796ee9c4ae17714268dc9083e8152431",
    "filename": "lucene/core/src/test/org/apache/lucene/index/TestIndexWriterReader.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterReader.java",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterReader.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterReader.java?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -150,7 +150,7 @@ public void testUpdateDocument() throws Exception {\n \n     Document newDoc = r1.document(10);\n     newDoc.removeField(\"id\");\n-    newDoc.add(newStringField(\"id\", Integer.toString(8000), Field.Store.YES));\n+    newDoc.add(new Field(\"id\", Integer.toString(8000), DocHelper.STRING_TYPE_STORED_WITH_TVS));\n     writer.updateDocument(new Term(\"id\", id10), newDoc);\n     assertFalse(r1.isCurrent());\n "
  },
  {
    "sha": "d5e082684802f00a1fb6c83287f45af05ae53ce1",
    "filename": "lucene/core/src/test/org/apache/lucene/index/TestMixedDocValuesUpdates.java",
    "status": "modified",
    "additions": 19,
    "deletions": 18,
    "changes": 37,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestMixedDocValuesUpdates.java",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestMixedDocValuesUpdates.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/core/src/test/org/apache/lucene/index/TestMixedDocValuesUpdates.java?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -456,7 +456,7 @@ public void testTryUpdateDocValues() throws IOException {\n     for (int i = 0; i < numDocs; i++) {\n       Document doc = new Document();\n       doc.add(new StringField(\"id\", \"\" + i, Store.YES));\n-      doc.add(new NumericDocValuesField(\"id\", i));\n+      doc.add(new NumericDocValuesField(\"numericId\", i));\n       doc.add(new BinaryDocValuesField(\"binaryId\", new BytesRef(new byte[] {(byte) i})));\n       writer.addDocument(doc);\n       if (random().nextBoolean()) {\n@@ -467,30 +467,30 @@ public void testTryUpdateDocValues() throws IOException {\n     doUpdate(\n         new Term(\"id\", \"\" + doc),\n         writer,\n-        new NumericDocValuesField(\"id\", doc + 1),\n+        new NumericDocValuesField(\"numericId\", doc + 1),\n         new BinaryDocValuesField(\"binaryId\", new BytesRef(new byte[] {(byte) (doc + 1)})));\n     IndexReader reader = writer.getReader();\n-    NumericDocValues idValues = null;\n+    NumericDocValues numericIdValues = null;\n     BinaryDocValues binaryIdValues = null;\n     for (LeafReaderContext c : reader.leaves()) {\n       TopDocs topDocs =\n           new IndexSearcher(c.reader()).search(new TermQuery(new Term(\"id\", \"\" + doc)), 10);\n       if (topDocs.totalHits.value == 1) {\n-        assertNull(idValues);\n+        assertNull(numericIdValues);\n         assertNull(binaryIdValues);\n-        idValues = c.reader().getNumericDocValues(\"id\");\n-        assertEquals(topDocs.scoreDocs[0].doc, idValues.advance(topDocs.scoreDocs[0].doc));\n+        numericIdValues = c.reader().getNumericDocValues(\"numericId\");\n+        assertEquals(topDocs.scoreDocs[0].doc, numericIdValues.advance(topDocs.scoreDocs[0].doc));\n         binaryIdValues = c.reader().getBinaryDocValues(\"binaryId\");\n         assertEquals(topDocs.scoreDocs[0].doc, binaryIdValues.advance(topDocs.scoreDocs[0].doc));\n       } else {\n         assertEquals(0, topDocs.totalHits.value);\n       }\n     }\n \n-    assertNotNull(idValues);\n+    assertNotNull(numericIdValues);\n     assertNotNull(binaryIdValues);\n \n-    assertEquals(doc + 1, idValues.longValue());\n+    assertEquals(doc + 1, numericIdValues.longValue());\n     assertEquals(new BytesRef(new byte[] {(byte) (doc + 1)}), binaryIdValues.binaryValue());\n     IOUtils.close(reader, writer, dir);\n   }\n@@ -709,7 +709,7 @@ public void testUpdateNotExistingFieldDV() throws IOException {\n       IllegalArgumentException iae =\n           expectThrows(IllegalArgumentException.class, () -> writer.addDocument(doc1));\n       assertEquals(\n-          \"cannot change DocValues type from NUMERIC to BINARY for field \\\"not_existing\\\"\",\n+          \"cannot change field \\\"not_existing\\\" from doc values type=NUMERIC to inconsistent doc values type=BINARY\",\n           iae.getMessage());\n \n       iae =\n@@ -720,12 +720,12 @@ public void testUpdateNotExistingFieldDV() throws IOException {\n                       new Term(\"id\", \"1\"),\n                       new BinaryDocValuesField(\"not_existing\", new BytesRef())));\n       assertEquals(\n-          \"cannot change DocValues type from NUMERIC to BINARY for field \\\"not_existing\\\"\",\n+          \"Can't update [BINARY] doc values; the field [not_existing] has inconsistent doc values' type of [NUMERIC].\",\n           iae.getMessage());\n     }\n   }\n \n-  public void testUpdateFieldWithNoPreviousDocValues() throws IOException {\n+  public void testUpdateFieldWithNoPreviousDocValuesThrowsError() throws IOException {\n     IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n     try (Directory dir = newDirectory();\n         IndexWriter writer = new IndexWriter(dir, conf)) {\n@@ -740,13 +740,14 @@ public void testUpdateFieldWithNoPreviousDocValues() throws IOException {\n       } else if (random().nextBoolean()) {\n         writer.commit();\n       }\n-      writer.updateDocValues(new Term(\"id\", \"1\"), new NumericDocValuesField(\"id\", 1));\n-      try (DirectoryReader reader = writer.getReader()) {\n-        NumericDocValues id = reader.leaves().get(0).reader().getNumericDocValues(\"id\");\n-        assertNotNull(id);\n-        assertTrue(id.advanceExact(0));\n-        assertEquals(1, id.longValue());\n-      }\n+      IllegalArgumentException exception =\n+          expectThrows(\n+              IllegalArgumentException.class,\n+              () ->\n+                  writer.updateDocValues(new Term(\"id\", \"1\"), new NumericDocValuesField(\"id\", 1)));\n+      assertEquals(\n+          \"Can't update [NUMERIC] doc values; the field [id] has inconsistent doc values' type of [NONE].\",\n+          exception.getMessage());\n     }\n   }\n }"
  },
  {
    "sha": "47f041314c92a2f3bbe81eec8beefcceb91bcf3e",
    "filename": "lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java",
    "status": "modified",
    "additions": 26,
    "deletions": 7,
    "changes": 33,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -1217,23 +1217,34 @@ public void testUpdateSegmentWithPostingButNoDocValues() throws Exception {\n     conf.setMergePolicy(NoMergePolicy.INSTANCE);\n     IndexWriter writer = new IndexWriter(dir, conf);\n \n-    // first segment with NDV\n+    // first segment with ndv and ndv2 fields\n     Document doc = new Document();\n     doc.add(new StringField(\"id\", \"doc0\", Store.NO));\n-    doc.add(new StringField(\"ndv\", \"mock-value\", Store.NO));\n     doc.add(new NumericDocValuesField(\"ndv\", 5));\n+    doc.add(new StringField(\"ndv2\", \"10\", Store.NO));\n+    doc.add(new NumericDocValuesField(\"ndv2\", 10));\n     writer.addDocument(doc);\n     writer.commit();\n \n-    // second segment with no NDV\n+    // second segment with no ndv and ndv2 fields\n     doc = new Document();\n     doc.add(new StringField(\"id\", \"doc1\", Store.NO));\n-    doc.add(new StringField(\"ndv\", \"mock-value\", Store.NO));\n     writer.addDocument(doc);\n     writer.commit();\n \n-    // update document in the second segment\n+    // update docValues of \"ndv\" field in the second segment\n+    // since global \"ndv\" field is docValues only field this is allowed\n     writer.updateNumericDocValue(new Term(\"id\", \"doc1\"), \"ndv\", 5L);\n+\n+    // update docValues of \"ndv2\" field in the second segment\n+    // since global \"ndv2\" field is not docValues only field this NOT allowed\n+    IllegalArgumentException exception =\n+        expectThrows(\n+            IllegalArgumentException.class,\n+            () -> writer.updateNumericDocValue(new Term(\"id\", \"doc1\"), \"ndv2\", 10L));\n+    String expectedErrMsg =\n+        \"Can't update [NUMERIC] doc values; the field [ndv2] must be doc values only field, but is also indexed with postings.\";\n+    assertEquals(expectedErrMsg, exception.getMessage());\n     writer.close();\n \n     DirectoryReader reader = DirectoryReader.open(dir);\n@@ -1262,13 +1273,21 @@ public void testUpdateNumericDVFieldWithSameNameAsPostingField() throws Exceptio\n     doc.add(new NumericDocValuesField(\"f\", 5));\n     writer.addDocument(doc);\n     writer.commit();\n-    writer.updateNumericDocValue(new Term(\"f\", \"mock-value\"), \"f\", 17L);\n+\n+    IllegalArgumentException exception =\n+        expectThrows(\n+            IllegalArgumentException.class,\n+            () -> writer.updateNumericDocValue(new Term(\"f\", \"mock-value\"), \"f\", 17L));\n+    String expectedErrMsg =\n+        \"Can't update [NUMERIC] doc values; the field [f] must be doc values only field, but is also indexed with postings.\";\n+    assertEquals(expectedErrMsg, exception.getMessage());\n+\n     writer.close();\n \n     DirectoryReader r = DirectoryReader.open(dir);\n     NumericDocValues ndv = r.leaves().get(0).reader().getNumericDocValues(\"f\");\n     assertEquals(0, ndv.nextDoc());\n-    assertEquals(17, ndv.longValue());\n+    assertEquals(5, ndv.longValue());\n     r.close();\n \n     dir.close();"
  },
  {
    "sha": "f0b5583d178b37a57c9c17070ab487ef6c0c9385",
    "filename": "lucene/core/src/test/org/apache/lucene/index/TestOmitNorms.java",
    "status": "modified",
    "additions": 22,
    "deletions": 178,
    "changes": 200,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestOmitNorms.java",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestOmitNorms.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/core/src/test/org/apache/lucene/index/TestOmitNorms.java?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -16,9 +16,6 @@\n  */\n package org.apache.lucene.index;\n \n-import static org.apache.lucene.search.DocIdSetIterator.NO_MORE_DOCS;\n-\n-import java.io.IOException;\n import org.apache.lucene.analysis.Analyzer;\n import org.apache.lucene.analysis.MockAnalyzer;\n import org.apache.lucene.document.Document;\n@@ -27,58 +24,11 @@\n import org.apache.lucene.document.TextField;\n import org.apache.lucene.store.Directory;\n import org.apache.lucene.util.LuceneTestCase;\n-import org.apache.lucene.util.TestUtil;\n \n public class TestOmitNorms extends LuceneTestCase {\n-  // Tests whether the DocumentWriter correctly enable the\n-  // omitNorms bit in the FieldInfo\n-  public void testOmitNorms() throws Exception {\n-    Directory ram = newDirectory();\n-    Analyzer analyzer = new MockAnalyzer(random());\n-    IndexWriter writer = new IndexWriter(ram, newIndexWriterConfig(analyzer));\n-    Document d = new Document();\n-\n-    // this field will have norms\n-    Field f1 = newTextField(\"f1\", \"This field has norms\", Field.Store.NO);\n-    d.add(f1);\n-\n-    // this field will NOT have norms\n-    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n-    customType.setOmitNorms(true);\n-    Field f2 = newField(\"f2\", \"This field has NO norms in all docs\", customType);\n-    d.add(f2);\n-\n-    writer.addDocument(d);\n-    writer.forceMerge(1);\n-    // now we add another document which has term freq for field f2 and not for f1 and verify if the\n-    // SegmentMerger\n-    // keep things constant\n-    d = new Document();\n-\n-    // Reverse\n-    d.add(newField(\"f1\", \"This field has norms\", customType));\n-\n-    d.add(newTextField(\"f2\", \"This field has NO norms in all docs\", Field.Store.NO));\n-\n-    writer.addDocument(d);\n-\n-    // force merge\n-    writer.forceMerge(1);\n-    // flush\n-    writer.close();\n-\n-    LeafReader reader = getOnlyLeafReader(DirectoryReader.open(ram));\n-    FieldInfos fi = reader.getFieldInfos();\n-    assertTrue(\"OmitNorms field bit should be set.\", fi.fieldInfo(\"f1\").omitsNorms());\n-    assertTrue(\"OmitNorms field bit should be set.\", fi.fieldInfo(\"f2\").omitsNorms());\n-\n-    reader.close();\n-    ram.close();\n-  }\n \n-  // Tests whether merging of docs that have different\n-  // omitNorms for the same field works\n-  public void testMixedMerge() throws Exception {\n+  // Tests that merging of docs with different omitNorms throws error\n+  public void testMixedMergeThrowsError() throws Exception {\n     Directory ram = newDirectory();\n     Analyzer analyzer = new MockAnalyzer(random());\n     IndexWriter writer =\n@@ -90,42 +40,42 @@ public void testMixedMerge() throws Exception {\n     Document d = new Document();\n \n     // this field will have norms\n-    Field f1 = newTextField(\"f1\", \"This field has norms\", Field.Store.NO);\n+    FieldType fieldType1 = new FieldType(TextField.TYPE_NOT_STORED);\n+    fieldType1.setOmitNorms(false);\n+    fieldType1.setStoreTermVectors(false);\n+    Field f1 = new Field(\"f1\", \"This field has norms\", fieldType1);\n     d.add(f1);\n \n     // this field will NOT have norms\n-    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n-    customType.setOmitNorms(true);\n-    Field f2 = newField(\"f2\", \"This field has NO norms in all docs\", customType);\n+    FieldType fieldType2 = new FieldType(TextField.TYPE_NOT_STORED);\n+    fieldType2.setOmitNorms(true);\n+    fieldType2.setStoreTermVectors(false);\n+    Field f2 = new Field(\"f2\", \"This field has NO norms in all docs\", fieldType2);\n     d.add(f2);\n \n     for (int i = 0; i < 30; i++) {\n       writer.addDocument(d);\n     }\n \n-    // now we add another document which has norms for field f2 and not for f1 and verify if the\n-    // SegmentMerger\n-    // keep things constant\n-    d = new Document();\n+    // reverse omitNorms options for f1 and f2\n+    Document d2 = new Document();\n+    d2.add(new Field(\"f1\", \"This field has NO norms\", fieldType2));\n+    d2.add(new Field(\"f2\", \"This field has norms\", fieldType1));\n \n-    // Reverese\n-    d.add(newField(\"f1\", \"This field has norms\", customType));\n+    IllegalArgumentException exception =\n+        expectThrows(IllegalArgumentException.class, () -> writer.addDocument(d2));\n+    assertEquals(\n+        \"cannot change field \\\"f1\\\" from omitNorms=false to inconsistent omitNorms=true\",\n+        exception.getMessage());\n \n-    d.add(newTextField(\"f2\", \"This field has NO norms in all docs\", Field.Store.NO));\n-\n-    for (int i = 0; i < 30; i++) {\n-      writer.addDocument(d);\n-    }\n-\n-    // force merge\n     writer.forceMerge(1);\n-    // flush\n     writer.close();\n \n     LeafReader reader = getOnlyLeafReader(DirectoryReader.open(ram));\n     FieldInfos fi = reader.getFieldInfos();\n-    assertTrue(\"OmitNorms field bit should be set.\", fi.fieldInfo(\"f1\").omitsNorms());\n-    assertTrue(\"OmitNorms field bit should be set.\", fi.fieldInfo(\"f2\").omitsNorms());\n+    // assert original omitNorms\n+    assertTrue(\"OmitNorms field bit must not be set.\", fi.fieldInfo(\"f1\").omitsNorms() == false);\n+    assertTrue(\"OmitNorms field bit must be set.\", fi.fieldInfo(\"f2\").omitsNorms());\n \n     reader.close();\n     ram.close();\n@@ -223,110 +173,4 @@ public void testNoNrmFile() throws Throwable {\n     assertNoNrm(ram);\n     ram.close();\n   }\n-\n-  /**\n-   * Tests various combinations of omitNorms=true/false, the field not existing at all, ensuring\n-   * that only omitNorms is 'viral'. Internally checks that MultiNorms.norms() is consistent\n-   * (returns the same bytes) as the fully merged equivalent.\n-   */\n-  public void testOmitNormsCombos() throws IOException {\n-    // indexed with norms\n-    FieldType customType = new FieldType(TextField.TYPE_STORED);\n-    Field norms = new Field(\"foo\", \"a\", customType);\n-    // indexed without norms\n-    FieldType customType1 = new FieldType(TextField.TYPE_STORED);\n-    customType1.setOmitNorms(true);\n-    Field noNorms = new Field(\"foo\", \"a\", customType1);\n-    // not indexed, but stored\n-    FieldType customType2 = new FieldType();\n-    customType2.setStored(true);\n-    Field noIndex = new Field(\"foo\", \"a\", customType2);\n-    // not indexed but stored, omitNorms is set\n-    FieldType customType3 = new FieldType();\n-    customType3.setStored(true);\n-    customType3.setOmitNorms(true);\n-    Field noNormsNoIndex = new Field(\"foo\", \"a\", customType3);\n-    // not indexed nor stored (doesnt exist at all, we index a different field instead)\n-    Field emptyNorms = new Field(\"bar\", \"a\", customType);\n-\n-    assertNotNull(getNorms(\"foo\", norms, norms));\n-    assertNull(getNorms(\"foo\", norms, noNorms));\n-    assertNotNull(getNorms(\"foo\", norms, noIndex));\n-    assertNotNull(getNorms(\"foo\", norms, noNormsNoIndex));\n-    assertNotNull(getNorms(\"foo\", norms, emptyNorms));\n-    assertNull(getNorms(\"foo\", noNorms, noNorms));\n-    assertNull(getNorms(\"foo\", noNorms, noIndex));\n-    assertNull(getNorms(\"foo\", noNorms, noNormsNoIndex));\n-    assertNull(getNorms(\"foo\", noNorms, emptyNorms));\n-    assertNull(getNorms(\"foo\", noIndex, noIndex));\n-    assertNull(getNorms(\"foo\", noIndex, noNormsNoIndex));\n-    assertNull(getNorms(\"foo\", noIndex, emptyNorms));\n-    assertNull(getNorms(\"foo\", noNormsNoIndex, noNormsNoIndex));\n-    assertNull(getNorms(\"foo\", noNormsNoIndex, emptyNorms));\n-    assertNull(getNorms(\"foo\", emptyNorms, emptyNorms));\n-  }\n-\n-  /**\n-   * Indexes at least 1 document with f1, and at least 1 document with f2. returns the norms for\n-   * \"field\".\n-   */\n-  NumericDocValues getNorms(String field, Field f1, Field f2) throws IOException {\n-    Directory dir = newDirectory();\n-    IndexWriterConfig iwc =\n-        newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy());\n-    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n-\n-    // add f1\n-    Document d = new Document();\n-    d.add(f1);\n-    riw.addDocument(d);\n-\n-    // add f2\n-    d = new Document();\n-    d.add(f2);\n-    riw.addDocument(d);\n-\n-    // add a mix of f1's and f2's\n-    int numExtraDocs = TestUtil.nextInt(random(), 1, 1000);\n-    for (int i = 0; i < numExtraDocs; i++) {\n-      d = new Document();\n-      d.add(random().nextBoolean() ? f1 : f2);\n-      riw.addDocument(d);\n-    }\n-\n-    IndexReader ir1 = riw.getReader();\n-    // todo: generalize\n-    NumericDocValues norms1 = MultiDocValues.getNormValues(ir1, field);\n-\n-    // fully merge and validate MultiNorms against single segment.\n-    riw.forceMerge(1);\n-    DirectoryReader ir2 = riw.getReader();\n-    NumericDocValues norms2 = getOnlyLeafReader(ir2).getNormValues(field);\n-\n-    if (norms1 == null) {\n-      assertNull(norms2);\n-    } else {\n-      while (true) {\n-        int norms1DocID = norms1.nextDoc();\n-        int norms2DocID = norms2.nextDoc();\n-        while (norms1DocID < norms2DocID) {\n-          assertEquals(0, norms1.longValue());\n-          norms1DocID = norms1.nextDoc();\n-        }\n-        while (norms2DocID < norms1DocID) {\n-          assertEquals(0, norms2.longValue());\n-          norms2DocID = norms2.nextDoc();\n-        }\n-        if (norms1.docID() == NO_MORE_DOCS) {\n-          break;\n-        }\n-        assertEquals(norms1.longValue(), norms2.longValue());\n-      }\n-    }\n-    ir1.close();\n-    ir2.close();\n-    riw.close();\n-    dir.close();\n-    return norms1;\n-  }\n }"
  },
  {
    "sha": "83c6ce4f400499935e021bad643afc2b587afb91",
    "filename": "lucene/core/src/test/org/apache/lucene/index/TestPointValues.java",
    "status": "modified",
    "additions": 39,
    "deletions": 27,
    "changes": 66,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestPointValues.java",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestPointValues.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/core/src/test/org/apache/lucene/index/TestPointValues.java?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -74,7 +74,8 @@ public void testIllegalDimChangeOneDoc() throws Exception {\n               w.addDocument(doc);\n             });\n     assertEquals(\n-        \"cannot change point dimension count from 1 to 2 for field=\\\"dim\\\"\", expected.getMessage());\n+        \"Inconsistency of field data structures across documents for field [dim] of doc [0].\",\n+        expected.getMessage());\n     w.close();\n     dir.close();\n   }\n@@ -96,8 +97,8 @@ public void testIllegalDimChangeTwoDocs() throws Exception {\n               w.addDocument(doc2);\n             });\n     assertEquals(\n-        \"cannot change point dimension count from 1 to 2 for field=\\\"dim\\\"\", expected.getMessage());\n-\n+        \"Inconsistency of field data structures across documents for field [dim] of doc [1].\",\n+        expected.getMessage());\n     w.close();\n     dir.close();\n   }\n@@ -120,8 +121,9 @@ public void testIllegalDimChangeTwoSegments() throws Exception {\n               w.addDocument(doc2);\n             });\n     assertEquals(\n-        \"cannot change point dimension count from 1 to 2 for field=\\\"dim\\\"\", expected.getMessage());\n-\n+        \"cannot change field \\\"dim\\\" from points dimensionCount=1, indexDimensionCount=1, numBytes=4 \"\n+            + \"to inconsistent dimensionCount=2, indexDimensionCount=2, numBytes=4\",\n+        expected.getMessage());\n     w.close();\n     dir.close();\n   }\n@@ -146,8 +148,9 @@ public void testIllegalDimChangeTwoWriters() throws Exception {\n               w2.addDocument(doc2);\n             });\n     assertEquals(\n-        \"cannot change point dimension count from 1 to 2 for field=\\\"dim\\\"\", expected.getMessage());\n-\n+        \"cannot change field \\\"dim\\\" from points dimensionCount=1, indexDimensionCount=1, numBytes=4 \"\n+            + \"to inconsistent dimensionCount=2, indexDimensionCount=2, numBytes=4\",\n+        expected.getMessage());\n     w2.close();\n     dir.close();\n   }\n@@ -172,9 +175,11 @@ public void testIllegalDimChangeViaAddIndexesDirectory() throws Exception {\n             () -> {\n               w2.addIndexes(new Directory[] {dir});\n             });\n-    assertEquals(\n-        \"cannot change point dimension count from 2 to 1 for field=\\\"dim\\\"\", expected.getMessage());\n \n+    assertEquals(\n+        \"cannot change field \\\"dim\\\" from points dimensionCount=2, indexDimensionCount=2, numBytes=4 \"\n+            + \"to inconsistent dimensionCount=1, indexDimensionCount=1, numBytes=4\",\n+        expected.getMessage());\n     IOUtils.close(w2, dir, dir2);\n   }\n \n@@ -200,8 +205,9 @@ public void testIllegalDimChangeViaAddIndexesCodecReader() throws Exception {\n               w2.addIndexes(new CodecReader[] {(CodecReader) getOnlyLeafReader(r)});\n             });\n     assertEquals(\n-        \"cannot change point dimension count from 2 to 1 for field=\\\"dim\\\"\", expected.getMessage());\n-\n+        \"cannot change field \\\"dim\\\" from points dimensionCount=2, indexDimensionCount=2, numBytes=4 \"\n+            + \"to inconsistent dimensionCount=1, indexDimensionCount=1, numBytes=4\",\n+        expected.getMessage());\n     IOUtils.close(r, w2, dir, dir2);\n   }\n \n@@ -228,8 +234,9 @@ public void testIllegalDimChangeViaAddIndexesSlowCodecReader() throws Exception\n               TestUtil.addIndexesSlowly(w2, r);\n             });\n     assertEquals(\n-        \"cannot change point dimension count from 2 to 1 for field=\\\"dim\\\"\", expected.getMessage());\n-\n+        \"cannot change field \\\"dim\\\" from points dimensionCount=2, indexDimensionCount=2, numBytes=4 \"\n+            + \"to inconsistent dimensionCount=1, indexDimensionCount=1, numBytes=4\",\n+        expected.getMessage());\n     IOUtils.close(r, w2, dir, dir2);\n   }\n \n@@ -247,8 +254,8 @@ public void testIllegalNumBytesChangeOneDoc() throws Exception {\n               w.addDocument(doc);\n             });\n     assertEquals(\n-        \"cannot change point numBytes from 4 to 6 for field=\\\"dim\\\"\", expected.getMessage());\n-\n+        \"Inconsistency of field data structures across documents for field [dim] of doc [0].\",\n+        expected.getMessage());\n     w.close();\n     dir.close();\n   }\n@@ -270,8 +277,8 @@ public void testIllegalNumBytesChangeTwoDocs() throws Exception {\n               w.addDocument(doc2);\n             });\n     assertEquals(\n-        \"cannot change point numBytes from 4 to 6 for field=\\\"dim\\\"\", expected.getMessage());\n-\n+        \"Inconsistency of field data structures across documents for field [dim] of doc [1].\",\n+        expected.getMessage());\n     w.close();\n     dir.close();\n   }\n@@ -294,8 +301,9 @@ public void testIllegalNumBytesChangeTwoSegments() throws Exception {\n               w.addDocument(doc2);\n             });\n     assertEquals(\n-        \"cannot change point numBytes from 4 to 6 for field=\\\"dim\\\"\", expected.getMessage());\n-\n+        \"cannot change field \\\"dim\\\" from points dimensionCount=1, indexDimensionCount=1, numBytes=4 \"\n+            + \"to inconsistent dimensionCount=1, indexDimensionCount=1, numBytes=6\",\n+        expected.getMessage());\n     w.close();\n     dir.close();\n   }\n@@ -321,8 +329,9 @@ public void testIllegalNumBytesChangeTwoWriters() throws Exception {\n               w2.addDocument(doc2);\n             });\n     assertEquals(\n-        \"cannot change point numBytes from 4 to 6 for field=\\\"dim\\\"\", expected.getMessage());\n-\n+        \"cannot change field \\\"dim\\\" from points dimensionCount=1, indexDimensionCount=1, numBytes=4 \"\n+            + \"to inconsistent dimensionCount=1, indexDimensionCount=1, numBytes=6\",\n+        expected.getMessage());\n     w2.close();\n     dir.close();\n   }\n@@ -349,8 +358,9 @@ public void testIllegalNumBytesChangeViaAddIndexesDirectory() throws Exception {\n               w2.addIndexes(new Directory[] {dir});\n             });\n     assertEquals(\n-        \"cannot change point numBytes from 6 to 4 for field=\\\"dim\\\"\", expected.getMessage());\n-\n+        \"cannot change field \\\"dim\\\" from points dimensionCount=1, indexDimensionCount=1, numBytes=6 \"\n+            + \"to inconsistent dimensionCount=1, indexDimensionCount=1, numBytes=4\",\n+        expected.getMessage());\n     IOUtils.close(w2, dir, dir2);\n   }\n \n@@ -377,8 +387,9 @@ public void testIllegalNumBytesChangeViaAddIndexesCodecReader() throws Exception\n               w2.addIndexes(new CodecReader[] {(CodecReader) getOnlyLeafReader(r)});\n             });\n     assertEquals(\n-        \"cannot change point numBytes from 6 to 4 for field=\\\"dim\\\"\", expected.getMessage());\n-\n+        \"cannot change field \\\"dim\\\" from points dimensionCount=1, indexDimensionCount=1, numBytes=6 \"\n+            + \"to inconsistent dimensionCount=1, indexDimensionCount=1, numBytes=4\",\n+        expected.getMessage());\n     IOUtils.close(r, w2, dir, dir2);\n   }\n \n@@ -405,8 +416,9 @@ public void testIllegalNumBytesChangeViaAddIndexesSlowCodecReader() throws Excep\n               TestUtil.addIndexesSlowly(w2, r);\n             });\n     assertEquals(\n-        \"cannot change point numBytes from 6 to 4 for field=\\\"dim\\\"\", expected.getMessage());\n-\n+        \"cannot change field \\\"dim\\\" from points dimensionCount=1, indexDimensionCount=1, numBytes=6 \"\n+            + \"to inconsistent dimensionCount=1, indexDimensionCount=1, numBytes=4\",\n+        expected.getMessage());\n     IOUtils.close(r, w2, dir, dir2);\n   }\n "
  },
  {
    "sha": "2187e425386c9774099604cf40e8c5d4824b115a",
    "filename": "lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets.java",
    "status": "modified",
    "additions": 0,
    "deletions": 35,
    "changes": 35,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets.java",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets.java?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -363,41 +363,6 @@ public void testRandom() throws Exception {\n     dir.close();\n   }\n \n-  public void testWithUnindexedFields() throws Exception {\n-    Directory dir = newDirectory();\n-    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, iwc);\n-    for (int i = 0; i < 100; i++) {\n-      Document doc = new Document();\n-      // ensure at least one doc is indexed with offsets\n-      if (i < 99 && random().nextInt(2) == 0) {\n-        // stored only\n-        FieldType ft = new FieldType();\n-        ft.setStored(true);\n-        doc.add(new Field(\"foo\", \"boo!\", ft));\n-      } else {\n-        FieldType ft = new FieldType(TextField.TYPE_STORED);\n-        ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n-        if (random().nextBoolean()) {\n-          // store some term vectors for the checkindex cross-check\n-          ft.setStoreTermVectors(true);\n-          ft.setStoreTermVectorPositions(true);\n-          ft.setStoreTermVectorOffsets(true);\n-        }\n-        doc.add(new Field(\"foo\", \"bar\", ft));\n-      }\n-      riw.addDocument(doc);\n-    }\n-    CompositeReader ir = riw.getReader();\n-    FieldInfos fis = FieldInfos.getMergedFieldInfos(ir);\n-    assertEquals(\n-        IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n-        fis.fieldInfo(\"foo\").getIndexOptions());\n-    ir.close();\n-    ir.close();\n-    riw.close();\n-    dir.close();\n-  }\n-\n   public void testAddFieldTwice() throws Exception {\n     Directory dir = newDirectory();\n     RandomIndexWriter iw = new RandomIndexWriter(random(), dir);"
  },
  {
    "sha": "ecd567cf604b4814edde1d6ea16323803a919943",
    "filename": "lucene/core/src/test/org/apache/lucene/index/TestSortingCodecReader.java",
    "status": "modified",
    "additions": 8,
    "deletions": 6,
    "changes": 14,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestSortingCodecReader.java",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestSortingCodecReader.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/core/src/test/org/apache/lucene/index/TestSortingCodecReader.java?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -114,8 +114,8 @@ public void testSortOnAddIndicesRandom() throws IOException {\n         for (int i = 0; i < numDocs; i++) {\n           int docId = docIds.get(i);\n           Document doc = new Document();\n-          doc.add(new StringField(\"id\", Integer.toString(docId), Field.Store.YES));\n-          doc.add(new LongPoint(\"id\", docId));\n+          doc.add(new StringField(\"string_id\", Integer.toString(docId), Field.Store.YES));\n+          doc.add(new LongPoint(\"point_id\", docId));\n           String s = RandomStrings.randomRealisticUnicodeOfLength(random(), 25);\n           doc.add(new TextField(\"text_field\", s, Field.Store.YES));\n           doc.add(new BinaryDocValuesField(\"text_field\", new BytesRef(s)));\n@@ -150,7 +150,7 @@ public void testSortOnAddIndicesRandom() throws IOException {\n           iw.addDocument(doc);\n           if (i > 0 && random().nextInt(5) == 0) {\n             final int id = RandomPicks.randomFrom(random(), docIds.subList(0, i));\n-            iw.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n+            iw.deleteDocuments(new Term(\"string_id\", Integer.toString(id)));\n           }\n         }\n         iw.commit();\n@@ -240,13 +240,15 @@ public void testSortOnAddIndicesRandom() throws IOException {\n                       .terms(\"term_vectors\")\n                       .iterator()\n                       .seekExact(new BytesRef(\"test\" + ids.longValue())));\n-              assertEquals(Long.toString(ids.longValue()), leaf.document(idNext).get(\"id\"));\n+              assertEquals(Long.toString(ids.longValue()), leaf.document(idNext).get(\"string_id\"));\n               IndexSearcher searcher = new IndexSearcher(r);\n-              TopDocs result = searcher.search(LongPoint.newExactQuery(\"id\", ids.longValue()), 1);\n+              TopDocs result =\n+                  searcher.search(LongPoint.newExactQuery(\"point_id\", ids.longValue()), 1);\n               assertEquals(1, result.totalHits.value);\n               assertEquals(idNext, result.scoreDocs[0].doc);\n \n-              result = searcher.search(new TermQuery(new Term(\"id\", \"\" + ids.longValue())), 1);\n+              result =\n+                  searcher.search(new TermQuery(new Term(\"string_id\", \"\" + ids.longValue())), 1);\n               assertEquals(1, result.totalHits.value);\n               assertEquals(idNext, result.scoreDocs[0].doc);\n             }"
  },
  {
    "sha": "1cbf3c75d33fb4dd07b9ed85df26208ee7bfe736",
    "filename": "lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2.java",
    "status": "modified",
    "additions": 58,
    "deletions": 63,
    "changes": 121,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2.java",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2.java?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -26,6 +26,7 @@\n import java.util.List;\n import java.util.Map;\n import java.util.Random;\n+import java.util.concurrent.ConcurrentHashMap;\n import org.apache.lucene.analysis.MockAnalyzer;\n import org.apache.lucene.document.Document;\n import org.apache.lucene.document.Field;\n@@ -39,6 +40,7 @@\n import org.apache.lucene.util.BytesRef;\n import org.apache.lucene.util.LuceneTestCase;\n import org.apache.lucene.util.TestUtil;\n+import org.junit.Before;\n \n public class TestStressIndexing2 extends LuceneTestCase {\n   static int maxFields = 4;\n@@ -47,6 +49,13 @@\n   static int mergeFactor = 3;\n   static int maxBufferedDocs = 3;\n   static int seed = 0;\n+  private static Map<String, FieldType> fieldTypes;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    super.setUp();\n+    fieldTypes = new ConcurrentHashMap<>();\n+  }\n \n   public void testRandomIWReader() throws Throwable {\n     Directory dir = newMaybeVirusCheckingDirectory();\n@@ -128,7 +137,6 @@ public void testMultiConfig() throws Throwable {\n     }\n   }\n \n-  static Term idTerm = new Term(\"id\", \"\");\n   IndexingThread[] threads;\n   static Comparator<IndexableField> fieldNameComparator =\n       new Comparator<IndexableField>() {\n@@ -807,71 +815,58 @@ public void indexDoc() throws IOException {\n       Field idField = newField(\"id\", idString, customType1);\n       fields.add(idField);\n \n-      Map<String, FieldType> tvTypes = new HashMap<>();\n-\n       int nFields = nextInt(maxFields);\n       for (int i = 0; i < nFields; i++) {\n-\n         String fieldName = \"f\" + nextInt(100);\n-        FieldType customType;\n-\n-        // Use the same term vector settings if we already\n-        // added this field to the doc:\n-        FieldType oldTVType = tvTypes.get(fieldName);\n-        if (oldTVType != null) {\n-          customType = new FieldType(oldTVType);\n-        } else {\n-          customType = new FieldType();\n-          switch (nextInt(4)) {\n-            case 0:\n-              break;\n-            case 1:\n-              customType.setStoreTermVectors(true);\n-              break;\n-            case 2:\n-              customType.setStoreTermVectors(true);\n-              customType.setStoreTermVectorPositions(true);\n-              break;\n-            case 3:\n-              customType.setStoreTermVectors(true);\n-              customType.setStoreTermVectorOffsets(true);\n-              break;\n-          }\n-          FieldType newType = new FieldType(customType);\n-          newType.freeze();\n-          tvTypes.put(fieldName, newType);\n-        }\n-\n-        switch (nextInt(4)) {\n-          case 0:\n-            customType.setStored(true);\n-            customType.setOmitNorms(true);\n-            customType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n-            customType.freeze();\n-            fields.add(newField(fieldName, getString(1), customType));\n-            break;\n-          case 1:\n-            customType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n-            customType.setTokenized(true);\n-            customType.freeze();\n-            fields.add(newField(fieldName, getString(0), customType));\n-            break;\n-          case 2:\n-            customType.setStored(true);\n-            customType.setStoreTermVectors(false);\n-            customType.setStoreTermVectorOffsets(false);\n-            customType.setStoreTermVectorPositions(false);\n-            customType.freeze();\n-            fields.add(newField(fieldName, getString(0), customType));\n-            break;\n-          case 3:\n-            customType.setStored(true);\n-            customType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n-            customType.setTokenized(true);\n-            customType.freeze();\n-            fields.add(newField(fieldName, getString(bigFieldSize), customType));\n-            break;\n-        }\n+        // Use the same field type if we already added this field to the index\n+        FieldType fieldType =\n+            fieldTypes.computeIfAbsent(\n+                fieldName,\n+                fn -> {\n+                  FieldType ft = new FieldType();\n+                  switch (nextInt(4)) {\n+                    case 0:\n+                      break;\n+                    case 1:\n+                      ft.setStoreTermVectors(true);\n+                      break;\n+                    case 2:\n+                      ft.setStoreTermVectors(true);\n+                      ft.setStoreTermVectorPositions(true);\n+                      break;\n+                    case 3:\n+                      ft.setStoreTermVectors(true);\n+                      ft.setStoreTermVectorOffsets(true);\n+                      break;\n+                  }\n+                  switch (nextInt(4)) {\n+                    case 0:\n+                      ft.setStored(true);\n+                      ft.setOmitNorms(true);\n+                      ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n+                      break;\n+                    case 1:\n+                      ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n+                      ft.setTokenized(true);\n+                      break;\n+                    case 2:\n+                      ft.setStored(true);\n+                      ft.setStoreTermVectors(false);\n+                      ft.setStoreTermVectorOffsets(false);\n+                      ft.setStoreTermVectorPositions(false);\n+                      break;\n+                    case 3:\n+                      ft.setStored(true);\n+                      ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);\n+                      ft.setTokenized(true);\n+                      break;\n+                  }\n+                  ft.freeze();\n+                  return ft;\n+                });\n+        int nTokens = nextInt(3);\n+        nTokens = nTokens < 2 ? nTokens : bigFieldSize;\n+        fields.add(newField(fieldName, getString(nTokens), fieldType));\n       }\n \n       if (sameFieldOrder) {"
  },
  {
    "sha": "c019db8e980c6928719f8541ba6d852e5a9e47ee",
    "filename": "lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter.java",
    "status": "modified",
    "additions": 12,
    "deletions": 11,
    "changes": 23,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter.java",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/core/src/test/org/apache/lucene/index/TestTermVectorsWriter.java?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -16,6 +16,9 @@\n  */\n package org.apache.lucene.index;\n \n+import static org.hamcrest.CoreMatchers.anyOf;\n+import static org.hamcrest.CoreMatchers.startsWith;\n+\n import java.io.IOException;\n import org.apache.lucene.analysis.Analyzer;\n import org.apache.lucene.analysis.CachingTokenFilter;\n@@ -35,6 +38,7 @@\n import org.apache.lucene.util.BytesRef;\n import org.apache.lucene.util.LuceneTestCase;\n import org.apache.lucene.util.TestUtil;\n+import org.hamcrest.MatcherAssert;\n \n /** tests for writing term vectors */\n public class TestTermVectorsWriter extends LuceneTestCase {\n@@ -649,18 +653,15 @@ private void doTestMixup(FieldType ft1, FieldType ft2) throws IOException {\n \n     // ensure broken doc hits exception\n     IllegalArgumentException expected =\n-        expectThrows(\n-            IllegalArgumentException.class,\n-            () -> {\n-              iw.addDocument(doc);\n-            });\n+        expectThrows(IllegalArgumentException.class, () -> iw.addDocument(doc));\n     assertNotNull(expected.getMessage());\n-    assertTrue(\n-        expected\n-            .getMessage()\n-            .startsWith(\n-                \"all instances of a given field name must have the same term vectors settings\"));\n-\n+    MatcherAssert.assertThat(\n+        expected.getMessage(),\n+        anyOf(\n+            startsWith(\n+                \"all instances of a given field name must have the same term vectors settings\"),\n+            startsWith(\n+                \"Inconsistency of field data structures across documents for field [field]\")));\n     // ensure good docs are still ok\n     IndexReader ir = iw.getReader();\n     assertEquals(3, ir.numDocs());"
  },
  {
    "sha": "05142b8613b905cd323c78d29005404e814c9b44",
    "filename": "lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestBackwardsCompatibility.java",
    "status": "modified",
    "additions": 5,
    "deletions": 0,
    "changes": 5,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestBackwardsCompatibility.java",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestBackwardsCompatibility.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestBackwardsCompatibility.java?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -51,6 +51,11 @@\n \n   public static final String oldTaxonomyIndexName = \"taxonomy.8.6.3-cfs\";\n \n+  // LUCENE-9334 requires consistency of field data structures between documents.\n+  // Old taxonomy index had $full_path$ field indexed only with postings,\n+  // It is not allowed to add the same field $full_path$ indexed with BinaryDocValues\n+  // for a new segment, that this test is trying to do.\n+  @AwaitsFix(bugUrl = \"https://issues.apache.org/jira/browse/LUCENE-9334\")\n   public void testCreateNewTaxonomy() throws IOException {\n     createNewTaxonomyIndex(oldTaxonomyIndexName);\n   }"
  },
  {
    "sha": "d8fe9e60badeae09153a97cb83f26afb4511776b",
    "filename": "lucene/grouping/src/test/org/apache/lucene/search/grouping/TestDistinctValuesCollector.java",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/apache/lucene/blob/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/grouping/src/test/org/apache/lucene/search/grouping/TestDistinctValuesCollector.java",
    "raw_url": "https://github.com/apache/lucene/raw/0fe3493110ac2a5f750ad41f732436daff6c69f5/lucene/grouping/src/test/org/apache/lucene/search/grouping/TestDistinctValuesCollector.java",
    "contents_url": "https://api.github.com/repos/apache/lucene/contents/lucene/grouping/src/test/org/apache/lucene/search/grouping/TestDistinctValuesCollector.java?ref=0fe3493110ac2a5f750ad41f732436daff6c69f5",
    "patch": "@@ -112,7 +112,7 @@ public void testSimple() throws Exception {\n \n     // 6 -- no author field\n     doc = new Document();\n-    doc.add(new TextField(\"content\", \"random word stuck in alot of other text\", Field.Store.YES));\n+    doc.add(new TextField(\"content\", \"random word stuck in alot of other text\", Field.Store.NO));\n     addField(doc, COUNT_FIELD, \"1\");\n     doc.add(new StringField(\"id\", \"6\", Field.Store.NO));\n     w.addDocument(doc);"
  }
]
