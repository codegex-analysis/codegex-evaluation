[
  {
    "sha": "817fafe82fec915acdb62465b82ace44171fd62d",
    "filename": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java",
    "status": "modified",
    "additions": 1,
    "deletions": 0,
    "changes": 1,
    "blob_url": "https://github.com/apache/hive/blob/16652e111b8ca63a79f67a05e1a5f4a6c2124cbf/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java",
    "raw_url": "https://github.com/apache/hive/raw/16652e111b8ca63a79f67a05e1a5f4a6c2124cbf/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java?ref=16652e111b8ca63a79f67a05e1a5f4a6c2124cbf",
    "patch": "@@ -2133,6 +2133,7 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal\n     HIVETESTCURRENTTIMESTAMP(\"hive.test.currenttimestamp\", null, \"current timestamp for test\", false),\n     HIVETESTMODEROLLBACKTXN(\"hive.test.rollbacktxn\", false, \"For testing only.  Will mark every ACID transaction aborted\", false),\n     HIVETESTMODEFAILCOMPACTION(\"hive.test.fail.compaction\", false, \"For testing only.  Will cause CompactorMR to fail.\", false),\n+    HIVETESTMODEFAILAFTERCOMPACTION(\"hive.test.fail.after.compaction\", false, \"For testing only.  Will cause CompactorMR to fail all opening txn and creating directories for compaction.\", false),\n     HIVETESTMODEFAILLOADDYNAMICPARTITION(\"hive.test.fail.load.dynamic.partition\", false, \"For testing only.  Will cause loadDynamicPartition to fail.\", false),\n     HIVETESTMODEFAILHEARTBEATER(\"hive.test.fail.heartbeater\", false, \"For testing only.  Will cause Heartbeater to fail.\", false),\n     TESTMODE_BUCKET_CODEC_VERSION(\"hive.test.bucketcodec.version\", 1,"
  },
  {
    "sha": "3abc5754a3ac99988a7b11ce95585f25ba535585",
    "filename": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java",
    "status": "modified",
    "additions": 27,
    "deletions": 1,
    "changes": 28,
    "blob_url": "https://github.com/apache/hive/blob/16652e111b8ca63a79f67a05e1a5f4a6c2124cbf/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java",
    "raw_url": "https://github.com/apache/hive/raw/16652e111b8ca63a79f67a05e1a5f4a6c2124cbf/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java?ref=16652e111b8ca63a79f67a05e1a5f4a6c2124cbf",
    "patch": "@@ -25,6 +25,7 @@\n import org.apache.hadoop.hive.metastore.HiveMetaStoreUtils;\n import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n import org.apache.hadoop.hive.metastore.MetaStoreThread;\n+import org.apache.hadoop.hive.metastore.api.CompactionType;\n import org.apache.hadoop.hive.metastore.api.MetaException;\n import org.apache.hadoop.hive.metastore.api.Partition;\n import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n@@ -523,6 +524,12 @@ protected Boolean findNextCompactionAndExecute(boolean computeStats) throws Inte\n       final StatsUpdater su = computeStats ? StatsUpdater.init(ci, msc.findColumnsWithStats(\n           CompactionInfo.compactionInfoToStruct(ci)), conf,\n           runJobAsSelf(ci.runAs) ? ci.runAs : t.getOwner()) : null;\n+      // result directory for compactor to write new files\n+\n+      final Path resultDir = QueryCompactor.Util.getCompactionResultDir(sd, tblValidWriteIds, conf,\n+          ci.type == CompactionType.MAJOR,false, false, dir);\n+      final Path resultDeleteDeltaDir = (ci.type == CompactionType.MINOR) ? QueryCompactor.Util.getCompactionResultDir(sd,\n+          tblValidWriteIds, conf, false, true, false, dir) : null;\n \n       try {\n         failCompactionIfSetForTest();\n@@ -535,6 +542,7 @@ protected Boolean findNextCompactionAndExecute(boolean computeStats) throws Inte\n         */\n         QueryCompactor queryCompactor = QueryCompactorFactory.getQueryCompactor(t, conf, ci);\n         if (queryCompactor != null) {\n+\n           LOG.info(\"Will compact id: \" + ci.id + \" with query-based compactor class: \"\n               + queryCompactor.getClass().getName());\n           queryCompactor.runCompaction(conf, t, p, sd, tblValidWriteIds, ci, dir);\n@@ -545,6 +553,8 @@ protected Boolean findNextCompactionAndExecute(boolean computeStats) throws Inte\n \n         heartbeater.cancel();\n \n+        failAfterCompactionIfSetForTest();\n+\n         verifyTableIdHasNotChanged(ci, t1);\n \n         LOG.info(\"Completed \" + ci.type.toString() + \" compaction for \" + ci.getFullPartitionName() + \" in \"\n@@ -553,8 +563,18 @@ protected Boolean findNextCompactionAndExecute(boolean computeStats) throws Inte\n         compactionTxn.wasSuccessful();\n       } catch (Throwable e) {\n         LOG.error(\"Caught exception while trying to compact \" + ci +\n-            \".  Marking failed to avoid repeated failures\", e);\n+            \". Deleting the result directories created by the compactor \"+ resultDir +\n+            \" Marking failed to avoid repeated failures\", e);\n         markFailed(ci, e);\n+        // remove the new directories created by compaction\n+        if (resultDir != null) {\n+          FileSystem fs = resultDir.getFileSystem(conf);\n+          fs.delete(resultDir, true);\n+        }\n+        if (resultDeleteDeltaDir != null) {\n+          FileSystem fs = resultDeleteDeltaDir.getFileSystem(conf);\n+          fs.delete(resultDeleteDeltaDir, true);\n+        }\n       }\n     } catch (TException | IOException t) {\n       LOG.error(\"Caught an exception in the main loop of compactor worker \" + workerName, t);\n@@ -604,6 +624,12 @@ private void failCompactionIfSetForTest() {\n     }\n   }\n \n+  private void failAfterCompactionIfSetForTest() {\n+    if(conf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST) && conf.getBoolVar(HiveConf.ConfVars.HIVETESTMODEFAILAFTERCOMPACTION)) {\n+      throw new RuntimeException(HiveConf.ConfVars.HIVETESTMODEFAILAFTERCOMPACTION.name() + \"=true\");\n+    }\n+  }\n+\n   private void runCompactionViaMrJob(CompactionInfo ci, Table t, Partition p, StorageDescriptor sd,\n       ValidCompactorWriteIdList tblValidWriteIds, StringBuilder jobName, AcidDirectory dir, StatsUpdater su)\n       throws IOException, HiveException, InterruptedException {"
  },
  {
    "sha": "e8a6f15c45960876f9f3fb364a1f0e607f4e933c",
    "filename": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands3.java",
    "status": "modified",
    "additions": 82,
    "deletions": 0,
    "changes": 82,
    "blob_url": "https://github.com/apache/hive/blob/16652e111b8ca63a79f67a05e1a5f4a6c2124cbf/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands3.java",
    "raw_url": "https://github.com/apache/hive/raw/16652e111b8ca63a79f67a05e1a5f4a6c2124cbf/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands3.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands3.java?ref=16652e111b8ca63a79f67a05e1a5f4a6c2124cbf",
    "patch": "@@ -463,6 +463,88 @@ public void testCompactionAbort() throws Exception {\n     runCleaner(hiveConf);\n   }\n \n+  @Test\n+  public void testMajorCompactionAbortLeftoverFiles() throws Exception {\n+    MetastoreConf.setBoolVar(hiveConf, MetastoreConf.ConfVars.CREATE_TABLES_AS_ACID, true);\n+\n+    dropTable(new String[] {\"T\"});\n+    //note: transaction names T1, T2, etc below, are logical, the actual txnid will be different\n+    runStatementOnDriver(\"create table T (a int, b int) stored as orc\");\n+    runStatementOnDriver(\"insert into T values(0,2)\");//makes delta_1_1 in T1\n+    runStatementOnDriver(\"insert into T values(1,4)\");//makes delta_2_2 in T2\n+\n+    //create failed compaction attempt so that compactor txn is aborted\n+    HiveConf.setBoolVar(hiveConf, HiveConf.ConfVars.HIVETESTMODEFAILAFTERCOMPACTION, true);\n+    runStatementOnDriver(\"alter table T compact 'major'\");\n+    runWorker(hiveConf);\n+\n+    TxnStore txnHandler = TxnUtils.getTxnStore(hiveConf);\n+    ShowCompactResponse resp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(\"Unexpected number of compactions in history\",\n+        1, resp.getCompactsSize());\n+    Assert.assertEquals(\"Unexpected 0th compaction state\",\n+        TxnStore.FAILED_RESPONSE, resp.getCompacts().get(0).getState());\n+    GetOpenTxnsResponse openResp =  txnHandler.getOpenTxns();\n+    Assert.assertEquals(openResp.toString(), 1, openResp.getOpen_txnsSize());\n+    //check that the compactor txn is aborted\n+    Assert.assertTrue(openResp.toString(), BitSet.valueOf(openResp.getAbortedBits()).get(0));\n+\n+    FileSystem fs = FileSystem.get(hiveConf);\n+    Path warehousePath = new Path(getWarehouseDir());\n+    FileStatus[] actualList = fs.listStatus(new Path(warehousePath + \"/t\"),\n+        FileUtils.HIDDEN_FILES_PATH_FILTER);\n+\n+    // we expect all the t/base_* files to be removed by the compactor failure\n+    String[] expectedList = new String[] {\n+        \"/t/delta_0000001_0000001_0000\",\n+        \"/t/delta_0000002_0000002_0000\",\n+    };\n+    checkExpectedFiles(actualList, expectedList, warehousePath.toString());\n+    //delete metadata about aborted txn from txn_components and files (if any)\n+    runCleaner(hiveConf);\n+  }\n+\n+  @Test\n+  public void testMinorCompactionAbortLeftoverFiles() throws Exception {\n+    MetastoreConf.setBoolVar(hiveConf, MetastoreConf.ConfVars.CREATE_TABLES_AS_ACID, true);\n+\n+    dropTable(new String[] {\"T\"});\n+    //note: transaction names T1, T2, etc below, are logical, the actual txnid will be different\n+    runStatementOnDriver(\"create table T (a int, b int) stored as orc\");\n+    runStatementOnDriver(\"insert into T values(0,2)\");//makes delta_1_1 in T1\n+    runStatementOnDriver(\"insert into T values(1,4)\");//makes delta_2_2 in T2\n+\n+    //create failed compaction attempt so that compactor txn is aborted\n+    HiveConf.setBoolVar(hiveConf, HiveConf.ConfVars.HIVETESTMODEFAILAFTERCOMPACTION, true);\n+    runStatementOnDriver(\"alter table T compact 'minor'\");\n+    runWorker(hiveConf);\n+\n+    TxnStore txnHandler = TxnUtils.getTxnStore(hiveConf);\n+    ShowCompactResponse resp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(\"Unexpected number of compactions in history\",\n+        1, resp.getCompactsSize());\n+    Assert.assertEquals(\"Unexpected 0th compaction state\",\n+        TxnStore.FAILED_RESPONSE, resp.getCompacts().get(0).getState());\n+    GetOpenTxnsResponse openResp =  txnHandler.getOpenTxns();\n+    Assert.assertEquals(openResp.toString(), 1, openResp.getOpen_txnsSize());\n+    //check that the compactor txn is aborted\n+    Assert.assertTrue(openResp.toString(), BitSet.valueOf(openResp.getAbortedBits()).get(0));\n+\n+    FileSystem fs = FileSystem.get(hiveConf);\n+    Path warehousePath = new Path(getWarehouseDir());\n+    FileStatus[] actualList = fs.listStatus(new Path(warehousePath + \"/t\"),\n+        FileUtils.HIDDEN_FILES_PATH_FILTER);\n+\n+    // we expect all the t/base_* files to be removed by the compactor failure\n+    String[] expectedList = new String[] {\n+        \"/t/delta_0000001_0000001_0000\",\n+        \"/t/delta_0000002_0000002_0000\",\n+    };\n+    checkExpectedFiles(actualList, expectedList, warehousePath.toString());\n+    //delete metadata about aborted txn from txn_components and files (if any)\n+    runCleaner(hiveConf);\n+  }\n+\n   /**\n    * Not enough deltas to compact, no need to clean: there is absolutely nothing to do.\n    */"
  }
]
