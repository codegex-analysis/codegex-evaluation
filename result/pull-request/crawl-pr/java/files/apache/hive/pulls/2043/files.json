[
  {
    "sha": "92645bccca28113c0a855dd90da1275cde514546",
    "filename": "common/src/java/org/apache/hadoop/hive/common/FileUtils.java",
    "status": "modified",
    "additions": 25,
    "deletions": 1,
    "changes": 26,
    "blob_url": "https://github.com/apache/hive/blob/cccf1375fdcc8ae3582155870b974fc205b10eb3/common/src/java/org/apache/hadoop/hive/common/FileUtils.java",
    "raw_url": "https://github.com/apache/hive/raw/cccf1375fdcc8ae3582155870b974fc205b10eb3/common/src/java/org/apache/hadoop/hive/common/FileUtils.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/common/src/java/org/apache/hadoop/hive/common/FileUtils.java?ref=cccf1375fdcc8ae3582155870b974fc205b10eb3",
    "patch": "@@ -26,7 +26,6 @@\n import java.net.URI;\n import java.net.URISyntaxException;\n import java.nio.ByteBuffer;\n-import java.nio.file.Files;\n import java.security.AccessControlException;\n import java.security.PrivilegedExceptionAction;\n import java.util.ArrayList;\n@@ -691,6 +690,31 @@ public static boolean distCp(FileSystem srcFS, List<Path> srcPaths, Path dst,\n     return copied;\n   }\n \n+  public static boolean distCpWithSnapshot(FileSystem srcFs, String snap1,\n+      String snap2, List<Path> srcPaths, Path dst,\n+      UserGroupInformation proxyUser, HiveConf conf, HadoopShims shims) {\n+    boolean copied;\n+    try {\n+      if (proxyUser == null) {\n+        copied =\n+            shims.runDistCpWithSnapshots(snap1, snap2, srcPaths, dst, conf);\n+      } else {\n+        copied = shims\n+            .runDistCpWithSnapshotsAs(snap1, snap2, srcPaths, dst, conf,\n+                proxyUser);\n+      }\n+    } catch (Exception e) {\n+      LOG.error(\"Can not copy using snapshot from source: {}, target: {}\",\n+          srcPaths, dst);\n+      copied = false;\n+    }\n+    if(copied)\n+      LOG.info(\"Successfully copied using snapshots source {} and dest {} \"\n+              + \"using snapshots {} and {}\",\n+          srcPaths, dst, snap1, snap2);\n+    return copied;\n+  }\n+\n   /**\n    * Move a particular file or directory to the trash.\n    * @param fs FileSystem to use"
  },
  {
    "sha": "7826cf636c8fc9280f34385d051ea0ab2a457a7a",
    "filename": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java",
    "status": "modified",
    "additions": 11,
    "deletions": 0,
    "changes": 11,
    "blob_url": "https://github.com/apache/hive/blob/cccf1375fdcc8ae3582155870b974fc205b10eb3/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java",
    "raw_url": "https://github.com/apache/hive/raw/cccf1375fdcc8ae3582155870b974fc205b10eb3/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java?ref=cccf1375fdcc8ae3582155870b974fc205b10eb3",
    "patch": "@@ -667,6 +667,17 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal\n         + \" table or partition level. If hive.exec.parallel \\n\"\n         + \"is set to true then max worker threads created for copy can be hive.exec.parallel.thread.number(determines \\n\"\n         + \"number of copy tasks in parallel) * hive.repl.parallel.copy.tasks \"),\n+    REPL_SNAPSHOT_DIFF_FOR_EXTERNAL_TABLE_COPY(\"hive.repl.externaltable.snapshotdiff.copy\",\n+        false,\"Use snapshot diff for copying data from source to \"\n+        + \"destination cluster for external table in distcp\"),\n+    REPL_SNAPSHOT_PREFIX_FOR_EXTERNAL_TABLE_COPY(\"hive.repl.externaltable.snapshot.prefix\",\n+        \"\",\"prefix to be used for creating snapshots for external table \"\n+        + \"copy if hive.repl.externaltable.snapshotdiff.copy is set to true, \"\n+        + \"each policy should have unique snapshot prefix\"),\n+    REPL_SNAPSHOT_EXTERNAL_TABLE_PATHS(\"hive.repl.externatable.snapshot.paths\",\n+        \"\",\"A comma seperated list of paths, where an external table or \"\n+        + \"partition lies apart from the database location, the specified \"\n+        + \"path shall be copied using snapshots\"),\n     LOCALSCRATCHDIR(\"hive.exec.local.scratchdir\",\n         \"${system:java.io.tmpdir}\" + File.separator + \"${system:user.name}\",\n         \"Local scratch space for Hive jobs\"),"
  },
  {
    "sha": "6cf55afe617470e39794eb33b692eff6427af34e",
    "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosUsingSnapshots.java",
    "status": "added",
    "additions": 876,
    "deletions": 0,
    "changes": 876,
    "blob_url": "https://github.com/apache/hive/blob/cccf1375fdcc8ae3582155870b974fc205b10eb3/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosUsingSnapshots.java",
    "raw_url": "https://github.com/apache/hive/raw/cccf1375fdcc8ae3582155870b974fc205b10eb3/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosUsingSnapshots.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosUsingSnapshots.java?ref=cccf1375fdcc8ae3582155870b974fc205b10eb3",
    "patch": "@@ -0,0 +1,876 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.parse;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.protocol.HdfsConstants;\n+import org.apache.hadoop.hive.conf.Constants;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.conf.MetastoreConf;\n+import org.apache.hadoop.hive.metastore.messaging.json.gzip.GzipJSONMessageEncoder;\n+import org.apache.hadoop.hive.metastore.utils.FileUtils;\n+import org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils;\n+import org.apache.hadoop.hive.ql.metadata.Hive;\n+import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.parse.repl.metric.MetricCollector;\n+import org.apache.hadoop.hive.ql.parse.repl.metric.event.ReplicationMetric;\n+import org.apache.hadoop.hive.ql.parse.repl.metric.event.Stage;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.test.LambdaTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.hive.ql.parse.repl.metric.event.Metadata.ReplicationType.BOOTSTRAP;\n+import static org.apache.hadoop.hive.ql.parse.repl.metric.event.Metadata.ReplicationType.INCREMENTAL;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TestReplicationScenariosUsingSnapshots extends BaseReplicationAcrossInstances {\n+\n+  String extraPrimaryDb;\n+\n+  @BeforeClass\n+  public static void classLevelSetup() throws Exception {\n+    HashMap<String, String> overrides = new HashMap<>();\n+    overrides.put(MetastoreConf.ConfVars.EVENT_MESSAGE_FACTORY.getHiveName(),\n+        GzipJSONMessageEncoder.class.getCanonicalName());\n+    overrides.put(HiveConf.ConfVars.REPL_DUMP_METADATA_ONLY.varname, \"false\");\n+    overrides.put(HiveConf.ConfVars.REPL_INCLUDE_EXTERNAL_TABLES.varname, \"true\");\n+    overrides.put(HiveConf.ConfVars.HIVE_DISTCP_DOAS_USER.varname,\n+        UserGroupInformation.getCurrentUser().getUserName());\n+    overrides.put(HiveConf.ConfVars.REPL_RUN_DATA_COPY_TASKS_ON_TARGET.varname, \"false\");\n+    overrides.put(HiveConf.ConfVars.REPL_SNAPSHOT_PREFIX_FOR_EXTERNAL_TABLE_COPY.varname, \"s0\");\n+    overrides.put(HiveConf.ConfVars.REPL_EXTERNAL_WAREHOUSE_SINGLE_COPY_TASK.varname, \"true\");\n+    overrides.put(HiveConf.ConfVars.REPL_SNAPSHOT_DIFF_FOR_EXTERNAL_TABLE_COPY.varname, \"true\");\n+\n+    internalBeforeClassSetup(overrides, TestReplicationScenarios.class);\n+  }\n+\n+  @Before\n+  public void setup() throws Throwable {\n+    super.setup();\n+    extraPrimaryDb = \"extra_\" + primaryDbName;\n+\n+  }\n+\n+  @After\n+  public void tearDown() throws Throwable {\n+    primary.run(\"drop database if exists \" + extraPrimaryDb + \" cascade\");\n+    super.tearDown();\n+  }\n+\n+\n+  @Test\n+  public void testBasicReplicationWithSnapshots() throws Throwable {\n+\n+    // Create a partitioned and non-partitioned table and call dump.\n+    WarehouseInstance.Tuple tuple = primary\n+        .run(\"use \" + primaryDbName)\n+        .run(\"create external table table1 (id int)\")\n+        .run(\"insert into table table1 values (1)\")\n+        .run(\"insert into table table1 values (2)\")\n+        .run(\"create external table table2 (place string) partitioned by \"\n+            + \"(country string)\")\n+        .run(\"insert into table table2 partition(country='india') values \"\n+            + \"('bangalore')\")\n+        .run(\"insert into table table2 partition(country='us') values \"\n+            + \"('austin')\")\n+        .run(\"insert into table table2 partition(country='france') values \"\n+            + \"('paris')\")\n+        .dump(primaryDbName);\n+\n+    // Call load, For the first time, only snapshots would be created and distCp\n+    // would run from source snapshot to target.\n+    replica.load(replicatedDbName, primaryDbName)\n+        .run(\"use \" + replicatedDbName)\n+        .run(\"show tables like 'table1'\")\n+        .verifyResult(\"table1\")\n+        .run(\"show tables like 'table2'\")\n+        .verifyResult(\"table2\")\n+        .run(\"repl status \" + replicatedDbName)\n+        .verifyResult(tuple.lastReplicationId)\n+        .run(\"select country from table2 where country = 'us'\")\n+        .verifyResult(\"us\")\n+        .run(\"select country from table2 where country = 'france'\")\n+        .verifyResult(\"france\")\n+        .run(\"show partitions table2\")\n+        .verifyResults(new String[] {\"country\"\n+        + \"=france\", \"country=india\", \"country=us\"})\n+        .run(\"select * from table1\").\n+        verifyResults(new String[]{\"1\",\"2\"});\n+\n+    // Verify Snapshots are created in source.\n+    validateInitialSnapshotsCreated();\n+\n+    String hiveDumpLocation = tuple.dumpLocation + File.separator + ReplUtils.REPL_HIVE_BASE_DIR;\n+    // Ckpt should be set on bootstrapped db.\n+    replica.verifyIfCkptSet(replicatedDbName, hiveDumpLocation);\n+\n+    // Create a new table and do dump, for these also it should do a normal\n+    // distcp and copy from snapshot directory\n+\n+    tuple = primary.run(\"use \" + primaryDbName)\n+        .run(\"create external table table3 (id int)\")\n+        .run(\"insert into table table3 values (10)\")\n+        .run(\"create external table table4 as select id from table3\")\n+        .dump(primaryDbName);\n+\n+    // verify that the table info is written correctly for incremental\n+\n+    replica.load(replicatedDbName, primaryDbName)\n+        .run(\"use \" + replicatedDbName)\n+        .run(\"show tables like 'table3'\")\n+        .verifyResult(\"table3\")\n+        .run(\"select id from table3\")\n+        .verifyResult(\"10\")\n+        .run(\"select id from table4\")\n+        .verifyResult(\"10\");\n+\n+    // Check the new snapshots are created.\n+    validateDiffSnapshotsCreated();\n+\n+    // Try deleting a directory and add data to a already dumped and loaded table\n+    // for using snapshot diff\n+    tuple = primary.run(\"use \" + primaryDbName)\n+        .run(\"drop table table2\")\n+        .run(\"insert into table1 values (3),(4) \")\n+        .dump(primaryDbName);\n+\n+    // Check if the dropped table isn't there and the new data is available.\n+    replica.load(replicatedDbName, primaryDbName)\n+        .run(\"use \" + replicatedDbName)\n+        .run(\"select * from table1 order by id\")\n+        .verifyResults(new String[]{\"1\",\"2\",\"3\",\"4\"})\n+        .run(\"show tables like 'table2'\")\n+        .verifyFailure(new String[] {\"table2\" });\n+  }\n+\n+  @Test\n+  public void testReplicationWithCustomPathsAtDest() throws Throwable {\n+    testReplicationWithCustomPaths(true);\n+  }\n+\n+  @Test\n+  public void testReplicationWithCustomPathsAtSource() throws Throwable {\n+    testReplicationWithCustomPaths(false);\n+  }\n+\n+  public void testReplicationWithCustomPaths(boolean runAtDest) throws Throwable {\n+\n+    Path externalTableLocation = new Path(\n+        \"/\" + testName.getMethodName() + \"/\" + primaryDbName + \"/\" + \"a/\");\n+    DistributedFileSystem fs = primary.miniDFSCluster.getFileSystem();\n+    fs.mkdirs(externalTableLocation, new FsPermission(\"777\"));\n+\n+    List<String> withClause = Arrays.asList(\"'distcp.options.update'=''\",\n+        \"'\" + HiveConf.ConfVars.REPL_RUN_DATA_COPY_TASKS_ON_TARGET.varname\n+            + \"'='\"+runAtDest+\"'\");\n+\n+    primary.run(\"use \" + primaryDbName).run(\n+        \"create external table a (i int, j int) \"\n+            + \"row format delimited fields terminated by ',' \" + \"location '\"\n+            + externalTableLocation.toUri() + \"'\")\n+        .dump(primaryDbName, withClause);\n+\n+    replica.load(replicatedDbName, primaryDbName, withClause)\n+        .run(\"use \" + replicatedDbName).run(\"show tables like 'a'\")\n+        .verifyResults(Collections.singletonList(\"a\")).run(\"select * From a\")\n+        .verifyResults(Collections.emptyList());\n+\n+    // Check if initial snapshots are created.\n+    validateInitialSnapshotsCreated();\n+\n+    //externally add data to location\n+    try (FSDataOutputStream outputStream = fs\n+        .create(new Path(externalTableLocation, \"file1.txt\"))) {\n+      outputStream.write(\"1,2\\n\".getBytes());\n+      outputStream.write(\"13,21\\n\".getBytes());\n+    }\n+    WarehouseInstance.Tuple incrementalTuple =\n+        primary.run(\"create table b (i int)\").dump(primaryDbName, withClause);\n+\n+    replica.load(replicatedDbName, primaryDbName, withClause)\n+        .run(\"select i From a\").verifyResults(new String[] {\"1\", \"13\"})\n+        .run(\"select j from a\").verifyResults(new String[] {\"2\", \"21\"});\n+\n+    // Check if the diff snapshots are created after incremental.\n+    validateDiffSnapshotsCreated();\n+\n+    // alter table location to something new.\n+    externalTableLocation = new Path(\n+        \"/\" + testName.getMethodName() + \"/\" + primaryDbName\n+            + \"/new_location/a/\");\n+    primary.run(\"use \" + primaryDbName)\n+        .run(\"alter table a set location '\" + externalTableLocation + \"'\")\n+        .dump(primaryDbName, withClause);\n+\n+    replica.load(replicatedDbName, primaryDbName, withClause)\n+        .run(\"use \" + replicatedDbName).run(\"select i From a\")\n+        .verifyResults(Collections.emptyList());\n+  }\n+\n+  @Test\n+  public void testIncrementalReplication() throws Throwable {\n+    WarehouseInstance.Tuple tuple = primary.dump(primaryDbName);\n+    replica.load(replicatedDbName, primaryDbName);\n+    Path externalTableLocation =\n+        new Path(\"/\" + testName.getMethodName() + \"/t1/\");\n+    DistributedFileSystem fs = primary.miniDFSCluster.getFileSystem();\n+    fs.mkdirs(externalTableLocation, new FsPermission(\"777\"));\n+\n+    tuple = primary.run(\"use \" + primaryDbName)\n+        .run(\"create external table t1 (place string) partitioned by (country string) row format \"\n+            + \"delimited fields terminated by ',' location '\" + externalTableLocation.toString()\n+            + \"'\")\n+        .run(\"alter table t1 add partition(country='india')\")\n+        .run(\"alter table t1 add partition(country='us')\")\n+        .dump(primaryDbName);\n+\n+\n+    // Add new data externally, to a partition, but under the partition level top directory\n+    // Also, it is added after dumping the events so data should not be seen at target after REPL LOAD.\n+    Path partitionDir = new Path(externalTableLocation, \"country=india\");\n+    try (FSDataOutputStream outputStream = fs.create(new Path(partitionDir, \"file.txt\"))) {\n+      outputStream.write(\"pune\\n\".getBytes());\n+      outputStream.write(\"mumbai\\n\".getBytes());\n+    }\n+\n+    try (FSDataOutputStream outputStream = fs.create(new Path(partitionDir, \"file1.txt\"))) {\n+      outputStream.write(\"bangalore\\n\".getBytes());\n+    }\n+\n+    replica.load(replicatedDbName, primaryDbName)\n+        .run(\"use \" + replicatedDbName)\n+        .run(\"show tables like 't1'\")\n+        .verifyResult(\"t1\")\n+        .run(\"show partitions t1\")\n+        .verifyResults(new String[] { \"country=india\", \"country=us\" })\n+        .run(\"select place from t1 order by place\")\n+        .verifyResults(new String[] {})\n+        .verifyReplTargetProperty(replicatedDbName);\n+\n+    // Check if the initial snapshots are created.\n+    validateInitialSnapshotsCreated();\n+\n+    // The Data should be seen after next dump-and-load cycle.\n+    tuple = primary.run(\"use \" + primaryDbName)\n+        .dump(primaryDbName);\n+    replica.load(replicatedDbName, primaryDbName)\n+        .run(\"use \" + replicatedDbName)\n+        .run(\"show tables like 't1'\")\n+        .verifyResult(\"t1\")\n+        .run(\"show partitions t1\")\n+        .verifyResults(new String[] {\"country=india\", \"country=us\"})\n+        .run(\"select place from t1 order by place\")\n+        .verifyResults(new String[] {\"bangalore\", \"mumbai\", \"pune\"})\n+        .verifyReplTargetProperty(replicatedDbName);\n+\n+    // Check if diff snapshots are created.\n+    validateDiffSnapshotsCreated();\n+\n+    // Delete one of the file and update another one.\n+    fs.delete(new Path(partitionDir, \"file.txt\"), true);\n+    fs.delete(new Path(partitionDir, \"file1.txt\"), true);\n+    try (FSDataOutputStream outputStream = fs.create(new Path(partitionDir, \"file1.txt\"))) {\n+      outputStream.write(\"chennai\\n\".getBytes());\n+    }\n+\n+    // Repl load with zero events but external tables location info should present.\n+    tuple = primary.dump(primaryDbName);\n+\n+    replica.load(replicatedDbName, primaryDbName)\n+        .run(\"use \" + replicatedDbName)\n+        .run(\"show tables like 't1'\")\n+        .verifyResult(\"t1\")\n+        .run(\"show partitions t1\")\n+        .verifyResults(new String[] { \"country=india\", \"country=us\" })\n+        .run(\"select place from t1 order by place\")\n+        .verifyResults(new String[] { \"chennai\" })\n+        .verifyReplTargetProperty(replicatedDbName);\n+\n+    Hive hive = Hive.get(replica.getConf());\n+    Set<Partition> partitions =\n+        hive.getAllPartitionsOf(hive.getTable(replicatedDbName + \".t1\"));\n+    List<String> paths = partitions.stream().map(p -> p.getDataLocation().toUri().getPath())\n+        .collect(Collectors.toList());\n+\n+    tuple = primary\n+        .run(\"alter table t1 drop partition (country='india')\")\n+        .run(\"alter table t1 drop partition (country='us')\")\n+        .dump(primaryDbName);\n+\n+    replica.load(replicatedDbName, primaryDbName)\n+        .run(\"select * From t1\")\n+        .verifyResults(new String[] {})\n+        .verifyReplTargetProperty(replicatedDbName);\n+\n+    for (String path : paths) {\n+      assertTrue(replica.miniDFSCluster.getFileSystem().exists(new Path(path)));\n+    }\n+  }\n+\n+\n+  @Test\n+  public void testExternalTableWithPartitions() throws Throwable {\n+    Path externalTableLocation =\n+        new Path(\"/\" + testName.getMethodName() + \"/t2/\");\n+    DistributedFileSystem fs = primary.miniDFSCluster.getFileSystem();\n+    fs.mkdirs(externalTableLocation, new FsPermission(\"777\"));\n+\n+    List<String> withClause = ReplicationTestUtils.includeExternalTableClause(true);\n+\n+    WarehouseInstance.Tuple tuple = primary.run(\"use \" + primaryDbName)\n+        .run(\"create external table t2 (place string) partitioned by (country string) row format \"\n+            + \"delimited fields terminated by ',' location '\" + externalTableLocation.toString()\n+            + \"'\")\n+        .run(\"insert into t2 partition(country='india') values ('bangalore')\")\n+        .dump(primaryDbName, withClause);\n+\n+\n+    replica.load(replicatedDbName, primaryDbName, withClause)\n+        .run(\"use \" + replicatedDbName)\n+        .run(\"show tables like 't2'\")\n+        .verifyResults(new String[] { \"t2\" })\n+        .run(\"select place from t2\")\n+        .verifyResults(new String[] { \"bangalore\" })\n+        .verifyReplTargetProperty(replicatedDbName);\n+\n+\n+    // add new  data externally, to a partition, but under the table level top directory\n+    Path partitionDir = new Path(externalTableLocation, \"country=india\");\n+    try (FSDataOutputStream outputStream = fs.create(new Path(partitionDir, \"file.txt\"))) {\n+      outputStream.write(\"pune\\n\".getBytes());\n+      outputStream.write(\"mumbai\\n\".getBytes());\n+    }\n+\n+    tuple = primary.run(\"use \" + primaryDbName)\n+        .run(\"insert into t2 partition(country='australia') values ('sydney')\")\n+        .dump(primaryDbName, withClause);\n+\n+\n+    replica.load(replicatedDbName, primaryDbName, withClause)\n+        .run(\"use \" + replicatedDbName)\n+        .run(\"select distinct(country) from t2\")\n+        .verifyResults(new String[] { \"india\", \"australia\" })\n+        .run(\"select place from t2 where country='india'\")\n+        .verifyResults(new String[] { \"bangalore\", \"pune\", \"mumbai\" })\n+        .run(\"select place from t2 where country='australia'\")\n+        .verifyResults(new String[] { \"sydney\" })\n+        .run(\"show partitions t2\")\n+        .verifyResults(new String[] {\"country=australia\", \"country=india\"})\n+        .verifyReplTargetProperty(replicatedDbName);\n+\n+    String tmpLocation2 = \"/tmp1/\" + System.nanoTime() + \"_2\";\n+    primary.miniDFSCluster.getFileSystem().mkdirs(new Path(tmpLocation2), new FsPermission(\"777\"));\n+\n+    // Try alter table location and then dump.\n+    primary.run(\"use \" + primaryDbName)\n+        .run(\"insert into table t2 partition(country='france') values ('lyon')\")\n+        .run(\"alter table t2 set location '\" + tmpLocation2 + \"'\")\n+        .dump(primaryDbName, withClause);\n+\n+    replica.load(replicatedDbName, primaryDbName, withClause)\n+        .run(\"show partitions t2\").verifyResults(\n+        new String[] {\"country=australia\", \"country=india\", \"country=france\"});\n+\n+    // Test alter partition location.\n+    String tmpLocation3 = \"/tmp1/\" + System.nanoTime() + \"_3\";\n+    primary.miniDFSCluster.getFileSystem()\n+        .mkdirs(new Path(tmpLocation2), new FsPermission(\"777\"));\n+    primary.run(\"use \" + primaryDbName).run(\n+        \"alter table t2 partition (country='australia') set location '\"\n+            + tmpLocation3 + \"'\").run(\n+        \"insert into table t2 partition(country='australia') values \"\n+            + \"('sydney')\").dump(primaryDbName, withClause);\n+\n+    replica.load(replicatedDbName, primaryDbName, withClause)\n+        .run(\"select place from t2 where country='australia'\")\n+        .verifyResult(\"sydney\");\n+  }\n+\n+  @Test\n+  public void testSnapshotCleanupsOnDatabaseLocationChange() throws Throwable {\n+    Path externalDatabaseLocation =\n+        new Path(\"/\" + testName.getMethodName() + \"/externalDatabase/\");\n+    DistributedFileSystem fs = primary.miniDFSCluster.getFileSystem();\n+    fs.mkdirs(externalDatabaseLocation, new FsPermission(\"777\"));\n+\n+    Path externalDatabaseLocationAlter =\n+        new Path(\"/\" + testName.getMethodName() + \"/externalDatabaseAlter/\");\n+    fs.mkdirs(externalDatabaseLocationAlter, new FsPermission(\"777\"));\n+\n+    Path externalDatabaseLocationDest = new Path(REPLICA_EXTERNAL_BASE,\n+        testName.getMethodName() + \"/externalDatabase/\");\n+\n+    Path externalDatabaseLocationAlterDest = new Path(REPLICA_EXTERNAL_BASE,\n+        testName.getMethodName() + \"/externalDatabaseAlter/\");\n+\n+    List<String> withClause =\n+        ReplicationTestUtils.includeExternalTableClause(true);\n+\n+    // Create a normal and partitioned table in the database location.\n+\n+    WarehouseInstance.Tuple tuple = primary.run(\"use \" + primaryDbName)\n+        .run(\"alter database \" + primaryDbName + \" set location '\" + externalDatabaseLocation + \"'\")\n+        .run(\"create external table emp1 (id int)\")\n+        .run(\"insert into emp1 values(1),(2)\")\n+        .run(\"create external table t2 (place string) partitioned by (country \"\n+           + \"string) row format delimited fields terminated by ','\")\n+        .run(\"insert into t2 partition(country='india') values ('bangalore')\")\n+        .dump(primaryDbName, withClause);\n+\n+    // Do a load, post that one snapshot should have been created in the source\n+    // for each table\n+\n+    replica.load(replicatedDbName, primaryDbName, withClause)\n+        .run(\"use \" + replicatedDbName)\n+        .run(\"show tables like 'emp1'\")\n+        .verifyResults(new String[] {\"emp1\"})\n+        .run(\"select id from emp1\")\n+        .verifyResults(new String[] {\"1\", \"2\"})\n+        .run(\"select place from t2\")\n+        .verifyResults(new String[] {\"bangalore\"})\n+        .verifyReplTargetProperty(replicatedDbName);\n+\n+    // Check the directory is snapshotable.\n+    assertTrue(fs.getFileStatus(externalDatabaseLocation).isSnapshotEnabled());\n+\n+    // Check if the snapshot exists at source.\n+    assertNotNull(fs.getFileStatus(\n+        new Path(externalDatabaseLocation, \".snapshot/s0initial\")));\n+\n+    // Check if the destination is snapshottable.\n+    assertTrue(\n+        fs.getFileStatus(externalDatabaseLocationDest).isSnapshotEnabled());\n+\n+    // Check if snapshot exist at destination.\n+    assertNotNull(fs.getFileStatus(\n+        new Path(externalDatabaseLocationDest, \".snapshot/s0old\")));\n+\n+    // Alter database location and create another table inside it.\n+\n+    tuple = primary.run(\"use \" + primaryDbName).run(\n+        \"alter database \" + primaryDbName + \" set location '\"\n+            + externalDatabaseLocationAlter + \"'\")\n+        .run(\"create external table empnew (id int)\")\n+        .run(\"insert into empnew values (3),(4)\").run(\"drop table emp\")\n+        .run(\"insert into t2 partition(country='france') \" + \"values('paris')\")\n+        .dump(primaryDbName, withClause);\n+\n+    // Do a load and see if everything is correct.\n+\n+    replica.load(replicatedDbName, primaryDbName, withClause)\n+        .run(\"use \" + replicatedDbName)\n+        .run(\"show tables like 'emp'\")\n+        .verifyFailure(new String[] {\"emp\"})\n+        .run(\"show tables like 'empnew'\")\n+        .verifyResults(new String[] {\"empnew\"})\n+        .run(\"select id from empnew\")\n+        .verifyResults(new String[] {\"3\", \"4\"})\n+        .run(\"select place from t2 where country='france'\")\n+        .verifyResults(new String[] {\"paris\"})\n+        .run(\"select place from t2 where country='india'\")\n+        .verifyResults(new String[] {\"bangalore\"})\n+        .verifyReplTargetProperty(replicatedDbName);\n+\n+    // Check if the new database location is snapshottable.\n+    assertTrue(\n+        fs.getFileStatus(externalDatabaseLocationAlter).isSnapshotEnabled());\n+\n+    // Check if the snapshot exists at source.\n+    assertNotNull(fs.getFileStatus(\n+        new Path(externalDatabaseLocationAlter, \".snapshot/s0initial\")));\n+\n+    // Check if the old snapshot got deleted at source.\n+    LambdaTestUtils.intercept(FileNotFoundException.class, () -> fs\n+        .getFileStatus(\n+            new Path(externalDatabaseLocation, \".snapshot/s0initial\")));\n+\n+    // Check if the new destination database location is snapshottable.\n+    assertTrue(fs.getFileStatus(externalDatabaseLocationAlterDest)\n+        .isSnapshotEnabled());\n+\n+    // Check if snapshot exist at destination.\n+    assertNotNull(fs.getFileStatus(\n+        new Path(externalDatabaseLocationAlterDest, \".snapshot/s0old\")));\n+\n+    //Check if the destination old snapshot is deleted.\n+    LambdaTestUtils.intercept(FileNotFoundException.class, () -> fs\n+        .getFileStatus(\n+            new Path(externalDatabaseLocationDest, \".snapshot/s0old\")));\n+\n+  }\n+\n+  @Test\n+  public void testFallbackToNormalDistCp() throws Throwable {\n+    DistributedFileSystem fs = primary.miniDFSCluster.getFileSystem();\n+    List<String> withClause = ReplicationTestUtils.includeExternalTableClause(true);\n+\n+    // Create a table which couldn't create snapshot during dump.\n+    Path externalTableLocationBootStrap =\n+        new Path(\"/\" + testName.getMethodName() + \"source1/tablesource/\");\n+\n+    fs.mkdirs(externalTableLocationBootStrap, new FsPermission(\"777\"));\n+\n+    // Allow snapshot on the parent of table directory, the creation of\n+    // snapshot shall fail for the table directory during dump.\n+    fs.allowSnapshot(new Path(\"/\" + testName.getMethodName() + \"source1/\"));\n+\n+    // Create a table which fails to create snapshot for one partition.\n+    Path externalTableLocationPartTable = new Path(\n+        \"/\" + testName.getMethodName() + \"source2\" + \"/tablesourcepartition\"\n+            + \"/\");\n+    fs.mkdirs(externalTableLocationPartTable, new FsPermission(\"777\"));\n+\n+    String tmpLocation2 = \"/tmp3/partition/partition\" + System.nanoTime() +\n+        \"_1\";\n+   fs.mkdirs(new Path(tmpLocation2), new FsPermission(\"777\"));\n+\n+   // Allow snapshot on the parent of the partition directory. to prevent\n+    // creation of snapshot for partition directory\n+   fs.allowSnapshot(new Path(tmpLocation2).getParent());\n+\n+    // Create a table which can not create snapshot on the destination.\n+    Path externalTableLocationDest =\n+        new Path(\"/\" + testName.getMethodName() + \"dest1/tabledest/\");\n+    fs.mkdirs(externalTableLocationDest, new FsPermission(\"777\"));\n+\n+    Path externalTableLocationDestInDest =\n+        new Path(REPLICA_EXTERNAL_BASE, externalTableLocationDest);\n+\n+    // Allow snapshot on the parent of destination, in this case creation of\n+    // snapshot shall fail post copy, The fallback will happen during next\n+    // copy, since the destination snapshot isn't present.\n+    fs.allowSnapshot(externalTableLocationDestInDest.getParent());\n+\n+    WarehouseInstance.Tuple tuple = primary.run(\"use \" + primaryDbName)\n+        .run(\"create external table tablesource (place string) row format \"\n+            + \"delimited fields terminated by ',' location '\" + externalTableLocationBootStrap.toString()\n+            + \"'\")\n+        .run(\"insert into tablesource values ('bangalore')\")\n+        .run(\"create external table tablesourcepartition (place string) partitioned by \"\n+            + \"(country string) row format \"\n+            + \"delimited fields terminated by ',' location '\" + externalTableLocationPartTable.toString()\n+            + \"'\")\n+        .run(\"insert into tablesourcepartition partition(country='india') values \"\n+            + \"('lucknow')\")\n+        .run(\"ALTER TABLE tablesourcepartition ADD PARTITION (country='france') LOCATION '\" + tmpLocation2 + \"'\")\n+        .run(\"insert into tablesourcepartition partition(country='france') \"\n+            + \"values ('paris')\")\n+        .run(\"create external table tabledest (place string) \"\n+            + \"partitioned by (country string) row format \"\n+            + \"delimited fields terminated by ',' location '\" + externalTableLocationDest.toString()\n+            + \"'\")\n+        .run(\"insert into tabledest partition(country='india') values\"\n+            + \" ('kanpur')\")\n+        .dump(primaryDbName, withClause);\n+\n+    // Load and check if the data is there, we should have fallback to normal\n+    // mode\n+\n+    replica.load(replicatedDbName, primaryDbName, withClause)\n+        .run(\"use \" + replicatedDbName)\n+        .run(\"select place from tablesource\")\n+        .verifyResults(new String[] {\"bangalore\"})\n+        .run(\"select place from tablesourcepartition where country='france'\")\n+        .verifyResults(new String[] {\"paris\"})\n+        .run(\"select place from tabledest\")\n+        .verifyResults(new String[] {\"kanpur\"})\n+        .verifyReplTargetProperty(replicatedDbName);\n+\n+    // Add some more data to tabledest and try incremental.\n+\n+    primary.run(\"use \" + primaryDbName)\n+        .run(\"insert into tabledest partition(country='india') values\"\n+            + \" ('lucknow')\")\n+        .run(\"insert into tabledest partition(country='usa') values\"\n+            + \" ('New York')\")\n+        .dump(primaryDbName, withClause);\n+\n+    replica.load(replicatedDbName, primaryDbName, withClause)\n+        .run(\"use \" + replicatedDbName)\n+        .run(\"select place from tabledest where country='usa'\")\n+        .verifyResults(new String[] {\"New York\"})\n+        .run(\"select place from tabledest where country='india'\")\n+        .verifyResults(new String[]{\"kanpur\",\"lucknow\"});\n+  }\n+\n+  @Test\n+  public void testCustomPathTableSnapshots() throws Throwable {\n+    Path externalTableLocation1 =\n+        new Path(\"/\" + testName.getMethodName() + \"/t1/\");\n+    DistributedFileSystem fs = primary.miniDFSCluster.getFileSystem();\n+    fs.mkdirs(externalTableLocation1, new FsPermission(\"777\"));\n+\n+    Path externalTableLocation2 =\n+        new Path(\"/\" + testName.getMethodName() + \"/t2/\");\n+    fs.mkdirs(externalTableLocation1, new FsPermission(\"777\"));\n+\n+    Path externalTablePart1 =\n+        new Path(\"/\" + testName.getMethodName() + \"/part1/\");\n+    fs.mkdirs(externalTableLocation1, new FsPermission(\"777\"));\n+\n+    Path externalTablePart2 =\n+        new Path(\"/\" + testName.getMethodName() + \"/part2/\");\n+    fs.mkdirs(externalTableLocation1, new FsPermission(\"777\"));\n+\n+\n+    List<String> withClause = ReplicationTestUtils.includeExternalTableClause(true);\n+    withClause.add(\n+        \"'hive.repl.externatable.snapshot.paths'='\" + externalTableLocation2\n+            .makeQualified(fs.getUri(), fs.getWorkingDirectory()).toString()\n+            + \",\" + externalTablePart1\n+            .makeQualified(fs.getUri(), fs.getWorkingDirectory()).toString()\n+            + \"'\");\n+\n+    WarehouseInstance.Tuple tuple = primary.run(\"use \" + primaryDbName)\n+        .run(\"create external table t1 (place string) partitioned by (country\"\n+            + \" string) row format \"\n+            + \"delimited fields terminated by ',' location '\" + externalTableLocation1.toString()\n+            + \"'\")\n+        .run(\"insert into t1 partition(country='india') values ('bangalore')\")\n+        .run(\"create external table t2 (place string) partitioned by (country\"\n+            + \" string) row format \"\n+            + \"delimited fields terminated by ',' location '\" + externalTableLocation2.toString()\n+            + \"'\")\n+        .run(\"insert into t1 partition(country='australia') values \"\n+            + \"('sydney')\")\n+        .run(\"insert into t2 partition(country='nepal') values \"\n+            + \"('kathmandu')\")\n+        .run(\"alter table t1 add partition (country='france') location '\"+externalTablePart1.toString() + \"'\")\n+        .run(\"insert into t1 partition(country='france') values ('paris')\")\n+        .run(\"alter table t2 add partition (country='china') location '\"+externalTablePart2.toString() + \"'\")\n+        .run(\"insert into t2 partition(country='china') values ('beejing')\")\n+        .dump(primaryDbName, withClause);\n+\n+    replica.load(replicatedDbName, primaryDbName, withClause)\n+        .run(\"use \" + replicatedDbName)\n+        .run(\"show tables like 't1'\")\n+        .verifyResults(new String[] {\"t1\"})\n+        .run(\"show tables like 't2'\")\n+        .verifyResults(new String[] {\"t2\"})\n+        .run(\"select place from t1 where country='india'\")\n+        .verifyResults(new String[] {\"bangalore\"})\n+        .run(\"select place from t1 where country='france'\")\n+        .verifyResults(new String[] {\"paris\"})\n+        .run(\"select place from t2 where country='nepal'\")\n+        .verifyResults(new String[] {\"kathmandu\"})\n+        .run(\"select place from t2 where country='china'\")\n+        .verifyResults(new String[] {\"beejing\"})\n+        .verifyReplTargetProperty(replicatedDbName);\n+\n+\n+    // Check if the t2 directory is snapshotoble and the snapshot is there.\n+    assertTrue(fs.getFileStatus(externalTableLocation2).isSnapshotEnabled());\n+    assertNotNull(fs.getFileStatus(new Path(externalTableLocation2,\n+        \".snapshot/s0initial\")));\n+\n+    // Check if the partition directory is snapshottable and has snapshots\n+    assertTrue(fs.getFileStatus(externalTablePart1).isSnapshotEnabled());\n+    assertNotNull(\n+        fs.getFileStatus(new Path(externalTablePart1, \".snapshot/s0initial\")));\n+\n+    // Check the table not specified as configuration is not snapshottable\n+    assertFalse(fs.getFileStatus(externalTableLocation1).isSnapshotEnabled());\n+\n+    // Check the partition not specified as configuration is not snapshottable\n+    assertFalse(fs.getFileStatus(externalTablePart2).isSnapshotEnabled());\n+  }\n+\n+  @Test\n+  public void testSnapshotMetrics() throws Throwable {\n+    conf.set(Constants.SCHEDULED_QUERY_SCHEDULENAME,\"metrics_test\");\n+    List<String> withClause =\n+        ReplicationTestUtils.includeExternalTableClause(true);\n+    MetricCollector collector = MetricCollector.getInstance();\n+    Path externalDatabaseLocation =\n+        new Path(\"/\" + testName.getMethodName() + \"/externalDatabase/\");\n+    DistributedFileSystem fs = primary.miniDFSCluster.getFileSystem();\n+    fs.mkdirs(externalDatabaseLocation, new FsPermission(\"777\"));\n+\n+    Path externalTableLocation1 =\n+        new Path(\"/\" + testName.getMethodName() + \"/t1/\");\n+     fs = primary.miniDFSCluster.getFileSystem();\n+    fs.mkdirs(externalTableLocation1, new FsPermission(\"777\"));\n+    withClause.add(\n+        \"'hive.repl.externatable.snapshot.paths'='\" + externalTableLocation1\n+            .makeQualified(fs.getUri(), fs.getWorkingDirectory()).toString()\n+            + \"'\");\n+\n+    // Allow snapshot on the parent of the database location and the table,\n+    // so as to make the creation of snapshots fail on the database directory.\n+    fs.allowSnapshot(externalDatabaseLocation.getParent());\n+\n+    WarehouseInstance.Tuple tuple = primary.run(\"use \" + primaryDbName)\n+        .run(\"alter database \" + primaryDbName + \" set location '\" + externalDatabaseLocation + \"'\")\n+        .run(\"create external table emp1 (id int)\")\n+        .run(\"insert into emp1 values(1),(2)\")\n+        .run(\"create external table exttab (place string) row format delimited fields terminated by ','\"\n+            + \" location '\" + externalTableLocation1.toString() +\"'\")\n+        .run(\"insert into exttab values('lucknow')\")\n+        .dump(primaryDbName,withClause);\n+\n+    // Check in the metrics if the entry in there.\n+    Iterator<ReplicationMetric> itr = collector.getMetrics().iterator();\n+    while (itr.hasNext()) {\n+      ReplicationMetric elem = itr.next();\n+      assertEquals(BOOTSTRAP, elem.getMetadata().getReplicationType());\n+      List<Stage> stages = elem.getProgress().getStages();\n+      for (Stage stage : stages) {\n+        HashSet<String> paths = stage.getFailedSnapshotsPaths();\n+        assertEquals(2, paths.size());\n+        assertTrue(paths.contains(\n+            FileUtils.makeQualified(externalTableLocation1, conf).toString()));\n+        assertTrue(paths.contains(\n+            FileUtils.makeQualified(externalDatabaseLocation, conf)\n+                .toString()));\n+      }\n+    }\n+\n+    // Load and check if the data and table are there.\n+    replica.load(replicatedDbName, primaryDbName, withClause)\n+        .run(\"use \" + replicatedDbName)\n+        .run(\"show tables like 'emp1'\")\n+        .verifyResults(new String[] {\"emp1\"})\n+        .run(\"select id from emp1\")\n+        .verifyResults(new String[] {\"1\", \"2\"})\n+        .run(\"show tables like 'exttab'\")\n+        .verifyResults(new String[]{\"exttab\"})\n+        .run(\"select place from exttab\")\n+        .verifyResults(new String[] {\"lucknow\"})\n+        .verifyReplTargetProperty(replicatedDbName);\n+\n+    // Add some data and try incremental dump.\n+\n+    tuple = primary.run(\"use \" + primaryDbName)\n+        .run(\"insert into emp1 values(3),(4)\")\n+        .run(\"insert into exttab values('agra')\")\n+        .dump(primaryDbName, withClause);\n+\n+     itr = collector.getMetrics().iterator();\n+    while (itr.hasNext()) {\n+      ReplicationMetric elem = itr.next();\n+      assertEquals(INCREMENTAL, elem.getMetadata().getReplicationType());\n+      List<Stage> stages = elem.getProgress().getStages();\n+      for (Stage stage : stages) {\n+        HashSet<String> paths = stage.getFailedSnapshotsPaths();\n+        assertEquals(2, paths.size());\n+        assertTrue(paths.contains(\n+            FileUtils.makeQualified(externalTableLocation1, conf).toString()));\n+        assertTrue(paths.contains(\n+            FileUtils.makeQualified(externalDatabaseLocation, conf)\n+                .toString()));\n+      }\n+    }\n+\n+    replica.load(replicatedDbName, primaryDbName, withClause)\n+        .run(\"use \" + replicatedDbName)\n+        .run(\"show tables like 'emp1'\")\n+        .verifyResults(new String[] {\"emp1\"})\n+        .run(\"select id from emp1\")\n+        .verifyResults(new String[] {\"1\", \"2\", \"3\", \"4\"})\n+        .run(\"show tables like 'exttab'\")\n+        .verifyResults(new String[]{\"exttab\"})\n+        .run(\"select place from exttab\")\n+        .verifyResults(new String[] {\"lucknow\", \"agra\"})\n+        .verifyReplTargetProperty(replicatedDbName);\n+\n+    // Disallow snapshots on the paths to see, if the metrics get cleared.\n+    fs.disallowSnapshot(externalDatabaseLocation.getParent());\n+\n+    primary.run(\"use \" + primaryDbName)\n+        .run(\"insert into emp1 values(5),(6)\")\n+        .run(\"insert into exttab values('kanpur')\")\n+        .dump(primaryDbName, withClause);\n+\n+    itr = collector.getMetrics().iterator();\n+    while (itr.hasNext()) {\n+      ReplicationMetric elem = itr.next();\n+      assertEquals(INCREMENTAL, elem.getMetadata().getReplicationType());\n+      List<Stage> stages = elem.getProgress().getStages();\n+      for (Stage stage : stages) {\n+        HashSet<String> paths = stage.getFailedSnapshotsPaths();\n+        assertEquals(0, paths.size());\n+      }\n+    }\n+  }\n+\n+  // Verifies if the diff snapshots are created for source and\n+  // target database.\n+  private void validateDiffSnapshotsCreated() throws Exception {\n+    String externalWhLocation =\n+        primary.getDatabase(primaryDbName).getLocationUri();\n+    Path externalWhLocationPath = new Path(externalWhLocation);\n+    DistributedFileSystem dfs =\n+        (DistributedFileSystem) externalWhLocationPath.getFileSystem(conf);\n+    assertNotNull(\n+        dfs.getFileStatus(new Path(externalWhLocationPath, \".snapshot/s0old\")));\n+    assertNotNull(dfs.getFileStatus(\n+        new Path(externalWhLocationPath, \".snapshot/s0initial\")));\n+  }\n+\n+  // Verifies if the initial rounds are snapshots are created for source and\n+  // target database.\n+  private void validateInitialSnapshotsCreated() throws Exception {\n+    String externalWhLocation =\n+        primary.getDatabase(primaryDbName).getLocationUri();\n+    Path externalWhLocationPath = new Path(externalWhLocation);\n+    DistributedFileSystem dfs =\n+        (DistributedFileSystem) externalWhLocationPath.getFileSystem(conf);\n+\n+    // Check whether the source warehouse location got snapshottable\n+    assertTrue(\n+        \"Snapshot not enabled for the source external warehouse location\",\n+        dfs.getFileStatus(externalWhLocationPath).isSnapshotEnabled());\n+    // Check whether the initial snapshot got created in the source db location.\n+    assertNotNull(dfs.getFileStatus(\n+        new Path(externalWhLocationPath, \".snapshot/s0initial\")));\n+\n+    // Verify Snapshots are created in target.\n+    Path externalWhLocationPathTarget = new Path(REPLICA_EXTERNAL_BASE,\n+        externalWhLocationPath.toUri().getPath().replaceFirst(\"/\", \"\"));\n+    DistributedFileSystem dfsTarget =\n+        (DistributedFileSystem) externalWhLocationPathTarget\n+            .getFileSystem(conf);\n+    assertTrue(\n+        \"Snapshot not enabled for the target external warehouse location\",\n+        dfsTarget.getFileStatus(externalWhLocationPathTarget).isSnapshotEnabled());\n+\n+    // Check whether the snapshot got created in the source db location.\n+    assertNotNull(dfsTarget.getFileStatus(\n+        new Path(externalWhLocationPathTarget, \".snapshot\" + \"/s0old\")));\n+  }\n+}"
  },
  {
    "sha": "3a51a6aee1b56d83c57c625cc61f473ed60efb87",
    "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/repl/AckTask.java",
    "status": "modified",
    "additions": 4,
    "deletions": 0,
    "changes": 4,
    "blob_url": "https://github.com/apache/hive/blob/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/AckTask.java",
    "raw_url": "https://github.com/apache/hive/raw/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/AckTask.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/AckTask.java?ref=cccf1375fdcc8ae3582155870b974fc205b10eb3",
    "patch": "@@ -18,7 +18,10 @@\n \n package org.apache.hadoop.hive.ql.exec.repl;\n \n+import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.ql.ErrorMsg;\n import org.apache.hadoop.hive.ql.exec.Task;\n import org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils;\n@@ -47,6 +50,7 @@ public int execute() {\n       }\n       Path ackPath = work.getAckFilePath();\n       Utils.create(ackPath, conf);\n+//      createSnapshot();\n       LOG.info(\"Created ack file : {} \", ackPath);\n     } catch (Exception e) {\n       setException(e);"
  },
  {
    "sha": "93363c70000a9fdf19d81207e63c8a201a5b1e49",
    "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/repl/DirCopyTask.java",
    "status": "modified",
    "additions": 101,
    "deletions": 8,
    "changes": 109,
    "blob_url": "https://github.com/apache/hive/blob/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/DirCopyTask.java",
    "raw_url": "https://github.com/apache/hive/raw/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/DirCopyTask.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/DirCopyTask.java?ref=cccf1375fdcc8ae3582155870b974fc205b10eb3",
    "patch": "@@ -21,11 +21,14 @@\n import org.apache.hadoop.fs.permission.AclEntry;\n import org.apache.hadoop.fs.permission.AclStatus;\n import org.apache.hadoop.fs.permission.AclUtil;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.protocol.HdfsConstants;\n import org.apache.hadoop.hive.common.FileUtils;\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.ql.ErrorMsg;\n import org.apache.hadoop.hive.ql.exec.Task;\n import org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils;\n+import org.apache.hadoop.hive.ql.exec.repl.util.SnapshotUtils;\n import org.apache.hadoop.hive.ql.exec.util.Retryable;\n import org.apache.hadoop.hive.ql.parse.repl.CopyUtils;\n import org.apache.hadoop.hive.ql.plan.api.StageType;\n@@ -162,14 +165,21 @@ public int execute() {\n           // do we create a new conf and only here provide this additional option so that we get away from\n           // differences of data in two location for the same directories ?\n           // basically add distcp.options.delete to hiveconf new object ?\n-          FileUtils.distCp(\n-            sourcePath.getFileSystem(conf), // source file system\n-            Collections.singletonList(sourcePath),  // list of source paths\n-            targetPath,\n-            false,\n-            proxyUser,\n-            conf,\n-            ShimLoader.getHadoopShims());\n+          if (!getWork().getCopyMode()\n+              .equals(SnapshotUtils.SnapshotCopyMode.FALLBACK_COPY)) {\n+            LOG.info(\"Using Snapshot mode of copy for source: {} and target:\"\n+                + \" {}\", sourcePath, targetPath);\n+            copyUsingDistCpSnapshots(sourcePath, targetPath, proxyUser);\n+            // Use distcp with snapshots for copy.\n+          } else {\n+            LOG.info(\"Using Normal copy for source: {} and target: {}\",\n+                sourcePath, targetPath);\n+            FileUtils\n+                .distCp(sourcePath.getFileSystem(conf), // source file system\n+                    Collections.singletonList(sourcePath), // list of source paths\n+                    targetPath, false, proxyUser, conf,\n+                    ShimLoader.getHadoopShims());\n+          }\n           return 0;\n         } finally {\n           if (proxyUser != null) {\n@@ -203,4 +213,87 @@ public String getName() {\n   public boolean canExecuteInParallel() {\n     return true;\n   }\n+\n+  void copyUsingDistCpSnapshots(Path sourcePath, Path targetPath,\n+      UserGroupInformation proxyUser) throws IOException {\n+\n+    DistributedFileSystem sourceFs = SnapshotUtils.getDFS(sourcePath, conf);\n+    DistributedFileSystem targetFs = SnapshotUtils.getDFS(targetPath, conf);\n+    if (sourceFs == null || targetFs == null) {\n+      LOG.error(\"Source and Destination filesystem are not \"\n+              + \"DistributedFileSystem, using normal copy instead of snapshot \"\n+              + \"copy, Source Path {}, Target Path {}, Source fs is {}, and \"\n+              + \"Target fs {}\", sourcePath, targetPath,\n+          sourcePath.getFileSystem(conf).getClass(),\n+          targetPath.getFileSystem(conf).getClass());\n+      FileUtils.distCp(sourcePath.getFileSystem(conf), // source file system\n+          Collections.singletonList(sourcePath), // list of source paths\n+          targetPath, false, proxyUser, conf, ShimLoader.getHadoopShims());\n+      // Since source/dest aren't DFS, no point trying to create snapshot at\n+      // target, return from here.\n+      return;\n+    }\n+    String prefix = conf.getVar(\n+        HiveConf.ConfVars.REPL_SNAPSHOT_PREFIX_FOR_EXTERNAL_TABLE_COPY);\n+    if (getWork().getCopyMode()\n+        .equals(SnapshotUtils.SnapshotCopyMode.DIFF_COPY)) {\n+      LOG.info(\"Using snapshot diff copy for source: {} and target: {}\",\n+          sourcePath, targetPath);\n+      boolean result = FileUtils\n+          .distCpWithSnapshot(sourceFs, prefix + \"old\", prefix + \"initial\",\n+              Collections.singletonList(sourcePath), targetPath, proxyUser,\n+              conf, ShimLoader.getHadoopShims());\n+      if (!result) {\n+        LOG.error(\"Can not copy using snapshot diff for source: {} and \"\n+                + \"target: {}. Falling back to normal copy.\", sourcePath,\n+            targetPath);\n+        FileUtils.distCp(sourcePath.getFileSystem(conf), // source file system\n+            Collections.singletonList(sourcePath), // list of source paths\n+            targetPath, false, proxyUser, conf, ShimLoader.getHadoopShims());\n+      }\n+    } else if (getWork().getCopyMode()\n+        .equals(SnapshotUtils.SnapshotCopyMode.INITIAL_COPY)) {\n+      LOG.info(\"Using snapshot initial copy for source: {} and target: {}\",\n+          sourcePath, targetPath);\n+      // Try allowSnapshot at target first, if success or already allowed\n+      // will use snapshots to copy else fallback.\n+      boolean isTargetSnapshottable =\n+          SnapshotUtils.allowSnapshot(targetFs, targetPath);\n+      if (!isTargetSnapshottable) {\n+        // We can not allow creating snapshot at target, so no point moving\n+        // to snapshot mode of copy, fallback to normal copy.\n+        LOG.error(\"Can not copy from initial snapshot directory for source: {} \"\n+            + \"and target: {}. Since target is not snapshottable. Falling \"\n+            + \"back to normal copy.\", sourcePath, targetPath);\n+        FileUtils.distCp(sourcePath.getFileSystem(conf), // source file system\n+            Collections.singletonList(sourcePath), // list of source paths\n+            targetPath, false, proxyUser, conf, ShimLoader.getHadoopShims());\n+        // Returning to avoid creating snapshots at target.\n+        return;\n+      } else {\n+        // Get the path relative to the initial snapshot for copy.\n+        Path snapRelPath = new Path(sourcePath,\n+            HdfsConstants.DOT_SNAPSHOT_DIR + \"/\" + prefix + \"initial\");\n+        boolean result = FileUtils.distCp(sourcePath.getFileSystem(conf), //\n+            // source file system\n+            Collections.singletonList(snapRelPath), // source path relative to\n+            // snapshot\n+            targetPath, false, proxyUser, conf, ShimLoader.getHadoopShims());\n+        if (!result) {\n+          LOG.error(\n+              \"Can not copy from initial snapshot directory for source: {} \"\n+                  + \"and target: {}. Falling back to normal copy.\", snapRelPath,\n+              targetPath);\n+          FileUtils.distCp(sourcePath.getFileSystem(conf), // source file system\n+              Collections.singletonList(sourcePath), // list of source paths\n+              targetPath, false, proxyUser, conf, ShimLoader.getHadoopShims());\n+        }\n+      }\n+    }\n+    // Create snapshot at target Filesystem, ignore exceptions if any,\n+    // since copy is success, if creation fails, the next iteration shall\n+    // handle.\n+    SnapshotUtils.deleteSnapshotSafe(targetFs, targetPath, prefix + \"old\");\n+    SnapshotUtils.createSnapshotSafe(targetFs, targetPath, prefix + \"old\");\n+  }\n }"
  },
  {
    "sha": "caa1b6998e04f7f5fa5ee8421a6965e60a023d91",
    "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/repl/DirCopyWork.java",
    "status": "modified",
    "additions": 22,
    "deletions": 1,
    "changes": 23,
    "blob_url": "https://github.com/apache/hive/blob/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/DirCopyWork.java",
    "raw_url": "https://github.com/apache/hive/raw/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/DirCopyWork.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/DirCopyWork.java?ref=cccf1375fdcc8ae3582155870b974fc205b10eb3",
    "patch": "@@ -18,11 +18,14 @@\n package org.apache.hadoop.hive.ql.exec.repl;\n \n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.exec.repl.util.SnapshotUtils;\n import org.apache.hadoop.hive.ql.exec.repl.util.StringConvertibleObject;\n import org.apache.hadoop.hive.ql.parse.repl.metric.ReplicationMetricCollector;\n import org.apache.hadoop.hive.ql.plan.Explain;\n import java.io.Serializable;\n \n+import static org.apache.hadoop.hive.ql.exec.repl.util.SnapshotUtils.SnapshotCopyMode.FALLBACK_COPY;\n+\n /**\n  * DirCopyWork, mainly to be used to copy External table data.\n  */\n@@ -36,6 +39,7 @@\n   private Path fullyQualifiedTargetPath;\n   private String dumpDirectory;\n   private transient ReplicationMetricCollector metricCollector;\n+  private SnapshotUtils.SnapshotCopyMode copyMode= FALLBACK_COPY;\n \n   public DirCopyWork(ReplicationMetricCollector metricCollector, String dumpDirectory) {\n     this.metricCollector = metricCollector;\n@@ -46,11 +50,19 @@ public DirCopyWork(Path fullyQualifiedSourcePath, Path fullyQualifiedTargetPath)\n     this.fullyQualifiedSourcePath = fullyQualifiedSourcePath;\n     this.fullyQualifiedTargetPath = fullyQualifiedTargetPath;\n   }\n+\n+  public DirCopyWork(Path fullyQualifiedSourcePath,\n+      Path fullyQualifiedTargetPath, SnapshotUtils.SnapshotCopyMode copyMode) {\n+    this(fullyQualifiedSourcePath, fullyQualifiedTargetPath);\n+    this.copyMode = copyMode;\n+  }\n+\n   @Override\n   public String toString() {\n     return \"DirCopyWork{\"\n             + \"fullyQualifiedSourcePath=\" + getFullyQualifiedSourcePath()\n             + \", fullyQualifiedTargetPath=\" + getFullyQualifiedTargetPath()\n+            + \", copyMode=\" + getCopyMode()\n             + '}';\n   }\n \n@@ -70,12 +82,18 @@ public String getDumpDirectory() {\n     return dumpDirectory;\n   }\n \n+  public SnapshotUtils.SnapshotCopyMode getCopyMode() {\n+    return copyMode;\n+  }\n+\n   @Override\n   public String convertToString() {\n     StringBuilder objInStr = new StringBuilder();\n     objInStr.append(fullyQualifiedSourcePath)\n             .append(URI_SEPARATOR)\n-            .append(fullyQualifiedTargetPath);\n+            .append(fullyQualifiedTargetPath)\n+            .append(URI_SEPARATOR)\n+            .append(copyMode);\n     return objInStr.toString();\n   }\n \n@@ -84,5 +102,8 @@ public void loadFromString(String objectInStr) {\n     String paths[] = objectInStr.split(URI_SEPARATOR);\n     this.fullyQualifiedSourcePath = new Path(paths[0]);\n     this.fullyQualifiedTargetPath = new Path(paths[1]);\n+    if (paths.length > 2) {\n+      this.copyMode = SnapshotUtils.SnapshotCopyMode.valueOf(paths[2]);\n+    }\n   }\n }"
  },
  {
    "sha": "b995ebba9d3e42193b97042425096c7e1586fc31",
    "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java",
    "status": "modified",
    "additions": 40,
    "deletions": 11,
    "changes": 51,
    "blob_url": "https://github.com/apache/hive/blob/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java",
    "raw_url": "https://github.com/apache/hive/raw/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java?ref=cccf1375fdcc8ae3582155870b974fc205b10eb3",
    "patch": "@@ -115,6 +115,9 @@\n import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.REPL_BOOTSTRAP_DUMP_ABORT_WRITE_TXN_AFTER_TIMEOUT;\n import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.REPL_DUMP_METADATA_ONLY;\n import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.REPL_EXTERNAL_WAREHOUSE_SINGLE_COPY_TASK;\n+import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.REPL_SNAPSHOT_DIFF_FOR_EXTERNAL_TABLE_COPY;\n+import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.REPL_SNAPSHOT_EXTERNAL_TABLE_PATHS;\n+import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.REPL_SNAPSHOT_PREFIX_FOR_EXTERNAL_TABLE_COPY;\n import static org.apache.hadoop.hive.metastore.ReplChangeManager.getReplPolicyIdString;\n import static org.apache.hadoop.hive.ql.exec.repl.ReplExternalTables.Writer;\n import static org.apache.hadoop.hive.ql.exec.repl.ReplAck.LOAD_ACKNOWLEDGEMENT;\n@@ -440,8 +443,9 @@ private boolean previousReplScopeModified() {\n    * Decide whether to dump external tables data. If external tables are enabled for replication,\n    * then need to dump it's data in all the incremental dumps.\n    * @return true if need to dump external table data and false if not.\n+   * @param conf\n    */\n-  private boolean shouldDumpExternalTableLocation() {\n+  public static boolean shouldDumpExternalTableLocation(HiveConf conf) {\n     return conf.getBoolVar(HiveConf.ConfVars.REPL_INCLUDE_EXTERNAL_TABLES)\n             && (!conf.getBoolVar(REPL_DUMP_METADATA_ONLY) &&\n             !conf.getBoolVar(HiveConf.ConfVars.REPL_DUMP_METADATA_ONLY_FOR_EXTERNAL_TABLE));\n@@ -538,6 +542,7 @@ private Long incrementalDump(Path dumpRoot, DumpMetaData dmd, Path cmRoot, Hive\n     long bootDumpBeginReplId = -1;\n \n     List<String> tableList = work.replScope.includeAllTables() ? null : new ArrayList<>();\n+    HashSet<String> failedSnapshotPaths = new HashSet<>();\n \n     // If we are bootstrapping ACID tables, we need to perform steps similar to a regular\n     // bootstrap (See bootstrapDump() for more details. Only difference here is instead of\n@@ -663,16 +668,23 @@ private Long incrementalDump(Path dumpRoot, DumpMetaData dmd, Path cmRoot, Hive\n               conf.getBoolVar(REPL_EXTERNAL_WAREHOUSE_SINGLE_COPY_TASK)\n                   && work.replScope.includeAllTables();\n           boolean isExternalTablePresent = false;\n+          boolean isSnapshotEnabed =\n+              conf.getBoolVar(REPL_SNAPSHOT_DIFF_FOR_EXTERNAL_TABLE_COPY);\n+          String snapshotPrefix =\n+              conf.getVar(REPL_SNAPSHOT_PREFIX_FOR_EXTERNAL_TABLE_COPY);\n+          List<String> snapCustomPaths = Arrays.asList(\n+              conf.getVar(REPL_SNAPSHOT_EXTERNAL_TABLE_PATHS).split(\",\"));\n           for (String tableName : Utils.matchesTbl(hiveDb, dbName, work.replScope)) {\n             try {\n               Table table = hiveDb.getTable(dbName, tableName);\n \n               // Dump external table locations if required.\n               if (TableType.EXTERNAL_TABLE.equals(table.getTableType())\n-                      && shouldDumpExternalTableLocation()) {\n+                      && shouldDumpExternalTableLocation(conf)) {\n                 dbPath = new Path(hiveDb.getDatabase(dbName).getLocationUri());\n                 writer.dataLocationDump(table, extTableFileList, dbPath,\n-                    !isSingleCopyTaskForExternalTables, conf);\n+                    !isSingleCopyTaskForExternalTables, isSnapshotEnabed,\n+                    snapshotPrefix, snapCustomPaths, failedSnapshotPaths, conf);\n                 isExternalTablePresent=true;\n               }\n \n@@ -693,17 +705,21 @@ private Long incrementalDump(Path dumpRoot, DumpMetaData dmd, Path cmRoot, Hive\n           }\n           // if it is not a table level replication, add a single task for\n           // the database default location for external tables...\n-          if (isExternalTablePresent && shouldDumpExternalTableLocation()\n+          if (isExternalTablePresent && shouldDumpExternalTableLocation(conf)\n               && isSingleCopyTaskForExternalTables) {\n             // Using the lower case of the database name, to keep it\n             // consistent with the name used during bootstrap.\n-            writer.dbLocationDump(dbName.toLowerCase(), dbPath, extTableFileList, conf);\n+            writer\n+                .dbLocationDump(dbName.toLowerCase(), dbPath, extTableFileList,\n+                    conf, isSnapshotEnabed, snapshotPrefix,\n+                    failedSnapshotPaths);\n           }\n         }\n         dumpTableListToDumpLocation(tableList, dumpRoot, dbName, conf);\n       }\n       setDataCopyIterators(extTableFileList, managedTblList);\n-      work.getMetricCollector().reportStageEnd(getName(), Status.SUCCESS, lastReplId);\n+      work.getMetricCollector().reportStageEnd(getName(), Status.SUCCESS,\n+          lastReplId, failedSnapshotPaths);\n       return lastReplId;\n     }\n   }\n@@ -875,6 +891,7 @@ Long bootStrapDump(Path dumpRoot, DumpMetaData dmd, Path cmRoot, Hive hiveDb)\n     Long bootDumpBeginReplId = queryState.getConf().getLong(ReplUtils.LAST_REPL_ID_KEY, -1L);\n     assert (bootDumpBeginReplId >= 0L);\n     List<String> tableList;\n+    HashSet<String> failedSnapshotPaths = new HashSet<>();\n \n     LOG.info(\"Bootstrap Dump for db {}\", work.dbNameOrPattern);\n     long timeoutInMs = HiveConf.getTimeVar(conf,\n@@ -945,6 +962,13 @@ Long bootStrapDump(Path dumpRoot, DumpMetaData dmd, Path cmRoot, Hive hiveDb)\n               conf.getBoolVar(REPL_EXTERNAL_WAREHOUSE_SINGLE_COPY_TASK)\n                   && work.replScope.includeAllTables();\n           boolean isExternalTablePresent = false;\n+          boolean isSnapshotEnabed =\n+              conf.getBoolVar(REPL_SNAPSHOT_DIFF_FOR_EXTERNAL_TABLE_COPY);\n+          String snapshotPrefix =\n+              conf.getVar(REPL_SNAPSHOT_PREFIX_FOR_EXTERNAL_TABLE_COPY);\n+          List<String> snapCustomPaths = Arrays.asList(\n+              conf.getVar(REPL_SNAPSHOT_EXTERNAL_TABLE_PATHS).split(\",\"));\n+\n           for (String tblName : Utils.matchesTbl(hiveDb, dbName, work.replScope)) {\n             Table table = null;\n             try {\n@@ -959,11 +983,13 @@ Long bootStrapDump(Path dumpRoot, DumpMetaData dmd, Path cmRoot, Hive hiveDb)\n               }\n \n               LOG.debug(\"Dumping table: \" + tblName + \" to db root \" + dbRoot.toUri());\n-              if (shouldDumpExternalTableLocation()\n+              if (shouldDumpExternalTableLocation(conf)\n                       && TableType.EXTERNAL_TABLE.equals(tableTuple.object.getTableType())) {\n                 LOG.debug(\"Adding table {} to external tables list\", tblName);\n                 writer.dataLocationDump(tableTuple.object, extTableFileList,\n-                    new Path(db.getLocationUri()), !isSingleTaskForExternalDb, conf);\n+                    new Path(db.getLocationUri()), !isSingleTaskForExternalDb,\n+                    isSnapshotEnabed, snapshotPrefix, snapCustomPaths,\n+                    failedSnapshotPaths, conf);\n                 isExternalTablePresent = true;\n               }\n               dumpTable(dbName, tblName, validTxnList, dbRoot, dbDataRoot,\n@@ -982,9 +1008,11 @@ Long bootStrapDump(Path dumpRoot, DumpMetaData dmd, Path cmRoot, Hive hiveDb)\n \n           // if it is not a table level replication, add a single task for\n           // the database default location for external tables.\n-          if (isExternalTablePresent && shouldDumpExternalTableLocation()\n+          if (isExternalTablePresent && shouldDumpExternalTableLocation(conf)\n               && isSingleTaskForExternalDb) {\n-            writer.dbLocationDump(dbName, new Path(db.getLocationUri()), extTableFileList, conf);\n+            writer.dbLocationDump(dbName, new Path(db.getLocationUri()),\n+                extTableFileList, conf, isSnapshotEnabed, snapshotPrefix,\n+                failedSnapshotPaths);\n           }\n           dumpTableListToDumpLocation(tableList, dumpRoot, dbName, conf);\n         } catch (Exception e) {\n@@ -1006,7 +1034,8 @@ Long bootStrapDump(Path dumpRoot, DumpMetaData dmd, Path cmRoot, Hive hiveDb)\n           }\n         }\n         replLogger.endLog(bootDumpBeginReplId.toString());\n-        work.getMetricCollector().reportStageEnd(getName(), Status.SUCCESS, bootDumpBeginReplId);\n+        work.getMetricCollector().reportStageEnd(getName(), Status.SUCCESS,\n+            bootDumpBeginReplId, failedSnapshotPaths);\n       }\n       work.setFunctionCopyPathIterator(functionsBinaryCopyPaths.iterator());\n       setDataCopyIterators(extTableFileList, managedTblList);"
  },
  {
    "sha": "06e8928fbfdbb47acd8525db0c6f0c9b2885bd8d",
    "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplExternalTables.java",
    "status": "modified",
    "additions": 97,
    "deletions": 8,
    "changes": 105,
    "blob_url": "https://github.com/apache/hive/blob/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplExternalTables.java",
    "raw_url": "https://github.com/apache/hive/raw/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplExternalTables.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplExternalTables.java?ref=cccf1375fdcc8ae3582155870b974fc205b10eb3",
    "patch": "@@ -20,6 +20,7 @@\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n import org.apache.hadoop.hive.common.FileUtils;\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.metastore.TableType;\n@@ -28,6 +29,7 @@\n import org.apache.hadoop.hive.ql.ErrorMsg;\n import org.apache.hadoop.hive.ql.exec.repl.util.FileList;\n import org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils;\n+import org.apache.hadoop.hive.ql.exec.repl.util.SnapshotUtils;\n import org.apache.hadoop.hive.ql.exec.util.Retryable;\n import org.apache.hadoop.hive.ql.metadata.Hive;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n@@ -53,6 +55,10 @@\n import java.util.Set;\n import java.util.concurrent.Callable;\n \n+import static org.apache.hadoop.hive.ql.exec.repl.util.SnapshotUtils.SnapshotCopyMode.DIFF_COPY;\n+import static org.apache.hadoop.hive.ql.exec.repl.util.SnapshotUtils.SnapshotCopyMode.FALLBACK_COPY;\n+import static org.apache.hadoop.hive.ql.exec.repl.util.SnapshotUtils.SnapshotCopyMode.INITIAL_COPY;\n+\n /**\n  * Format of the file used to dump information about external tables:\n  * <p>\n@@ -139,8 +145,10 @@ private boolean shouldWrite() {\n      * table if the table is partitioned and the partition location is outside the table.\n      * It returns list of all the external table locations.\n      */\n-    void dataLocationDump(Table table, FileList fileList,\n-        Path dbLoc, boolean isTableLevelReplication, HiveConf conf)\n+    void dataLocationDump(Table table, FileList fileList, Path dbLoc, boolean isTableLevelReplication,\n+        boolean isSnapshotEnabed, String snapshotPrefix,\n+        List<String> snapCustomPaths, HashSet<String> failedSnapshotPaths,\n+        HiveConf conf)\n             throws InterruptedException, IOException, HiveException {\n       if (!shouldWrite()) {\n         return;\n@@ -155,7 +163,11 @@ void dataLocationDump(Table table, FileList fileList,\n           .isPathWithinSubtree(table.getDataLocation(), dbLoc)) {\n         write(lineFor(table.getTableName(), fullyQualifiedDataLocation,\n             hiveConf));\n-        dirLocationToCopy(fileList, fullyQualifiedDataLocation, conf);\n+        dirLocationToCopy(fileList, fullyQualifiedDataLocation, conf,\n+            isSnapshotEnabed ?\n+                isSnapshotRequiredForPath(table.getDataLocation(),\n+                    snapCustomPaths) :\n+                false, snapshotPrefix, failedSnapshotPaths);\n       }\n       if (table.isPartitioned()) {\n         List<Partition> partitions;\n@@ -187,24 +199,42 @@ void dataLocationDump(Table table, FileList fileList,\n             fullyQualifiedDataLocation = PathBuilder\n                 .fullyQualifiedHDFSUri(partition.getDataLocation(), FileSystem.get(hiveConf));\n             write(lineFor(table.getTableName(), fullyQualifiedDataLocation, hiveConf));\n-            dirLocationToCopy(fileList, fullyQualifiedDataLocation, conf);\n+            dirLocationToCopy(fileList, fullyQualifiedDataLocation, conf,  isSnapshotEnabed ?\n+                    isSnapshotRequiredForPath(fullyQualifiedDataLocation,\n+                        snapCustomPaths) :\n+                    false,\n+                snapshotPrefix, failedSnapshotPaths);\n           }\n         }\n       }\n     }\n \n     void dbLocationDump(String dbName, Path dbLocation, FileList fileList,\n-        HiveConf conf) throws Exception {\n+        HiveConf conf, boolean isSnapshotEnabed, String snapshotPrefix,\n+        HashSet<String> failedSnapshotPaths) throws Exception {\n       Path fullyQualifiedDataLocation = PathBuilder\n           .fullyQualifiedHDFSUri(dbLocation, FileSystem.get(hiveConf));\n       write(lineFor(dbName, fullyQualifiedDataLocation, hiveConf));\n-      dirLocationToCopy(fileList, fullyQualifiedDataLocation, conf);\n+      dirLocationToCopy(fileList, fullyQualifiedDataLocation, conf,\n+          isSnapshotEnabed, snapshotPrefix, failedSnapshotPaths);\n+    }\n+\n+    private boolean isSnapshotRequiredForPath(Path tableLocation,\n+        List<String> customPaths) {\n+      return customPaths.contains(tableLocation.toString());\n     }\n \n-    private void dirLocationToCopy(FileList fileList, Path sourcePath, HiveConf conf)\n+\n+    private void dirLocationToCopy(FileList fileList, Path sourcePath,\n+        HiveConf conf, boolean createSnapshot, String snapshotPrefix,\n+        HashSet<String> numFailedSnapshotPaths)\n             throws HiveException {\n       Path basePath = getExternalTableBaseDir(conf);\n       Path targetPath = externalTableDataPath(conf, basePath, sourcePath);\n+      // creates snapshots at source filesystem, if required.\n+      SnapshotUtils.SnapshotCopyMode copyMode =\n+          createSnapshotsAtSource(sourcePath, snapshotPrefix, createSnapshot,\n+              conf, numFailedSnapshotPaths);\n       //Here, when src and target are HA clusters with same NS, then sourcePath would have the correct host\n       //whereas the targetPath would have an host that refers to the target cluster. This is fine for\n       //data-copy running during dump as the correct logical locations would be used. But if data-copy runs during\n@@ -221,7 +251,66 @@ private void dirLocationToCopy(FileList fileList, Path sourcePath, HiveConf conf\n         targetPath = new Path(Utils.replaceHost(targetPath.toString(), sourcePath.toUri().getHost()));\n         sourcePath = new Path(Utils.replaceHost(sourcePath.toString(), remoteNS));\n       }\n-      fileList.add(new DirCopyWork(sourcePath, targetPath).convertToString());\n+      fileList.add(\n+          new DirCopyWork(sourcePath, targetPath, copyMode).convertToString());\n+    }\n+\n+    private SnapshotUtils.SnapshotCopyMode createSnapshotsAtSource(\n+        Path sourcePath, String snapshotPrefix, boolean isSnapshotEnabled,\n+        HiveConf conf, HashSet<String> numFailedSnapshotPaths) {\n+      if (!isSnapshotEnabled) {\n+        LOG.info(\"Snapshot copy not enabled for path {} Will use normal \"\n+                + \"distCp for copying data.\",\n+            sourcePath);\n+        return FALLBACK_COPY;\n+      }\n+      DistributedFileSystem sourceDfs = SnapshotUtils.getDFS(sourcePath, conf);\n+\n+      if (sourceDfs != null) {\n+        try {\n+          // check if second snapshot exists.\n+          boolean isSecondSnapAvlb = SnapshotUtils\n+              .isSnapshotAvailable(sourceDfs, sourcePath, snapshotPrefix,\n+                  \"old\");\n+          if (isSecondSnapAvlb) {\n+            sourceDfs.deleteSnapshot(sourcePath, snapshotPrefix + \"old\");\n+            sourceDfs.renameSnapshot(sourcePath, snapshotPrefix + \"initial\",\n+                snapshotPrefix + \"old\");\n+            sourceDfs.createSnapshot(sourcePath, snapshotPrefix + \"initial\");\n+            return DIFF_COPY;\n+          } else {\n+            // Check if first snapshot is available\n+            boolean isFirstSnapshotAvailable = SnapshotUtils\n+                .isSnapshotAvailable(sourceDfs, sourcePath, snapshotPrefix,\n+                    \"initial\");\n+            if (isFirstSnapshotAvailable) {\n+              sourceDfs.renameSnapshot(sourcePath, snapshotPrefix + \"initial\",\n+                  snapshotPrefix + \"old\");\n+              sourceDfs.createSnapshot(sourcePath, snapshotPrefix + \"initial\");\n+              return DIFF_COPY;\n+            } else {\n+              if (SnapshotUtils.allowSnapshot(sourceDfs, sourcePath)) {\n+                ReplUtils.createSnapshotSafe(sourceDfs, sourcePath,\n+                    snapshotPrefix + \"initial\");\n+                return INITIAL_COPY;\n+              } else {\n+                numFailedSnapshotPaths.add(sourcePath.toString());\n+                LOG.error(\"Can not allow Snapshot for path {}\", sourcePath);\n+                return FALLBACK_COPY;\n+              }\n+            }\n+          }\n+        } catch (Exception e) {\n+          LOG.error(\"Error encountered during snapshot setup for path {}\",\n+              sourcePath, e);\n+          numFailedSnapshotPaths.add(sourcePath.toString());\n+          return FALLBACK_COPY;\n+        }\n+      } else {\n+        LOG.warn(\"Can not get DistributedFileSystem for path {} . Will use \"\n+            + \"normal distCp mode for copy.\", sourcePath);\n+        return FALLBACK_COPY;\n+      }\n     }\n \n     private static String lineFor(String tableName, Path dataLoc, HiveConf hiveConf)"
  },
  {
    "sha": "d1eb89ef0705eb2c2f60cb23b331f1f491d40e71",
    "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplStateLogWork.java",
    "status": "modified",
    "additions": 4,
    "deletions": 1,
    "changes": 5,
    "blob_url": "https://github.com/apache/hive/blob/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplStateLogWork.java",
    "raw_url": "https://github.com/apache/hive/raw/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplStateLogWork.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplStateLogWork.java?ref=cccf1375fdcc8ae3582155870b974fc205b10eb3",
    "patch": "@@ -30,6 +30,8 @@\n import org.apache.hadoop.hive.ql.plan.Explain.Level;\n \n import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashSet;\n import java.util.Map;\n \n \n@@ -150,7 +152,8 @@ public void replStateLog() throws SemanticException {\n       if (StringUtils.isEmpty(lastReplId) || \"null\".equalsIgnoreCase(lastReplId)) {\n         metricCollector.reportStageEnd(\"REPL_LOAD\", Status.SUCCESS);\n       } else {\n-        metricCollector.reportStageEnd(\"REPL_LOAD\", Status.SUCCESS, Long.parseLong(lastReplId));\n+        metricCollector.reportStageEnd(\"REPL_LOAD\", Status.SUCCESS,\n+            Long.parseLong(lastReplId), new HashSet<String>());\n       }\n       metricCollector.reportEnd(Status.SUCCESS);\n       break;"
  },
  {
    "sha": "65adf4a36e4f199555bbdec817a5b01788809adf",
    "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/ReplUtils.java",
    "status": "modified",
    "additions": 66,
    "deletions": 0,
    "changes": 66,
    "blob_url": "https://github.com/apache/hive/blob/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/ReplUtils.java",
    "raw_url": "https://github.com/apache/hive/raw/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/ReplUtils.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/ReplUtils.java?ref=cccf1375fdcc8ae3582155870b974fc205b10eb3",
    "patch": "@@ -21,6 +21,8 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.PathFilter;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.protocol.HdfsConstants;\n import org.apache.hadoop.hive.common.TableName;\n import org.apache.hadoop.hive.common.repl.ReplConst;\n import org.apache.hadoop.hive.common.repl.ReplScope;\n@@ -63,6 +65,7 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.io.UnsupportedEncodingException;\n import java.nio.charset.StandardCharsets;\n@@ -453,4 +456,67 @@ public static Path getLatestDumpPath(Path dumpRoot, HiveConf conf) throws IOExce\n     }\n     return null;\n   }\n+\n+  public static void createSnapshotSafe(FileSystem dfs,\n+      Path snapshotPath, String snapshotName) {\n+    try {\n+      dfs.createSnapshot(snapshotPath, snapshotName);\n+    } catch (IOException e) {\n+      LOG.debug(\"Couldn't create the snapshot {} under path {}\", snapshotName,\n+          snapshotPath, e);\n+    }\n+  }\n+\n+  public static void deleteSnapshotSafe(DistributedFileSystem dfs,\n+      Path snapshotPath, String snapshotName) {\n+    try {\n+      dfs.deleteSnapshot(snapshotPath, snapshotName);\n+    } catch (IOException e) {\n+      LOG.debug(\"Couldn't delete the snapshot {} under path {}\", snapshotName,\n+          snapshotPath, e);\n+    }\n+  }\n+\n+  public static void renameSnapshotSafe(DistributedFileSystem dfs,\n+      Path snapshotPath, String oldSnapshotName, String newSnapshotName) {\n+    try {\n+      dfs.renameSnapshot(snapshotPath, oldSnapshotName, newSnapshotName);\n+    } catch (IOException e) {\n+      LOG.debug(\"Couldn't rename the snapshot {} to {} under path {}\",\n+          oldSnapshotName, newSnapshotName, snapshotPath, e);\n+    }\n+  }\n+\n+  public static boolean allowSanapshotSafe(DistributedFileSystem dfs,\n+      Path snapshotPath) throws IOException {\n+    if (dfs.getFileStatus(snapshotPath).isSnapshotEnabled()) {\n+      return true;\n+    } else {\n+      try {\n+        dfs.allowSnapshot(snapshotPath);\n+      } catch (Exception e) {\n+        LOG.error(\"Cannot allow snapshot on path {}\", snapshotPath, e);\n+        throw e;\n+      }\n+    }\n+    return true;\n+  }\n+\n+  public static void disAllowSanapshotSafe(DistributedFileSystem dfs,\n+      Path snapshotPath) {\n+    try {\n+      dfs.disallowSnapshot(snapshotPath);\n+    } catch (IOException e) {\n+      LOG.debug(\"Couldn't disallow snapshot on {}\", snapshotPath, e);\n+    }\n+  }\n+\n+\n+\n+  public static String getRelativePath(Path sourceRootPath, Path childPath) {\n+    String childPathString = childPath.toUri().getPath();\n+    String sourceRootPathString = sourceRootPath.toUri().getPath();\n+    return sourceRootPathString.equals(\"/\") ? childPathString :\n+        childPathString.substring(sourceRootPathString.length());\n+  }\n }"
  },
  {
    "sha": "c536aab6b46ec986dd937a5150575ba6fcceea59",
    "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/SnapshotUtils.java",
    "status": "added",
    "additions": 120,
    "deletions": 0,
    "changes": 120,
    "blob_url": "https://github.com/apache/hive/blob/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/SnapshotUtils.java",
    "raw_url": "https://github.com/apache/hive/raw/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/SnapshotUtils.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/SnapshotUtils.java?ref=cccf1375fdcc8ae3582155870b974fc205b10eb3",
    "patch": "@@ -0,0 +1,120 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.exec.repl.util;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.protocol.HdfsConstants;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+\n+public class SnapshotUtils {\n+\n+  private static transient Logger LOG = LoggerFactory.getLogger(SnapshotUtils.class);\n+\n+  public static DistributedFileSystem getDFS(Path path, HiveConf conf) {\n+    try {\n+      FileSystem fs = path.getFileSystem(conf);\n+      if (fs instanceof DistributedFileSystem) {\n+        return (DistributedFileSystem) fs;\n+      } else {\n+        LOG.info(\"FileSystem for {} is not DistributedFileSystem\", path);\n+      }\n+    } catch (Exception e) {\n+      LOG.error(\"Can not get filesystem for path {}\", path);\n+    }\n+    return null;\n+  }\n+\n+  public static boolean isSnapshotAvailable(DistributedFileSystem dfs,\n+      Path path, String snapshotPrefix, String snapshotName)\n+      throws IOException {\n+    boolean isSnapAvlb = true;\n+    try {\n+      FileStatus fileStatus = dfs.getFileStatus(new Path(path,\n+          HdfsConstants.DOT_SNAPSHOT_DIR + \"/\" + snapshotPrefix + snapshotName));\n+      isSnapAvlb = fileStatus != null;\n+    } catch (FileNotFoundException fnf) {\n+      isSnapAvlb = false;\n+    }\n+    LOG.debug(\"Snapshot for path {} is {}\", path,\n+        isSnapAvlb ? \"available\" : \"unavailable\");\n+    return isSnapAvlb;\n+  }\n+\n+  public static void deleteSnapshotSafe(DistributedFileSystem dfs,\n+      Path snapshotPath, String snapshotName) {\n+    try {\n+      dfs.deleteSnapshot(snapshotPath, snapshotName);\n+    } catch (IOException e) {\n+      LOG.debug(\"Couldn't delete the snapshot {} under path {}\", snapshotName,\n+          snapshotPath, e);\n+    }\n+  }\n+\n+  public static void disallowSnapshot(DistributedFileSystem dfs,\n+      Path snapshotPath) {\n+    try {\n+      // Check if the directory is snapshottable.\n+      if (dfs.getFileStatus(snapshotPath).isSnapshotEnabled()) {\n+        dfs.disallowSnapshot(snapshotPath);\n+      }\n+    } catch (Exception e) {\n+      LOG.warn(\"Could not disallow snapshot for path {}\", snapshotPath, e);\n+    }\n+  }\n+\n+  public static boolean allowSnapshot(DistributedFileSystem dfs,\n+      Path snapshotPath) {\n+    try {\n+      // Check if the directory is already snapshottable.\n+      if (dfs.getFileStatus(snapshotPath).isSnapshotEnabled()) {\n+        return true;\n+      } else {\n+        dfs.allowSnapshot(snapshotPath);\n+      }\n+    } catch (Exception e) {\n+      LOG.error(\"Cannot allow snapshot on path {}\", snapshotPath, e);\n+      return false;\n+    }\n+    return true;\n+  }\n+\n+  public enum SnapshotCopyMode {\n+    INITIAL_COPY,\n+    DIFF_COPY,\n+    FALLBACK_COPY;\n+  }\n+\n+  public static void createSnapshotSafe(FileSystem dfs, Path snapshotPath,\n+      String snapshotName) {\n+    try {\n+      dfs.createSnapshot(snapshotPath, snapshotName);\n+    } catch (IOException e) {\n+      LOG.warn(\"Couldn't create the snapshot {} under path {}\", snapshotName,\n+          snapshotPath, e);\n+    }\n+  }\n+\n+}"
  },
  {
    "sha": "4e076b652b060dd05c00fe0d857e4a6330735abe",
    "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/events/AlterDatabaseHandler.java",
    "status": "modified",
    "additions": 39,
    "deletions": 1,
    "changes": 40,
    "blob_url": "https://github.com/apache/hive/blob/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/events/AlterDatabaseHandler.java",
    "raw_url": "https://github.com/apache/hive/raw/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/events/AlterDatabaseHandler.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/events/AlterDatabaseHandler.java?ref=cccf1375fdcc8ae3582155870b974fc205b10eb3",
    "patch": "@@ -17,15 +17,29 @@\n  */\n package org.apache.hadoop.hive.ql.parse.repl.dump.events;\n \n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.api.Database;\n import org.apache.hadoop.hive.metastore.api.NotificationEvent;\n import org.apache.hadoop.hive.metastore.messaging.AlterDatabaseMessage;\n+import org.apache.hadoop.hive.ql.exec.repl.util.SnapshotUtils;\n import org.apache.hadoop.hive.ql.parse.repl.DumpType;\n import org.apache.hadoop.hive.ql.parse.repl.load.DumpMetaData;\n \n+import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.REPL_SNAPSHOT_DIFF_FOR_EXTERNAL_TABLE_COPY;\n+import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.REPL_SNAPSHOT_PREFIX_FOR_EXTERNAL_TABLE_COPY;\n+import static org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.shouldDumpExternalTableLocation;\n+\n class AlterDatabaseHandler extends AbstractEventHandler<AlterDatabaseMessage> {\n \n-  AlterDatabaseHandler(NotificationEvent event) {\n+  private final Database before;\n+  private final Database after;\n+\n+  AlterDatabaseHandler(NotificationEvent event) throws Exception {\n     super(event);\n+    before = eventMessage.getDbObjBefore();\n+    after = eventMessage.getDbObjAfter();\n   }\n \n   @Override\n@@ -37,10 +51,34 @@ AlterDatabaseMessage eventMessage(String stringRepresentation) {\n   public void handle(Context withinContext) throws Exception {\n     LOG.info(\"Processing#{} ALTER_DATABASE message : {}\", fromEventId(), eventMessageAsJSON);\n     DumpMetaData dmd = withinContext.createDmd(this);\n+    processSnapshots(withinContext.hiveConf);\n     dmd.setPayload(eventMessageAsJSON);\n     dmd.write();\n   }\n \n+  void processSnapshots(HiveConf conf) {\n+    // if the snapshot diff is enabled delete the snapshot on the db location\n+    // if the location has been changed.\n+    if (shouldDumpExternalTableLocation(conf) && conf\n+        .getBoolVar(REPL_SNAPSHOT_DIFF_FOR_EXTERNAL_TABLE_COPY)) {\n+      if (!before.getLocationUri().equals(after.getLocationUri())) {\n+        String snapshotPrefix =\n+            conf.getVar(REPL_SNAPSHOT_PREFIX_FOR_EXTERNAL_TABLE_COPY);\n+        Path beforePath = new Path(before.getLocationUri());\n+        LOG.info(\"Processing snapshots for path {} and database {}\", beforePath,\n+            before);\n+        DistributedFileSystem dfs = SnapshotUtils.getDFS(beforePath, conf);\n+        if (dfs != null) {\n+          SnapshotUtils\n+              .deleteSnapshotSafe(dfs, beforePath, snapshotPrefix + \"old\");\n+          SnapshotUtils\n+              .deleteSnapshotSafe(dfs, beforePath, snapshotPrefix + \"initial\");\n+          SnapshotUtils.disallowSnapshot(dfs, beforePath);\n+        }\n+      }\n+    }\n+  }\n+\n   @Override\n   public DumpType dumpType() {\n     return DumpType.EVENT_ALTER_DATABASE;"
  },
  {
    "sha": "e0ae8bc61dab630fcfdc9a77f1ef63f83dc8c5cf",
    "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AbstractMessageHandler.java",
    "status": "modified",
    "additions": 3,
    "deletions": 0,
    "changes": 3,
    "blob_url": "https://github.com/apache/hive/blob/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AbstractMessageHandler.java",
    "raw_url": "https://github.com/apache/hive/raw/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AbstractMessageHandler.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AbstractMessageHandler.java?ref=cccf1375fdcc8ae3582155870b974fc205b10eb3",
    "patch": "@@ -22,11 +22,14 @@\n import org.apache.hadoop.hive.ql.hooks.ReadEntity;\n import org.apache.hadoop.hive.ql.hooks.WriteEntity;\n import org.apache.hadoop.hive.ql.parse.repl.load.UpdatedMetaDataTracker;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n import java.util.HashSet;\n import java.util.Set;\n \n abstract class AbstractMessageHandler implements MessageHandler {\n+  static final Logger LOG = LoggerFactory.getLogger(AbstractMessageHandler.class);\n   final HashSet<ReadEntity> readEntitySet = new HashSet<>();\n   final HashSet<WriteEntity> writeEntitySet = new HashSet<>();\n   final UpdatedMetaDataTracker updatedMetadata = new UpdatedMetaDataTracker();"
  },
  {
    "sha": "0989f4e28fc95dcb4bc06e93b541626bb90eca20",
    "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AlterDatabaseHandler.java",
    "status": "modified",
    "additions": 45,
    "deletions": 0,
    "changes": 45,
    "blob_url": "https://github.com/apache/hive/blob/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AlterDatabaseHandler.java",
    "raw_url": "https://github.com/apache/hive/raw/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AlterDatabaseHandler.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/AlterDatabaseHandler.java?ref=cccf1375fdcc8ae3582155870b974fc205b10eb3",
    "patch": "@@ -17,6 +17,9 @@\n  */\n package org.apache.hadoop.hive.ql.parse.repl.load.message;\n \n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.metastore.ReplChangeManager;\n import org.apache.hadoop.hive.metastore.api.Database;\n import org.apache.hadoop.hive.metastore.messaging.AlterDatabaseMessage;\n@@ -28,6 +31,7 @@\n import org.apache.hadoop.hive.ql.exec.Task;\n import org.apache.hadoop.hive.ql.exec.TaskFactory;\n import org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils;\n+import org.apache.hadoop.hive.ql.exec.repl.util.SnapshotUtils;\n import org.apache.hadoop.hive.ql.parse.ReplicationSpec;\n import org.apache.hadoop.hive.ql.parse.SemanticException;\n import org.apache.hadoop.hive.ql.parse.repl.dump.Utils;\n@@ -38,6 +42,13 @@\n import java.util.List;\n import java.util.Map;\n \n+import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.REPL_EXTERNAL_TABLE_BASE_DIR;\n+import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.REPL_SNAPSHOT_DIFF_FOR_EXTERNAL_TABLE_COPY;\n+import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.REPL_SNAPSHOT_PREFIX_FOR_EXTERNAL_TABLE_COPY;\n+import static org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.shouldDumpExternalTableLocation;\n+import static org.apache.hadoop.hive.ql.exec.repl.ReplExternalTables.externalTableDataPath;\n+import static org.apache.hadoop.hive.ql.exec.repl.ReplExternalTables.getExternalTableBaseDir;\n+\n /**\n  * AlterDatabaseHandler.\n  * Handler at target warehouse for the EVENT_ALTER_DATABASE type of messages\n@@ -86,11 +97,45 @@\n       // Only database object is updated\n       updatedMetadata.set(context.dmd.getEventTo().toString(), actualDbName,\n               null, null);\n+      processSnapshots(context.hiveConf, oldDb, newDb);\n       return Collections.singletonList(alterDbTask);\n     } catch (Exception e) {\n       throw (e instanceof SemanticException)\n           ? (SemanticException) e\n           : new SemanticException(\"Error reading message members\", e);\n     }\n   }\n+\n+  private void processSnapshots(HiveConf conf, Database before,\n+      Database after) {\n+    try {\n+      // if the snapshot diff is enabled delete the snapshot on the db location\n+      // if the location has been changed.\n+      if (conf.getBoolVar(REPL_SNAPSHOT_DIFF_FOR_EXTERNAL_TABLE_COPY)\n+          && shouldDumpExternalTableLocation(conf)) {\n+        if (!before.getLocationUri().equals(after.getLocationUri())) {\n+          String snapshotPrefix =\n+              conf.getVar(REPL_SNAPSHOT_PREFIX_FOR_EXTERNAL_TABLE_COPY);\n+          Path beforePath = new Path(before.getLocationUri());\n+          DistributedFileSystem dfs = SnapshotUtils.getDFS(beforePath, conf);\n+          if (dfs != null) {\n+            Path snapshotPath =\n+                externalTableDataPath(conf, getExternalTableBaseDir(conf),\n+                    beforePath);\n+            LOG.info(\"Processing snapshots message for path {} and database {}\",\n+                snapshotPath, before);\n+            SnapshotUtils\n+                .deleteSnapshotSafe(dfs, snapshotPath, snapshotPrefix + \"old\");\n+            SnapshotUtils.deleteSnapshotSafe(dfs, snapshotPath,\n+                snapshotPrefix + \"initial\");\n+            SnapshotUtils.disallowSnapshot(dfs, snapshotPath);\n+          }\n+        }\n+      }\n+    } catch (Exception e) {\n+      LOG.error(\n+          \"Can not clear snapshots for alter database {} from location \" + \"{}\",\n+          before, before.getLocationUri(), e);\n+    }\n+  }\n }"
  },
  {
    "sha": "065a88100bf2a747865d73aefefcd407a9135cbb",
    "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/repl/metric/ReplicationMetricCollector.java",
    "status": "modified",
    "additions": 20,
    "deletions": 2,
    "changes": 22,
    "blob_url": "https://github.com/apache/hive/blob/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/metric/ReplicationMetricCollector.java",
    "raw_url": "https://github.com/apache/hive/raw/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/metric/ReplicationMetricCollector.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/metric/ReplicationMetricCollector.java?ref=cccf1375fdcc8ae3582155870b974fc205b10eb3",
    "patch": "@@ -17,6 +17,7 @@\n  */\n package org.apache.hadoop.hive.ql.parse.repl.metric;\n \n+import com.google.common.annotations.VisibleForTesting;\n import org.apache.hadoop.hive.conf.Constants;\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.metastore.utils.StringUtils;\n@@ -30,6 +31,7 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import java.util.HashSet;\n import java.util.Map;\n \n /**\n@@ -40,9 +42,11 @@\n   private ReplicationMetric replicationMetric;\n   private MetricCollector metricCollector;\n   private boolean isEnabled;\n+  private static boolean enableForTests;\n \n   public ReplicationMetricCollector(String dbName, Metadata.ReplicationType replicationType,\n                              String stagingDir, long dumpExecutionId, HiveConf conf) {\n+    checkEnabledForTests(conf);\n     String policy = conf.get(Constants.SCHEDULED_QUERY_SCHEDULENAME);\n     long executionId = conf.getLong(Constants.SCHEDULED_QUERY_EXECUTIONID, 0L);\n     if (!StringUtils.isEmpty(policy) && executionId > 0) {\n@@ -69,8 +73,8 @@ public void reportStageStart(String stageName, Map<String, Long> metricMap) thro\n     }\n   }\n \n-\n-  public void reportStageEnd(String stageName, Status status, long lastReplId) throws SemanticException {\n+  public void reportStageEnd(String stageName, Status status, long lastReplId,\n+      HashSet<String> failedSnapshotPaths) throws SemanticException {\n     if (isEnabled) {\n       LOG.debug(\"Stage ended {}, {}, {}\", stageName, status, lastReplId );\n       Progress progress = replicationMetric.getProgress();\n@@ -80,6 +84,7 @@ public void reportStageEnd(String stageName, Status status, long lastReplId) thr\n       }\n       stage.setStatus(status);\n       stage.setEndTime(System.currentTimeMillis());\n+      stage.setFailedSnapshotsPaths(failedSnapshotPaths);\n       progress.addStage(stage);\n       replicationMetric.setProgress(progress);\n       Metadata metadata = replicationMetric.getMetadata();\n@@ -156,4 +161,17 @@ public void reportEnd(Status status) throws SemanticException {\n       metricCollector.addMetric(replicationMetric);\n     }\n   }\n+\n+  // Utility methods to enable metrics without running scheduler for testing.\n+  @VisibleForTesting\n+  public static void isMetricsEnabledForTests(boolean enable) {\n+    enableForTests = enable;\n+  }\n+\n+  private void checkEnabledForTests(HiveConf conf) {\n+    if (enableForTests) {\n+      conf.set(Constants.SCHEDULED_QUERY_SCHEDULENAME, \"pol\");\n+      conf.setLong(Constants.SCHEDULED_QUERY_EXECUTIONID, 1L);\n+    }\n+  }\n }"
  },
  {
    "sha": "eb44e6e7a08c857787beafba9c1ff7e3fc75e571",
    "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/repl/metric/event/Stage.java",
    "status": "modified",
    "additions": 11,
    "deletions": 0,
    "changes": 11,
    "blob_url": "https://github.com/apache/hive/blob/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/metric/event/Stage.java",
    "raw_url": "https://github.com/apache/hive/raw/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/metric/event/Stage.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/metric/event/Stage.java?ref=cccf1375fdcc8ae3582155870b974fc205b10eb3",
    "patch": "@@ -19,6 +19,7 @@\n \n import java.util.ArrayList;\n import java.util.HashMap;\n+import java.util.HashSet;\n import java.util.List;\n import java.util.Map;\n \n@@ -32,6 +33,7 @@\n   private long endTime;\n   private Map<String, Metric> metrics = new HashMap<>();\n   private String errorLogPath;\n+  private HashSet<String> failedSnapshotsPaths = new HashSet<>();\n \n   public Stage() {\n \n@@ -52,6 +54,7 @@ public Stage(Stage stage) {\n       this.metrics.put(metric.getName(), new Metric(metric));\n     }\n     this.errorLogPath = stage.errorLogPath;\n+    this.failedSnapshotsPaths = stage.failedSnapshotsPaths;\n   }\n \n   public String getName() {\n@@ -106,4 +109,12 @@ public String getErrorLogPath() {\n   public void setErrorLogPath(String errorLogPath) {\n     this.errorLogPath = errorLogPath;\n   }\n+\n+  public void setFailedSnapshotsPaths(HashSet<String> failedSnapshotsPaths) {\n+    this.failedSnapshotsPaths = failedSnapshotsPaths;\n+  }\n+\n+  public HashSet<String> getFailedSnapshotsPaths() {\n+    return failedSnapshotsPaths;\n+  }\n }"
  },
  {
    "sha": "182b1fcbcc3d6687ef75a50df413866c0a5513a8",
    "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/repl/metric/event/StageMapper.java",
    "status": "modified",
    "additions": 7,
    "deletions": 0,
    "changes": 7,
    "blob_url": "https://github.com/apache/hive/blob/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/metric/event/StageMapper.java",
    "raw_url": "https://github.com/apache/hive/raw/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/metric/event/StageMapper.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/metric/event/StageMapper.java?ref=cccf1375fdcc8ae3582155870b974fc205b10eb3",
    "patch": "@@ -20,6 +20,7 @@\n import org.apache.hive.common.util.SuppressFBWarnings;\n \n import java.util.ArrayList;\n+import java.util.HashSet;\n import java.util.List;\n \n /**\n@@ -40,6 +41,8 @@\n \n   private String errorLogPath;\n \n+  private HashSet<String> failedSnapshotsPaths = new HashSet<>();\n+\n   public StageMapper() {\n \n   }\n@@ -66,4 +69,8 @@ public long getEndTime() {\n   public String getErrorLogPath() {\n     return errorLogPath;\n   }\n+\n+  public HashSet<String> getFailedSnapshotsPaths() {\n+    return failedSnapshotsPaths;\n+  }\n }"
  },
  {
    "sha": "d94cacd4538bed5bb9cd7834e21f79cfbe6385af",
    "filename": "ql/src/test/org/apache/hadoop/hive/ql/parse/repl/metric/TestReplicationMetricCollector.java",
    "status": "modified",
    "additions": 5,
    "deletions": 4,
    "changes": 9,
    "blob_url": "https://github.com/apache/hive/blob/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/test/org/apache/hadoop/hive/ql/parse/repl/metric/TestReplicationMetricCollector.java",
    "raw_url": "https://github.com/apache/hive/raw/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/test/org/apache/hadoop/hive/ql/parse/repl/metric/TestReplicationMetricCollector.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/parse/repl/metric/TestReplicationMetricCollector.java?ref=cccf1375fdcc8ae3582155870b974fc205b10eb3",
    "patch": "@@ -40,6 +40,7 @@\n import org.mockito.Mockito;\n import org.mockito.junit.MockitoJUnitRunner;\n \n+import java.util.HashSet;\n import java.util.Map;\n import java.util.HashMap;\n import java.util.List;\n@@ -132,7 +133,7 @@ public void testSuccessBootstrapDumpMetrics() throws Exception {\n     actualMetrics = MetricCollector.getInstance().getMetrics();\n     Assert.assertEquals(1, actualMetrics.size());\n \n-    bootstrapDumpMetricCollector.reportStageEnd(\"dump\", Status.SUCCESS, 10);\n+    bootstrapDumpMetricCollector.reportStageEnd(\"dump\", Status.SUCCESS, 10,  new HashSet<String>());\n     bootstrapDumpMetricCollector.reportEnd(Status.SUCCESS);\n     actualMetrics = MetricCollector.getInstance().getMetrics();\n     Assert.assertEquals(1, actualMetrics.size());\n@@ -173,7 +174,7 @@ public void testSuccessIncrDumpMetrics() throws Exception {\n     actualMetrics = MetricCollector.getInstance().getMetrics();\n     Assert.assertEquals(1, actualMetrics.size());\n \n-    incrDumpMetricCollector.reportStageEnd(\"dump\", Status.SUCCESS, 10);\n+    incrDumpMetricCollector.reportStageEnd(\"dump\", Status.SUCCESS, 10,  new HashSet<String>());\n     incrDumpMetricCollector.reportEnd(Status.SUCCESS);\n     actualMetrics = MetricCollector.getInstance().getMetrics();\n     Assert.assertEquals(1, actualMetrics.size());\n@@ -215,7 +216,7 @@ public void testSuccessBootstrapLoadMetrics() throws Exception {\n     actualMetrics = MetricCollector.getInstance().getMetrics();\n     Assert.assertEquals(1, actualMetrics.size());\n \n-    bootstrapLoadMetricCollector.reportStageEnd(\"dump\", Status.SUCCESS, 10);\n+    bootstrapLoadMetricCollector.reportStageEnd(\"dump\", Status.SUCCESS, 10,  new HashSet<String>());\n     bootstrapLoadMetricCollector.reportEnd(Status.SUCCESS);\n     actualMetrics = MetricCollector.getInstance().getMetrics();\n     Assert.assertEquals(1, actualMetrics.size());\n@@ -257,7 +258,7 @@ public void testSuccessIncrLoadMetrics() throws Exception {\n     actualMetrics = MetricCollector.getInstance().getMetrics();\n     Assert.assertEquals(1, actualMetrics.size());\n \n-    incrLoadMetricCollector.reportStageEnd(\"dump\", Status.SUCCESS, 10);\n+    incrLoadMetricCollector.reportStageEnd(\"dump\", Status.SUCCESS, 10,  new HashSet<String>());\n     incrLoadMetricCollector.reportEnd(Status.SUCCESS);\n     actualMetrics = MetricCollector.getInstance().getMetrics();\n     Assert.assertEquals(1, actualMetrics.size());"
  },
  {
    "sha": "a84b9fbf8b26f1990a86613cf9f84397b37e7bed",
    "filename": "ql/src/test/org/apache/hadoop/hive/ql/parse/repl/metric/TestReplicationMetricSink.java",
    "status": "modified",
    "additions": 4,
    "deletions": 2,
    "changes": 6,
    "blob_url": "https://github.com/apache/hive/blob/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/test/org/apache/hadoop/hive/ql/parse/repl/metric/TestReplicationMetricSink.java",
    "raw_url": "https://github.com/apache/hive/raw/cccf1375fdcc8ae3582155870b974fc205b10eb3/ql/src/test/org/apache/hadoop/hive/ql/parse/repl/metric/TestReplicationMetricSink.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/parse/repl/metric/TestReplicationMetricSink.java?ref=cccf1375fdcc8ae3582155870b974fc205b10eb3",
    "patch": "@@ -46,6 +46,7 @@\n \n import java.util.Arrays;\n import java.util.HashMap;\n+import java.util.HashSet;\n import java.util.List;\n import java.util.Map;\n \n@@ -80,7 +81,8 @@ public void testSuccessBootstrapDumpMetrics() throws Exception {\n     bootstrapDumpMetricCollector.reportStageProgress(\"dump\", ReplUtils.MetricName.TABLES.name(), 1);\n     bootstrapDumpMetricCollector.reportStageProgress(\"dump\", ReplUtils.MetricName.TABLES.name(), 2);\n     bootstrapDumpMetricCollector.reportStageProgress(\"dump\", ReplUtils.MetricName.FUNCTIONS.name(), 1);\n-    bootstrapDumpMetricCollector.reportStageEnd(\"dump\", Status.SUCCESS, 10);\n+    bootstrapDumpMetricCollector.reportStageEnd(\"dump\", Status.SUCCESS, 10,\n+        new HashSet<String>());\n     bootstrapDumpMetricCollector.reportEnd(Status.SUCCESS);\n \n     Metadata expectedMetadata = new Metadata(\"testAcidTablesReplLoadBootstrapIncr_1592205875387\",\n@@ -139,7 +141,7 @@ public void testSuccessBootstrapDumpMetrics() throws Exception {\n     metricMap.put(ReplUtils.MetricName.EVENTS.name(), (long) 10);\n     incrementDumpMetricCollector.reportStageStart(\"dump\", metricMap);\n     incrementDumpMetricCollector.reportStageProgress(\"dump\", ReplUtils.MetricName.EVENTS.name(), 10);\n-    incrementDumpMetricCollector.reportStageEnd(\"dump\", Status.SUCCESS, 10);\n+    incrementDumpMetricCollector.reportStageEnd(\"dump\", Status.SUCCESS, 10, new HashSet<String>());\n     incrementDumpMetricCollector.reportEnd(Status.SUCCESS);\n \n     expectedMetadata = new Metadata(\"testAcidTablesReplLoadBootstrapIncr_1592205875387\","
  },
  {
    "sha": "d97b97718454fa2b0ff5d6a4966df2a295d1ff3a",
    "filename": "shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java",
    "status": "modified",
    "additions": 67,
    "deletions": 2,
    "changes": 69,
    "blob_url": "https://github.com/apache/hive/blob/cccf1375fdcc8ae3582155870b974fc205b10eb3/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java",
    "raw_url": "https://github.com/apache/hive/raw/cccf1375fdcc8ae3582155870b974fc205b10eb3/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java?ref=cccf1375fdcc8ae3582155870b974fc205b10eb3",
    "patch": "@@ -1114,6 +1114,18 @@ public void setStoragePolicy(Path path, StoragePolicyValue policy)\n   List<String> constructDistCpParams(List<Path> srcPaths, Path dst, Configuration conf) {\n     // -update and -delete are mandatory options for directory copy to work.\n     // -pbx is default preserve options if user doesn't pass any.\n+    List<String> params = constructDistCpDefaultParams(conf);\n+    if (!params.contains(\"-delete\")) {\n+      params.add(\"-delete\");\n+    }\n+    for (Path src : srcPaths) {\n+      params.add(src.toString());\n+    }\n+    params.add(dst.toString());\n+    return params;\n+  }\n+\n+  private List<String> constructDistCpDefaultParams(Configuration conf) {\n     List<String> params = new ArrayList<String>();\n     boolean needToAddPreserveOption = true;\n     for (Map.Entry<String,String> entry : conf.getPropsWithPrefix(DISTCP_OPTIONS_PREFIX).entrySet()){\n@@ -1133,13 +1145,26 @@ public void setStoragePolicy(Path path, StoragePolicyValue policy)\n     if (!params.contains(\"-update\")) {\n       params.add(\"-update\");\n     }\n-    if (!params.contains(\"-delete\")) {\n-      params.add(\"-delete\");\n+    return params;\n+  }\n+\n+  List<String> constructDistCpWithSnapshotParams(List<Path> srcPaths, Path dst,\n+      String sourceSnap, String destSnap, Configuration conf) {\n+    List<String> params = constructDistCpDefaultParams(conf);\n+    if (params.contains(\"-delete\")) {\n+      params.remove(\"-delete\");\n     }\n+\n+    // Add distCp snapshot diff parameters.\n+    params.add(\"-diff\");\n+    params.add(sourceSnap);\n+    params.add(destSnap);\n+\n     for (Path src : srcPaths) {\n       params.add(src.toString());\n     }\n     params.add(dst.toString());\n+\n     return params;\n   }\n \n@@ -1188,6 +1213,46 @@ public boolean runDistCp(List<Path> srcPaths, Path dst, Configuration conf) thro\n     }\n   }\n \n+  public boolean runDistCpWithSnapshots(String snap1, String snap2,\n+      List<Path> srcPaths, Path dst, Configuration conf) throws IOException {\n+    DistCpOptions options =\n+        new DistCpOptions.Builder(srcPaths, dst).withSyncFolder(true)\n+            .withUseDiff(snap1, snap2).preserve(FileAttribute.BLOCKSIZE)\n+            .preserve(FileAttribute.XATTR).build();\n+\n+    List<String> params =\n+        constructDistCpWithSnapshotParams(srcPaths, dst, snap1, snap2, conf);\n+\n+    try {\n+      conf.setBoolean(\"mapred.mapper.new-api\", true);\n+      DistCp distcp = new DistCp(conf, options);\n+      if (distcp.run(params.toArray(new String[0])) == 0) {\n+        return true;\n+      } else {\n+        return false;\n+      }\n+    } catch (Exception e) {\n+      throw new IOException(\"Cannot execute DistCp process: \" + e, e);\n+    } finally {\n+      conf.setBoolean(\"mapred.mapper.new-api\", false);\n+    }\n+  }\n+\n+  public boolean runDistCpWithSnapshotsAs(String snap1, String snap2,\n+      List<Path> srcPaths, Path dst, Configuration conf,\n+      UserGroupInformation proxyUser) throws IOException {\n+    try {\n+      return proxyUser.doAs(new PrivilegedExceptionAction<Boolean>() {\n+        @Override\n+        public Boolean run() throws Exception {\n+          return runDistCpWithSnapshots(snap1, snap2, srcPaths, dst, conf);\n+        }\n+      });\n+    } catch (InterruptedException e) {\n+      throw new IOException(e);\n+    }\n+  }\n+\n   private static Boolean hdfsEncryptionSupport;\n \n   @SuppressFBWarnings(value = \"LI_LAZY_INIT_STATIC\", justification = \"All threads set the same value despite data race\")"
  },
  {
    "sha": "313f71168440d3c8dc14d996ca91fa4cb52aa51a",
    "filename": "shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShims.java",
    "status": "modified",
    "additions": 35,
    "deletions": 0,
    "changes": 35,
    "blob_url": "https://github.com/apache/hive/blob/cccf1375fdcc8ae3582155870b974fc205b10eb3/shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShims.java",
    "raw_url": "https://github.com/apache/hive/raw/cccf1375fdcc8ae3582155870b974fc205b10eb3/shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShims.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShims.java?ref=cccf1375fdcc8ae3582155870b974fc205b10eb3",
    "patch": "@@ -532,6 +532,41 @@ boolean runDistCpAs(List<Path> srcPaths, Path dst, Configuration conf, UserGroup\n    */\n   public boolean runDistCp(List<Path> srcPaths, Path dst, Configuration conf) throws IOException;\n \n+  /**\n+   * Copies a source dir/file to a destination by orchestrating the copy between hdfs nodes.\n+   * This distributed process is meant to copy huge files that could take some time if a single\n+   * copy is done. This method allows to specify usage of -diff feature of\n+   * distcp\n+   * @param snap1    initial snapshot\n+   * @param snap2    final snapshot\n+   * @param srcPaths List of Path to the source files or directories to copy\n+   * @param dst      Path to the destination file or directory\n+   * @param conf     The hadoop configuration object\n+   * @return True if it is successfull; False otherwise.\n+   */\n+  boolean runDistCpWithSnapshots(String snap1, String snap2,\n+      List<Path> srcPaths, Path dst, Configuration conf) throws IOException;\n+\n+  /**\n+   * Copies a source dir/file to a destination by orchestrating the copy between hdfs nodes.\n+   * This distributed process is meant to copy huge files that could take some time if a single\n+   * copy is done. This method allows to specify usage of -diff feature of\n+   * distcp. This is a variation which allows proxying as a different\n+   * user to perform\n+   * the distcp, and requires that the caller have requisite proxy user privileges.\n+   * @param snap1     initial snapshot\n+   * @param snap2     final snapshot\n+   * @param srcPaths  List of Path to the source files or directories to copy\n+   * @param dst       Path to the destination file or directory\n+   * @param conf      The hadoop configuration object\n+   * @param proxyUser The user to perform the distcp as\n+   * @return True if it is successfull; False otherwise.\n+   */\n+  boolean runDistCpWithSnapshotsAs(String snap1, String snap2,\n+      List<Path> srcPaths, Path dst, Configuration conf,\n+      UserGroupInformation proxyUser) throws IOException;\n+\n+\n   /**\n    * This interface encapsulates methods used to get encryption information from\n    * HDFS paths."
  }
]
