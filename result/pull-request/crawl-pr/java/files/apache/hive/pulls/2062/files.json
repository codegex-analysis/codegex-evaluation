[
  {
    "sha": "d78376be8e88cd65690efdf3d873ce7e45610407",
    "filename": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java",
    "status": "modified",
    "additions": 12,
    "deletions": 3,
    "changes": 15,
    "blob_url": "https://github.com/apache/hive/blob/e11aba0e72917b739c613177a20646640bdcf8ce/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java",
    "raw_url": "https://github.com/apache/hive/raw/e11aba0e72917b739c613177a20646640bdcf8ce/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java?ref=e11aba0e72917b739c613177a20646640bdcf8ce",
    "patch": "@@ -31,9 +31,11 @@\n import org.apache.hadoop.hive.metastore.api.Table;\n import org.apache.hadoop.hive.metastore.api.TxnType;\n import org.apache.hadoop.hive.metastore.conf.MetastoreConf;\n+import org.apache.hadoop.hive.metastore.metrics.MetricsConstants;\n import org.apache.hadoop.hive.metastore.txn.TxnStatus;\n import org.apache.hadoop.hive.ql.io.AcidDirectory;\n import org.apache.hadoop.hive.ql.io.AcidUtils;\n+import org.apache.hadoop.hive.ql.log.PerfLogger;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n import org.apache.hive.common.util.Ref;\n import org.apache.thrift.TException;\n@@ -387,12 +389,14 @@ private ExecutorService getTimeoutHandlingExecutor() {\n    * in case of timeout.\n    * @param computeStats If true then for MR compaction the stats are regenerated\n    * @return Returns true, if there was compaction in the queue, and we started working on it.\n-   * @throws InterruptedException is thrown when the process is interrupted because of timeout for example\n    */\n   @VisibleForTesting\n-  protected Boolean findNextCompactionAndExecute(boolean computeStats) throws InterruptedException {\n+  protected Boolean findNextCompactionAndExecute(boolean computeStats) {\n     // Make sure nothing escapes this run method and kills the metastore at large,\n     // so wrap it in a big catch Throwable statement.\n+    PerfLogger perfLogger = SessionState.getPerfLogger(false);\n+    String workerMetric = null;\n+\n     CompactionHeartbeater heartbeater = null;\n     CompactionInfo ci = null;\n     try (CompactionTxn compactionTxn = new CompactionTxn()) {\n@@ -410,9 +414,11 @@ protected Boolean findNextCompactionAndExecute(boolean computeStats) throws Inte\n       if (ci == null) {\n         return false;\n       }\n-\n       checkInterrupt();\n \n+      workerMetric = MetricsConstants.COMPACTION_WORKER_CYCLE + \"_\" + ci.type;\n+      perfLogger.perfLogBegin(CLASS_NAME, workerMetric);\n+\n       // Find the table we will be working with.\n       Table t1;\n       try {\n@@ -569,6 +575,9 @@ protected Boolean findNextCompactionAndExecute(boolean computeStats) throws Inte\n       if (heartbeater != null) {\n         heartbeater.cancel();\n       }\n+      if (workerMetric != null) {\n+        perfLogger.perfLogEnd(CLASS_NAME, workerMetric);\n+      }\n     }\n     return true;\n   }"
  },
  {
    "sha": "dcf1d1edd8ea0fd949d074daf6451f39334cc3ac",
    "filename": "ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestCompactionMetrics.java",
    "status": "modified",
    "additions": 58,
    "deletions": 8,
    "changes": 66,
    "blob_url": "https://github.com/apache/hive/blob/e11aba0e72917b739c613177a20646640bdcf8ce/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestCompactionMetrics.java",
    "raw_url": "https://github.com/apache/hive/raw/e11aba0e72917b739c613177a20646640bdcf8ce/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestCompactionMetrics.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestCompactionMetrics.java?ref=e11aba0e72917b739c613177a20646640bdcf8ce",
    "patch": "@@ -17,6 +17,9 @@\n  */\n package org.apache.hadoop.hive.ql.txn.compactor;\n \n+import org.apache.hadoop.hive.common.metrics.MetricsTestUtils;\n+import org.apache.hadoop.hive.common.metrics.common.MetricsFactory;\n+import org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics;\n import com.google.common.collect.Lists;\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.metastore.api.CommitTxnRequest;\n@@ -51,12 +54,13 @@\n   private static final String INITIATED_METRICS_KEY = MetricsConstants.COMPACTION_STATUS_PREFIX + TxnStore.INITIATED_RESPONSE;\n   private static final String INITIATOR_CYCLE_KEY = MetricsConstants.API_PREFIX + MetricsConstants.COMPACTION_INITIATOR_CYCLE;\n   private static final String CLEANER_CYCLE_KEY = MetricsConstants.API_PREFIX + MetricsConstants.COMPACTION_CLEANER_CYCLE;\n+  private static final String WORKER_CYCLE_KEY = MetricsConstants.API_PREFIX + MetricsConstants.COMPACTION_WORKER_CYCLE;\n \n   @Test\n-  public void testInitiatorMetricsEnabled() throws Exception {\n+  public void testInitiatorPerfMetricsEnabled() throws Exception {\n     MetastoreConf.setBoolVar(conf, MetastoreConf.ConfVars.METRICS_ENABLED, true);\n     Metrics.initialize(conf);\n-    int originalValue = Metrics.getOrCreateGauge(INITIATED_METRICS_KEY).intValue();\n+    Metrics.getOrCreateGauge(INITIATED_METRICS_KEY).set(0);\n     long initiatorCycles = Objects.requireNonNull(Metrics.getOrCreateTimer(INITIATOR_CYCLE_KEY)).getCount();\n     Table t = newTable(\"default\", \"ime\", true);\n     List<LockComponent> components = new ArrayList<>();\n@@ -95,12 +99,12 @@ public void testInitiatorMetricsEnabled() throws Exception {\n \n     runAcidMetricService();\n \n-    Assert.assertEquals(originalValue + 10,\n+    Assert.assertEquals(10,\n         Metrics.getOrCreateGauge(INITIATED_METRICS_KEY).intValue());\n   }\n \n   @Test\n-  public void testInitiatorMetricsDisabled() throws Exception {\n+  public void testInitiatorPerfMetricsDisabled() throws Exception {\n     MetastoreConf.setBoolVar(conf, MetastoreConf.ConfVars.METRICS_ENABLED, false);\n     Metrics.initialize(conf);\n     int originalValue = Metrics.getOrCreateGauge(INITIATED_METRICS_KEY).intValue();\n@@ -137,17 +141,17 @@ public void testInitiatorMetricsDisabled() throws Exception {\n     ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n     List<ShowCompactResponseElement> compacts = rsp.getCompacts();\n     Assert.assertEquals(10, compacts.size());\n+    Assert.assertEquals(initiatorCycles,\n+        Objects.requireNonNull(Metrics.getOrCreateTimer(INITIATOR_CYCLE_KEY)).getCount());\n \n     runAcidMetricService();\n \n     Assert.assertEquals(originalValue,\n         Metrics.getOrCreateGauge(INITIATED_METRICS_KEY).intValue());\n-    Assert.assertEquals(initiatorCycles,\n-        Objects.requireNonNull(Metrics.getOrCreateTimer(INITIATOR_CYCLE_KEY)).getCount());\n   }\n \n   @Test\n-  public void testCleanerMetricsEnabled() throws Exception {\n+  public void testCleanerPerfMetricsEnabled() throws Exception {\n     MetastoreConf.setBoolVar(conf, MetastoreConf.ConfVars.METRICS_ENABLED, true);\n     Metrics.initialize(conf);\n \n@@ -209,7 +213,7 @@ public void testCleanerMetricsEnabled() throws Exception {\n   }\n \n   @Test\n-  public void testCleanerMetricsDisabled() throws Exception {\n+  public void testCleanerPerfMetricsDisabled() throws Exception {\n     MetastoreConf.setBoolVar(conf, MetastoreConf.ConfVars.METRICS_ENABLED, false);\n     Metrics.initialize(conf);\n \n@@ -241,6 +245,52 @@ public void testCleanerMetricsDisabled() throws Exception {\n         Metrics.getOrCreateTimer(CLEANER_CYCLE_KEY + \"_\" + CompactionType.MAJOR)).getCount());\n   }\n \n+  @Test\n+  public void testWorkerPerfMetrics() throws Exception {\n+    HiveConf.setBoolVar(conf, HiveConf.ConfVars.HIVE_SERVER2_METRICS_ENABLED, true);\n+    MetricsFactory.close();\n+    MetricsFactory.init(conf);\n+\n+    conf.setIntVar(HiveConf.ConfVars.COMPACTOR_MAX_NUM_DELTA, 1);\n+    Table t = newTable(\"default\", \"mapwb\", true);\n+    Partition p = newPartition(t, \"today\");\n+\n+    addBaseFile(t, p, 20L, 20);\n+    addDeltaFile(t, p, 21L, 22L, 2);\n+    addDeltaFile(t, p, 23L, 24L, 2);\n+\n+    burnThroughTransactions(\"default\", \"mapwb\", 25);\n+\n+    CompactionRequest rqst = new CompactionRequest(\"default\", \"mapwb\", CompactionType.MINOR);\n+    rqst.setPartitionname(\"ds=today\");\n+    txnHandler.compact(rqst);\n+\n+    startWorker();\n+\n+    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(1, rsp.getCompactsSize());\n+    Assert.assertEquals(TxnStore.CLEANING_RESPONSE, rsp.getCompacts().get(0).getState());\n+\n+    CodahaleMetrics metrics = (CodahaleMetrics) MetricsFactory.getInstance();\n+    String json = metrics.dumpJson();\n+    MetricsTestUtils.verifyMetricsJson(json, MetricsTestUtils.TIMER,\n+        WORKER_CYCLE_KEY + \"_\" + CompactionType.MINOR, 1);\n+\n+    rqst = new CompactionRequest(\"default\", \"mapwb\", CompactionType.MAJOR);\n+    rqst.setPartitionname(\"ds=today\");\n+    txnHandler.compact(rqst);\n+\n+    startWorker();\n+\n+    rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(2, rsp.getCompactsSize());\n+    Assert.assertEquals(TxnStore.CLEANING_RESPONSE, rsp.getCompacts().get(0).getState());\n+\n+    json = metrics.dumpJson();\n+    MetricsTestUtils.verifyMetricsJson(json, MetricsTestUtils.TIMER,\n+        WORKER_CYCLE_KEY + \"_\" + CompactionType.MAJOR, 1);\n+  }\n+\n   @Test\n   public void testUpdateCompactionMetrics() {\n     Metrics.initialize(conf);"
  },
  {
    "sha": "2375fb7c6ab5efee69bd1a34db72bdd46b608837",
    "filename": "ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java",
    "status": "modified",
    "additions": 23,
    "deletions": 29,
    "changes": 52,
    "blob_url": "https://github.com/apache/hive/blob/e11aba0e72917b739c613177a20646640bdcf8ce/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java",
    "raw_url": "https://github.com/apache/hive/raw/e11aba0e72917b739c613177a20646640bdcf8ce/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java?ref=e11aba0e72917b739c613177a20646640bdcf8ce",
    "patch": "@@ -59,9 +59,9 @@\n import java.util.List;\n import java.util.Map;\n import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Executors;\n-import java.util.concurrent.Future;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicBoolean;\n \n@@ -73,8 +73,9 @@\n  * Need to change some of these to have better test coverage.\n  */\n public class TestWorker extends CompactorTest {\n-  static final private String CLASS_NAME = TestWorker.class.getName();\n-  static final private Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n+\n+  private static final String CLASS_NAME = TestWorker.class.getName();\n+  private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n \n   @Test\n   public void nothing() throws Exception {\n@@ -1117,34 +1118,23 @@ public void testTimeoutWithoutInterrupt() throws Exception {\n   private void runTimeoutTest(long timeout, boolean runForever, boolean swallowInterrupt) throws Exception {\n     ExecutorService executor = Executors.newSingleThreadExecutor();\n     HiveConf timeoutConf = new HiveConf(conf);\n-    TimeoutWorker timeoutWorker;\n-\n     timeoutConf.setTimeVar(HiveConf.ConfVars.HIVE_COMPACTOR_WORKER_TIMEOUT, timeout, TimeUnit.MILLISECONDS);\n-    timeoutWorker = getTimeoutWorker(timeoutConf, executor, runForever, swallowInterrupt);\n-\n-    // Wait until at least 1st loop is finished\n-    while (!timeoutWorker.looped.get()) {\n-      Thread.sleep(10L);\n-    }\n-\n-    timeoutWorker.looped.set(false);\n-\n-    // Wait until the 2nd loop is finished\n-    while (!timeoutWorker.looped.get()) {\n-      Thread.sleep(10L);\n-    }\n \n+    TimeoutWorker timeoutWorker = getTimeoutWorker(timeoutConf, executor,\n+        runForever, swallowInterrupt, new CountDownLatch(2));\n+    // Wait until the 2nd cycle is finished\n+    timeoutWorker.looped.await();\n     timeoutWorker.stop.set(true);\n     executor.shutdownNow();\n   }\n \n   private TimeoutWorker getTimeoutWorker(HiveConf conf, ExecutorService executor, boolean runForever,\n-      boolean swallowInterrupt) throws Exception {\n-    TimeoutWorker timeoutWorker = new TimeoutWorker(runForever, swallowInterrupt);\n+      boolean swallowInterrupt, CountDownLatch looped) throws Exception {\n+    TimeoutWorker timeoutWorker = new TimeoutWorker(runForever, swallowInterrupt, looped);\n     timeoutWorker.setThreadId((int)timeoutWorker.getId());\n     timeoutWorker.setConf(conf);\n     timeoutWorker.init(new AtomicBoolean(false));\n-    executor.submit(() -> timeoutWorker.run());\n+    executor.submit(timeoutWorker);\n     return timeoutWorker;\n   }\n \n@@ -1156,29 +1146,33 @@ public void tearDown() throws Exception {\n   private static final class TimeoutWorker extends Worker {\n     private boolean runForever;\n     private boolean swallowInterrupt;\n-    private AtomicBoolean looped;\n+    private CountDownLatch looped;\n \n-    private TimeoutWorker(boolean runForever, boolean swallowInterrupt) {\n+    private TimeoutWorker(boolean runForever, boolean swallowInterrupt, CountDownLatch looped) {\n       this.runForever = runForever;\n       this.swallowInterrupt = swallowInterrupt;\n-      this.looped = new AtomicBoolean(false);\n+      this.looped = looped;\n     }\n \n-    protected Boolean findNextCompactionAndExecute(boolean computeStats) throws InterruptedException {\n-      looped.set(true);\n+    @Override\n+    protected Boolean findNextCompactionAndExecute(boolean computeStats) {\n       if (runForever) {\n         while (!stop.get()) {\n           try {\n-            looped.set(true);\n             Thread.sleep(Long.MAX_VALUE);\n           } catch (InterruptedException ie) {\n             if (!swallowInterrupt) {\n-              throw ie;\n+              break;\n+            }\n+            try {\n+              Thread.sleep(Long.MAX_VALUE);\n+            } catch (InterruptedException e) {\n             }\n-            Thread.sleep(Long.MAX_VALUE);\n           }\n+          looped.countDown();\n         }\n       }\n+      looped.countDown();\n       return true;\n     }\n   }"
  },
  {
    "sha": "8a77c733d133c6d2c58be0ae75a33774123a76dc",
    "filename": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/metrics/MetricsConstants.java",
    "status": "modified",
    "additions": 1,
    "deletions": 0,
    "changes": 1,
    "blob_url": "https://github.com/apache/hive/blob/e11aba0e72917b739c613177a20646640bdcf8ce/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/metrics/MetricsConstants.java",
    "raw_url": "https://github.com/apache/hive/raw/e11aba0e72917b739c613177a20646640bdcf8ce/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/metrics/MetricsConstants.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/metrics/MetricsConstants.java?ref=e11aba0e72917b739c613177a20646640bdcf8ce",
    "patch": "@@ -24,6 +24,7 @@\n   public static final String COMPACTION_OLDEST_ENQUEUE_AGE = \"compaction_oldest_enqueue_age_in_sec\";\n   public static final String COMPACTION_INITIATOR_CYCLE = \"compaction_initiator_cycle\";\n   public static final String COMPACTION_CLEANER_CYCLE = \"compaction_cleaner_cycle\";\n+  public static final String COMPACTION_WORKER_CYCLE = \"compaction_worker_cycle\";\n \n   public static final String TOTAL_API_CALLS = \"total_api_calls\";\n "
  }
]
