[
  {
    "sha": "dfc3187efbd4156cb271129f8877d6455d97a861",
    "filename": "accumulo-handler/pom.xml",
    "status": "modified",
    "additions": 4,
    "deletions": 0,
    "changes": 4,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/accumulo-handler/pom.xml",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/accumulo-handler/pom.xml",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/accumulo-handler/pom.xml?ref=01a0201274b624c64c5a60c2421ef79f91f60828",
    "patch": "@@ -100,6 +100,10 @@\n                 <groupId>org.eclipse.jetty.orbit</groupId>\n                 <artifactId>javax.servlet</artifactId>\n             </exclusion>\n+            <exclusion>\n+              <groupId>org.pac4j</groupId>\n+              <artifactId>*</artifactId>\n+            </exclusion>\n         </exclusions>\n     </dependency>\n     <dependency>"
  },
  {
    "sha": "42cc87c1bb6092b4981f2e6a51533a66e464e55b",
    "filename": "beeline/src/java/org/apache/hive/beeline/Commands.java",
    "status": "modified",
    "additions": 2,
    "deletions": 1,
    "changes": 3,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/beeline/src/java/org/apache/hive/beeline/Commands.java",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/beeline/src/java/org/apache/hive/beeline/Commands.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/beeline/src/java/org/apache/hive/beeline/Commands.java?ref=01a0201274b624c64c5a60c2421ef79f91f60828",
    "patch": "@@ -1659,7 +1659,8 @@ public boolean connect(Properties props) throws IOException {\n     }\n \n     beeLine.info(\"Connecting to \" + url);\n-    if (Utils.parsePropertyFromUrl(url, JdbcConnectionParams.AUTH_PRINCIPAL) == null) {\n+    if (Utils.parsePropertyFromUrl(url, JdbcConnectionParams.AUTH_PRINCIPAL) == null\n+        && !JdbcConnectionParams.AUTH_SSO_BROWSER_MODE.equals(auth)) {\n       String urlForPrompt = url.substring(0, url.contains(\";\") ? url.indexOf(';') : url.length());\n       if (username == null) {\n         username = beeLine.getConsoleReader().readLine(\"Enter username for \" + urlForPrompt + \": \");"
  },
  {
    "sha": "1e85bcf28145c5041211c96bbe3cd556ff5bea90",
    "filename": "cli/pom.xml",
    "status": "modified",
    "additions": 6,
    "deletions": 0,
    "changes": 6,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/cli/pom.xml",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/cli/pom.xml",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/cli/pom.xml?ref=01a0201274b624c64c5a60c2421ef79f91f60828",
    "patch": "@@ -53,6 +53,12 @@\n       <groupId>org.apache.hive</groupId>\n       <artifactId>hive-service</artifactId>\n       <version>${project.version}</version>\n+      <exclusions>\n+        <exclusion>\n+          <groupId>org.pac4j</groupId>\n+          <artifactId>*</artifactId>\n+        </exclusion>\n+      </exclusions>\n     </dependency>\n     <dependency>\n       <groupId>org.apache.hive</groupId>"
  },
  {
    "sha": "6a3e706f2569c42fa27ad54147bb4d2205c6ff17",
    "filename": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java",
    "status": "modified",
    "additions": 73,
    "deletions": 4,
    "changes": 77,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java?ref=01a0201274b624c64c5a60c2421ef79f91f60828",
    "patch": "@@ -603,6 +603,11 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal\n         \"This is the fully qualified base directory on the target/replica warehouse under which data for \"\n             + \"external tables is stored. This is relative base path and hence prefixed to the source \"\n             + \"external table path on target cluster.\"),\n+    REPL_EXTERNAL_WAREHOUSE_SINGLE_COPY_TASK(\"hive.repl.external.warehouse.single.copy.task\",\n+        false, \"Should create single copy task for all the external tables \"\n+        + \"within the database default location for external tables, Would require more memory \"\n+        + \"for preparing the initial listing, Should be used if the memory \"\n+        + \"requirements can be fulfilled.\"),\n     REPL_INCLUDE_AUTHORIZATION_METADATA(\"hive.repl.include.authorization.metadata\", false,\n             \"This configuration will enable security and authorization related metadata along \"\n                     + \"with the hive data and metadata replication. \"),\n@@ -2137,7 +2142,6 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal\n             \"Dynamically allocate some bits from statement id when bucket id overflows. This allows having more than 4096 buckets.\"),\n     HIVETESTMODEACIDKEYIDXSKIP(\"hive.test.acid.key.index.skip\", false, \"For testing only. OrcRecordUpdater will skip \"\n         + \"generation of the hive.acid.key.index\", false),\n-\n     HIVEMERGEMAPFILES(\"hive.merge.mapfiles\", true,\n         \"Merge small files at the end of a map-only job\"),\n     HIVEMERGEMAPREDFILES(\"hive.merge.mapredfiles\", false,\n@@ -2612,7 +2616,7 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal\n         \"Whether to enable shared work extended optimizer. The optimizer tries to merge equal operators\\n\" +\n         \"after a work boundary after shared work optimizer has been executed. Requires hive.optimize.shared.work\\n\" +\n         \"to be set to true. Tez only.\"),\n-    HIVE_SHARED_WORK_SEMIJOIN_OPTIMIZATION(\"hive.optimize.shared.work.semijoin\", true,\n+    HIVE_SHARED_WORK_SEMIJOIN_OPTIMIZATION(\"hive.optimize.shared.work.semijoin\", false,\n         \"Whether to enable shared work extended optimizer for semijoins. The optimizer tries to merge\\n\" +\n         \"scan operators if one of them reads the full table, even if the other one is the target for\\n\" +\n         \"one or more semijoin edges. Tez only.\"),\n@@ -3904,15 +3908,16 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal\n \n     // HiveServer2 auth configuration\n     HIVE_SERVER2_AUTHENTICATION(\"hive.server2.authentication\", \"NONE\",\n-      new StringSet(\"NOSASL\", \"NONE\", \"LDAP\", \"KERBEROS\", \"PAM\", \"CUSTOM\"),\n+      new StringSet(\"NOSASL\", \"NONE\", \"LDAP\", \"KERBEROS\", \"PAM\", \"CUSTOM\", \"SAML\"),\n         \"Client authentication types.\\n\" +\n         \"  NONE: no authentication check\\n\" +\n         \"  LDAP: LDAP/AD based authentication\\n\" +\n         \"  KERBEROS: Kerberos/GSSAPI authentication\\n\" +\n         \"  CUSTOM: Custom authentication provider\\n\" +\n         \"          (Use with property hive.server2.custom.authentication.class)\\n\" +\n         \"  PAM: Pluggable authentication module\\n\" +\n-        \"  NOSASL:  Raw transport\"),\n+        \"  NOSASL:  Raw transport\\n\" +\n+        \"  SAML2: SAML 2.0 compliant authentication. This is only supported in http transport mode.\"),\n     HIVE_SERVER2_TRUSTED_DOMAIN(\"hive.server2.trusted.domain\", \"\",\n         \"Specifies the host or a domain to trust connections from. Authentication is skipped \" +\n         \"for any connection coming from a host whose hostname ends with the value of this\" +\n@@ -4018,6 +4023,70 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal\n       \"List of the underlying pam services that should be used when auth type is PAM\\n\" +\n       \"A file with the same name must exist in /etc/pam.d\"),\n \n+    // HS2 SAML2.0 configuration\n+    HIVE_SERVER2_SAML_KEYSTORE_PATH(\"hive.server2.saml2.keystore.path\", \"\",\n+        \"Keystore path to the saml2 client. This keystore is used to store the\\n\"\n+            + \" key pair used to sign the authentication requests when hive.server2.saml2.sign.requests\\n\"\n+            + \" is set to true. If the path doesn't exist, HiveServer2 will attempt to\\n\"\n+            + \" create a keystore using the default configurations otherwise it will use\\n\"\n+            + \" the one provided.\"),\n+    HIVE_SERVER2_SAML_KEYSTORE_PASSWORD(\"hive.server2.saml2.keystore.password\", \"\",\n+        \"Password to the keystore used to sign the authentication requests. By default,\\n\"\n+            + \" this must be set to a non-blank value if the authentication mode is SAML.\"),\n+    HIVE_SERVER2_SAML_PRIVATE_KEY_PASSWORD(\"hive.server2.saml2.private.key.password\", \"\",\n+        \"Password for the private key which is stored in the keystore pointed \\n\"\n+            + \" by hive.server2.saml2.keystore.path. This key is used to sign the authentication request\\n\"\n+            + \" if hive.server2.saml2.sign.requests is set to true.\"),\n+    HIVE_SERVER2_SAML_IDP_METADATA(\"hive.server2.saml2.idp.metadata\", \"\",\n+        \"IDP metadata file for the SAML configuration. This metadata file must be\\n\"\n+            + \" exported from the external identity provider. This is used to validate the SAML assertions\\n\"\n+            + \" received by HiveServer2.\"),\n+    HIVE_SERVER2_SAML_SP_ID(\"hive.server2.saml2.sp.entity.id\", \"\",\n+        \"Service provider entity id for this HiveServer2. This must match with the\\n\"\n+            + \" SP id on the external identity provider. If this is not set, HiveServer2 will use the\\n\"\n+            + \" callback url as the SP id.\"),\n+    HIVE_SERVER2_SAML_FORCE_AUTH(\"hive.server2.saml2.sp.force.auth\", \"false\",\n+        \"This is a boolean configuration which toggles the force authentication\\n\"\n+            + \" flag in the SAML authentication request. When set to true, the request generated\\n\"\n+            + \" to the IDP will ask the IDP to force the authentication again.\"),\n+    HIVE_SERVER2_SAML_AUTHENTICATION_LIFETIME(\n+        \"hive.server2.saml2.max.authentication.lifetime\", \"1h\",\n+        \"This configuration can be used to set the lifetime of the\\n\"\n+            + \" authentication response from IDP. Generally the IDP will not ask\\n\"\n+            + \" you enter credentials if you have a authenticated session with it already.\\n\"\n+            + \" The IDP will automatically generate an assertion in such a case. This configuration\\n\"\n+            + \" can be used to set the time limit for such assertions. Assertions which are\\n\"\n+            + \" older than this value will not be accepted by HiveServer2. The default\\n\"\n+            + \" is one hour.\"),\n+    HIVE_SERVER2_SAML_BLACKLISTED_SIGNATURE_ALGORITHMS(\n+        \"hive.server2.saml2.blacklisted.signature.algorithms\", \"\",\n+        \"Comma separated list of signature algorithm names which are not\\n\"\n+            + \" allowed by HiveServer2 during validation of the assertions received from IDP\"),\n+    HIVE_SERVER2_SAML_ACS_INDEX(\"hive.server2.saml2.acs.index\", \"\",\n+        \"This configuration specifies the assertion consumer service (ACS)\\n\"\n+            + \" index to be sent to the IDP in case it support multiple ACS URLs. This\\n\"\n+            + \" will also be used to pick the ACS URL from the IDP metadata for validation.\"),\n+    HIVE_SERVER2_SAML_CALLBACK_URL(\"hive.server2.saml2.sp.callback.url\", \"\",\n+        \"Callback URL where SAML responses should be posted. Currently this\\n\" +\n+            \" must be configured at the same port number as defined by hive.server2.thrift.http.port.\"),\n+    HIVE_SERVER2_SAML_WANT_ASSERTIONS_SIGNED(\"hive.server2.saml2.want.assertions.signed\", true,\n+        \"When this configuration is set to true, hive server2 will validate the signature\\n\"\n+            + \" of the assertions received at the callback url. For security reasons, it is recommended\"\n+            + \"that this value should be true.\"),\n+    HIVE_SERVER2_SAML_SIGN_REQUESTS(\"hive.server2.saml2.sign.requests\", false,\n+        \"When this configuration is set to true, HiveServer2 will sign the SAML requests\\n\" +\n+            \" which can be validated by the IDP provider.\"),\n+    HIVE_SERVER2_SAML_CALLBACK_TOKEN_TTL(\"hive.server2.saml2.callback.token.ttl\", \"30s\",\n+        new TimeValidator(TimeUnit.MILLISECONDS), \"Time for which the token issued by\\n\"\n+        + \"service provider is valid.\"),\n+    HIVE_SERVER2_SAML_GROUP_ATTRIBUTE_NAME(\"hive.server2.saml2.group.attribute.name\",\n+        \"\", \"The attribute name in the SAML assertion which would\\n\"\n+        + \" be used to compare for the group name matching. By default it is empty\\n\"\n+        + \" which would allow any authenticated user. If this value is set then\\n\"\n+            + \" then hive.server2.saml2.group.filter must be set to a non-empty value.\"),\n+    HIVE_SERVER2_SAML_GROUP_FILTER(\"hive.server2.saml2.group.filter\", \"\",\n+        \"Comma separated list of group names which will be allowed when SAML\\n\"\n+            + \" authentication is enabled.\"),\n     HIVE_SERVER2_ENABLE_DOAS(\"hive.server2.enable.doAs\", true,\n         \"Setting this property to true will have HiveServer2 execute\\n\" +\n         \"Hive operations as the user making the calls to it.\"),"
  },
  {
    "sha": "2a14e7a73edeee5d77f64bcc9506d0fe46fc3473",
    "filename": "data/conf/llap/hive-site.xml",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/data/conf/llap/hive-site.xml",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/data/conf/llap/hive-site.xml",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/data/conf/llap/hive-site.xml?ref=01a0201274b624c64c5a60c2421ef79f91f60828",
    "patch": "@@ -186,7 +186,7 @@\n \n <property>\n   <name>hive.exec.post.hooks</name>\n-  <value>org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, org.apache.hadoop.hive.ql.hooks.RuntimeStatsPersistenceCheckerHook, org.apache.hadoop.hive.ql.hooks.NoOperatorReuseCheckerHook</value>\n+  <value>org.apache.hadoop.hive.ql.hooks.PostExecutePrinter, org.apache.hadoop.hive.ql.hooks.RuntimeStatsPersistenceCheckerHook, org.apache.hadoop.hive.ql.hooks.OperatorHealthCheckerHook</value>\n   <description>Post Execute Hook for Tests</description>\n </property>\n "
  },
  {
    "sha": "bd205ff0136a8a7f7302aab907c7c6b11152615a",
    "filename": "data/files/orc_compressed",
    "status": "added",
    "additions": 0,
    "deletions": 0,
    "changes": 0,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/data/files/orc_compressed",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/data/files/orc_compressed",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/data/files/orc_compressed?ref=01a0201274b624c64c5a60c2421ef79f91f60828"
  },
  {
    "sha": "84990e2ddb1cb0e16ecad072b10b6725280de9bc",
    "filename": "data/files/orc_uncompressed",
    "status": "added",
    "additions": 0,
    "deletions": 0,
    "changes": 0,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/data/files/orc_uncompressed",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/data/files/orc_uncompressed",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/data/files/orc_uncompressed?ref=01a0201274b624c64c5a60c2421ef79f91f60828"
  },
  {
    "sha": "c6441410686fffa242739da2333df85920138631",
    "filename": "druid-handler/src/test/org/apache/hadoop/hive/druid/package-info.java",
    "status": "removed",
    "additions": 0,
    "deletions": 22,
    "changes": 22,
    "blob_url": "https://github.com/apache/hive/blob/5145468bc3cbe36c2b0e18cdd8ec435b753faabf/druid-handler/src/test/org/apache/hadoop/hive/druid/package-info.java",
    "raw_url": "https://github.com/apache/hive/raw/5145468bc3cbe36c2b0e18cdd8ec435b753faabf/druid-handler/src/test/org/apache/hadoop/hive/druid/package-info.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/druid-handler/src/test/org/apache/hadoop/hive/druid/package-info.java?ref=5145468bc3cbe36c2b0e18cdd8ec435b753faabf",
    "patch": "@@ -1,22 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-/**\n- * Package info.\n- */\n-package org.apache.hadoop.hive.druid;"
  },
  {
    "sha": "90742c9936c2cacc04c42b6a3f02e0097cf271c4",
    "filename": "hplsql/src/main/antlr4/org/apache/hive/hplsql/Hplsql.g4",
    "status": "modified",
    "additions": 2,
    "deletions": 0,
    "changes": 2,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/main/antlr4/org/apache/hive/hplsql/Hplsql.g4",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/main/antlr4/org/apache/hive/hplsql/Hplsql.g4",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/hplsql/src/main/antlr4/org/apache/hive/hplsql/Hplsql.g4?ref=01a0201274b624c64c5a60c2421ef79f91f60828",
    "patch": "@@ -492,6 +492,8 @@ create_routine_option :\n      \n drop_stmt :             // DROP statement\n        T_DROP T_TABLE (T_IF T_EXISTS)? table_name\n+     | T_DROP T_PACKAGE (T_IF T_EXISTS)? ident\n+     | T_DROP (T_PROCEDURE | T_FUNCTION) (T_IF T_EXISTS)? ident\n      | T_DROP (T_DATABASE | T_SCHEMA) (T_IF T_EXISTS)? expr\n      ;\n "
  },
  {
    "sha": "ca6bf92e54ddf62c0a13e2c31df1993be1e3a4e7",
    "filename": "hplsql/src/main/java/org/apache/hive/hplsql/Exec.java",
    "status": "modified",
    "additions": 90,
    "deletions": 36,
    "changes": 126,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/main/java/org/apache/hive/hplsql/Exec.java",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/main/java/org/apache/hive/hplsql/Exec.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/hplsql/src/main/java/org/apache/hive/hplsql/Exec.java?ref=01a0201274b624c64c5a60c2421ef79f91f60828",
    "patch": "@@ -29,6 +29,7 @@\n import java.util.HashMap;\n import java.util.Map;\n import java.util.Map.Entry;\n+import java.util.Optional;\n import java.util.Stack;\n \n import org.antlr.v4.runtime.ANTLRInputStream;\n@@ -47,13 +48,16 @@\n import org.apache.hive.hplsql.executor.QueryExecutor;\n import org.apache.hive.hplsql.executor.QueryResult;\n import org.apache.hive.hplsql.functions.BuiltinFunctions;\n-import org.apache.hive.hplsql.functions.Function;\n+import org.apache.hive.hplsql.functions.FunctionRegistry;\n import org.apache.hive.hplsql.functions.FunctionDatetime;\n import org.apache.hive.hplsql.functions.FunctionMisc;\n import org.apache.hive.hplsql.functions.FunctionOra;\n import org.apache.hive.hplsql.functions.FunctionString;\n-import org.apache.hive.hplsql.functions.HmsFunction;\n-import org.apache.hive.hplsql.functions.InMemoryFunction;\n+import org.apache.hive.hplsql.functions.HmsFunctionRegistry;\n+import org.apache.hive.hplsql.functions.InMemoryFunctionRegistry;\n+import org.apache.hive.hplsql.packages.HmsPackageRegistry;\n+import org.apache.hive.hplsql.packages.InMemoryPackageRegistry;\n+import org.apache.hive.hplsql.packages.PackageRegistry;\n \n /**\n  * HPL/SQL script executor\n@@ -70,11 +74,13 @@\n   Exec exec;\n   ParseTree tree = null;\n   private IMetaStoreClient msc;\n-  Function function;\n+  FunctionRegistry functions;\n   private BuiltinFunctions builtinFunctions;\n   private ResultListener resultListener = ResultListener.NONE;\n   QueryExecutor queryExecutor;\n   private HplSqlSessionState hplSqlSession;\n+  private PackageRegistry packageRegistry = new InMemoryPackageRegistry();\n+  private boolean packageLoading = false;\n \n   public enum OnError {EXCEPTION, SETERROR, STOP}\n \n@@ -328,16 +334,16 @@ public String callStackPop() {\n    */\n   public Var findVariable(String name) {\n     Var var;\n-    String name1 = name;\n+    String name1 = name.toUpperCase();\n     String name1a = null;\n     String name2 = null;\n     Scope cur = exec.currentScope;\n     Package pack;\n     Package packCallContext = exec.getPackageCallContext();\n     ArrayList<String> qualified = exec.meta.splitIdentifier(name);\n     if (qualified != null) {\n-      name1 = qualified.get(0);\n-      name2 = qualified.get(1);\n+      name1 = qualified.get(0).toUpperCase();\n+      name2 = qualified.get(1).toUpperCase();\n       pack = findPackage(name1);\n       if (pack != null) {        \n         var = pack.findVariable(name2);\n@@ -404,6 +410,24 @@ public Var findCursor(String name) {\n    * Find the package by name\n    */\n   Package findPackage(String name) {\n+    Package pkg = packages.get(name.toUpperCase());\n+    if (pkg != null) {\n+      return pkg;\n+    }\n+    Optional<String> source = exec.packageRegistry.getPackage(name);\n+    if (source.isPresent()) {\n+      HplsqlLexer lexer = new HplsqlLexer(new ANTLRInputStream(source.get()));\n+      CommonTokenStream tokens = new CommonTokenStream(lexer);\n+      HplsqlParser parser = new HplsqlParser(tokens);\n+      exec.packageLoading = true;\n+      try {\n+        visit(parser.program());\n+      } finally {\n+        exec.packageLoading = false;\n+      }\n+    } else {\n+      return null;\n+    }\n     return packages.get(name.toUpperCase());\n   }\n   \n@@ -751,7 +775,7 @@ public Var run() {\n       initRoutines = true;\n       visit(tree);\n       initRoutines = false;\n-      exec.function.exec(execMain, null);\n+      exec.functions.exec(execMain.toUpperCase(), null);\n     }\n     else {\n       visit(tree);\n@@ -790,9 +814,10 @@ public Integer init(String[] args) throws Exception {\n     new FunctionString(this, queryExecutor).register(builtinFunctions);\n     new FunctionOra(this, queryExecutor).register(builtinFunctions);\n     if (msc != null) {\n-      function = new HmsFunction(this, msc, builtinFunctions, hplSqlSession);\n+      functions = new HmsFunctionRegistry(this, msc, builtinFunctions, hplSqlSession);\n+      packageRegistry = new HmsPackageRegistry(msc, hplSqlSession);\n     } else {\n-      function = new InMemoryFunction(this, builtinFunctions);\n+      functions = new InMemoryFunctionRegistry(this, builtinFunctions);\n     }\n     addVariable(new Var(ERRORCODE, Var.Type.BIGINT, 0L));\n     addVariable(new Var(SQLCODE, Var.Type.BIGINT, 0L));\n@@ -1414,7 +1439,7 @@ public Integer visitCreate_database_stmt(HplsqlParser.Create_database_stmtContex\n    */\n   @Override \n   public Integer visitCreate_function_stmt(HplsqlParser.Create_function_stmtContext ctx) {\n-    exec.function.addUserFunction(ctx);\n+    exec.functions.addUserFunction(ctx);\n     addLocalUdf(ctx);\n     return 0; \n   }\n@@ -1425,42 +1450,71 @@ public Integer visitCreate_function_stmt(HplsqlParser.Create_function_stmtContex\n   @Override \n   public Integer visitCreate_package_stmt(HplsqlParser.Create_package_stmtContext ctx) { \n     String name = ctx.ident(0).getText().toUpperCase();\n-    exec.currentPackageDecl = new Package(name, exec, builtinFunctions);\n-    exec.packages.put(name, exec.currentPackageDecl);\n-    trace(ctx, \"CREATE PACKAGE\");\n-    exec.currentPackageDecl.createSpecification(ctx);\n-    exec.currentPackageDecl = null;\n+    if (exec.packageLoading) {\n+      exec.currentPackageDecl = new Package(name, exec, builtinFunctions);\n+      exec.packages.put(name, exec.currentPackageDecl);\n+      exec.currentPackageDecl.createSpecification(ctx);\n+      exec.currentPackageDecl = null;\n+    } else {\n+      trace(ctx, \"CREATE PACKAGE\");\n+      exec.packages.remove(name);\n+      exec.packageRegistry.createPackageHeader(name, getFormattedText(ctx), ctx.T_REPLACE() != null);\n+    }\n     return 0; \n   }\n \n   /**\n    * CREATE PACKAGE body statement\n    */\n   @Override \n-  public Integer visitCreate_package_body_stmt(HplsqlParser.Create_package_body_stmtContext ctx) { \n+  public Integer visitCreate_package_body_stmt(HplsqlParser.Create_package_body_stmtContext ctx) {\n     String name = ctx.ident(0).getText().toUpperCase();\n-    exec.currentPackageDecl = exec.packages.get(name);\n-    if (exec.currentPackageDecl == null) {\n-      exec.currentPackageDecl = new Package(name, exec, builtinFunctions);\n-      exec.currentPackageDecl.setAllMembersPublic(true);\n-      exec.packages.put(name, exec.currentPackageDecl);\n+    if (exec.packageLoading) {\n+      exec.currentPackageDecl = exec.packages.get(name);\n+      if (exec.currentPackageDecl == null) {\n+        exec.currentPackageDecl = new Package(name, exec, builtinFunctions);\n+        exec.currentPackageDecl.setAllMembersPublic(true);\n+        exec.packages.put(name, exec.currentPackageDecl);\n+      }\n+      exec.currentPackageDecl.createBody(ctx);\n+      exec.currentPackageDecl = null;\n+    } else {\n+      trace(ctx, \"CREATE PACKAGE BODY\");\n+      exec.packages.remove(name);\n+      exec.packageRegistry.createPackageBody(name,  getFormattedText(ctx), ctx.T_REPLACE() != null);\n     }\n-    trace(ctx, \"CREATE PACKAGE BODY\");\n-    exec.currentPackageDecl.createBody(ctx);\n-    exec.currentPackageDecl = null;\n-    return 0; \n+    return 0;\n   }\n \n   /**\n    * CREATE PROCEDURE statement\n    */\n   @Override \n   public Integer visitCreate_procedure_stmt(HplsqlParser.Create_procedure_stmtContext ctx) {\n-    exec.function.addUserProcedure(ctx);\n+    exec.functions.addUserProcedure(ctx);\n     addLocalUdf(ctx);                      // Add procedures as they can be invoked by functions\n     return 0; \n   }\n-  \n+\n+  public void dropProcedure(HplsqlParser.Drop_stmtContext ctx, String name, boolean checkIfExists) {\n+    if (checkIfExists && !functions.exists(name)) {\n+      trace(ctx, name + \" DOES NOT EXIST\");\n+      return;\n+    }\n+    functions.remove(name);\n+    trace(ctx, name + \" DROPPED\");\n+  }\n+\n+  public void dropPackage(HplsqlParser.Drop_stmtContext ctx, String name, boolean checkIfExists) {\n+    if (checkIfExists && !packageRegistry.getPackage(name).isPresent()) {\n+      trace(ctx, name + \" DOES NOT EXIST\");\n+      return;\n+    }\n+    packages.remove(name);\n+    packageRegistry.dropPackage(name);\n+    trace(ctx, name + \" DROPPED\");\n+  }\n+\n   /**\n    * CREATE INDEX statement\n    */\n@@ -1633,11 +1687,11 @@ public Integer visitExpr_cursor_attribute(HplsqlParser.Expr_cursor_attributeCont\n    */\n   @Override \n   public Integer visitExpr_func(HplsqlParser.Expr_funcContext ctx) {\n-    String name = ctx.ident().getText();  \n+    String name = ctx.ident().getText();\n     if (exec.buildSql) {\n       exec.execSql(name, ctx.expr_func_params());\n-    }\n-    else {\n+    } else {\n+      name = name.toUpperCase();\n       Package packCallContext = exec.getPackageCallContext();\n       ArrayList<String> qualified = exec.meta.splitIdentifier(name);\n       boolean executed = false;\n@@ -1651,7 +1705,7 @@ public Integer visitExpr_func(HplsqlParser.Expr_funcContext ctx) {\n         executed = packCallContext.execFunc(name, ctx.expr_func_params());\n       }\n       if (!executed) {        \n-        exec.function.exec(name, ctx.expr_func_params());\n+        exec.functions.exec(name, ctx.expr_func_params());\n       }\n     }\n     return 0;\n@@ -1685,7 +1739,7 @@ public void execSql(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n    * For example converts: select fn(col) from table to select hplsql('fn(:1)', col) from table\n    */\n   private boolean execUserSql(HplsqlParser.Expr_func_paramsContext ctx, String name) {\n-    if (!function.exists(name)) {\n+    if (!functions.exists(name)) {\n       return false;\n     }\n     StringBuilder sql = new StringBuilder();\n@@ -1826,7 +1880,7 @@ public Integer visitExec_stmt(HplsqlParser.Exec_stmtContext ctx) {\n    */\n   @Override \n   public Integer visitCall_stmt(HplsqlParser.Call_stmtContext ctx) {\n-    String name = ctx.ident().getText();\n+    String name = ctx.ident().getText().toUpperCase();\n     Package packCallContext = exec.getPackageCallContext();\n     ArrayList<String> qualified = exec.meta.splitIdentifier(name);\n     exec.inCallStmt = true;    \n@@ -1841,7 +1895,7 @@ public Integer visitCall_stmt(HplsqlParser.Call_stmtContext ctx) {\n       executed = packCallContext.execProc(name, ctx.expr_func_params(), false /*trace error if not exists*/);\n     }\n     if (!executed) {\n-      exec.function.exec(name, ctx.expr_func_params());\n+      exec.functions.exec(name, ctx.expr_func_params());\n     }\n     exec.inCallStmt = false;\n     return 0;\n@@ -2212,7 +2266,7 @@ public Integer visitIdent(HplsqlParser.IdentContext ctx) {\n       }\n     }\n     else {\n-      if (!exec.buildSql && !exec.inCallStmt && exec.function.exec(ident, null)) {\n+      if (!exec.buildSql && !exec.inCallStmt && exec.functions.exec(ident.toUpperCase(), null)) {\n         return 0;\n       } else {\n         exec.stackPush(new Var(Var.Type.IDENT, ident));"
  },
  {
    "sha": "a9b5ae7e36a017d9c5c8c9726e613744771d818a",
    "filename": "hplsql/src/main/java/org/apache/hive/hplsql/Package.java",
    "status": "modified",
    "additions": 19,
    "deletions": 24,
    "changes": 43,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/main/java/org/apache/hive/hplsql/Package.java",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/main/java/org/apache/hive/hplsql/Package.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/hplsql/src/main/java/org/apache/hive/hplsql/Package.java?ref=01a0201274b624c64c5a60c2421ef79f91f60828",
    "patch": "@@ -18,8 +18,11 @@\n \n package org.apache.hive.hplsql;\n \n+import static org.apache.hive.hplsql.functions.InMemoryFunctionRegistry.setCallParameters;\n+\n import java.util.ArrayList;\n import java.util.HashMap;\n+import java.util.List;\n import java.util.Map;\n \n import org.antlr.v4.runtime.ParserRuleContext;\n@@ -28,46 +31,45 @@\n import org.apache.hive.hplsql.HplsqlParser.Create_function_stmtContext;\n import org.apache.hive.hplsql.HplsqlParser.Create_procedure_stmtContext;\n import org.apache.hive.hplsql.functions.BuiltinFunctions;\n-import org.apache.hive.hplsql.functions.InMemoryFunction;\n+import org.apache.hive.hplsql.functions.InMemoryFunctionRegistry;\n \n /**\n  * Program package\n  */\n public class Package {\n   \n-  String name;\n-  ArrayList<Var> vars = new ArrayList<Var>();\n-  ArrayList<String> publicVars = new ArrayList<String>();\n-  ArrayList<String> publicFuncs = new ArrayList<String>();\n-  ArrayList<String> publicProcs = new ArrayList<String>();\n+  private String name;\n+  private List<Var> vars = new ArrayList<>();\n+  private List<String> publicFuncs = new ArrayList<>();\n+  private List<String> publicProcs = new ArrayList<>();\n   \n-  HashMap<String, Create_function_stmtContext> func = new HashMap<String, Create_function_stmtContext>();\n-  HashMap<String, Create_procedure_stmtContext> proc = new HashMap<String, Create_procedure_stmtContext>();\n+  HashMap<String, Create_function_stmtContext> func = new HashMap<>();\n+  HashMap<String, Create_procedure_stmtContext> proc = new HashMap<>();\n     \n   boolean allMembersPublic = false;\n     \n   Exec exec;\n-  InMemoryFunction function;\n+  InMemoryFunctionRegistry function;\n   boolean trace = false;\n   \n   Package(String name, Exec exec, BuiltinFunctions builtinFunctions) {\n     this.name = name;\n     this.exec = exec;\n-    this.function = new InMemoryFunction(exec, builtinFunctions);\n+    this.function = new InMemoryFunctionRegistry(exec, builtinFunctions);\n     this.trace = exec.getTrace();\n   }\n   \n   /**\n    * Add a local variable\n    */\n-  void addVariable(Var var) {\n+  public void addVariable(Var var) {\n     vars.add(var);\n   }\n   \n   /**\n    * Find the variable by name\n    */\n-  Var findVariable(String name) {\n+  public Var findVariable(String name) {\n     for (Var var : vars) {\n       if (name.equalsIgnoreCase(var.getName())) {\n         return var;\n@@ -79,7 +81,7 @@ Var findVariable(String name) {\n   /**\n    * Create the package specification\n    */\n-  void createSpecification(HplsqlParser.Create_package_stmtContext ctx) {\n+  public void createSpecification(HplsqlParser.Create_package_stmtContext ctx) {\n     int cnt = ctx.package_spec().package_spec_item().size();\n     for (int i = 0; i < cnt; i++) {\n       Package_spec_itemContext c = ctx.package_spec().package_spec_item(i);\n@@ -98,7 +100,7 @@ else if (c.T_PROC() != null || c.T_PROCEDURE() != null) {\n   /**\n    * Create the package body\n    */\n-  void createBody(HplsqlParser.Create_package_body_stmtContext ctx) {\n+  public void createBody(HplsqlParser.Create_package_body_stmtContext ctx) {\n     int cnt = ctx.package_body().package_body_item().size();\n     for (int i = 0; i < cnt; i++) {\n       Package_body_itemContext c = ctx.package_body().package_body_item(i);\n@@ -127,7 +129,7 @@ public boolean execFunc(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n     }\n     ArrayList<Var> actualParams = function.getActualCallParameters(ctx);\n     exec.enterScope(Scope.Type.ROUTINE, this);\n-    function.setCallParameters(ctx, actualParams, f.create_routine_params(), null, exec);\n+    setCallParameters(ctx, actualParams, f.create_routine_params(), null, exec);\n     visit(f.single_block_stmt());\n     exec.leaveScope(); \n     return true;\n@@ -155,7 +157,7 @@ public boolean execProc(String name, HplsqlParser.Expr_func_paramsContext ctx, b\n       visit(p.declare_block_inplace());\n     }\n     if (p.create_routine_params() != null) {\n-      function.setCallParameters(ctx, actualParams, p.create_routine_params(), out, exec);\n+      setCallParameters(ctx, actualParams, p.create_routine_params(), out, exec);\n     }\n     visit(p.proc_block());\n     exec.callStackPop();\n@@ -179,14 +181,7 @@ void setAllMembersPublic(boolean value) {\n   Integer visit(ParserRuleContext ctx) {\n     return exec.visit(ctx);  \n   } \n-  \n-  /**\n-   * Execute children rules\n-   */\n-  Integer visitChildren(ParserRuleContext ctx) {\n-    return exec.visitChildren(ctx);  \n-  }  \n-  \n+\n   /**\n    * Trace information\n    */"
  },
  {
    "sha": "463e3facb0486eab3adc39233356f9091558f80c",
    "filename": "hplsql/src/main/java/org/apache/hive/hplsql/Stmt.java",
    "status": "modified",
    "additions": 7,
    "deletions": 4,
    "changes": 11,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/main/java/org/apache/hive/hplsql/Stmt.java",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/main/java/org/apache/hive/hplsql/Stmt.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/hplsql/src/main/java/org/apache/hive/hplsql/Stmt.java?ref=01a0201274b624c64c5a60c2421ef79f91f60828",
    "patch": "@@ -383,8 +383,11 @@ public Integer drop(HplsqlParser.Drop_stmtContext ctx) {\n         sql += \"IF EXISTS \";\n       }\n       sql += evalPop(ctx.table_name()).toString();\n-    }\n-    else if (ctx.T_DATABASE() != null || ctx.T_SCHEMA() != null) {\n+    } else if (ctx.T_PACKAGE() != null) {\n+      exec.dropPackage(ctx, ctx.ident().getText().toUpperCase(), ctx.T_EXISTS() != null);\n+    } else if (ctx.T_PROCEDURE() != null || ctx.T_FUNCTION() != null) {\n+      exec.dropProcedure(ctx, ctx.ident().getText().toUpperCase(), ctx.T_EXISTS() != null);\n+    } else if (ctx.T_DATABASE() != null || ctx.T_SCHEMA() != null) {\n       sql = \"DROP DATABASE \";\n       if (ctx.T_EXISTS() != null) {\n         sql += \"IF EXISTS \";\n@@ -1045,8 +1048,8 @@ else if (trace) {\n    * EXEC to execute a stored procedure\n    */\n   public Boolean execProc(HplsqlParser.Exec_stmtContext ctx) { \n-    String name = evalPop(ctx.expr()).toString();\n-    if (exec.function.exec(name, ctx.expr_func_params())) {\n+    String name = evalPop(ctx.expr()).toString().toUpperCase();\n+    if (exec.functions.exec(name, ctx.expr_func_params())) {\n       return true;\n     }\n     return false;"
  },
  {
    "sha": "36def74ce9fb2121a0fd153392bb3cca1ed9e80e",
    "filename": "hplsql/src/main/java/org/apache/hive/hplsql/functions/FunctionRegistry.java",
    "status": "renamed",
    "additions": 2,
    "deletions": 1,
    "changes": 3,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/main/java/org/apache/hive/hplsql/functions/FunctionRegistry.java",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/main/java/org/apache/hive/hplsql/functions/FunctionRegistry.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/hplsql/src/main/java/org/apache/hive/hplsql/functions/FunctionRegistry.java?ref=01a0201274b624c64c5a60c2421ef79f91f60828",
    "patch": "@@ -20,9 +20,10 @@\n \n import org.apache.hive.hplsql.HplsqlParser;\n \n-public interface Function {\n+public interface FunctionRegistry {\n   boolean exec(String name, HplsqlParser.Expr_func_paramsContext ctx);\n   void addUserFunction(HplsqlParser.Create_function_stmtContext ctx);\n   void addUserProcedure(HplsqlParser.Create_procedure_stmtContext ctx);\n   boolean exists(String name);\n+  void remove(String name);\n }",
    "previous_filename": "hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java"
  },
  {
    "sha": "53656919d4331e86f3b661c9a60951012d783d8f",
    "filename": "hplsql/src/main/java/org/apache/hive/hplsql/functions/HmsFunctionRegistry.java",
    "status": "renamed",
    "additions": 15,
    "deletions": 5,
    "changes": 20,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/main/java/org/apache/hive/hplsql/functions/HmsFunctionRegistry.java",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/main/java/org/apache/hive/hplsql/functions/HmsFunctionRegistry.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/hplsql/src/main/java/org/apache/hive/hplsql/functions/HmsFunctionRegistry.java?ref=01a0201274b624c64c5a60c2421ef79f91f60828",
    "patch": "@@ -20,7 +20,7 @@\n \n package org.apache.hive.hplsql.functions;\n \n-import static org.apache.hive.hplsql.functions.InMemoryFunction.setCallParameters;\n+import static org.apache.hive.hplsql.functions.InMemoryFunctionRegistry.setCallParameters;\n \n import java.util.ArrayList;\n import java.util.HashMap;\n@@ -43,15 +43,15 @@\n import org.apache.hive.hplsql.Var;\n import org.apache.thrift.TException;\n \n-public class HmsFunction implements Function {\n+public class HmsFunctionRegistry implements FunctionRegistry {\n   private Exec exec;\n   private boolean trace;\n   private IMetaStoreClient msc;\n   private BuiltinFunctions builtinFunctions;\n   private HplSqlSessionState hplSqlSession;\n   private Map<String, ParserRuleContext> cache = new HashMap<>();\n \n-  public HmsFunction(Exec e, IMetaStoreClient msc, BuiltinFunctions builtinFunctions, HplSqlSessionState hplSqlSession) {\n+  public HmsFunctionRegistry(Exec e, IMetaStoreClient msc, BuiltinFunctions builtinFunctions, HplSqlSessionState hplSqlSession) {\n     this.exec = e;\n     this.msc = msc;\n     this.builtinFunctions = builtinFunctions;\n@@ -61,17 +61,27 @@ public HmsFunction(Exec e, IMetaStoreClient msc, BuiltinFunctions builtinFunctio\n \n   @Override\n   public boolean exists(String name) {\n-    name = name.toUpperCase();\n     return isCached(name) || getProcFromHMS(name).isPresent();\n   }\n \n+  @Override\n+  public void remove(String name) {\n+    try {\n+      msc.dropStoredProcedure(new StoredProcedureRequest(\n+              hplSqlSession.currentCatalog(),\n+              hplSqlSession.currentDatabase(),\n+              name));\n+    } catch (TException e) {\n+      throw new RuntimeException(e);\n+    }\n+  }\n+\n   protected boolean isCached(String name) {\n     return cache.containsKey(qualified(name));\n   }\n \n   @Override\n   public boolean exec(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n-    name = name.toUpperCase();\n     if (builtinFunctions.exec(name, ctx)) {\n       return true;\n     }",
    "previous_filename": "hplsql/src/main/java/org/apache/hive/hplsql/functions/HmsFunction.java"
  },
  {
    "sha": "40b8b5a3da9d5d0268dcbd29884eb67fdf44c485",
    "filename": "hplsql/src/main/java/org/apache/hive/hplsql/functions/InMemoryFunctionRegistry.java",
    "status": "renamed",
    "additions": 10,
    "deletions": 6,
    "changes": 16,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/main/java/org/apache/hive/hplsql/functions/InMemoryFunctionRegistry.java",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/main/java/org/apache/hive/hplsql/functions/InMemoryFunctionRegistry.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/hplsql/src/main/java/org/apache/hive/hplsql/functions/InMemoryFunctionRegistry.java?ref=01a0201274b624c64c5a60c2421ef79f91f60828",
    "patch": "@@ -39,28 +39,32 @@\n /**\n  * HPL/SQL functions\n  */\n-public class InMemoryFunction implements Function {\n+public class InMemoryFunctionRegistry implements FunctionRegistry {\n   Exec exec;\n   private BuiltinFunctions builtinFunctions;\n   HashMap<String, HplsqlParser.Create_function_stmtContext> funcMap = new HashMap<>();\n   HashMap<String, HplsqlParser.Create_procedure_stmtContext> procMap = new HashMap<>();\n   boolean trace = false;\n   \n-  public InMemoryFunction(Exec e, BuiltinFunctions builtinFunctions) {\n+  public InMemoryFunctionRegistry(Exec e, BuiltinFunctions builtinFunctions) {\n     this.exec = e;\n     this.trace = exec.getTrace();\n     this.builtinFunctions = builtinFunctions;\n   }\n \n   @Override\n   public boolean exists(String name) {\n-    name = name.toUpperCase();\n     return funcMap.containsKey(name) || procMap.containsKey(name);\n   }\n \n+  @Override\n+  public void remove(String name) {\n+    funcMap.remove(name);\n+    procMap.remove(name);\n+  }\n+\n   @Override\n   public boolean exec(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n-    name = name.toUpperCase();\n     if (builtinFunctions.exec(name, ctx)) {\n       return true;\n     }\n@@ -74,7 +78,7 @@ public boolean exec(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n    * Execute a user-defined function\n    */\n   private boolean execFunction(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n-    HplsqlParser.Create_function_stmtContext userCtx = funcMap.get(name.toUpperCase());\n+    HplsqlParser.Create_function_stmtContext userCtx = funcMap.get(name);\n     if (userCtx == null) {\n       return false;\n     }\n@@ -99,7 +103,7 @@ private boolean execProc(String name, HplsqlParser.Expr_func_paramsContext ctx)\n     if (trace) {\n       trace(ctx == null ? null : ctx.getParent(), \"EXEC PROCEDURE \" + name);\n     }\n-    HplsqlParser.Create_procedure_stmtContext procCtx = procMap.get(name.toUpperCase());    \n+    HplsqlParser.Create_procedure_stmtContext procCtx = procMap.get(name);\n     if (procCtx == null) {\n       trace(ctx.getParent(), \"Procedure not found\");\n       return false;",
    "previous_filename": "hplsql/src/main/java/org/apache/hive/hplsql/functions/InMemoryFunction.java"
  },
  {
    "sha": "e055881db49e5b7141f2c3eb647f56d8e056d918",
    "filename": "hplsql/src/main/java/org/apache/hive/hplsql/packages/HmsPackageRegistry.java",
    "status": "added",
    "additions": 101,
    "deletions": 0,
    "changes": 101,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/main/java/org/apache/hive/hplsql/packages/HmsPackageRegistry.java",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/main/java/org/apache/hive/hplsql/packages/HmsPackageRegistry.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/hplsql/src/main/java/org/apache/hive/hplsql/packages/HmsPackageRegistry.java?ref=01a0201274b624c64c5a60c2421ef79f91f60828",
    "patch": "@@ -0,0 +1,101 @@\n+/*\n+ *  Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hive.hplsql.packages;\n+\n+import java.util.Optional;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.AddPackageRequest;\n+import org.apache.hadoop.hive.metastore.api.DropPackageRequest;\n+import org.apache.hadoop.hive.metastore.api.GetPackageRequest;\n+import org.apache.hadoop.hive.metastore.api.Package;\n+import org.apache.hive.hplsql.HplSqlSessionState;\n+import org.apache.thrift.TException;\n+\n+public class HmsPackageRegistry implements PackageRegistry {\n+  private final IMetaStoreClient msc;\n+  private final HplSqlSessionState hplSqlSession;\n+\n+  public HmsPackageRegistry(IMetaStoreClient msc, HplSqlSessionState hplSqlSession) {\n+    this.msc = msc;\n+    this.hplSqlSession = hplSqlSession;\n+  }\n+\n+  @Override\n+  public Optional<String> getPackage(String name) {\n+    try {\n+      Package pkg = msc.findPackage(request(name));\n+      return pkg == null\n+              ? Optional.empty()\n+              : Optional.of(pkg.getHeader() + \";\\n\" + pkg.getBody());\n+    } catch (TException e) {\n+      throw new RuntimeException(e);\n+    }\n+  }\n+\n+  @Override\n+  public void createPackageHeader(String name, String header, boolean replace) {\n+    try {\n+      Package existing = msc.findPackage(request(name));\n+      if (existing != null && !replace)\n+        throw new RuntimeException(\"Package \" + name + \" already exists\");\n+      msc.addPackage(makePackage(name, header, \"\"));\n+    } catch (TException e) {\n+      throw new RuntimeException(e);\n+    }\n+  }\n+\n+  @Override\n+  public void createPackageBody(String name, String body, boolean replace) {\n+    try {\n+      Package existing = msc.findPackage(request(name));\n+      if (existing == null || StringUtils.isEmpty(existing.getHeader()))\n+        throw new RuntimeException(\"Package header does not exists \" + name);\n+      if (StringUtils.isNotEmpty(existing.getBody()) && !replace)\n+        throw new RuntimeException(\"Package body \" + name + \" already exists\");\n+      msc.addPackage(makePackage(name, existing.getHeader(), body));\n+    } catch (TException e) {\n+      throw new RuntimeException(e);\n+    }\n+  }\n+\n+  @Override\n+  public void dropPackage(String name) {\n+    try {\n+      msc.dropPackage(new DropPackageRequest(hplSqlSession.currentCatalog(), hplSqlSession.currentDatabase(), name));\n+    } catch (TException e) {\n+      throw new RuntimeException(e);\n+    }\n+  }\n+\n+  private GetPackageRequest request(String name) {\n+    return new GetPackageRequest(hplSqlSession.currentCatalog(), hplSqlSession.currentDatabase(), name.toUpperCase());\n+  }\n+\n+  private AddPackageRequest makePackage(String name, String header, String body) {\n+    return new AddPackageRequest(\n+            hplSqlSession.currentCatalog(),\n+            hplSqlSession.currentDatabase(),\n+            name.toUpperCase(),\n+            hplSqlSession.currentUser(),\n+            header,\n+            body);\n+  }\n+}"
  },
  {
    "sha": "c6ca6f1d95428cf83387551e011492e5b09c4dd4",
    "filename": "hplsql/src/main/java/org/apache/hive/hplsql/packages/InMemoryPackageRegistry.java",
    "status": "added",
    "additions": 69,
    "deletions": 0,
    "changes": 69,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/main/java/org/apache/hive/hplsql/packages/InMemoryPackageRegistry.java",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/main/java/org/apache/hive/hplsql/packages/InMemoryPackageRegistry.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/hplsql/src/main/java/org/apache/hive/hplsql/packages/InMemoryPackageRegistry.java?ref=01a0201274b624c64c5a60c2421ef79f91f60828",
    "patch": "@@ -0,0 +1,69 @@\n+/*\n+ *  Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hive.hplsql.packages;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import org.apache.commons.lang3.StringUtils;\n+\n+public class InMemoryPackageRegistry implements PackageRegistry {\n+  private Map<String, Source> registry = new HashMap<>();\n+\n+  @Override\n+  public Optional<String> getPackage(String name) {\n+    Source src = registry.get(name.toUpperCase());\n+    return src == null\n+            ? Optional.empty()\n+            : Optional.of(src.header + \";\\n\" + src.body);\n+  }\n+\n+  @Override\n+  public void createPackageHeader(String name, String header, boolean replace) {\n+    if (registry.containsKey(name) && !replace)\n+      throw new RuntimeException(\"Package \" + name + \" already exits\");\n+    registry.put(name, new Source(header, \"\"));\n+  }\n+\n+  @Override\n+  public void createPackageBody(String name, String body, boolean replace) {\n+    Source existing = registry.get(name);\n+    if (existing == null || StringUtils.isEmpty(existing.header))\n+      throw new RuntimeException(\"Package header does not exists \" + name);\n+    if (existing != null && StringUtils.isNotEmpty(existing.body) && !replace)\n+      throw new RuntimeException(\"Package body \" + name + \" already exits\");\n+    registry.getOrDefault(name, new Source(\"\", \"\")).body = body;\n+  }\n+\n+  @Override\n+  public void dropPackage(String name) {\n+    registry.remove(name);\n+  }\n+\n+  private static class Source {\n+    String header;\n+    String body;\n+\n+    public Source(String header, String body) {\n+      this.header = header;\n+      this.body = body;\n+    }\n+  }\n+}"
  },
  {
    "sha": "96bc0496876814875b041f314304bab573399785",
    "filename": "hplsql/src/main/java/org/apache/hive/hplsql/packages/PackageRegistry.java",
    "status": "added",
    "additions": 28,
    "deletions": 0,
    "changes": 28,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/main/java/org/apache/hive/hplsql/packages/PackageRegistry.java",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/main/java/org/apache/hive/hplsql/packages/PackageRegistry.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/hplsql/src/main/java/org/apache/hive/hplsql/packages/PackageRegistry.java?ref=01a0201274b624c64c5a60c2421ef79f91f60828",
    "patch": "@@ -0,0 +1,28 @@\n+/*\n+ *  Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hive.hplsql.packages;\n+\n+import java.util.Optional;\n+\n+public interface PackageRegistry {\n+  Optional<String> getPackage(String name);\n+  void createPackageHeader(String name, String header, boolean replace);\n+  void createPackageBody(String name, String body, boolean replace);\n+  void dropPackage(String name);\n+}"
  },
  {
    "sha": "a44b5752d1cadea5490772133f36891c96e7534d",
    "filename": "hplsql/src/test/java/org/apache/hive/hplsql/TestHplsqlLocal.java",
    "status": "modified",
    "additions": 13,
    "deletions": 3,
    "changes": 16,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/test/java/org/apache/hive/hplsql/TestHplsqlLocal.java",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/test/java/org/apache/hive/hplsql/TestHplsqlLocal.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/hplsql/src/test/java/org/apache/hive/hplsql/TestHplsqlLocal.java?ref=01a0201274b624c64c5a60c2421ef79f91f60828",
    "patch": "@@ -128,21 +128,31 @@ public void testCreatePackage3() throws Exception {\n     run(\"create_package3\");\n   }\n \n+  @Test\n+  public void testDropPackage() throws Exception {\n+    run(\"drop_package\");\n+  }\n+\n+  @Test\n+  public void testDropProcedure() throws Exception {\n+    run(\"drop_proc\");\n+  }\n+\n   @Test\n   public void testCreateProcedure() throws Exception {\n     run(\"create_procedure\");\n   }\n-  \n+\n   @Test\n   public void testCreateProcedure2() throws Exception {\n     run(\"create_procedure2\");\n   }\n-  \n+\n   @Test\n   public void testCreateProcedure3() throws Exception {\n     run(\"create_procedure3\");\n   }\n-  \n+\n   @Test\n   public void testCreateProcedure4() throws Exception {\n     run(\"create_procedure4\");"
  },
  {
    "sha": "ee4ba134439dd085bbd45bc073477554c92bb5df",
    "filename": "hplsql/src/test/queries/local/create_package3_include.sql",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/test/queries/local/create_package3_include.sql",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/test/queries/local/create_package3_include.sql",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/hplsql/src/test/queries/local/create_package3_include.sql?ref=01a0201274b624c64c5a60c2421ef79f91f60828",
    "patch": "@@ -6,6 +6,6 @@ create or replace package body a as\n procedure test() \n is \n begin \n-print \"test ok\"; \n+print 'test ok';\n end; \n end;\n\\ No newline at end of file"
  },
  {
    "sha": "4af459a155975521968e7f5b49f0b17eb3d4d235",
    "filename": "hplsql/src/test/queries/local/drop_package.sql",
    "status": "added",
    "additions": 21,
    "deletions": 0,
    "changes": 21,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/test/queries/local/drop_package.sql",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/test/queries/local/drop_package.sql",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/hplsql/src/test/queries/local/drop_package.sql?ref=01a0201274b624c64c5a60c2421ef79f91f60828",
    "patch": "@@ -0,0 +1,21 @@\n+CREATE PACKAGE Counter AS\n+  count INT := 0;\n+  FUNCTION current() RETURNS INT;\n+  PROCEDURE inc(i INT);\n+END;\n+\n+CREATE PACKAGE BODY Counter AS\n+  FUNCTION current() RETURNS INT IS BEGIN RETURN count; END;\n+  PROCEDURE inc(i INT) IS BEGIN count := count + i; END;\n+END;\n+\n+\n+Counter.inc(10);\n+\n+PRINT Counter.current();\n+\n+DROP PACKAGE Counter;\n+\n+PRINT Counter.current();\n+\n+DROP PACKAGE IF EXISTS Counter;\n\\ No newline at end of file"
  },
  {
    "sha": "3566436d30e7f9d58b65add1231d517ff301024d",
    "filename": "hplsql/src/test/queries/local/drop_proc.sql",
    "status": "added",
    "additions": 18,
    "deletions": 0,
    "changes": 18,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/test/queries/local/drop_proc.sql",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/test/queries/local/drop_proc.sql",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/hplsql/src/test/queries/local/drop_proc.sql?ref=01a0201274b624c64c5a60c2421ef79f91f60828",
    "patch": "@@ -0,0 +1,18 @@\n+CREATE PROCEDURE P1()\n+BEGIN\n+  PRINT 'P1';\n+END;\n+\n+CREATE FUNCTION F1() RETURNS STRING\n+BEGIN\n+  RETURN 'F1';\n+END;\n+\n+P1();\n+F1();\n+\n+DROP PROCEDURE P1;\n+DROP PROCEDURE F1;\n+\n+DROP PROCEDURE IF EXISTS P1;\n+DROP FUNCTION IF EXISTS F1;"
  },
  {
    "sha": "147506f733fe753c85166a06b9c7fe88fe44bb8f",
    "filename": "hplsql/src/test/results/local/create_package.out.txt",
    "status": "modified",
    "additions": 19,
    "deletions": 19,
    "changes": 38,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/test/results/local/create_package.out.txt",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/test/results/local/create_package.out.txt",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/hplsql/src/test/results/local/create_package.out.txt?ref=01a0201274b624c64c5a60c2421ef79f91f60828",
    "patch": "@@ -1,47 +1,47 @@\n Ln:1 CREATE PACKAGE\n-Ln:2 DECLARE a int = 3\n Ln:7 CREATE PACKAGE BODY\n-Ln:8 DECLARE b int = 1\n Ln:49 CREATE PROCEDURE SP2\n+Ln:2 DECLARE a int = 3\n+Ln:7 DECLARE b int = 1\n pack1.a: 3\n-Ln:57 EXEC PACKAGE FUNCTION PACK1.f1\n+Ln:57 EXEC PACKAGE FUNCTION PACK1.F1\n Ln:57 SET PARAM p1 = 3\n Ln:57 SET PARAM p2 = 5\n a: 3\n b: 1\n p1: 3\n p2: 5\n-Ln:16 IF\n-Ln:16 IF TRUE executed\n-EXEC PACKAGE FUNCTION PACK1.f2\n-Ln:29 RETURN\n+Ln:15 IF\n+Ln:15 IF TRUE executed\n+EXEC PACKAGE FUNCTION PACK1.F2\n+Ln:28 RETURN\n f2: 1\n-EXEC PACKAGE FUNCTION PACK1.f2\n-Ln:29 RETURN\n+EXEC PACKAGE FUNCTION PACK1.F2\n+Ln:28 RETURN\n pack1.f2: 1\n-Ln:20 EXEC PACKAGE PROCEDURE PACK1.sp1\n-Ln:20 SET PARAM p1 = 3\n+Ln:19 EXEC PACKAGE PROCEDURE PACK1.SP1\n+Ln:19 SET PARAM p1 = 3\n a: 3\n b: 1\n p1: 3\n-Ln:21 EXEC PROCEDURE SP2\n-Ln:21 SET PARAM p2 = 1\n+Ln:20 EXEC PROCEDURE SP2\n+Ln:20 SET PARAM p2 = 1\n pack1.a: 3\n p2: 1\n-Ln:22 EXEC PACKAGE PROCEDURE PACK1.sp3\n-Ln:22 SET PARAM p1 = 3\n+Ln:21 EXEC PACKAGE PROCEDURE PACK1.SP3\n+Ln:21 SET PARAM p1 = 3\n a: 3\n b: 1\n p1: 3\n-Ln:23 RETURN\n+Ln:22 RETURN\n pack1.f1: \n-Ln:58 EXEC PACKAGE PROCEDURE PACK1.sp1\n+Ln:58 EXEC PACKAGE PROCEDURE PACK1.SP1\n Ln:58 SET PARAM p1 = 1\n a: 3\n b: 1\n p1: 1\n-Ln:59 EXEC PACKAGE PROCEDURE PACK1.sp1\n+Ln:59 EXEC PACKAGE PROCEDURE PACK1.SP1\n Ln:59 SET PARAM p1 = 1\n a: 3\n b: 1\n-p1: 1\n\\ No newline at end of file\n+p1: 1"
  },
  {
    "sha": "9081887369d0e35c51a4d79be74ab6dacf84e4ac",
    "filename": "hplsql/src/test/results/local/create_package2.out.txt",
    "status": "modified",
    "additions": 9,
    "deletions": 9,
    "changes": 18,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/test/results/local/create_package2.out.txt",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/test/results/local/create_package2.out.txt",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/hplsql/src/test/results/local/create_package2.out.txt?ref=01a0201274b624c64c5a60c2421ef79f91f60828",
    "patch": "@@ -1,16 +1,16 @@\n Ln:1 CREATE PACKAGE\n-Ln:2 DECLARE session_count int = 0\n Ln:7 CREATE PACKAGE BODY\n-Ln:20 EXEC PACKAGE PROCEDURE USERS.add\n+Ln:2 DECLARE session_count int = 0\n+Ln:20 EXEC PACKAGE PROCEDURE USERS.ADD\n Ln:20 SET PARAM name = John\n-Ln:16 SET session_count = 1\n-Ln:21 EXEC PACKAGE PROCEDURE USERS.add\n+Ln:15 SET session_count = 1\n+Ln:21 EXEC PACKAGE PROCEDURE USERS.ADD\n Ln:21 SET PARAM name = Sarah\n-Ln:16 SET session_count = 2\n-Ln:22 EXEC PACKAGE PROCEDURE USERS.add\n+Ln:15 SET session_count = 2\n+Ln:22 EXEC PACKAGE PROCEDURE USERS.ADD\n Ln:22 SET PARAM name = Paul\n-Ln:16 SET session_count = 3\n+Ln:15 SET session_count = 3\n Ln:23 PRINT\n-EXEC PACKAGE FUNCTION USERS.get_count\n-Ln:11 RETURN\n+EXEC PACKAGE FUNCTION USERS.GET_COUNT\n+Ln:10 RETURN\n Number of users: 3\n\\ No newline at end of file"
  },
  {
    "sha": "e66171d36b94ac4f395e7bbfedc7ba3526a113c3",
    "filename": "hplsql/src/test/results/local/create_package3.out.txt",
    "status": "modified",
    "additions": 3,
    "deletions": 3,
    "changes": 6,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/test/results/local/create_package3.out.txt",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/test/results/local/create_package3.out.txt",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/hplsql/src/test/results/local/create_package3.out.txt?ref=01a0201274b624c64c5a60c2421ef79f91f60828",
    "patch": "@@ -1,5 +1,5 @@\n Ln:1 INCLUDE src/test/queries/local/create_package3_include.sql\n INCLUDE CONTENT src/test/queries/local/create_package3_include.sql (non-empty)\n-EXEC PACKAGE PROCEDURE A.test\n-Ln:9 PRINT\n-\"test ok\"\n\\ No newline at end of file\n+EXEC PACKAGE PROCEDURE A.TEST\n+Ln:8 PRINT\n+test ok\n\\ No newline at end of file"
  },
  {
    "sha": "8e3575531703dcfe6e4c03bea81d908292686ada",
    "filename": "hplsql/src/test/results/local/drop_package.out.txt",
    "status": "added",
    "additions": 16,
    "deletions": 0,
    "changes": 16,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/test/results/local/drop_package.out.txt",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/test/results/local/drop_package.out.txt",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/hplsql/src/test/results/local/drop_package.out.txt?ref=01a0201274b624c64c5a60c2421ef79f91f60828",
    "patch": "@@ -0,0 +1,16 @@\n+Ln:1 CREATE PACKAGE\n+Ln:7 CREATE PACKAGE BODY\n+Ln:2 DECLARE count INT = 0\n+Ln:13 EXEC PACKAGE PROCEDURE COUNTER.INC\n+Ln:13 SET PARAM i = 10\n+Ln:8 SET count = 10\n+Ln:15 PRINT\n+EXEC PACKAGE FUNCTION COUNTER.CURRENT\n+Ln:7 RETURN\n+10\n+Ln:17 DROP\n+Ln:17 COUNTER DROPPED\n+Ln:19 PRINT\n+null\n+Ln:21 DROP\n+Ln:21 COUNTER DOES NOT EXIST\n\\ No newline at end of file"
  },
  {
    "sha": "a57cdcd52f3a72c264ecce59602c88426b3a8328",
    "filename": "hplsql/src/test/results/local/drop_proc.out.txt",
    "status": "added",
    "additions": 16,
    "deletions": 0,
    "changes": 16,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/test/results/local/drop_proc.out.txt",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/hplsql/src/test/results/local/drop_proc.out.txt",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/hplsql/src/test/results/local/drop_proc.out.txt?ref=01a0201274b624c64c5a60c2421ef79f91f60828",
    "patch": "@@ -0,0 +1,16 @@\n+Ln:1 CREATE PROCEDURE P1\n+Ln:6 CREATE FUNCTION F1\n+EXEC PROCEDURE P1\n+Ln:3 PRINT\n+P1\n+EXEC FUNCTION F1\n+Ln:8 RETURN\n+F1\n+Ln:14 DROP\n+Ln:14 P1 DROPPED\n+Ln:15 DROP\n+Ln:15 F1 DROPPED\n+Ln:17 DROP\n+Ln:17 P1 DOES NOT EXIST\n+Ln:18 DROP\n+Ln:18 F1 DOES NOT EXIST\n\\ No newline at end of file"
  },
  {
    "sha": "89dba997cb218c2b87702a374edbd0a31f35262b",
    "filename": "itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/DummyRawStoreFailEvent.java",
    "status": "modified",
    "additions": 25,
    "deletions": 0,
    "changes": 25,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/DummyRawStoreFailEvent.java",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/DummyRawStoreFailEvent.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hcatalog-unit/src/test/java/org/apache/hive/hcatalog/listener/DummyRawStoreFailEvent.java?ref=01a0201274b624c64c5a60c2421ef79f91f60828",
    "patch": "@@ -19,10 +19,15 @@\n package org.apache.hive.hcatalog.listener;\n \n import org.apache.hadoop.hive.common.TableName;\n+import org.apache.hadoop.hive.metastore.api.AddPackageRequest;\n+import org.apache.hadoop.hive.metastore.api.DropPackageRequest;\n+import org.apache.hadoop.hive.metastore.api.GetPackageRequest;\n import org.apache.hadoop.hive.metastore.api.GetPartitionsFilterSpec;\n import org.apache.hadoop.hive.metastore.api.GetProjectionsSpec;\n import org.apache.hadoop.hive.metastore.api.ISchemaName;\n+import org.apache.hadoop.hive.metastore.api.ListPackageRequest;\n import org.apache.hadoop.hive.metastore.api.ListStoredProcedureRequest;\n+import org.apache.hadoop.hive.metastore.api.Package;\n import org.apache.hadoop.hive.metastore.api.SQLAllTableConstraints;\n import org.apache.hadoop.hive.metastore.api.SchemaVersionDescriptor;\n import org.apache.hadoop.hive.metastore.api.Catalog;\n@@ -1445,4 +1450,24 @@ public void dropStoredProcedure(String catName, String dbName, String funcName)\n     return objectStore.getAllStoredProcedures(request);\n   }\n \n+  @Override\n+  public void addPackage(AddPackageRequest request) throws MetaException, NoSuchObjectException {\n+    objectStore.addPackage(request);\n+  }\n+\n+  @Override\n+  public Package findPackage(GetPackageRequest request) {\n+    return objectStore.findPackage(request);\n+  }\n+\n+  @Override\n+  public List<String> listPackages(ListPackageRequest request) {\n+    return objectStore.listPackages(request);\n+  }\n+\n+  @Override\n+  public void dropPackage(DropPackageRequest request) {\n+    objectStore.dropPackage(request);\n+  }\n+\n }"
  },
  {
    "sha": "760df5f64b1ad2f195b27ad05feacec984a406b8",
    "filename": "itests/hive-jmh/src/main/java/org/apache/hive/benchmark/serde/HiveCharBench.java",
    "status": "added",
    "additions": 84,
    "deletions": 0,
    "changes": 84,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/itests/hive-jmh/src/main/java/org/apache/hive/benchmark/serde/HiveCharBench.java",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/itests/hive-jmh/src/main/java/org/apache/hive/benchmark/serde/HiveCharBench.java",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-jmh/src/main/java/org/apache/hive/benchmark/serde/HiveCharBench.java?ref=01a0201274b624c64c5a60c2421ef79f91f60828",
    "patch": "@@ -0,0 +1,84 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hive.benchmark.serde;\n+\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.hive.common.type.HiveChar;\n+import org.apache.hadoop.hive.serde2.io.HiveCharWritable;\n+import org.apache.hadoop.io.Text;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.RunnerException;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+\n+@State(Scope.Benchmark)\n+public class HiveCharBench {\n+\n+  @BenchmarkMode(Mode.AverageTime)\n+  @Fork(1)\n+  @State(Scope.Thread)\n+  @OutputTimeUnit(TimeUnit.MILLISECONDS)\n+  public static class BaseBench {\n+\n+    private Text value;\n+\n+    @Setup\n+    public void setup() throws Exception {\n+      value = new Text(\"    asdfghjk    \");\n+    }\n+\n+    @Benchmark\n+    @Warmup(iterations = 2, time = 2, timeUnit = TimeUnit.SECONDS)\n+    @Measurement(iterations = 10, time = 2, timeUnit = TimeUnit.SECONDS)\n+    public void testStrippedValueOld() {\n+      for (int i = 0; i < 10000; i++) {\n+        HiveChar hiveChar = new HiveChar(value.toString(), -1);\n+        new Text(StringUtils.stripEnd(hiveChar.getValue(), \" \"));\n+      }\n+    }\n+\n+    @Benchmark\n+    @Warmup(iterations = 2, time = 2, timeUnit = TimeUnit.SECONDS)\n+    @Measurement(iterations = 10, time = 2, timeUnit = TimeUnit.SECONDS)\n+    public void testStrippedValueNew() {\n+      for (int j = 0; j < 10000; j++) {\n+        byte[] input = value.getBytes();\n+        int i = input.length;\n+        while (i-- > 0 && (input[i] == 32 || input[i] == 0)) {\n+        }\n+        byte[] output = new byte[i + 1];\n+        System.arraycopy(input, 0, output, 0, i + 1);\n+        new Text(output);\n+      }\n+    }\n+  }\n+\n+  public static void main(String[] args) throws RunnerException {\n+    Options opt =\n+        new OptionsBuilder().include(\".*\" + HiveCharBench.class.getSimpleName() + \".*\").build();\n+    new Runner(opt).run();\n+  }\n+}"
  },
  {
    "sha": "18b4de937057a0fe2c9d11bedeb647ada249a11a",
    "filename": "itests/hive-unit/pom.xml",
    "status": "modified",
    "additions": 14,
    "deletions": 0,
    "changes": 14,
    "blob_url": "https://github.com/apache/hive/blob/01a0201274b624c64c5a60c2421ef79f91f60828/itests/hive-unit/pom.xml",
    "raw_url": "https://github.com/apache/hive/raw/01a0201274b624c64c5a60c2421ef79f91f60828/itests/hive-unit/pom.xml",
    "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/pom.xml?ref=01a0201274b624c64c5a60c2421ef79f91f60828",
    "patch": "@@ -30,6 +30,8 @@\n   <properties>\n     <hive.path.to.root>../..</hive.path.to.root>\n     <spark.home>${basedir}/${hive.path.to.root}/itests/hive-unit/target/spark</spark.home>\n+    <testcontainers.version>1.15.2</testcontainers.version>\n+    <htmlunit.version>2.45.0</htmlunit.version>\n   </properties>\n \n   <dependencies>\n@@ -427,6 +429,18 @@\n       <version>${plexus.version}</version>\n       <scope>test</scope>\n     </dependency>\n+    <dependency>\n+      <groupId>org.testcontainers</groupId>\n+      <artifactId>testcontainers</artifactId>\n+      <version>${testcontainers.version}</version>\n+      <scope>test</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>net.sourceforge.htmlunit</groupId>\n+      <artifactId>htmlunit</artifactId>\n+      <version>${htmlunit.version}</version>\n+      <scope>test</scope>\n+    </dependency>\n   </dependencies>\n \n   <profiles>"
  }
]
