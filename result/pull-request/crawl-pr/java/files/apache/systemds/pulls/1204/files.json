[
  {
    "sha": "942770f9b353c15beaa6a6cde9776324795e9d4c",
    "filename": "binningtest.dml",
    "status": "added",
    "additions": 96,
    "deletions": 0,
    "changes": 96,
    "blob_url": "https://github.com/apache/systemds/blob/debdcbaf10790b6480026528be195d208262665a/binningtest.dml",
    "raw_url": "https://github.com/apache/systemds/raw/debdcbaf10790b6480026528be195d208262665a/binningtest.dml",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/binningtest.dml?ref=debdcbaf10790b6480026528be195d208262665a",
    "patch": "@@ -0,0 +1,96 @@\n+#test = matrix(rand, rows = 20, cols =1);\n+\n+test = rand (rows = 493, cols = 1);\n+\n+ #for (i in 1:20)\n+   # {\n+\n+    #    print(\"col _ bins is: \" + as.scalar(test[i,1]));\n+   # }\n+\n+\n+bin_size = max (as.integer (nrow(test) / 10), 1);\n+\n+print(\"bin isze is:\" + bin_size);\n+\n+[col_bins, numb_bins_defined] = binning(test, bin_size, 10);\n+\n+print(numb_bins_defined);\n+print(as.scalar(col_bins[1]));\n+print(ncol(col_bins));\n+print(nrow(col_bins));\n+\n+\n+\n+#Function used to bin the col of the Matrix\n+# input sorted vector of double Values\n+# returns matrix of bins\n+binning =\n+    function (Matrix[double] col, int bin_size, int num_bins)\n+    return (Matrix[double] col_bins, int num_bins_defined)\n+{\n+    print(\"binning\");\n+\n+    size_of_col = nrow(col);\n+\n+    #print(\"size of col is = \" + size_of_col);\n+\n+    position_in_input_col = 1;\n+\n+    col_bins_temp = matrix(0, rows = num_bins + 1, cols = 1);\n+\n+    num_bins_defined = 0;\n+\n+    col_bins_temp[1] = col[1];\n+\n+     while((position_in_input_col < size_of_col) & (num_bins_defined < num_bins))\n+     {\n+        position_in_input_col = position_in_input_col + bin_size;\n+\n+        if(position_in_input_col >= size_of_col)\n+        {\n+            position_in_input_col = size_of_col;\n+        }\n+\n+        current_bin_entry = col[position_in_input_col];\n+\n+        col_bins_temp[num_bins_defined + 1] = current_bin_entry;\n+\n+        condition = 0;\n+\n+        while((position_in_input_col < size_of_col) & (condition == 0))\n+        {\n+            if(as.scalar(col[position_in_input_col + 1]) == as.scalar(current_bin_entry))\n+            {\n+                position_in_input_col = position_in_input_col + 1;\n+            }\n+            else\n+            {\n+                condition = 1;\n+            }\n+        }\n+\n+\n+        # I increase the current amount of bins\n+       # print(\"Number bins defined is:\" + num_bins_defined);\n+        num_bins_defined = num_bins_defined + 1;\n+     }\n+\n+    #Bins should be represented by the middle value (first bin + second bin /2)\n+\n+    #print(\"number of Bins defined: \" + num_bins_defined);\n+\n+    col_bins = matrix(0, rows = num_bins + 1, cols = 1);\n+\n+\n+\n+    for (i in 1:num_bins_defined)\n+    {\n+        col_bins[i] = (col_bins_temp[i] + col_bins_temp[i+1])/2;\n+    }\n+\n+    print(\"Value of num_bins is : \" + num_bins);\n+    print(\"Value of num_bins_defined is : \" + num_bins_defined);\n+\n+    #print(\"Going to return\");\n+}\n\\ No newline at end of file"
  },
  {
    "sha": "b75bb9181748c2a8c121160c93be956a4b7bfc56",
    "filename": "rftest.dml",
    "status": "added",
    "additions": 26,
    "deletions": 0,
    "changes": 26,
    "blob_url": "https://github.com/apache/systemds/blob/debdcbaf10790b6480026528be195d208262665a/rftest.dml",
    "raw_url": "https://github.com/apache/systemds/raw/debdcbaf10790b6480026528be195d208262665a/rftest.dml",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/rftest.dml?ref=debdcbaf10790b6480026528be195d208262665a",
    "patch": "@@ -0,0 +1,26 @@\n+print(\"starting rftest ...\\n\")\n+\n+\n+\n+#X = matrix(0, rows = 10,cols = 10);\n+X= read(\"/home/christof/Desktop/DILA_new/DILA/temp/linRegData.train.data.csv\",  data_type=\"matrix\", format=\"csv\");\n+#Y = matrix(0, rows = 10,cols = 10);\n+Y= read(\"/home/christof/Desktop/DILA_new/DILA/temp/linRegData.train.labels.csv\",  data_type=\"matrix\", format=\"csv\");\n+R = matrix(1, rows = 10,cols = 3);\n+bins = 20;\n+depth = 25;\n+num_leafs = 10;\n+num_samples = 3000;\n+num_trees = 10;\n+subsamp_rate = 1.0;\n+feature_subset = 0.5;\n+impurity = \"Gini\";\n+S_map = \" \";\n+C_map = \" \";\n+C = \" \";\n+fmt = \"text\";\n+\n+\n+#M = matrix(0, rows = 10,cols = 10);\n+M = randomForest(X, Y, R, bins, depth, num_leafs, num_samples, num_trees, subsamp_rate, feature_subset,\n+                                           impurity, S_map, C_map, C, fmt)"
  },
  {
    "sha": "b498cafd68c8602a2b334ffe6b044af417f15374",
    "filename": "scripts/builtin/randomForest.dml",
    "status": "added",
    "additions": 1506,
    "deletions": 0,
    "changes": 1506,
    "blob_url": "https://github.com/apache/systemds/blob/debdcbaf10790b6480026528be195d208262665a/scripts/builtin/randomForest.dml",
    "raw_url": "https://github.com/apache/systemds/raw/debdcbaf10790b6480026528be195d208262665a/scripts/builtin/randomForest.dml",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/scripts/builtin/randomForest.dml?ref=debdcbaf10790b6480026528be195d208262665a",
    "patch": "@@ -0,0 +1,1506 @@\n+#-------------------------------------------------------------\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+#\n+#-------------------------------------------------------------\n+\n+#\n+# THIS SCRIPT IMPLEMENTS CLASSIFICATION RANDOM FOREST WITH BOTH SCALE AND CATEGORICAL FEATURES\n+#\n+# INPUT         \t\tPARAMETERS:\n+# ---------------------------------------------------------------------------------------------\n+# NAME          \t\tTYPE     DEFAULT      MEANING\n+# ---------------------------------------------------------------------------------------------\n+# X             \t\tString   ---          Location to read feature matrix X; note that X needs to be both recoded and dummy coded\n+# Y \t\t\t\t\tString   ---\t\t  Location to read label matrix Y; note that Y needs to be both recoded and dummy coded\n+# R   \t  \t\t\t\tString   \" \"\t      Location to read the matrix R which for each feature in X contains the following information\n+#\t\t\t\t\t\t\t\t\t\t\t\t- R[,1]: column ids\n+#\t\t\t\t\t\t\t\t\t\t\t\t- R[,2]: start indices\n+#\t\t\t\t\t\t\t\t\t\t\t\t- R[,3]: end indices\n+#\t\t\t\t\t\t\t\t\t\t\t  If R is not provided by default all variables are assumed to be scale\n+# bins          \t\tInt \t 20\t\t\t  Number of equiheight bins per scale feature to choose thresholds\n+# depth         \t\tInt \t 25\t\t\t  Maximum depth of the learned tree\n+# num_leaf      \t\tInt      10           Number of samples when splitting stops and a leaf node is added\n+# num_samples   \t\tInt \t 3000\t\t  Number of samples at which point we switch to in-memory subtree building\n+# num_trees     \t\tInt \t 10\t\t\t  Number of trees to be learned in the random forest model\n+# subsamp_rate  \t\tDouble   1.0\t\t  Parameter controlling the size of each tree in the forest; samples are selected from a\n+#\t\t\t\t\t\t\t\t\t\t\t  Poisson distribution with parameter subsamp_rate (the default value is 1.0)\n+# feature_subset    \tDouble   0.5    \t  Parameter that controls the number of feature used as candidates for splitting at each tree node\n+#\t\t\t\t\t\t\t\t\t\t\t  as a power of number of features in the dataset;\n+#\t\t\t\t\t\t\t\t\t\t\t  by default square root of features (i.e., feature_subset = 0.5) are used at each tree node\n+# impurity      \t\tString   \"Gini\"    \t  Impurity measure: entropy or Gini (the default)\n+# M             \t\tString \t ---\t   \t  Location to write matrix M containing the learned tree\n+# C \t\t\t\t\tString   \" \"\t\t  Location to write matrix C containing the number of times samples are chosen in each tree of the random forest\n+# S_map\t\t\t\t\tString   \" \"\t\t  Location to write the mappings from scale feature ids to global feature ids\n+# C_map\t\t\t\t\tString   \" \"\t\t  Location to write the mappings from categorical feature ids to global feature ids\n+# fmt     \t    \t\tString   \"text\"       The output format of the model (matrix M), such as \"text\" or \"csv\"\n+# ---------------------------------------------------------------------------------------------\n+# OUTPUT:\n+# Matrix M where each column corresponds to a node in the learned tree and each row contains the following information:\n+#\t M[1,j]: id of node j (in a complete binary tree)\n+#\t M[2,j]: tree id to which node j belongs\n+#\t M[3,j]: Offset (no. of columns) to left child of j\n+#\t M[4,j]: Feature index of the feature that node j looks at if j is an internal node, otherwise 0\n+#\t M[5,j]: Type of the feature that node j looks at if j is an internal node: 1 for scale and 2 for categorical features,\n+#\t\t     otherwise the label that leaf node j is supposed to predict\n+#\t M[6,j]: 1 if j is an internal node and the feature chosen for j is scale, otherwise the size of the subset of values\n+#\t\t\t stored in rows 7,8,... if j is categorical\n+#\t M[7:,j]: Only applicable for internal nodes. Threshold the example's feature value is compared to is stored at M[7,j] if the feature chosen for j is scale;\n+# \t\t\t  If the feature chosen for j is categorical rows 7,8,... depict the value subset chosen for j\n+# -------------------------------------------------------------------------------------------\n+\n+\n+#Todo: Check if input through file and output as Matrix[Double] is ok\n+\n+s_randomForest = function (String X = \" \", String Y = \" \", String R = \" \", Integer bins = 20, Integer depth = 25,\n+                                            Integer num_leaf = 10, Integer num_samples = 3000, Integer num_trees = 1, Double subsamp_rate = 1.0, Double feature_subset = 0.5,\n+                                            String impurity = \"Gini\", String C = \" \", String C_map = \" \", String S_map = \" \", String fmt = \"text\")\n+   return(Double m)\n+{\n+\t #this is just a dummy function for now\n+\t m = 7;\n+\t print(\"entered s_randomForest\");\n+}\n+\n+m_randomForest = function(Matrix[Double] X, Matrix[Double] Y, Matrix[Double] R, Integer bins = 20, Integer depth = 25,\n+                           Integer num_leaf = 10, Integer num_samples = 3000, Integer num_trees = 1, Double subsamp_rate = 1.0, Double feature_subset = 0.5,\n+                           String impurity = \"Gini\", String C = \" \", String C_map = \" \", String S_map = \" \", String fmt = \"text\")\n+    return(Matrix[Double] M)\n+{\n+    print(\"started random-forest ...\")\n+\n+    if (length(X) == 0) {\n+        stop (\"Location for feature matrix X not found.\");\n+    }\n+\n+    if (length(Y) == 0) {\n+        stop (\"Location for label matrix Y not found.\");\n+    }\n+\n+\n+    fileR = R;\n+    fileC = C;\n+    fileS_map = S_map;\n+    fileC_map = C_map;\n+    num_bins = bins;\n+    threshold = num_samples;\n+    imp = impurity;\n+    fpow = feature_subset;\n+    fmt0 = fmt;\n+    rate = subsamp_rate;\n+\n+\n+    M = matrix(0, rows = 10,cols = 10);\n+\n+    #X = read(\"/home/deez/Downloads/temp/linRegData.train.data.csv\", data_type=\"matrix\", format=\"csv\");\n+    Y_bin = Y;\n+    num_records = nrow (X);\n+    num_classes = ncol (Y_bin);\n+\n+    # check if there is only one class label\n+    Y_bin_sum = sum (colSums (Y_bin) == num_records);\n+    if (Y_bin_sum == 1) {\n+    \tstop (\"Y contains only one class label. No model will be learned!\");\n+    } else if (Y_bin_sum > 1) {\n+    \tstop (\"Y is not properly dummy coded. Multiple columns of Y contain only ones!\")\n+    }\n+\n+    # split data into X_scale and X_cat\n+\n+    if (length(fileR) != 0) {\n+    \tR_tmp = fileR;\n+    \tR_tmp = order (target = R_tmp, by = 2); # sort by start indices\n+    \tdummy_coded = (R_tmp[,2] != R_tmp[,3]);\n+    \tR_scale = removeEmpty (target = R_tmp[,2:3] * (1 - dummy_coded), margin = \"rows\");\n+    \tR_cat = removeEmpty (target = R_tmp[,2:3] * dummy_coded, margin = \"rows\");\n+    \tif (fileS_map != \" \") {\n+    \t\tscale_feature_mapping = removeEmpty (target = (1 - dummy_coded) * seq (1, nrow (R_tmp)), margin = \"rows\");\n+    \t\t# write (scale_feature_mapping, fileS_map, format = fmtO);\n+    \t}\n+    \tif (fileC_map != \" \") {\n+    \t\tcat_feature_mapping = removeEmpty (target = dummy_coded * seq (1, nrow (R_tmp)), margin = \"rows\");\n+    \t\t# write (cat_feature_mapping, fileC_map, format = fmtO);\n+    \t}\n+    \tsum_dummy = sum (dummy_coded);\n+    \tif (sum_dummy == nrow (R_tmp)) { # all features categorical\n+    \t\tprint (\"All features categorical\");\n+    \t\tnum_cat_features = nrow (R_cat);\n+    \t\tnum_scale_features = 0;\n+    \t\tX_cat = X;\n+    \t\tdistinct_values = t (R_cat[,2] - R_cat[,1] + 1);\n+    \t\tdistinct_values_max = max (distinct_values);\n+    \t\tdistinct_values_offset = cumsum (t (distinct_values));\n+    \t\tdistinct_values_overall = sum (distinct_values);\n+    \t} else if (sum_dummy == 0) { # all features scale\n+    \t\tprint (\"All features scale\");\n+    \t\tnum_scale_features = ncol (X);\n+    \t\tnum_cat_features = 0;\n+    \t\tX_scale = X;\n+    \t\tdistinct_values_max = 1;\n+    \t} else { # some features scale some features categorical\n+    \t\tnum_cat_features = nrow (R_cat);\n+    \t\tnum_scale_features = nrow (R_scale);\n+    \t\tdistinct_values = t (R_cat[,2] - R_cat[,1] + 1);\n+    \t\tdistinct_values_max = max (distinct_values);\n+    \t\tdistinct_values_offset = cumsum (t (distinct_values));\n+    \t\tdistinct_values_overall = sum (distinct_values);\n+\n+    \t\tW = matrix (1, rows = num_cat_features, cols = 1) %*% matrix (\"1 -1\", rows = 1, cols = 2);\n+    \t\tW = matrix (W, rows = 2 * num_cat_features, cols = 1);\n+    \t\tif (as.scalar (R_cat[num_cat_features, 2]) == ncol (X)) {\n+    \t\t\tW[2 * num_cat_features,] = 0;\n+    \t\t}\n+\n+    \t\tlast = (R_cat[,2] != ncol (X));\n+    \t\tR_cat1 = (R_cat[,2] + 1) * last;\n+    \t\tR_cat[,2] = (R_cat[,2] * (1 - last)) + R_cat1;\n+    \t\tR_cat_vec = matrix (R_cat, rows = 2 * num_cat_features, cols = 1);\n+\n+    \t\tcol_tab = table (R_cat_vec, 1, W, ncol (X), 1);\n+    \t\tcol_ind = cumsum (col_tab);\n+\n+    \t\tcol_ind_cat = removeEmpty (target = col_ind * seq (1, ncol (X)), margin = \"rows\");\n+    \t\tcol_ind_scale = removeEmpty (target = (1 - col_ind) * seq (1, ncol (X)), margin = \"rows\");\n+    \t\tX_cat = X %*% table (col_ind_cat, seq (1, nrow (col_ind_cat)), ncol (X), nrow (col_ind_cat));\n+    \t\tX_scale = X %*% table (col_ind_scale, seq (1, nrow (col_ind_scale)), ncol (X), nrow (col_ind_scale));\n+    \t}\n+    } else { # only scale features exist\n+    \tprint (\"All features scale\");\n+    \tnum_scale_features = ncol (X);\n+    \tnum_cat_features = 0;\n+    \tX_scale = X;\n+    \tdistinct_values_max = 1;\n+    }\n+\n+    if (num_scale_features > 0) {\n+\n+    \tprint (\"COMPUTING BINNING...\");\n+    \tbin_size = max (as.integer (num_records / num_bins), 1);\n+    \tcount_thresholds = matrix (0, rows = 1, cols = num_scale_features)\n+    \tthresholds = matrix (0, rows = num_bins +1, cols = num_scale_features)\n+    \tparfor(i1 in 1:num_scale_features)\n+    \t{\n+    \t\tcol = order (target = X_scale[,i1], by = 1, decreasing = FALSE);\n+    \t\t#col_bins = matrix(0, rows = num_bins, cols = 1);\n+    \t\t[col_bins, num_bins_defined] = binning (col, bin_size, num_bins);\n+\n+    \t\t#print(as.scalar(col_bins));\n+    \t\tcount_thresholds[,i1] = num_bins_defined;\n+    \t\tthresholds[,i1] = col_bins;\n+    \t}\n+\n+    \tprint (\"PREPROCESSING SCALE FEATURE MATRIX...\");\n+    \tmin_num_bins = min (count_thresholds);\n+    \tmax_num_bins = max (count_thresholds);\n+    \ttotal_num_bins = sum (count_thresholds);\n+    \tcum_count_thresholds = t (cumsum (t (count_thresholds)));\n+    \tX_scale_ext = matrix (0, rows = num_records, cols = total_num_bins);\n+    \tparfor (i2 in 1:num_scale_features, check = 0) {\n+    \t\tXi2 = X_scale[,i2];\n+    \t\tcount_threshold = as.scalar (count_thresholds[,i2]);\n+    \t\toffset_feature = 1;\n+    \t\tif (i2 > 1) {\n+    \t\t\toffset_feature = offset_feature + as.integer (as.scalar (cum_count_thresholds[, (i2 - 1)]));\n+    \t\t}\n+\n+    \t\tti2 = t(thresholds[1:count_threshold, i2]);\n+    \t\tX_scale_ext[,offset_feature:(offset_feature + count_threshold - 1)] = outer (Xi2, ti2, \"<\");\n+    \t}\n+\n+    }\n+\n+\n+    num_features_total = num_scale_features + num_cat_features;\n+    num_feature_samples = as.integer (floor (num_features_total ^ fpow));\n+\n+\n+    #-------------------------------------------------------------------------------------\n+\n+    ##### INITIALIZATION\n+    L = matrix (1, rows = num_records, cols = num_trees); # last visited node id for each training sample\n+\n+    # create matrix of counts (generated by Poisson distribution) storing how many times each sample appears in each tree\n+    print (\"CONPUTING COUNTS...\");\n+    C = rand (rows = num_records, cols = num_trees, pdf = \"poisson\", lambda = rate);\n+    Ix_nonzero = (C != 0);\n+    L = L * Ix_nonzero;\n+    total_counts = sum (C);\n+\n+\n+    # model\n+    # LARGE leaf nodes\n+    # NC_large[,1]: node id\n+    # NC_large[,2]: tree id\n+    # NC_large[,3]: class label\n+    # NC_large[,4]: no. of misclassified samples\n+    # NC_large[,5]: 1 if special leaf (impure and 3 samples at that leaf > threshold) or 0 otherwise\n+    NC_large = matrix (0, rows = 5, cols = 1);\n+\n+    # SMALL leaf nodes\n+    # same schema as for LARGE leaf nodes (to be initialized)\n+    NC_small = matrix (0, rows = 5, cols = 1);\n+\n+    # LARGE internal nodes\n+    # Q_large[,1]: node id\n+    # Q_large[,2]: tree id\n+    Q_large = matrix (0, rows = 2, cols = num_trees);\n+    Q_large[1,] = matrix (1, rows = 1, cols = num_trees);\n+    Q_large[2,] = t (seq (1, num_trees));\n+\n+    # SMALL internal nodes\n+    # same schema as for LARGE internal nodes (to be initialized)\n+    Q_small = matrix (0, rows = 2, cols = 1);\n+\n+    # F_large[,1]: feature\n+    # F_large[,2]: type\n+    # F_large[,3]: offset\n+    F_large = matrix (0, rows = 3, cols = 1);\n+\n+    # same schema as for LARGE nodes\n+    F_small = matrix (0, rows = 3, cols = 1);\n+\n+    # split points for LARGE internal nodes\n+    S_large = matrix (0, rows = 1, cols = 1);\n+\n+    # split points for SMALL internal nodes\n+    S_small = matrix (0, rows = 1, cols = 1);\n+\n+    # initialize queue\n+    cur_nodes_large = matrix (1, rows = 2, cols = num_trees);\n+    cur_nodes_large[2,] = t (seq (1, num_trees));\n+\n+    num_cur_nodes_large = num_trees;\n+    num_cur_nodes_small = 0;\n+    level = 0;\n+\n+    while ((num_cur_nodes_large + num_cur_nodes_small) > 0 & level < depth) {\n+\n+    \tlevel = level + 1;\n+    \tprint (\" --- start level \" + level + \" --- \");\n+\n+    \t##### PREPARE MODEL\n+    \tif (num_cur_nodes_large > 0) { # LARGE nodes to process\n+    \t\tcur_Q_large = matrix (0, rows = 2, cols = 2 * num_cur_nodes_large);\n+    \t\tcur_NC_large = matrix (0, rows = 5, cols = 2 * num_cur_nodes_large);\n+    \t\tcur_F_large = matrix (0, rows = 3, cols = num_cur_nodes_large);\n+    \t\tcur_S_large = matrix (0, rows = 1, cols = num_cur_nodes_large * distinct_values_max);\n+    \t\tcur_nodes_small = matrix (0, rows = 3, cols = 2 * num_cur_nodes_large);\n+    \t}\n+\n+    \t##### LOOP OVER LARGE NODES...\n+    \tparfor (i6 in 1:num_cur_nodes_large, check = 0) {\n+\n+    \t\tcur_node = as.scalar (cur_nodes_large[1,i6]);\n+    \t\tcur_tree = as.scalar (cur_nodes_large[2,i6]);\n+\n+    \t\t# select sample features WOR\n+    \t\tfeature_samples = sample (num_features_total, num_feature_samples);\n+    \t\tfeature_samples = order (target = feature_samples, by = 1);\n+    \t\tnum_scale_feature_samples = sum (feature_samples <= num_scale_features);\n+    \t\tnum_cat_feature_samples = num_feature_samples - num_scale_feature_samples;\n+\n+    \t\t# --- find best split ---\n+    \t\t# samples that reach cur_node\n+    \t\tIx = (L[,cur_tree] == cur_node);\n+\n+    \t\tcur_Y_bin = Y_bin * (Ix * C[,cur_tree]);\n+    \t\tlabel_counts_overall = colSums (cur_Y_bin);\n+    \t\tlabel_sum_overall = sum (label_counts_overall);\n+    \t\tlabel_dist_overall = label_counts_overall / label_sum_overall;\n+\n+    \t\tif (imp == \"entropy\") {\n+    \t\t\tlabel_dist_zero = (label_dist_overall == 0);\n+    \t\t\tcur_impurity = - sum (label_dist_overall * log (label_dist_overall + label_dist_zero)); # / log (2); # impurity before\n+    \t\t} else { # imp == \"Gini\"\n+    \t\t\tcur_impurity = sum (label_dist_overall * (1 - label_dist_overall)); # impurity before\n+    \t\t}\n+    \t\tbest_scale_gain = 0;\n+    \t\tbest_cat_gain = 0;\n+    \t\tif (num_scale_features > 0 & num_scale_feature_samples > 0) {\n+\n+    \t\t\tscale_feature_samples = feature_samples[1:num_scale_feature_samples,];\n+\n+    \t\t\t# main operation\n+    \t\t\tlabel_counts_left_scale = t (t (cur_Y_bin) %*% X_scale_ext);\n+\n+    \t\t\t# compute left and right label distribution\n+    \t\t\tlabel_sum_left = rowSums (label_counts_left_scale);\n+    \t\t\tlabel_dist_left = label_counts_left_scale / label_sum_left;\n+    \t\t\tif (imp == \"entropy\") {\n+    \t\t\t\tlabel_dist_left = replace (target = label_dist_left, pattern = 0, replacement = 1);\n+    \t\t\t\tlog_label_dist_left = log (label_dist_left); # / log (2)\n+    \t\t\t\timpurity_left_scale = - rowSums (label_dist_left * log_label_dist_left);\n+    \t\t\t} else { # imp == \"Gini\"\n+    \t\t\t\timpurity_left_scale = rowSums (label_dist_left * (1 - label_dist_left));\n+    \t\t\t}\n+    \t\t\t#\n+    \t\t\tlabel_counts_right_scale = - label_counts_left_scale + label_counts_overall;\n+    \t\t\tlabel_sum_right = rowSums (label_counts_right_scale);\n+    \t\t\tlabel_dist_right = label_counts_right_scale / label_sum_right;\n+    \t\t\tif (imp == \"entropy\") {\n+    \t\t\t\tlabel_dist_right = replace (target = label_dist_right, pattern = 0, replacement = 1);\n+    \t\t\t\tlog_label_dist_right = log (label_dist_right); # / log (2)\n+    \t\t\t\timpurity_right_scale = - rowSums (label_dist_right * log_label_dist_right);\n+    \t\t\t} else { # imp == \"Gini\"\n+    \t\t\t\timpurity_right_scale = rowSums (label_dist_right * (1 - label_dist_right));\n+    \t\t\t}\n+\n+    \t\t\tI_gain_scale = cur_impurity - ( ( label_sum_left / label_sum_overall ) * impurity_left_scale + ( label_sum_right / label_sum_overall ) * impurity_right_scale);\n+\n+    \t\t\tI_gain_scale = replace (target = I_gain_scale, pattern = NaN, replacement = 0);\n+\n+    \t\t\t# determine best feature to split on and the split value\n+    \t\t\tfeature_start_ind = matrix (0, rows = 1, cols = num_scale_features);\n+    \t\t\tfeature_start_ind[1,1] = 1;\n+    \t\t\tif (num_scale_features > 1) {\n+    \t\t\t\tfeature_start_ind[1,2:num_scale_features] = cum_count_thresholds[1,1:(num_scale_features - 1)] + 1;\n+    \t\t\t}\n+    \t\t\tmax_I_gain_found = 0;\n+    \t\t\tmax_I_gain_found_ind = 0;\n+    \t\t\tbest_i = 0;\n+\n+    \t\t\tfor (i in 1:num_scale_feature_samples) { # assuming feature_samples is 5x1\n+    \t\t\t\tcur_feature_samples_bin = as.scalar (scale_feature_samples[i,]);\n+    \t\t\t\tcur_start_ind = as.scalar (feature_start_ind[,cur_feature_samples_bin]);\n+    \t\t\t\tcur_end_ind = as.scalar (cum_count_thresholds[,cur_feature_samples_bin]);\n+    \t\t\t\tI_gain_portion = I_gain_scale[cur_start_ind:cur_end_ind,];\n+    \t\t\t\tcur_max_I_gain = max (I_gain_portion);\n+    \t\t\t\tcur_max_I_gain_ind = as.scalar (rowIndexMax (t (I_gain_portion)));\n+    \t\t\t\tif (cur_max_I_gain > max_I_gain_found) {\n+    \t\t\t\t\tmax_I_gain_found = cur_max_I_gain;\n+    \t\t\t\t\tmax_I_gain_found_ind = cur_max_I_gain_ind;\n+    \t\t\t\t\tbest_i = i;\n+    \t\t\t\t}\n+    \t\t\t}\n+\n+    \t\t\tbest_scale_gain = max_I_gain_found;\n+    \t\t\tmax_I_gain_ind_scale = max_I_gain_found_ind;\n+    \t\t\tbest_scale_feature = 0;\n+    \t\t\tif (best_i > 0) {\n+    \t\t\t\tbest_scale_feature = as.scalar (scale_feature_samples[best_i,]);\n+    \t\t\t}\n+    \t\t\tbest_scale_split = max_I_gain_ind_scale;\n+    \t\t\tif (best_scale_feature > 1) {\n+    \t\t\t\tbest_scale_split = best_scale_split + as.scalar(cum_count_thresholds[,(best_scale_feature - 1)]);\n+    \t\t\t}\n+    \t\t}\n+\n+    \t\tif (num_cat_features > 0 & num_cat_feature_samples > 0){\n+\n+    \t\t\tcat_feature_samples = feature_samples[(num_scale_feature_samples + 1):(num_scale_feature_samples + num_cat_feature_samples),] - num_scale_features;\n+\n+    \t\t\t# initialization\n+    \t\t\tsplit_values_bin = matrix (0, rows = 1, cols = distinct_values_overall);\n+    \t\t\tsplit_values = split_values_bin;\n+    \t\t\tsplit_values_offset = matrix (0, rows = 1, cols = num_cat_features);\n+    \t\t\tI_gains = split_values_offset;\n+    \t\t\timpurities_left = split_values_offset;\n+    \t\t\timpurities_right = split_values_offset;\n+    \t\t\tbest_label_counts_left = matrix (0, rows = num_cat_features, cols = num_classes);\n+    \t\t\tbest_label_counts_right = matrix (0, rows = num_cat_features, cols = num_classes);\n+\n+    \t\t\t# main operation\n+    \t\t\tlabel_counts = t (t (cur_Y_bin) %*% X_cat);\n+\n+    \t\t\tparfor (i9 in 1:num_cat_feature_samples, check = 0){\n+\n+    \t\t\t\tcur_cat_feature = as.scalar (cat_feature_samples[i9,1]);\n+    \t\t\t\tstart_ind = 1;\n+    \t\t\t\tif (cur_cat_feature > 1) {\n+    \t\t\t\t\tstart_ind = start_ind + as.scalar (distinct_values_offset[(cur_cat_feature - 1),]);\n+    \t\t\t\t}\n+    \t\t\t\toffset = as.scalar (distinct_values[1,cur_cat_feature]);\n+\n+    \t\t\t\tcur_label_counts = label_counts[start_ind:(start_ind + offset - 1),];\n+\n+    \t\t\t\tlabel_sum = rowSums (cur_label_counts);\n+    \t\t\t\tlabel_dist = cur_label_counts / label_sum;\n+    \t\t\t\tif (imp == \"entropy\") {\n+    \t\t\t\t\tlabel_dist = replace (target = label_dist, pattern = 0, replacement = 1);\n+    \t\t\t\t\tlog_label_dist = log (label_dist); # / log(2)\n+    \t\t\t\t\timpurity_tmp = - rowSums (label_dist * log_label_dist);\n+    \t\t\t\t\timpurity_tmp = replace (target = impurity_tmp, pattern = NaN, replacement = 1/0);\n+    \t\t\t\t} else { # imp == \"Gini\"\n+    \t\t\t\t\timpurity_tmp = rowSums (label_dist * (1 - label_dist));\n+    \t\t\t\t}\n+\n+    \t\t\t\t# sort cur feature by impurity\n+    \t\t\t\tcur_distinct_values = seq (1, nrow (cur_label_counts));\n+    \t\t\t\tcur_distinct_values_impurity = cbind (cur_distinct_values, impurity_tmp);\n+    \t\t\t\tcur_feature_sorted = order (target = cur_distinct_values_impurity, by = 2, decreasing = FALSE);\n+    \t\t\t\tP = table (cur_distinct_values, cur_feature_sorted); # permutation matrix\n+    \t\t\t\tlabel_counts_sorted = P %*% cur_label_counts;\n+\n+    \t\t\t\t# compute left and right label distribution\n+    \t\t\t\tlabel_counts_left = cumsum (label_counts_sorted);\n+\n+    \t\t\t\tlabel_sum_left = rowSums (label_counts_left);\n+    \t\t\t\tlabel_dist_left = label_counts_left / label_sum_left;\n+    \t\t\t\tlabel_dist_left = replace (target = label_dist_left, pattern = NaN, replacement = 1);\n+    \t\t\t\tif (imp == \"entropy\") {\n+    \t\t\t\t\tlabel_dist_left = replace (target = label_dist_left, pattern = 0, replacement = 1);\n+    \t\t\t\t\tlog_label_dist_left = log (label_dist_left); # / log(2)\n+    \t\t\t\t\timpurity_left = - rowSums (label_dist_left * log_label_dist_left);\n+    \t\t\t\t} else { # imp == \"Gini\"\n+    \t\t\t\t\timpurity_left = rowSums (label_dist_left * (1 - label_dist_left));\n+    \t\t\t\t}\n+    \t\t\t\t#\n+    \t\t\t\tlabel_counts_right = - label_counts_left + label_counts_overall;\n+    \t\t\t\tlabel_sum_right = rowSums (label_counts_right);\n+    \t\t\t\tlabel_dist_right = label_counts_right / label_sum_right;\n+    \t\t\t\tlabel_dist_right = replace (target = label_dist_right, pattern = NaN, replacement = 1);\n+    \t\t\t\tif (imp == \"entropy\") {\n+    \t\t\t\t\tlabel_dist_right = replace (target = label_dist_right, pattern = 0, replacement = 1);\n+    \t\t\t\t\tlog_label_dist_right = log (label_dist_right); # / log (2)\n+    \t\t\t\t\timpurity_right = - rowSums (label_dist_right * log_label_dist_right);\n+    \t\t\t\t} else { # imp == \"Gini\"\n+    \t\t\t\t\timpurity_right = rowSums (label_dist_right * (1 - label_dist_right));\n+    \t\t\t\t}\n+    \t\t\t\tI_gain = cur_impurity - ( ( label_sum_left / label_sum_overall ) * impurity_left + ( label_sum_right / label_sum_overall ) * impurity_right);\n+\n+    \t\t\t\tIx_label_sum_left_zero = (label_sum_left == 0);\n+    \t\t\t\tIx_label_sum_right_zero = (label_sum_right == 0);\n+    \t\t\t\tIx_label_sum_zero = Ix_label_sum_left_zero * Ix_label_sum_right_zero;\n+    \t\t\t\tI_gain = I_gain * (1 - Ix_label_sum_zero);\n+\n+    \t\t\t\tI_gain[nrow (I_gain),] = 0; # last entry invalid\n+\n+    \t\t\t\tmax_I_gain_ind = as.scalar (rowIndexMax (t (I_gain)));\n+\n+    \t\t\t\tsplit_values[1, start_ind:(start_ind + max_I_gain_ind - 1)] = t (cur_feature_sorted[1:max_I_gain_ind,1]);\n+    \t\t\t\tfor (i10 in 1:max_I_gain_ind) {\n+    \t\t\t\t\tind = as.scalar (cur_feature_sorted[i10,1]);\n+    \t\t\t\t\tif (ind == 1) {\n+    \t\t\t\t\t\tsplit_values_bin[1,start_ind] = 1.0;\n+    \t\t\t\t\t} else {\n+    \t\t\t\t\t\tsplit_values_bin[1,(start_ind + ind - 1)] = 1.0;\n+    \t\t\t\t\t}\n+    \t\t\t\t}\n+    \t\t\t\tsplit_values_offset[1,cur_cat_feature] = max_I_gain_ind;\n+\n+    \t\t\t\tI_gains[1,cur_cat_feature] = max (I_gain);\n+\n+    \t\t\t\timpurities_left[1,cur_cat_feature] = as.scalar (impurity_left[max_I_gain_ind,]);\n+    \t\t\t\timpurities_right[1,cur_cat_feature] = as.scalar (impurity_right[max_I_gain_ind,]);\n+    \t\t\t\tbest_label_counts_left[cur_cat_feature,] = label_counts_left[max_I_gain_ind,];\n+    \t\t\t\tbest_label_counts_right[cur_cat_feature,] = label_counts_right[max_I_gain_ind,];\n+    \t\t\t}\n+\n+    \t\t\t# determine best feature to split on and the split values\n+    \t\t\tbest_cat_feature = as.scalar (rowIndexMax (I_gains));\n+    \t\t\tbest_cat_gain = max (I_gains);\n+    \t\t\tstart_ind = 1;\n+    \t\t\tif (best_cat_feature > 1) {\n+    \t\t\t\tstart_ind = start_ind + as.scalar (distinct_values_offset[(best_cat_feature - 1),]);\n+    \t\t\t}\n+    \t\t\toffset = as.scalar (distinct_values[1,best_cat_feature]);\n+    \t\t\tbest_split_values_bin = split_values_bin[1, start_ind:(start_ind + offset - 1)];\n+    \t\t}\n+\n+    \t\t# compare best scale feature to best cat. feature and pick the best one\n+    \t\tif (num_scale_features > 0 & num_scale_feature_samples > 0 & best_scale_gain >= best_cat_gain & best_scale_gain > 0) {\n+\n+    \t\t\t# --- update model ---\n+    \t\t\tcur_F_large[1,i6] = best_scale_feature;\n+    \t\t\tcur_F_large[2,i6] = 1;\n+    \t\t\tcur_F_large[3,i6] = 1;\n+    \t\t\tcur_S_large[1,(i6 - 1) * distinct_values_max + 1] = thresholds[max_I_gain_ind_scale, best_scale_feature];\n+\n+    \t\t\tleft_child = 2 * (cur_node - 1) + 1 + 1;\n+    \t\t\tright_child = 2 * (cur_node - 1) + 2 + 1;\n+\n+    \t\t\t# samples going to the left subtree\n+    \t\t\tIx_left = X_scale_ext[,best_scale_split];\n+\n+    \t\t\tIx_left = Ix * Ix_left;\n+    \t\t\tIx_right = Ix * (1 - Ix_left);\n+\n+    \t\t\tL[,cur_tree] = L[,cur_tree] * (1 - Ix_left) + (Ix_left * left_child);\n+    \t\t\tL[,cur_tree] = L[,cur_tree] * (1 - Ix_right) + (Ix_right * right_child);\n+\n+    \t\t\tleft_child_size = sum (Ix_left * C[,cur_tree]);\n+    \t\t\tright_child_size = sum (Ix_right * C[,cur_tree]);\n+\n+    \t\t\t# check if left or right child is a leaf\n+    \t\t\tleft_pure = FALSE;\n+    \t\t\tright_pure = FALSE;\n+    \t\t\tcur_impurity_left = as.scalar(impurity_left_scale[best_scale_split,]); # max_I_gain_ind_scale\n+    \t\t\tcur_impurity_right = as.scalar(impurity_right_scale[best_scale_split,]); # max_I_gain_ind_scale\n+    \t\t\tif ( (left_child_size <= num_leaf | cur_impurity_left == 0 | (level == depth)) &\n+    \t\t\t   (right_child_size <= num_leaf | cur_impurity_right == 0 | (level == depth)) |\n+    \t\t\t   (left_child_size <= threshold & right_child_size <= threshold & (level == depth)) ) { # both left and right nodes are leaf\n+\n+    \t\t\t\tcur_label_counts_left = label_counts_left_scale[best_scale_split,]; # max_I_gain_ind_scale\n+    \t\t\t\tcur_NC_large[1,(2 * (i6 - 1) + 1)] = left_child;\n+    \t\t\t\tcur_NC_large[2,(2 * (i6 - 1) + 1)] = cur_tree;\n+    \t\t\t\tcur_NC_large[3,(2 * (i6 - 1) + 1)] = as.scalar( rowIndexMax (cur_label_counts_left)); # leaf class label\n+    \t\t\t\tleft_pure = TRUE;\n+    \t\t\t\t# compute number of misclassified points\n+    \t\t\t\tcur_NC_large[4,(2 * (i6 - 1) + 1)] = left_child_size - max (cur_label_counts_left);\n+\n+    \t\t\t\tcur_label_counts_right = label_counts_overall - cur_label_counts_left;\n+    \t\t\t\tcur_NC_large[1,(2 * i6)] = right_child;\n+    \t\t\t\tcur_NC_large[2,(2 * i6)] = cur_tree;\n+    \t\t\t\tcur_NC_large[3,(2 * i6)] = as.scalar( rowIndexMax (cur_label_counts_right)); # leaf class label\n+    \t\t\t\tright_pure = TRUE;\n+    \t\t\t\t# compute number of misclassified pints\n+    \t\t\t\tcur_NC_large[4,(2 * i6)] = right_child_size - max (cur_label_counts_right);\n+\n+    \t\t\t} else if (left_child_size <= num_leaf | cur_impurity_left == 0 | (level == depth) |\n+    \t\t\t\t\t  (left_child_size <= threshold & (level == depth))) {\n+\n+    \t\t\t\tcur_label_counts_left = label_counts_left_scale[best_scale_split,]; # max_I_gain_ind_scale\n+    \t\t\t\tcur_NC_large[1,(2 * (i6 - 1) + 1)] = left_child;\n+    \t\t\t\tcur_NC_large[2,(2 * (i6 - 1) + 1)] = cur_tree;\n+    \t\t\t\tcur_NC_large[3,(2 * (i6 - 1) + 1)] = as.scalar( rowIndexMax (cur_label_counts_left)); # leaf class label\n+    \t\t\t\tleft_pure = TRUE;\n+    \t\t\t\t# compute number of misclassified points\n+    \t\t\t\tcur_NC_large[4,(2 * (i6 - 1) + 1)] = left_child_size - max (cur_label_counts_left);\n+\n+    \t\t\t} else if (right_child_size <= num_leaf | cur_impurity_right == 0 | (level == depth) |\n+    \t\t\t\t\t  (right_child_size <= threshold & (level == depth))) {\n+\n+    \t\t\t\tcur_label_counts_right = label_counts_right_scale[best_scale_split,]; # max_I_gain_ind_scale\n+    \t\t\t\tcur_NC_large[1,(2 * i6)] = right_child;\n+    \t\t\t\tcur_NC_large[2,(2 * i6)] = cur_tree;\n+    \t\t\t\tcur_NC_large[3,(2 * i6)] = as.scalar( rowIndexMax (cur_label_counts_right)); # leaf class label\n+    \t\t\t\tright_pure = TRUE;\n+    \t\t\t\t# compute number of misclassified pints\n+    \t\t\t\tcur_NC_large[4,(2 * i6)] = right_child_size - max (cur_label_counts_right);\n+\n+    \t\t\t}\n+\n+    \t\t} else if (num_cat_features > 0 & num_cat_feature_samples > 0 & best_cat_gain > 0) {\n+\n+    \t\t\t# --- update model ---\n+    \t\t\tcur_F_large[1,i6] = best_cat_feature;\n+    \t\t\tcur_F_large[2,i6] = 2;\n+    \t\t\toffset_nonzero = as.scalar (split_values_offset[1,best_cat_feature]);\n+    \t\t\tS_start_ind = (i6 - 1) * distinct_values_max + 1;\n+    \t\t\tcur_F_large[3,i6] = offset_nonzero;\n+    \t\t\tcur_S_large[1,S_start_ind:(S_start_ind + offset_nonzero - 1)] = split_values[1,start_ind:(start_ind + offset_nonzero - 1)];\n+\n+    \t\t\tleft_child = 2 * (cur_node - 1) + 1 + 1;\n+    \t\t\tright_child = 2 * (cur_node - 1) + 2 + 1;\n+\n+    \t\t\t# samples going to the left subtree\n+    \t\t\tIx_left = rowSums (X_cat[,start_ind:(start_ind + offset - 1)] * best_split_values_bin);\n+    \t\t\tIx_left = (Ix_left >= 1);\n+\n+    \t\t\tIx_left = Ix * Ix_left;\n+    \t\t\tIx_right = Ix * (1 - Ix_left);\n+\n+    \t\t\tL[,cur_tree] = L[,cur_tree] * (1 - Ix_left) + (Ix_left * left_child);\n+    \t\t\tL[,cur_tree] = L[,cur_tree] * (1 - Ix_right) + (Ix_right * right_child);\n+\n+    \t\t\tleft_child_size = sum (Ix_left * C[,cur_tree]);\n+    \t\t\tright_child_size = sum (Ix_right * C[,cur_tree]);\n+\n+    \t\t\t# check if left or right child is a leaf\n+    \t\t\tleft_pure = FALSE;\n+    \t\t\tright_pure = FALSE;\n+    \t\t\tcur_impurity_left = as.scalar(impurities_left[,best_cat_feature]);\n+    \t\t\tcur_impurity_right = as.scalar(impurities_right[,best_cat_feature]);\n+    \t\t\tif ( (left_child_size <= num_leaf | cur_impurity_left == 0 | (level == depth)) &\n+    \t\t\t   (right_child_size <= num_leaf | cur_impurity_right == 0 | (level == depth)) |\n+    \t\t\t   (left_child_size <= threshold & right_child_size <= threshold & (level == depth)) ) { # both left and right nodes are leaf\n+\n+    \t\t\t\tcur_label_counts_left = best_label_counts_left[best_cat_feature,];\n+    \t\t\t\tcur_NC_large[1,(2 * (i6 - 1) + 1)] = left_child;\n+    \t\t\t\tcur_NC_large[2,(2 * (i6 - 1) + 1)] = cur_tree;\n+    \t\t\t\tcur_NC_large[3,(2 * (i6 - 1) + 1)] = as.scalar( rowIndexMax (cur_label_counts_left)); # leaf class label\n+    \t\t\t\tleft_pure = TRUE;\n+    \t\t\t\t# compute number of misclassified points\n+    \t\t\t\tcur_NC_large[4,(2 * (i6 - 1) + 1)] = left_child_size - max (cur_label_counts_left);\n+\n+    \t\t\t\tcur_label_counts_right = label_counts_overall - cur_label_counts_left;\n+    \t\t\t\tcur_NC_large[1,(2 * i6)] = right_child;\n+    \t\t\t\tcur_NC_large[2,(2 * i6)] = cur_tree;\n+    \t\t\t\tcur_NC_large[3,(2 * i6)] = as.scalar( rowIndexMax (cur_label_counts_right)); # leaf class label\n+    \t\t\t\tright_pure = TRUE;\n+    \t\t\t\t# compute number of misclassified pints\n+    \t\t\t\tcur_NC_large[4,(2 * i6)] = right_child_size - max (cur_label_counts_right);\n+\n+    \t\t\t} else if (left_child_size <= num_leaf | cur_impurity_left == 0 | (level == depth) |\n+    \t\t\t\t\t  (left_child_size <= threshold & (level == depth))) {\n+\n+    \t\t\t\tcur_label_counts_left = best_label_counts_left[best_cat_feature,];\n+    \t\t\t\tcur_NC_large[1,(2 * (i6 - 1) + 1)] = left_child;\n+    \t\t\t\tcur_NC_large[2,(2 * (i6 - 1) + 1)] = cur_tree;\n+    \t\t\t\tcur_NC_large[3,(2 * (i6 - 1) + 1)] = as.scalar( rowIndexMax (cur_label_counts_left)); # leaf class label\n+    \t\t\t\tleft_pure = TRUE;\n+    \t\t\t\t# compute number of misclassified points\n+    \t\t\t\tcur_NC_large[4,(2 * (i6 - 1) + 1)] = left_child_size - max (cur_label_counts_left);\n+\n+    \t\t\t} else if (right_child_size <= num_leaf | cur_impurity_right == 0 | (level == depth) |\n+    \t\t\t\t\t  (right_child_size <= threshold & (level == depth))) {\n+\n+    \t\t\t\tcur_label_counts_right = best_label_counts_right[best_cat_feature,];\n+    \t\t\t\tcur_NC_large[1,(2 * i6)] = right_child;\n+    \t\t\t\tcur_NC_large[2,(2 * i6)] = cur_tree;\n+    \t\t\t\tcur_NC_large[3,(2 * i6)] = as.scalar( rowIndexMax (cur_label_counts_right)); # leaf class label\n+    \t\t\t\tright_pure = TRUE;\n+    \t\t\t\t# compute number of misclassified pints\n+    \t\t\t\tcur_NC_large[4,(2 * i6)] = right_child_size - max (cur_label_counts_right);\n+\n+    \t\t\t}\n+    \t\t} else {\n+\n+    \t\t\tprint (\"NUMBER OF SAMPLES AT NODE \" + cur_node + \" in tree \" + cur_tree + \" CANNOT BE REDUCED TO MATCH \" + num_leaf + \". THIS NODE IS DECLARED AS LEAF!\");\n+    \t\t\tright_pure = TRUE;\n+    \t\t\tleft_pure = TRUE;\n+    \t\t\tcur_NC_large[1,(2 * (i6 - 1) + 1)] = cur_node;\n+    \t\t\tcur_NC_large[2,(2 * (i6 - 1) + 1)] = cur_tree;\n+    \t\t\tclass_label = as.scalar (rowIndexMax (label_counts_overall));\n+    \t\t\tcur_NC_large[3,(2 * (i6 - 1) + 1)] = class_label;\n+    \t\t\tcur_NC_large[4,(2 * (i6 - 1) + 1)] = label_sum_overall - max (label_counts_overall);\n+    \t\t\tcur_NC_large[5,(2 * (i6 - 1) + 1)] = 1; # special leaf\n+\n+    \t\t}\n+\n+    \t\t# add nodes to Q\n+    \t\tif (!left_pure) {\n+    \t\t\tif (left_child_size > threshold) {\n+    \t\t\t\tcur_Q_large[1,(2 * (i6 - 1)+ 1)] = left_child;\n+    \t\t\t\tcur_Q_large[2,(2 * (i6 - 1)+ 1)] = cur_tree;\n+    \t\t\t} else {\n+    \t\t\t\tcur_nodes_small[1,(2 * (i6 - 1)+ 1)] = left_child;\n+    \t\t\t\tcur_nodes_small[2,(2 * (i6 - 1)+ 1)] = left_child_size;\n+    \t\t\t\tcur_nodes_small[3,(2 * (i6 - 1)+ 1)] = cur_tree;\n+    \t\t\t}\n+    \t\t}\n+    \t\tif (!right_pure) {\n+    \t\t\tif (right_child_size > threshold) {\n+    \t\t\t\tcur_Q_large[1,(2 * i6)] = right_child;\n+    \t\t\t\tcur_Q_large[2,(2 * i6)] = cur_tree;\n+    \t\t\t} else{\n+    \t\t\t\tcur_nodes_small[1,(2 * i6)] = right_child;\n+    \t\t\t\tcur_nodes_small[2,(2 * i6)] = right_child_size;\n+    \t\t\t\tcur_nodes_small[3,(2 * i6)] = cur_tree;\n+    \t\t\t}\n+    \t\t}\n+    \t}\n+\n+\n+\n+    \t##### PREPARE MODEL FOR LARGE NODES\n+    \tif (num_cur_nodes_large > 0) {\n+    \t\tcur_Q_large = removeEmpty (target = cur_Q_large, margin = \"cols\");\n+    \t\tif (as.scalar (cur_Q_large[1,1]) != 0) Q_large = cbind (Q_large, cur_Q_large);\n+    \t\tcur_NC_large = removeEmpty (target = cur_NC_large, margin = \"cols\");\n+    \t\tif (as.scalar (cur_NC_large[1,1]) != 0) NC_large = cbind (NC_large, cur_NC_large);\n+\n+    \t\tcur_F_large = removeEmpty (target = cur_F_large, margin = \"cols\");\n+    \t\tif (as.scalar (cur_F_large[1,1]) != 0) F_large = cbind (F_large, cur_F_large);\n+    \t\tcur_S_large = removeEmpty (target = cur_S_large, margin = \"cols\");\n+    \t\tif (as.scalar (cur_S_large[1,1]) != 0) S_large = cbind (S_large, cur_S_large);\n+\n+    \t\tnum_cur_nodes_large_pre = 2 * num_cur_nodes_large;\n+    \t\tif (as.scalar (cur_Q_large[1,1]) == 0) {\n+    \t\t\tnum_cur_nodes_large = 0;\n+    \t\t} else {\n+    \t\t\tcur_nodes_large = cur_Q_large;\n+    \t\t\tnum_cur_nodes_large = ncol (cur_Q_large);\n+    \t\t}\n+    \t}\n+\n+    \t##### PREPARE MODEL FOR SMALL NODES\n+    \tcur_nodes_small_nonzero = removeEmpty (target = cur_nodes_small, margin = \"cols\");\n+    \tif (as.scalar (cur_nodes_small_nonzero[1,1]) != 0) { # if SMALL nodes exist\n+    \t\tnum_cur_nodes_small = ncol (cur_nodes_small_nonzero);\n+    \t}\n+\n+    \tif (num_cur_nodes_small > 0) { # SMALL nodes to process\n+    \t\treserve_len = sum (2 ^ (ceil (log (cur_nodes_small_nonzero[2,]) / log (2)))) + num_cur_nodes_small;\n+    \t\tcur_Q_small =  matrix (0, rows = 2, cols = reserve_len);\n+    \t\tcur_F_small = matrix (0, rows = 3, cols = reserve_len);\n+    \t\tcur_NC_small = matrix (0, rows = 5, cols = reserve_len);\n+    \t\tcur_S_small = matrix (0, rows = 1, cols = reserve_len * distinct_values_max); # split values of the best feature\n+    \t}\n+\n+\n+\n+    \t##### LOOP OVER SMALL NODES...\n+\n+\n+    \tfor (i7 in 1:num_cur_nodes_small) {\n+\n+\n+\n+    \t\tcur_node_small = as.scalar (cur_nodes_small_nonzero[1,i7]);\n+    \t\tcur_tree_small = as.scalar (cur_nodes_small_nonzero[3,i7]);\n+    \t\t# build dataset for SMALL node\n+    \t\tIx = (L[,cur_tree_small] == cur_node_small);\n+    \t\t#print(as.scalar(Ix[0,0]));\n+    \t\tif (num_scale_features > 0) {\n+    \t\t\tX_scale_ext_small = removeEmpty (target = X_scale_ext, margin = \"rows\", select = Ix);\n+    \t\t}\n+    \t\tif (num_cat_features > 0) {\n+    \t\t\tX_cat_small = removeEmpty (target = X_cat, margin = \"rows\", select = Ix);\n+    \t\t}\n+\n+    \t\tL_small = removeEmpty (target = L * Ix, margin = \"rows\");\n+    \t\tC_small = removeEmpty (target = C * Ix, margin = \"rows\");\n+    \t\tY_bin_small = removeEmpty (target = Y_bin * Ix, margin = \"rows\");\n+\n+    \t\t# compute offset\n+    \t\toffsets = cumsum (t (2 ^ ceil (log (cur_nodes_small_nonzero[2,]) / log (2))));\n+    \t\tstart_ind_global = 1;\n+    \t\tif (i7 > 1) {\n+    \t\t\tstart_ind_global = start_ind_global + as.scalar (offsets[(i7 - 1),]);\n+    \t\t}\n+    \t\tstart_ind_S_global = 1;\n+    \t\tif (i7 > 1) {\n+    \t\t\tstart_ind_S_global = start_ind_S_global + (as.scalar (offsets[(i7 - 1),]) * distinct_values_max);\n+    \t\t}\n+\n+\n+    \t\tQ = matrix (0, rows = 2, cols = 1);\n+    \t\tQ[1,1] = cur_node_small;\n+    \t\tQ[2,1] = cur_tree_small;\n+    \t\tF = matrix (0, rows = 3, cols = 1);\n+    \t\tNC = matrix (0, rows = 5, cols = 1);\n+    \t\tS = matrix (0, rows = 1, cols = 1);\n+\n+    \t\tcur_nodes_ = matrix (cur_node_small, rows = 2, cols = 1);\n+    \t\tcur_nodes_[1,1] = cur_node_small;\n+    \t\tcur_nodes_[2,1] = cur_tree_small;\n+\n+    \t\tnum_cur_nodes = 1;\n+    \t\tlevel_ = level;\n+\n+\n+\n+\n+    \t\twhile (num_cur_nodes > 0 & level_ < depth) {\n+\n+    \t\t\tlevel_ = level_ + 1;\n+\n+    \t\t\tcur_Q = matrix (0, rows = 2, cols = 2 * num_cur_nodes);\n+    \t\t\tcur_F = matrix (0, rows = 3, cols = num_cur_nodes);\n+    \t\t\tcur_NC = matrix (0, rows = 5, cols = 2 * num_cur_nodes);\n+    \t\t\tcur_S = matrix (0, rows = 1, cols = num_cur_nodes * distinct_values_max);\n+\n+    \t\t\tparfor (i8 in 1:num_cur_nodes, check = 0) {\n+\n+    \t\t\t\tcur_node = as.scalar (cur_nodes_[1,i8]);\n+    \t\t\t\tcur_tree = as.scalar (cur_nodes_[2,i8]);\n+\n+    \t\t\t\t# select sample features WOR\n+    \t\t\t\tfeature_samples = sample (num_features_total, num_feature_samples);\n+    \t\t\t\tfeature_samples = order (target = feature_samples, by = 1);\n+    \t\t\t\tnum_scale_feature_samples = sum (feature_samples <= num_scale_features);\n+    \t\t\t\tnum_cat_feature_samples = num_feature_samples - num_scale_feature_samples;\n+\n+    \t\t\t\t# --- find best split ---\n+    \t\t\t\t# samples that reach cur_node\n+    \t\t\t\tIx = (L_small[,cur_tree] == cur_node);\n+    \t\t\t\tcur_Y_bin = Y_bin_small * (Ix * C_small[,cur_tree]);\n+    \t\t\t\tlabel_counts_overall = colSums (cur_Y_bin);\n+\n+    \t\t\t\tlabel_sum_overall = sum (label_counts_overall);\n+    \t\t\t\tlabel_dist_overall = label_counts_overall / label_sum_overall;\n+    \t\t\t\tif (imp == \"entropy\") {\n+    \t\t\t\t\tlabel_dist_zero = (label_dist_overall == 0);\n+    \t\t\t\t\tcur_impurity = - sum (label_dist_overall * log (label_dist_overall + label_dist_zero)); # / log (2);\n+    \t\t\t\t} else { # imp == \"Gini\"\n+    \t\t\t\t\tcur_impurity = sum (label_dist_overall * (1 - label_dist_overall));\n+    \t\t\t\t}\n+    \t\t\t\tbest_scale_gain = 0;\n+    \t\t\t\tbest_cat_gain = 0;\n+\n+\n+\n+    \t\t\t\tif (num_scale_features > 0 & num_scale_feature_samples > 0) {\n+\n+    \t\t\t\t\tscale_feature_samples = feature_samples[1:num_scale_feature_samples,];\n+\n+    \t\t\t\t\t# main operation\n+    \t\t\t\t\tlabel_counts_left_scale = t (t (cur_Y_bin) %*% X_scale_ext_small);\n+\n+    \t\t\t\t\t# compute left and right label distribution\n+    \t\t\t\t\tlabel_sum_left = rowSums (label_counts_left_scale);\n+    \t\t\t\t\tlabel_dist_left = label_counts_left_scale / label_sum_left;\n+    \t\t\t\t\tif (imp == \"entropy\") {\n+    \t\t\t\t\t\tlabel_dist_left = replace (target = label_dist_left, pattern = 0, replacement = 1);\n+    \t\t\t\t\t\tlog_label_dist_left = log (label_dist_left); # / log (2)\n+    \t\t\t\t\t\timpurity_left_scale = - rowSums (label_dist_left * log_label_dist_left);\n+    \t\t\t\t\t} else { # imp == \"Gini\"\n+    \t\t\t\t\t\timpurity_left_scale = rowSums (label_dist_left * (1 - label_dist_left));\n+    \t\t\t\t\t}\n+    \t\t\t\t\t#\n+    \t\t\t\t\tlabel_counts_right_scale = - label_counts_left_scale + label_counts_overall;\n+    \t\t\t\t\tlabel_sum_right = rowSums (label_counts_right_scale);\n+    \t\t\t\t\tlabel_dist_right = label_counts_right_scale / label_sum_right;\n+    \t\t\t\t\tif (imp == \"entropy\") {\n+    \t\t\t\t\t\tlabel_dist_right = replace (target = label_dist_right, pattern = 0, replacement = 1);\n+    \t\t\t\t\t\tlog_label_dist_right = log (label_dist_right); # log (2)\n+    \t\t\t\t\t\timpurity_right_scale = - rowSums (label_dist_right * log_label_dist_right);\n+    \t\t\t\t\t} else { # imp == \"Gini\"\n+    \t\t\t\t\t\timpurity_right_scale = rowSums (label_dist_right * (1 - label_dist_right));\n+    \t\t\t\t\t}\n+    \t\t\t\t\tI_gain_scale = cur_impurity - ( ( label_sum_left / label_sum_overall ) * impurity_left_scale + ( label_sum_right / label_sum_overall ) * impurity_right_scale);\n+\n+    \t\t\t\t\tI_gain_scale = replace (target = I_gain_scale, pattern = NaN, replacement = 0);\n+\n+    \t\t\t\t\t# determine best feature to split on and the split value\n+    \t\t\t\t\tfeature_start_ind = matrix (0, rows = 1, cols = num_scale_features);\n+    \t\t\t\t\tfeature_start_ind[1,1] = 1;\n+    \t\t\t\t\tif (num_scale_features > 1) {\n+    \t\t\t\t\t\tfeature_start_ind[1,2:num_scale_features] = cum_count_thresholds[1,1:(num_scale_features - 1)] + 1;\n+    \t\t\t\t\t}\n+    \t\t\t\t\tmax_I_gain_found = 0;\n+    \t\t\t\t\tmax_I_gain_found_ind = 0;\n+    \t\t\t\t\tbest_i = 0;\n+\n+    \t\t\t\t\tfor (i in 1:num_scale_feature_samples) { # assuming feature_samples is 5x1\n+    \t\t\t\t\t\tcur_feature_samples_bin = as.scalar (scale_feature_samples[i,]);\n+    \t\t\t\t\t\tcur_start_ind = as.scalar (feature_start_ind[,cur_feature_samples_bin]);\n+    \t\t\t\t\t\tcur_end_ind = as.scalar (cum_count_thresholds[,cur_feature_samples_bin]);\n+    \t\t\t\t\t\tI_gain_portion = I_gain_scale[cur_start_ind:cur_end_ind,];\n+    \t\t\t\t\t\tcur_max_I_gain = max (I_gain_portion);\n+    \t\t\t\t\t\tcur_max_I_gain_ind = as.scalar (rowIndexMax (t (I_gain_portion)));\n+    \t\t\t\t\t\tif (cur_max_I_gain > max_I_gain_found) {\n+    \t\t\t\t\t\t\tmax_I_gain_found = cur_max_I_gain;\n+    \t\t\t\t\t\t\tmax_I_gain_found_ind = cur_max_I_gain_ind;\n+    \t\t\t\t\t\t\tbest_i = i;\n+    \t\t\t\t\t\t}\n+    \t\t\t\t\t}\n+\n+    \t\t\t\t\tbest_scale_gain = max_I_gain_found;\n+    \t\t\t\t\tmax_I_gain_ind_scale = max_I_gain_found_ind;\n+    \t\t\t\t\tbest_scale_feature = 0;\n+    \t\t\t\t\tif (best_i > 0) {\n+    \t\t\t\t\t\tbest_scale_feature = as.scalar (scale_feature_samples[best_i,]);\n+    \t\t\t\t\t}\n+    \t\t\t\t\tbest_scale_split = max_I_gain_ind_scale;\n+    \t\t\t\t\tif (best_scale_feature > 1) {\n+    \t\t\t\t\t\tbest_scale_split = best_scale_split + as.scalar(cum_count_thresholds[,(best_scale_feature - 1)]);\n+    \t\t\t\t\t}\n+    \t\t\t\t}\n+\n+\n+\n+\n+    \t\t\t\tif (num_cat_features > 0 & num_cat_feature_samples > 0){\n+\n+    \t\t\t\t\tcat_feature_samples = feature_samples[(num_scale_feature_samples + 1):(num_scale_feature_samples + num_cat_feature_samples),] - num_scale_features;\n+\n+    \t\t\t\t\t# initialization\n+    \t\t\t\t\tsplit_values_bin = matrix (0, rows = 1, cols = distinct_values_overall);\n+    \t\t\t\t\tsplit_values = split_values_bin;\n+    \t\t\t\t\tsplit_values_offset = matrix (0, rows = 1, cols = num_cat_features);\n+    \t\t\t\t\tI_gains = split_values_offset;\n+    \t\t\t\t\timpurities_left = split_values_offset;\n+    \t\t\t\t\timpurities_right = split_values_offset;\n+    \t\t\t\t\tbest_label_counts_left = matrix (0, rows = num_cat_features, cols = num_classes);\n+    \t\t\t\t\tbest_label_counts_right = matrix (0, rows = num_cat_features, cols = num_classes);\n+\n+    \t\t\t\t\t# main operation\n+    \t\t\t\t\tlabel_counts = t (t (cur_Y_bin) %*% X_cat_small);\n+\n+    \t\t\t\t\tparfor (i9 in 1:num_cat_feature_samples, check = 0){\n+\n+    \t\t\t\t\t\tcur_cat_feature = as.scalar (cat_feature_samples[i9,1]);\n+    \t\t\t\t\t\tstart_ind = 1;\n+    \t\t\t\t\t\tif (cur_cat_feature > 1) {\n+    \t\t\t\t\t\t\tstart_ind = start_ind + as.scalar (distinct_values_offset[(cur_cat_feature - 1),]);\n+    \t\t\t\t\t\t}\n+    \t\t\t\t\t\toffset = as.scalar (distinct_values[1,cur_cat_feature]);\n+\n+    \t\t\t\t\t\tcur_label_counts = label_counts[start_ind:(start_ind + offset - 1),];\n+\n+    \t\t\t\t\t\tlabel_sum = rowSums (cur_label_counts);\n+    \t\t\t\t\t\tlabel_dist = cur_label_counts / label_sum;\n+    \t\t\t\t\t\tif (imp == \"entropy\") {\n+    \t\t\t\t\t\t\tlabel_dist = replace (target = label_dist, pattern = 0, replacement = 1);\n+    \t\t\t\t\t\t\tlog_label_dist = log (label_dist); # / log(2)\n+    \t\t\t\t\t\t\timpurity_tmp = - rowSums (label_dist * log_label_dist);\n+    \t\t\t\t\t\t\timpurity_tmp = replace (target = impurity_tmp, pattern = NaN, replacement = 1/0);\n+    \t\t\t\t\t\t} else { # imp == \"Gini\"\n+    \t\t\t\t\t\t\timpurity_tmp = rowSums (label_dist * (1 - label_dist));\n+    \t\t\t\t\t\t}\n+\n+    \t\t\t\t\t\t# sort cur feature by impurity\n+    \t\t\t\t\t\tcur_distinct_values = seq (1, nrow (cur_label_counts));\n+    \t\t\t\t\t\tcur_distinct_values_impurity = cbind (cur_distinct_values, impurity_tmp);\n+    \t\t\t\t\t\tcur_feature_sorted = order (target = cur_distinct_values_impurity, by = 2, decreasing = FALSE);\n+    \t\t\t\t\t\tP = table (cur_distinct_values, cur_feature_sorted); # permutation matrix\n+    \t\t\t\t\t\tlabel_counts_sorted = P %*% cur_label_counts;\n+\n+    \t\t\t\t\t\t# compute left and right label distribution\n+    \t\t\t\t\t\tlabel_counts_left = cumsum (label_counts_sorted);\n+\n+    \t\t\t\t\t\tlabel_sum_left = rowSums (label_counts_left);\n+    \t\t\t\t\t\tlabel_dist_left = label_counts_left / label_sum_left;\n+    \t\t\t\t\t\tlabel_dist_left = replace (target = label_dist_left, pattern = NaN, replacement = 1);\n+    \t\t\t\t\t\tif (imp == \"entropy\") {\n+    \t\t\t\t\t\t\tlabel_dist_left = replace (target = label_dist_left, pattern = 0, replacement = 1);\n+    \t\t\t\t\t\t\tlog_label_dist_left = log (label_dist_left); # / log(2)\n+    \t\t\t\t\t\t\timpurity_left = - rowSums (label_dist_left * log_label_dist_left);\n+    \t\t\t\t\t\t} else { # imp == \"Gini\"\n+    \t\t\t\t\t\t\timpurity_left = rowSums (label_dist_left * (1 - label_dist_left));\n+    \t\t\t\t\t\t}\n+    \t\t\t\t\t\t#\n+    \t\t\t\t\t\tlabel_counts_right = - label_counts_left + label_counts_overall;\n+    \t\t\t\t\t\tlabel_sum_right = rowSums (label_counts_right);\n+    \t\t\t\t\t\tlabel_dist_right = label_counts_right / label_sum_right;\n+    \t\t\t\t\t\tlabel_dist_right = replace (target = label_dist_right, pattern = NaN, replacement = 1);\n+    \t\t\t\t\t\tif (imp == \"entropy\") {\n+    \t\t\t\t\t\t\tlabel_dist_right = replace (target = label_dist_right, pattern = 0, replacement = 1);\n+    \t\t\t\t\t\t\tlog_label_dist_right = log (label_dist_right); # / log (2)\n+    \t\t\t\t\t\t\timpurity_right = - rowSums (label_dist_right * log_label_dist_right);\n+    \t\t\t\t\t\t} else { # imp == \"Gini\"\n+    \t\t\t\t\t\t\timpurity_right = rowSums (label_dist_right * (1 - label_dist_right));\n+    \t\t\t\t\t\t}\n+    \t\t\t\t\t\tI_gain = cur_impurity - ( ( label_sum_left / label_sum_overall ) * impurity_left + ( label_sum_right / label_sum_overall ) * impurity_right);\n+\n+    \t\t\t\t\t\tIx_label_sum_left_zero = (label_sum_left == 0);\n+    \t\t\t\t\t\tIx_label_sum_right_zero = (label_sum_right == 0);\n+    \t\t\t\t\t\tIx_label_sum_zero = Ix_label_sum_left_zero * Ix_label_sum_right_zero;\n+    \t\t\t\t\t\tI_gain = I_gain * (1 - Ix_label_sum_zero);\n+\n+    \t\t\t\t\t\tI_gain[nrow (I_gain),] = 0; # last entry invalid\n+\n+    \t\t\t\t\t\tmax_I_gain_ind = as.scalar (rowIndexMax (t (I_gain)));\n+\n+    \t\t\t\t\t\tsplit_values[1, start_ind:(start_ind + max_I_gain_ind - 1)] = t (cur_feature_sorted[1:max_I_gain_ind,1]);\n+    \t\t\t\t\t\tfor (i10 in 1:max_I_gain_ind) {\n+    \t\t\t\t\t\t\tind = as.scalar (cur_feature_sorted[i10,1]);\n+    \t\t\t\t\t\t\tif (ind == 1) {\n+    \t\t\t\t\t\t\t\tsplit_values_bin[1,start_ind] = 1.0;\n+    \t\t\t\t\t\t\t} else {\n+    \t\t\t\t\t\t\t\tsplit_values_bin[1,(start_ind + ind - 1)] = 1.0;\n+    \t\t\t\t\t\t\t}\n+    \t\t\t\t\t\t}\n+    \t\t\t\t\t\tsplit_values_offset[1,cur_cat_feature] = max_I_gain_ind;\n+\n+    \t\t\t\t\t\tI_gains[1,cur_cat_feature] = max (I_gain);\n+\n+    \t\t\t\t\t\timpurities_left[1,cur_cat_feature] = as.scalar (impurity_left[max_I_gain_ind,]);\n+    \t\t\t\t\t\timpurities_right[1,cur_cat_feature] = as.scalar (impurity_right[max_I_gain_ind,]);\n+    \t\t\t\t\t\tbest_label_counts_left[cur_cat_feature,] = label_counts_left[max_I_gain_ind,];\n+    \t\t\t\t\t\tbest_label_counts_right[cur_cat_feature,] = label_counts_right[max_I_gain_ind,];\n+    \t\t\t\t\t}\n+\n+    \t\t\t\t\t# determine best feature to split on and the split values\n+    \t\t\t\t\tbest_cat_feature = as.scalar (rowIndexMax (I_gains));\n+    \t\t\t\t\tbest_cat_gain = max (I_gains);\n+    \t\t\t\t\tstart_ind = 1;\n+    \t\t\t\t\tif (best_cat_feature > 1) {\n+    \t\t\t\t\t\tstart_ind = start_ind + as.scalar (distinct_values_offset[(best_cat_feature - 1),]);\n+    \t\t\t\t\t}\n+    \t\t\t\t\toffset = as.scalar (distinct_values[1,best_cat_feature]);\n+    \t\t\t\t\tbest_split_values_bin = split_values_bin[1, start_ind:(start_ind + offset - 1)];\n+    \t\t\t\t}\n+\n+    \t\t\t\t# compare best scale feature to best cat. feature and pick the best one\n+    \t\t\t\tif (num_scale_features > 0 & num_scale_feature_samples > 0 & best_scale_gain >= best_cat_gain & best_scale_gain > 0) {\n+\n+    \t\t\t\t\t# --- update model ---\n+    \t\t\t\t\tcur_F[1,i8] = best_scale_feature;\n+    \t\t\t\t\tcur_F[2,i8] = 1;\n+    \t\t\t\t\tcur_F[3,i8] = 1;\n+    \t\t\t\t\tcur_S[1,(i8 - 1) * distinct_values_max + 1] = thresholds[max_I_gain_ind_scale, best_scale_feature];\n+\n+    \t\t\t\t\tleft_child = 2 * (cur_node - 1) + 1 + 1;\n+    \t\t\t\t\tright_child = 2 * (cur_node - 1) + 2 + 1;\n+\n+    \t\t\t\t\t# samples going to the left subtree\n+    \t\t\t\t\tIx_left = X_scale_ext_small[, best_scale_split];\n+\n+    \t\t\t\t\tIx_left = Ix * Ix_left;\n+    \t\t\t\t\tIx_right = Ix * (1 - Ix_left);\n+\n+    \t\t\t\t\tL_small[,cur_tree] = L_small[,cur_tree] * (1 - Ix_left) + (Ix_left * left_child);\n+    \t\t\t\t\tL_small[,cur_tree] = L_small[,cur_tree] * (1 - Ix_right) + (Ix_right * right_child);\n+\n+    \t\t\t\t\tleft_child_size = sum (Ix_left * C_small[,cur_tree]);\n+    \t\t\t\t\tright_child_size = sum (Ix_right * C_small[,cur_tree]);\n+\n+    \t\t\t\t\t# check if left or right child is a leaf\n+    \t\t\t\t\tleft_pure = FALSE;\n+    \t\t\t\t\tright_pure = FALSE;\n+    \t\t\t\t\tcur_impurity_left = as.scalar(impurity_left_scale[best_scale_split,]);\n+    \t\t\t\t\tcur_impurity_right = as.scalar(impurity_right_scale[best_scale_split,]);\n+    \t\t\t\t\tif ( (left_child_size <= num_leaf | cur_impurity_left == 0 | level_ == depth) &\n+    \t\t\t\t\t   (right_child_size <= num_leaf | cur_impurity_right == 0 | level_ == depth) ) { # both left and right nodes are leaf\n+\n+    \t\t\t\t\t\tcur_label_counts_left = label_counts_left_scale[best_scale_split,];\n+    \t\t\t\t\t\tcur_NC[1,(2 * (i8 - 1) + 1)] = left_child;\n+    \t\t\t\t\t\tcur_NC[2,(2 * (i8 - 1) + 1)] = cur_tree;\n+    \t\t\t\t\t\tcur_NC[3,(2 * (i8 - 1) + 1)] = as.scalar( rowIndexMax (cur_label_counts_left)); # leaf class label\n+    \t\t\t\t\t\tleft_pure = TRUE;\n+    \t\t\t\t\t\t# compute number of misclassified points\n+    \t\t\t\t\t\tcur_NC[4,(2 * (i8 - 1) + 1)] = left_child_size - max (cur_label_counts_left);\n+\n+    \t\t\t\t\t\tcur_label_counts_right = label_counts_overall - cur_label_counts_left;\n+    \t\t\t\t\t\tcur_NC[1,(2 * i8)] = right_child;\n+    \t\t\t\t\t\tcur_NC[2,(2 * i8)] = cur_tree;\n+    \t\t\t\t\t\tcur_NC[3,(2 * i8)] = as.scalar( rowIndexMax (cur_label_counts_right)); # leaf class label\n+    \t\t\t\t\t\tright_pure = TRUE;\n+    \t\t\t\t\t\t# compute number of misclassified points\n+    \t\t\t\t\t\tcur_NC[4,(2 * i8)] = right_child_size - max (cur_label_counts_right);\n+\n+    \t\t\t\t\t} else if (left_child_size <= num_leaf | cur_impurity_left == 0 | level_ == depth) {\n+\n+    \t\t\t\t\t\tcur_label_counts_left = label_counts_left_scale[best_scale_split,];\n+    \t\t\t\t\t\tcur_NC[1,(2 * (i8 - 1) + 1)] = left_child;\n+    \t\t\t\t\t\tcur_NC[2,(2 * (i8 - 1) + 1)] = cur_tree;\n+    \t\t\t\t\t\tcur_NC[3,(2 * (i8 - 1) + 1)] = as.scalar( rowIndexMax (cur_label_counts_left)); # leaf class label\n+    \t\t\t\t\t\tleft_pure = TRUE;\n+    \t\t\t\t\t\t# compute number of misclassified points\n+    \t\t\t\t\t\tcur_NC[4,(2 * (i8 - 1) + 1)] = left_child_size - max (cur_label_counts_left);\n+\n+    \t\t\t\t\t} else if (right_child_size <= num_leaf | cur_impurity_right == 0 | level_ == depth) {\n+\n+    \t\t\t\t\t\tcur_label_counts_right = label_counts_right_scale[best_scale_split,];\n+    \t\t\t\t\t\tcur_NC[1,(2 * i8)] = right_child;\n+    \t\t\t\t\t\tcur_NC[2,(2 * i8)] = cur_tree;\n+    \t\t\t\t\t\tcur_NC[3,(2 * i8)] = as.scalar( rowIndexMax (cur_label_counts_right)); # leaf class label\n+    \t\t\t\t\t\tright_pure = TRUE;\n+    \t\t\t\t\t\t# compute number of misclassified points\n+    \t\t\t\t\t\tcur_NC[4,(2 * i8)] = right_child_size - max (cur_label_counts_right);\n+\n+    \t\t\t\t\t}\n+\n+    \t\t\t\t} else if (num_cat_features > 0 & num_cat_feature_samples > 0 & best_cat_gain > 0) {\n+\n+    \t\t\t\t\t# --- update model ---\n+    \t\t\t\t\tcur_F[1,i8] = best_cat_feature;\n+    \t\t\t\t\tcur_F[2,i8] = 2;\n+    \t\t\t\t\toffset_nonzero = as.scalar (split_values_offset[1,best_cat_feature]);\n+    \t\t\t\t\tS_start_ind = (i8 - 1) * distinct_values_max + 1;\n+    \t\t\t\t\tcur_F[3,i8] = offset_nonzero;\n+    \t\t\t\t\tcur_S[1,S_start_ind:(S_start_ind + offset_nonzero - 1)] = split_values[1,start_ind:(start_ind + offset_nonzero - 1)];\n+\n+    \t\t\t\t\tleft_child = 2 * (cur_node - 1) + 1 + 1;\n+    \t\t\t\t\tright_child = 2 * (cur_node - 1) + 2 + 1;\n+\n+    \t\t\t\t\t# samples going to the left subtree\n+    \t\t\t\t\tIx_left = rowSums (X_cat_small[,start_ind:(start_ind + offset - 1)] * best_split_values_bin);\n+    \t\t\t\t\tIx_left = (Ix_left >= 1);\n+\n+    \t\t\t\t\tIx_left = Ix * Ix_left;\n+    \t\t\t\t\tIx_right = Ix * (1 - Ix_left);\n+\n+    \t\t\t\t\tL_small[,cur_tree] = L_small[,cur_tree] * (1 - Ix_left) + (Ix_left * left_child);\n+    \t\t\t\t\tL_small[,cur_tree] = L_small[,cur_tree] * (1 - Ix_right) + (Ix_right * right_child);\n+\n+    \t\t\t\t\tleft_child_size = sum (Ix_left * C_small[,cur_tree]);\n+    \t\t\t\t\tright_child_size = sum (Ix_right * C_small[,cur_tree]);\n+\n+    \t\t\t\t\t# check if left or right child is a leaf\n+    \t\t\t\t\tleft_pure = FALSE;\n+    \t\t\t\t\tright_pure = FALSE;\n+    \t\t\t\t\tcur_impurity_left = as.scalar(impurities_left[,best_cat_feature]);\n+    \t\t\t\t\tcur_impurity_right = as.scalar(impurities_right[,best_cat_feature]);\n+    \t\t\t\t\tif ( (left_child_size <= num_leaf | cur_impurity_left == 0 | level_ == depth) &\n+    \t\t\t\t\t   (right_child_size <= num_leaf | cur_impurity_right == 0 | level_ == depth) ) { # both left and right nodes are leaf\n+\n+    \t\t\t\t\t\tcur_label_counts_left = best_label_counts_left[best_cat_feature,];\n+    \t\t\t\t\t\tcur_NC[1,(2 * (i8 - 1) + 1)] = left_child;\n+    \t\t\t\t\t\tcur_NC[2,(2 * (i8 - 1) + 1)] = cur_tree;\n+    \t\t\t\t\t\tcur_NC[3,(2 * (i8 - 1) + 1)] = as.scalar( rowIndexMax (cur_label_counts_left)); # leaf class label\n+    \t\t\t\t\t\tleft_pure = TRUE;\n+    \t\t\t\t\t\t# compute number of misclassified points\n+    \t\t\t\t\t\tcur_NC[4,(2 * (i8 - 1) + 1)] = left_child_size - max (cur_label_counts_left);\n+\n+    \t\t\t\t\t\tcur_label_counts_right = label_counts_overall - cur_label_counts_left;\n+    \t\t\t\t\t\tcur_NC[1,(2 * i8)] = right_child;\n+    \t\t\t\t\t\tcur_NC[2,(2 * i8)] = cur_tree;\n+    \t\t\t\t\t\tcur_NC[3,(2 * i8)] = as.scalar( rowIndexMax (cur_label_counts_right)); # leaf class label\n+    \t\t\t\t\t\tright_pure = TRUE;\n+    \t\t\t\t\t\t# compute number of misclassified points\n+    \t\t\t\t\t\tcur_NC[4,(2 * i8)] = right_child_size - max (cur_label_counts_right);\n+\n+    \t\t\t\t\t} else if (left_child_size <= num_leaf | cur_impurity_left == 0 | level_ == depth) {\n+\n+    \t\t\t\t\t\tcur_label_counts_left = best_label_counts_left[best_cat_feature,];\n+    \t\t\t\t\t\tcur_NC[1,(2 * (i8 - 1) + 1)] = left_child;\n+    \t\t\t\t\t\tcur_NC[2,(2 * (i8 - 1) + 1)] = cur_tree;\n+    \t\t\t\t\t\tcur_NC[3,(2 * (i8 - 1) + 1)] = as.scalar( rowIndexMax (cur_label_counts_left)); # leaf class label\n+    \t\t\t\t\t\tleft_pure = TRUE;\n+    \t\t\t\t\t\t# compute number of misclassified points\n+    \t\t\t\t\t\tcur_NC[4,(2 * (i8 - 1) + 1)] = left_child_size - max (cur_label_counts_left);\n+\n+    \t\t\t\t\t} else if (right_child_size <= num_leaf | cur_impurity_right == 0 | level_ == depth) {\n+    \t\t\t\t\t\tcur_label_counts_right = best_label_counts_right[best_cat_feature,];\n+    \t\t\t\t\t\tcur_NC[1,(2 * i8)] = right_child;\n+    \t\t\t\t\t\tcur_NC[2,(2 * i8)] = cur_tree;\n+    \t\t\t\t\t\tcur_NC[3,(2 * i8)] = as.scalar( rowIndexMax (cur_label_counts_right)); # leaf class label\n+    \t\t\t\t\t\tright_pure = TRUE;\n+    \t\t\t\t\t\t# compute number of misclassified points\n+    \t\t\t\t\t\tcur_NC[4,(2 * i8)] = right_child_size - max (cur_label_counts_right);\n+\n+    \t\t\t\t\t}\n+    \t\t\t\t} else {\n+\n+    \t\t\t\t\tprint (\"NUMBER OF SAMPLES AT NODE \" + cur_node + \" in tree \" + cur_tree + \" CANNOT BE REDUCED TO MATCH \" + num_leaf + \". THIS NODE IS DECLARED AS LEAF!\");\n+    \t\t\t\t\tright_pure = TRUE;\n+    \t\t\t\t\tleft_pure = TRUE;\n+    \t\t\t\t\tcur_NC[1,(2 * (i8 - 1) + 1)] = cur_node;\n+    \t\t\t\t\tcur_NC[2,(2 * (i8 - 1) + 1)] = cur_tree;\n+    \t\t\t\t\tclass_label = as.scalar (rowIndexMax (label_counts_overall));\n+    \t\t\t\t\tcur_NC[3,(2 * (i8 - 1) + 1)] = class_label;\n+    \t\t\t\t\tcur_NC[4,(2 * (i8 - 1) + 1)] = label_sum_overall - max (label_counts_overall);\n+    \t\t\t\t\tcur_NC[5,(2 * (i8 - 1) + 1)] = 1; # special leaf\n+\n+    \t\t\t\t}\n+\n+    \t\t\t\t# add nodes to Q\n+    \t\t\t\tif (!left_pure) {\n+    \t\t\t\t\tcur_Q[1,(2 * (i8 - 1)+ 1)] = left_child;\n+    \t\t\t\t\tcur_Q[2,(2 * (i8 - 1)+ 1)] = cur_tree;\n+    \t\t\t\t}\n+    \t\t\t\tif (!right_pure) {\n+    \t\t\t\t\tcur_Q[1,(2 * i8)] = right_child;\n+    \t\t\t\t\tcur_Q[2,(2 * i8)] = cur_tree;\n+    \t\t\t\t}\n+    \t\t\t}\n+\n+    \t\t\tcur_Q = removeEmpty (target = cur_Q, margin = \"cols\");\n+    \t\t\tQ = cbind (Q, cur_Q);\n+    \t\t\tNC = cbind (NC, cur_NC);\n+    \t\t\tF = cbind (F, cur_F);\n+    \t\t\tS = cbind (S, cur_S);\n+\n+    \t\t\tnum_cur_nodes_pre = 2 * num_cur_nodes;\n+    \t\t\tif (as.scalar (cur_Q[1,1]) == 0) {\n+    \t\t\t\tnum_cur_nodes = 0;\n+    \t\t\t} else {\n+    \t\t\t\tcur_nodes_ = cur_Q;\n+    \t\t\t\tnum_cur_nodes = ncol (cur_Q);\n+    \t\t\t}\n+    \t\t}\n+\n+\n+\n+\n+    \t\tcur_Q_small[,start_ind_global:(start_ind_global + ncol (Q) - 1)] = Q;\n+    \t\tcur_NC_small[,start_ind_global:(start_ind_global + ncol (NC) - 1)] = NC;\n+    \t\tcur_F_small[,start_ind_global:(start_ind_global + ncol (F) - 1)] = F;\n+    \t\tcur_S_small[,start_ind_S_global:(start_ind_S_global + ncol (S) - 1)] = S;\n+    \t}\n+\n+\n+    \t##### PREPARE MODEL FOR SMALL NODES\n+    \tif (num_cur_nodes_small > 0) {\t# small nodes already processed\n+    \t\tcur_Q_small = removeEmpty (target = cur_Q_small, margin = \"cols\");\n+    \t\tif (as.scalar (cur_Q_small[1,1]) != 0) Q_small = cbind (Q_small, cur_Q_small);\n+    \t\tcur_NC_small = removeEmpty (target = cur_NC_small, margin = \"cols\");\n+    \t\tif (as.scalar (cur_NC_small[1,1]) != 0) NC_small = cbind (NC_small, cur_NC_small);\n+\n+    \t\tcur_F_small = removeEmpty (target = cur_F_small, margin = \"cols\"); #\n+    \t\tif (as.scalar (cur_F_small[1,1]) != 0) F_small = cbind (F_small, cur_F_small);\n+    \t\tcur_S_small = removeEmpty (target = cur_S_small, margin = \"cols\"); #\n+    \t\tif (as.scalar (cur_S_small[1,1]) != 0) S_small = cbind (S_small, cur_S_small);\n+\n+    \t\tnum_cur_nodes_small = 0; # reset\n+    \t}\n+\n+    \tprint (\" --- end level \" + level + \", remaining no. of LARGE nodes to expand \" + num_cur_nodes_large + \" --- \");\n+    }\n+\n+    #### prepare model\n+    print (\"PREPARING MODEL...\")\n+    ### large nodes\n+    if (as.scalar (Q_large[1,1]) == 0 & ncol (Q_large) > 1) {\n+    \tQ_large = Q_large[,2:ncol (Q_large)];\n+    }\n+    if (as.scalar (NC_large[1,1]) == 0 & ncol (NC_large) > 1) {\n+    \tNC_large = NC_large[,2:ncol (NC_large)];\n+    }\n+    if (as.scalar (S_large[1,1]) == 0 & ncol (S_large) > 1) {\n+    \tS_large = S_large[,2:ncol (S_large)];\n+    }\n+    if (as.scalar (F_large[1,1]) == 0 & ncol (F_large) > 1) {\n+    \tF_large = F_large[,2:ncol (F_large)];\n+    }\n+    ### small nodes\n+    if (as.scalar (Q_small[1,1]) == 0 & ncol (Q_small) > 1) {\n+    \tQ_small = Q_small[,2:ncol (Q_small)];\n+    }\n+    if (as.scalar (NC_small[1,1]) == 0 & ncol (NC_small) > 1) {\n+    \tNC_small = NC_small[,2:ncol (NC_small)];\n+    }\n+    if (as.scalar (S_small[1,1]) == 0 & ncol (S_small) > 1) {\n+    \tS_small = S_small[,2:ncol (S_small)];\n+    }\n+    if (as.scalar (F_small[1,1]) == 0 & ncol (F_small) > 1) {\n+    \tF_small = F_small[,2:ncol (F_small)];\n+    }\n+\n+    # check for special leaves and if there are any remove them from Q_large and Q_small\n+    special_large_leaves_ind = NC_large[5,];\n+    num_special_large_leaf = sum (special_large_leaves_ind);\n+    if (num_special_large_leaf > 0) {\n+    \tprint (\"PROCESSING \" + num_special_large_leaf + \" SPECIAL LARGE LEAVES...\");\n+    \tspecial_large_leaves = removeEmpty (target = NC_large[1:2,] * special_large_leaves_ind, margin = \"cols\");\n+    \tlarge_internal_ind = 1 - colSums (outer (t (special_large_leaves[1,]), Q_large[1,], \"==\") * outer (t (special_large_leaves[2,]), Q_large[2,], \"==\"));\n+    \tQ_large = removeEmpty (target = Q_large * large_internal_ind, margin = \"cols\");\n+    \tF_large = removeEmpty (target = F_large, margin = \"cols\"); # remove special leaves from F\n+    }\n+\n+    special_small_leaves_ind = NC_small[5,];\n+    num_special_small_leaf = sum (special_small_leaves_ind);\n+    if (num_special_small_leaf > 0) {\n+    \tprint (\"PROCESSING \" + num_special_small_leaf + \" SPECIAL SMALL LEAVES...\");\n+    \tspecial_small_leaves = removeEmpty (target = NC_small[1:2,] * special_small_leaves_ind, margin = \"cols\");\n+    \tsmall_internal_ind = 1 - colSums (outer (t (special_small_leaves[1,]), Q_small[1,], \"==\") * outer (t (special_small_leaves[2,]), Q_small[2,], \"==\"));\n+    \tQ_small = removeEmpty (target = Q_small * small_internal_ind, margin = \"cols\");\n+    \tF_small = removeEmpty (target = F_small, margin = \"cols\"); # remove special leaves from F\n+    }\n+\n+    # model corresponding to large internal nodes\n+    no_large_internal_node = FALSE;\n+    if (as.scalar (Q_large[1,1]) != 0) {\n+    \tprint (\"PROCESSING LARGE INTERNAL NODES...\");\n+    \tnum_large_internal = ncol (Q_large);\n+    \tmax_offset = max (max (F_large[3,]), max (F_small[3,]));\n+    \tM1_large = matrix (0, rows = 6 + max_offset, cols = num_large_internal);\n+    \tM1_large[1:2,] = Q_large;\n+    \tM1_large[4:6,] = F_large;\n+    \t# process S_large\n+    \tcum_offsets_large = cumsum (t (F_large[3,]));\n+    \tparfor (it in 1:num_large_internal, check = 0) {\n+    \t\tstart_ind = 1;\n+    \t\tif (it > 1) {\n+    \t\t\tstart_ind = start_ind + as.scalar (cum_offsets_large[(it - 1),]);\n+    \t\t}\n+    \t\toffset = as.scalar (F_large[3,it]);\n+    \t\tM1_large[7:(7 + offset - 1),it] = t (S_large[1,start_ind:(start_ind + offset - 1)]);\n+    \t}\n+    } else {\n+    \tprint (\"No LARGE internal nodes available\");\n+    \tno_large_internal_node = TRUE;\n+    }\n+\n+    # model corresponding to small internal nodes\n+    no_small_internal_node = FALSE;\n+    if (as.scalar (Q_small[1,1]) != 0) {\n+    \tprint (\"PROCESSING SMALL INTERNAL NODES...\");\n+    \tnum_small_internal = ncol (Q_small);\n+    \tM1_small = matrix (0, rows = 6 + max_offset, cols = num_small_internal);\n+    \tM1_small[1:2,] = Q_small;\n+    \tM1_small[4:6,] = F_small;\n+    \t# process S_small\n+    \tcum_offsets_small = cumsum (t (F_small[3,]));\n+    \tparfor (it in 1:num_small_internal, check = 0) {\n+    \t\tstart_ind = 1;\n+    \t\tif (it > 1) {\n+    \t\t\tstart_ind = start_ind + as.scalar (cum_offsets_small[(it - 1),]);\n+    \t\t}\n+    \t\toffset = as.scalar (F_small[3,it]);\n+    \t\tM1_small[7:(7 + offset - 1),it] = t (S_small[1,start_ind:(start_ind + offset - 1)]);\n+    \t}\n+    } else {\n+    \tprint (\"No SMALL internal nodes available\");\n+    \tno_small_internal_node = TRUE;\n+    }\n+\n+    # model corresponding to large leaf nodes\n+    no_large_leaf_node = FALSE;\n+    if (as.scalar (NC_large[1,1]) != 0) {\n+    \tprint (\"PROCESSING LARGE LEAF NODES...\");\n+    \tnum_large_leaf = ncol (NC_large);\n+    \tM2_large = matrix (0, rows = 6 + max_offset, cols = num_large_leaf);\n+    \tM2_large[1:2,] = NC_large[1:2,];\n+    \tM2_large[5:7,] = NC_large[3:5,];\n+    } else {\n+    \tprint (\"No LARGE leaf nodes available\");\n+    \tno_large_leaf_node = TRUE;\n+    }\n+\n+    # model corresponding to small leaf nodes\n+    no_small_leaf_node = FALSE;\n+    if (as.scalar (NC_small[1,1]) != 0) {\n+    \tprint (\"PROCESSING SMALL LEAF NODES...\");\n+    \tnum_small_leaf = ncol (NC_small);\n+    \tM2_small = matrix (0, rows = 6 + max_offset, cols = num_small_leaf);\n+    \tM2_small[1:2,] = NC_small[1:2,];\n+    \tM2_small[5:7,] = NC_small[3:5,];\n+    } else {\n+    \tprint (\"No SMALL leaf nodes available\");\n+    \tno_small_leaf_node = TRUE;\n+    }\n+\n+    if (no_large_internal_node) {\n+    \tM1 = M1_small;\n+    } else if (no_small_internal_node) {\n+    \tM1 = M1_large;\n+    } else {\n+    \tM1 = cbind (M1_large, M1_small);\n+    }\n+\n+    if (no_large_leaf_node) {\n+    \tM2 = M2_small;\n+    } else if (no_small_leaf_node) {\n+    \tM2 = M2_large;\n+    } else {\n+    \tM2 = cbind (M2_large, M2_small);\n+    }\n+\n+    M = cbind (M1, M2);\n+    M = t (order (target = t (M), by = 1)); # sort by node id\n+    M = t (order (target = t (M), by = 2)); # sort by tree id\n+\n+\n+    # removing redundant subtrees\n+    if (ncol (M) > 1) {\n+    \tprint (\"CHECKING FOR REDUNDANT SUBTREES...\");\n+    \tred_leaf = TRUE;\n+    \tprocess_red_subtree = FALSE;\n+    \tinvalid_node_ind = matrix (0, rows = 1, cols = ncol (M));\n+    \twhile (red_leaf & ncol (M) > 1) {\n+    \t\tleaf_ind = (M[4,] == 0);\n+    \t\tlabels = M[5,] * leaf_ind;\n+    \t\ttree_ids = M[2,];\n+    \t\tparent_ids = floor (M[1,] /2);\n+    \t\tcond1 = (labels[,1:(ncol (M) - 1)] == labels[,2:ncol (M)]); # siebling leaves with same label\n+    \t\tcond2 = (parent_ids[,1:(ncol (M) - 1)] == parent_ids[,2:ncol (M)]); # same parents\n+    \t\tcond3 = (tree_ids[,1:(ncol (M) - 1)] == tree_ids[,2:ncol (M)]); # same tree\n+    \t\tred_leaf_ind =  cond1 * cond2 * cond3 * leaf_ind[,2:ncol (M)];\n+\n+    \t\tif (sum (red_leaf_ind) > 0) { # if redundant subtrees exist\n+    \t\t\tred_leaf_ids = M[1:2,2:ncol (M)] * red_leaf_ind;\n+    \t\t\tred_leaf_ids_nonzero = removeEmpty (target = red_leaf_ids, margin = \"cols\");\n+    \t\t\tparfor (it in 1:ncol (red_leaf_ids_nonzero), check = 0){\n+    \t\t\t\tcur_right_leaf_id = as.scalar (red_leaf_ids_nonzero[1,it]);\n+    \t\t\t\tcur_parent_id = floor (cur_right_leaf_id / 2);\n+    \t\t\t\tcur_tree_id = as.scalar (red_leaf_ids_nonzero[2,it]);\n+    \t\t\t\tcur_right_leaf_pos = as.scalar (rowIndexMax ((M[1,] == cur_right_leaf_id) * (M[2,] == cur_tree_id)));\n+    \t\t\t\tcur_parent_pos = as.scalar(rowIndexMax ((M[1,] == cur_parent_id) * (M[2,] == cur_tree_id)));\n+    \t\t\t\tM[3:nrow (M), cur_parent_pos] = M[3:nrow (M), cur_right_leaf_pos];\n+    \t\t\t\tM[4,cur_right_leaf_pos] = -1;\n+    \t\t\t\tM[4,cur_right_leaf_pos - 1] = -1;\n+    \t\t\t\tinvalid_node_ind[1,cur_right_leaf_pos] = 1;\n+    \t\t\t\tinvalid_node_ind[1,cur_right_leaf_pos - 1] = 1;\n+    \t\t\t}\n+    \t\t\tprocess_red_subtree = TRUE;\n+    \t\t} else {\n+    \t\t\tred_leaf = FALSE;\n+    \t\t}\n+    \t}\n+\n+    \tif (process_red_subtree) {\n+    \t\tprint (\"REMOVING REDUNDANT SUBTREES...\");\n+    \t\tvalid_node_ind = (invalid_node_ind == 0);\n+    \t\tM = removeEmpty (target = M * valid_node_ind, margin = \"cols\");\n+    \t}\n+    }\n+\n+    internal_ind = (M[4,] > 0);\n+    internal_ids = M[1:2,] * internal_ind;\n+    internal_ids_nonzero = removeEmpty (target = internal_ids, margin = \"cols\");\n+    if (as.scalar (internal_ids_nonzero[1,1]) > 0) { # if internal nodes exist\n+        a1 = internal_ids_nonzero[1,];\n+        a2 = internal_ids_nonzero[1,] * 2;\n+        vcur_tree_id = internal_ids_nonzero[2,];\n+        pos_a1 = rowIndexMax( outer(t(a1), M[1,], \"==\") * outer(t(vcur_tree_id), M[2,], \"==\") );\n+        pos_a2 = rowIndexMax( outer(t(a2), M[1,], \"==\") * outer(t(vcur_tree_id), M[2,], \"==\") );\n+        M[3,] = t(table(pos_a1, 1, pos_a2 - pos_a1, ncol(M), 1));\n+    }\n+    else {\n+        print (\"All trees in the random forest contain only one leaf!\");\n+    }\n+    if (fileC != \" \") {\n+    \t#write (C, fileC, format = fmtO);\n+    }\n+\n+\n+}\n+\n+#Function used to bin the col of the Matrix\n+# input sorted vector of double Values\n+# returns matrix of bins\n+binning =\n+    function (Matrix[double] col, int bin_size, int num_bins)\n+    return (Matrix[double] col_bins, int num_bins_defined)\n+{\n+    #print(\"binning\");\n+\n+    size_of_col = nrow(col);\n+\n+    #print(\"size of col is = \" + size_of_col);\n+\n+    position_in_input_col = 1;\n+\n+    col_bins_temp = matrix(0, rows = num_bins + 1, cols = 1);\n+\n+    num_bins_defined = 0;\n+\n+    col_bins_temp[1] = col[1];\n+\n+    condition_2 = 0;\n+\n+    while((position_in_input_col < size_of_col) & (condition_2 == 0))\n+    {\n+        if(as.scalar(col[position_in_input_col + 1]) == as.scalar(col_bins_temp[1]))\n+        {\n+            position_in_input_col = position_in_input_col + 1;\n+        }\n+        else\n+        {\n+            condition_2 = 1;\n+        }\n+    }\n+\n+     while((position_in_input_col < size_of_col) & (num_bins_defined < num_bins))\n+     {\n+        position_in_input_col = position_in_input_col + bin_size;\n+\n+        if(position_in_input_col >= size_of_col)\n+        {\n+            position_in_input_col = size_of_col;\n+        }\n+\n+        current_bin_entry = col[position_in_input_col];\n+\n+        col_bins_temp[num_bins_defined + 1] = current_bin_entry;\n+\n+        condition = 0;\n+\n+        while((position_in_input_col < size_of_col) & (condition == 0))\n+        {\n+            if(as.scalar(col[position_in_input_col + 1]) == as.scalar(current_bin_entry))\n+            {\n+                position_in_input_col = position_in_input_col + 1;\n+            }\n+            else\n+            {\n+                condition = 1;\n+            }\n+        }\n+\n+\n+        # I increase the current amount of bins\n+        # print(\"Number bins defined is:\" + num_bins_defined);\n+        num_bins_defined = num_bins_defined + 1;\n+     }\n+\n+    #Bins should be represented by the middle value (first bin + second bin /2)\n+    col_bins = matrix(0, rows = num_bins +1, cols = 1);\n+\n+\n+    for (i in 1:num_bins_defined)\n+    {\n+        col_bins[i] = (col_bins_temp[i] + col_bins_temp[i+1])/2;\n+    }\n+\n+}"
  },
  {
    "sha": "189307a03c581d46c0d680e3048680712a56b0c2",
    "filename": "src/test/java/org/apache/sysds/test/functions/builtin/BuiltinRandomForestTest.java",
    "status": "added",
    "additions": 87,
    "deletions": 0,
    "changes": 87,
    "blob_url": "https://github.com/apache/systemds/blob/debdcbaf10790b6480026528be195d208262665a/src/test/java/org/apache/sysds/test/functions/builtin/BuiltinRandomForestTest.java",
    "raw_url": "https://github.com/apache/systemds/raw/debdcbaf10790b6480026528be195d208262665a/src/test/java/org/apache/sysds/test/functions/builtin/BuiltinRandomForestTest.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/test/java/org/apache/sysds/test/functions/builtin/BuiltinRandomForestTest.java?ref=debdcbaf10790b6480026528be195d208262665a",
    "patch": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.sysds.test.functions.builtin;\n+\n+import org.junit.Test;\n+import org.apache.sysds.api.DMLScript;\n+import org.apache.sysds.common.Types;\n+import org.apache.sysds.hops.OptimizerUtils;\n+import org.apache.sysds.lops.LopProperties;\n+import org.apache.sysds.test.AutomatedTestBase;\n+import org.apache.sysds.test.TestConfiguration;\n+import org.apache.sysds.test.TestUtils;\n+\n+\n+/** \n+ * TODO FIX Stability. The test currently sometimes fails due to differences in test executions and random behaviour in operations.\n+*/\n+public class BuiltinKmeansTest extends AutomatedTestBase\n+{\n+\tprivate final static String TEST_NAME = \"kmeans\";\n+\tprivate final static String TEST_DIR = \"functions/builtin/\";\n+\tprivate static final String TEST_CLASS_DIR = TEST_DIR + BuiltinKmeansTest.class.getSimpleName() + \"/\";\n+\tprivate final static double eps = 1e-10;\n+\tprivate final static int rows = 1320;\n+\tprivate final static int cols = 32;\n+\tprivate final static double spSparse = 0.3;\n+\tprivate final static double spDense = 0.7;\n+\tprivate final static double max_iter = 50;\n+\n+\t@Override\n+\tpublic void setUp() {\n+\t\tTestUtils.clearAssertionInformation();\n+\t\taddTestConfiguration(TEST_NAME,new TestConfiguration(TEST_CLASS_DIR, TEST_NAME,new String[]{\"C\"}));\n+\t}\n+\n+\n+\tprivate void runRandomForestTest(int depth, int bins, int num_leafs,\n+\t\tint num_samples, int num_trees, double subsamp_rate, double, feature_subset,\n+\t\tString impurity)\n+\t{\n+\t\tTypes.ExecMode platformOld = setExecMode(instType);\n+\n+\t\tboolean oldFlag = OptimizerUtils.ALLOW_ALGEBRAIC_SIMPLIFICATION;\n+\t\tboolean sparkConfigOld = DMLScript.USE_LOCAL_SPARK_CONFIG;\n+\n+\t\ttry\n+\t\t{\n+\t\t\tString HOME = SCRIPT_DIR + TEST_DIR;\n+\n+\t\t\tfullDMLScriptName = HOME + TEST_NAME + \".dml\";\n+\n+\t\t\tOptimizerUtils.ALLOW_ALGEBRAIC_SIMPLIFICATION = rewrites;\n+\n+\t\t\t//generate actual datasets\n+\t\t\tdouble[][] X = getRandomMatrix(rows, cols, 0, 1, sparsity, 714);\n+\t\t\twriteInputMatrixWithMTD(\"X\", X, true);\n+\n+\n+\t\t\t//TODO: check how to run actual tests\n+\t\t\trunTest(true, false, null, -1);\n+\t\t}\n+\t\tfinally {\n+\t\t\trtplatform = platformOld;\n+\t\t\tDMLScript.USE_LOCAL_SPARK_CONFIG = sparkConfigOld;\n+\t\t\tOptimizerUtils.ALLOW_ALGEBRAIC_SIMPLIFICATION = oldFlag;\n+\t\t\tOptimizerUtils.ALLOW_AUTO_VECTORIZATION = true;\n+\t\t\tOptimizerUtils.ALLOW_OPERATOR_FUSION = true;\n+\t\t}\n+\t}\n+}"
  }
]
