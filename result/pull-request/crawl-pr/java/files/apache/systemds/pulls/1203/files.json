[
  {
    "sha": "b6e0d97170c100a748e2b1cbacb0c528c221d552",
    "filename": "src/main/java/org/apache/sysds/runtime/instructions/cp/MultiReturnParameterizedBuiltinCPInstruction.java",
    "status": "modified",
    "additions": 21,
    "deletions": 21,
    "changes": 42,
    "blob_url": "https://github.com/apache/systemds/blob/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/instructions/cp/MultiReturnParameterizedBuiltinCPInstruction.java",
    "raw_url": "https://github.com/apache/systemds/raw/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/instructions/cp/MultiReturnParameterizedBuiltinCPInstruction.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/instructions/cp/MultiReturnParameterizedBuiltinCPInstruction.java?ref=cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd",
    "patch": "@@ -33,14 +33,14 @@\n import org.apache.sysds.runtime.matrix.data.FrameBlock;\n import org.apache.sysds.runtime.matrix.data.MatrixBlock;\n import org.apache.sysds.runtime.matrix.operators.Operator;\n-import org.apache.sysds.runtime.transform.encode.Encoder;\n import org.apache.sysds.runtime.transform.encode.EncoderFactory;\n+import org.apache.sysds.runtime.transform.encode.MultiColumnEncoder;\n \n public class MultiReturnParameterizedBuiltinCPInstruction extends ComputationCPInstruction {\n \tprotected final ArrayList<CPOperand> _outputs;\n \n \tprivate MultiReturnParameterizedBuiltinCPInstruction(Operator op, CPOperand input1, CPOperand input2,\n-\t\t\tArrayList<CPOperand> outputs, String opcode, String istr) {\n+\t\tArrayList<CPOperand> outputs, String opcode, String istr) {\n \t\tsuper(CPType.MultiReturnBuiltin, op, input1, input2, outputs.get(0), opcode, istr);\n \t\t_outputs = outputs;\n \t}\n@@ -49,25 +49,25 @@ public CPOperand getOutput(int i) {\n \t\treturn _outputs.get(i);\n \t}\n \n-\tpublic List<CPOperand> getOutputs(){\n+\tpublic List<CPOperand> getOutputs() {\n \t\treturn _outputs;\n \t}\n \n \tpublic String[] getOutputNames() {\n-\t\treturn _outputs.stream().map(output -> output.getName()).toArray(String[]::new);\n+\t\treturn _outputs.stream().map(CPOperand::getName).toArray(String[]::new);\n \t}\n \n-\tpublic static MultiReturnParameterizedBuiltinCPInstruction parseInstruction ( String str ) {\n+\tpublic static MultiReturnParameterizedBuiltinCPInstruction parseInstruction(String str) {\n \t\tString[] parts = InstructionUtils.getInstructionPartsWithValueType(str);\n \t\tArrayList<CPOperand> outputs = new ArrayList<>();\n \t\tString opcode = parts[0];\n-\t\t\n-\t\tif ( opcode.equalsIgnoreCase(\"transformencode\") ) {\n+\n+\t\tif(opcode.equalsIgnoreCase(\"transformencode\")) {\n \t\t\t// one input and two outputs\n \t\t\tCPOperand in1 = new CPOperand(parts[1]);\n \t\t\tCPOperand in2 = new CPOperand(parts[2]);\n-\t\t\toutputs.add ( new CPOperand(parts[3], ValueType.FP64, DataType.MATRIX) );\n-\t\t\toutputs.add ( new CPOperand(parts[4], ValueType.STRING, DataType.FRAME) );\n+\t\t\toutputs.add(new CPOperand(parts[3], ValueType.FP64, DataType.MATRIX));\n+\t\t\toutputs.add(new CPOperand(parts[4], ValueType.STRING, DataType.FRAME));\n \t\t\treturn new MultiReturnParameterizedBuiltinCPInstruction(null, in1, in2, outputs, opcode, str);\n \t\t}\n \t\telse {\n@@ -76,20 +76,20 @@ public static MultiReturnParameterizedBuiltinCPInstruction parseInstruction ( St\n \n \t}\n \n-\t@Override \n+\t@Override\n \tpublic void processInstruction(ExecutionContext ec) {\n-\t\t//obtain and pin input frame\n+\t\t// obtain and pin input frame\n \t\tFrameBlock fin = ec.getFrameInput(input1.getName());\n \t\tString spec = ec.getScalarInput(input2).getStringValue();\n-\t\tString[] colnames = fin.getColumnNames(); \n-\t\t\n-\t\t//execute block transform encode\n-\t\tEncoder encoder = EncoderFactory.createEncoder(spec, colnames, fin.getNumColumns(), null);\n-\t\tMatrixBlock data = encoder.encode(fin, new MatrixBlock(fin.getNumRows(), fin.getNumColumns(), false)); //build and apply\n+\t\tString[] colnames = fin.getColumnNames();\n+\n+\t\t// execute block transform encode\n+\t\tMultiColumnEncoder encoder = EncoderFactory.createEncoder(spec, colnames, fin.getNumColumns(), null);\n+\t\tMatrixBlock data = encoder.encode(fin); // build and apply\n \t\tFrameBlock meta = encoder.getMetaData(new FrameBlock(fin.getNumColumns(), ValueType.STRING));\n \t\tmeta.setColumnNames(colnames);\n-\t\t\n-\t\t//release input and outputs\n+\n+\t\t// release input and outputs\n \t\tec.releaseFrameInput(input1.getName());\n \t\tec.setMatrixOutput(getOutput(0).getName(), data);\n \t\tec.setFrameOutput(getOutput(1).getName(), meta);\n@@ -103,10 +103,10 @@ public boolean hasSingleLineage() {\n \t@Override\n \t@SuppressWarnings({\"rawtypes\", \"unchecked\"})\n \tpublic Pair[] getLineageItems(ExecutionContext ec) {\n-\t\tLineageItem[] inputLineage = LineageItemUtils.getLineage(ec, input1,input2,input3);\n+\t\tLineageItem[] inputLineage = LineageItemUtils.getLineage(ec, input1, input2, input3);\n \t\tArrayList<Pair> items = new ArrayList<>();\n-\t\tfor (CPOperand out : _outputs)\n+\t\tfor(CPOperand out : _outputs)\n \t\t\titems.add(Pair.of(out.getName(), new LineageItem(getOpcode(), inputLineage)));\n-\t\treturn items.toArray(new Pair[items.size()]);\n+\t\treturn items.toArray(new Pair[0]);\n \t}\n }"
  },
  {
    "sha": "ec7c1c4d833417ea0b8e1e5065947219665c09fa",
    "filename": "src/main/java/org/apache/sysds/runtime/instructions/cp/ParameterizedBuiltinCPInstruction.java",
    "status": "modified",
    "additions": 183,
    "deletions": 179,
    "changes": 362,
    "blob_url": "https://github.com/apache/systemds/blob/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/instructions/cp/ParameterizedBuiltinCPInstruction.java",
    "raw_url": "https://github.com/apache/systemds/raw/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/instructions/cp/ParameterizedBuiltinCPInstruction.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/instructions/cp/ParameterizedBuiltinCPInstruction.java?ref=cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd",
    "patch": "@@ -55,8 +55,8 @@\n import org.apache.sysds.runtime.transform.TfUtils;\n import org.apache.sysds.runtime.transform.decode.Decoder;\n import org.apache.sysds.runtime.transform.decode.DecoderFactory;\n-import org.apache.sysds.runtime.transform.encode.Encoder;\n import org.apache.sysds.runtime.transform.encode.EncoderFactory;\n+import org.apache.sysds.runtime.transform.encode.MultiColumnEncoder;\n import org.apache.sysds.runtime.transform.meta.TfMetaUtils;\n import org.apache.sysds.runtime.transform.tokenize.Tokenizer;\n import org.apache.sysds.runtime.transform.tokenize.TokenizerFactory;\n@@ -74,166 +74,164 @@\n \tprotected final LinkedHashMap<String, String> params;\n \n \tprotected ParameterizedBuiltinCPInstruction(Operator op, LinkedHashMap<String, String> paramsMap, CPOperand out,\n-\t\t\tString opcode, String istr) {\n+\t\tString opcode, String istr) {\n \t\tsuper(CPType.ParameterizedBuiltin, op, null, null, out, opcode, istr);\n \t\tparams = paramsMap;\n \t}\n-\t\n-\tpublic HashMap<String,String> getParameterMap() { \n-\t\treturn params; \n+\n+\tpublic HashMap<String, String> getParameterMap() {\n+\t\treturn params;\n \t}\n-\t\n+\n \tpublic String getParam(String key) {\n \t\treturn getParameterMap().get(key);\n \t}\n-\t\n+\n \tpublic static LinkedHashMap<String, String> constructParameterMap(String[] params) {\n \t\t// process all elements in \"params\" except first(opcode) and last(output)\n-\t\tLinkedHashMap<String,String> paramMap = new LinkedHashMap<>();\n-\t\t\n+\t\tLinkedHashMap<String, String> paramMap = new LinkedHashMap<>();\n+\n \t\t// all parameters are of form <name=value>\n \t\tString[] parts;\n-\t\tfor ( int i=1; i <= params.length-2; i++ ) {\n+\t\tfor(int i = 1; i <= params.length - 2; i++) {\n \t\t\tparts = params[i].split(Lop.NAME_VALUE_SEPARATOR);\n \t\t\tparamMap.put(parts[0], parts[1]);\n \t\t}\n-\t\t\n+\n \t\treturn paramMap;\n \t}\n-\t\n-\tpublic static ParameterizedBuiltinCPInstruction parseInstruction ( String str ) {\n+\n+\tpublic static ParameterizedBuiltinCPInstruction parseInstruction(String str) {\n \t\tString[] parts = InstructionUtils.getInstructionPartsWithValueType(str);\n \t\t// first part is always the opcode\n \t\tString opcode = parts[0];\n \t\t// last part is always the output\n-\t\tCPOperand out = new CPOperand( parts[parts.length-1] ); \n+\t\tCPOperand out = new CPOperand(parts[parts.length - 1]);\n \n \t\t// process remaining parts and build a hash map\n-\t\tLinkedHashMap<String,String> paramsMap = constructParameterMap(parts);\n+\t\tLinkedHashMap<String, String> paramsMap = constructParameterMap(parts);\n \n \t\t// determine the appropriate value function\n \t\tValueFunction func = null;\n-\t\tif ( opcode.equalsIgnoreCase(\"cdf\") ) {\n-\t\t\tif ( paramsMap.get(\"dist\") == null ) \n+\t\tif(opcode.equalsIgnoreCase(\"cdf\")) {\n+\t\t\tif(paramsMap.get(\"dist\") == null)\n \t\t\t\tthrow new DMLRuntimeException(\"Invalid distribution: \" + str);\n \t\t\tfunc = ParameterizedBuiltin.getParameterizedBuiltinFnObject(opcode, paramsMap.get(\"dist\"));\n \t\t\t// Determine appropriate Function Object based on opcode\n \t\t\treturn new ParameterizedBuiltinCPInstruction(new SimpleOperator(func), paramsMap, out, opcode, str);\n \t\t}\n-\t\telse if ( opcode.equalsIgnoreCase(\"invcdf\") ) {\n-\t\t\tif ( paramsMap.get(\"dist\") == null ) \n+\t\telse if(opcode.equalsIgnoreCase(\"invcdf\")) {\n+\t\t\tif(paramsMap.get(\"dist\") == null)\n \t\t\t\tthrow new DMLRuntimeException(\"Invalid distribution: \" + str);\n \t\t\tfunc = ParameterizedBuiltin.getParameterizedBuiltinFnObject(opcode, paramsMap.get(\"dist\"));\n \t\t\t// Determine appropriate Function Object based on opcode\n \t\t\treturn new ParameterizedBuiltinCPInstruction(new SimpleOperator(func), paramsMap, out, opcode, str);\n \t\t}\n-\t\telse if ( opcode.equalsIgnoreCase(\"groupedagg\")) {\n+\t\telse if(opcode.equalsIgnoreCase(\"groupedagg\")) {\n \t\t\t// check for mandatory arguments\n \t\t\tString fnStr = paramsMap.get(\"fn\");\n-\t\t\tif ( fnStr == null ) \n+\t\t\tif(fnStr == null)\n \t\t\t\tthrow new DMLRuntimeException(\"Function parameter is missing in groupedAggregate.\");\n-\t\t\tif ( fnStr.equalsIgnoreCase(\"centralmoment\") ) {\n-\t\t\t\tif ( paramsMap.get(\"order\") == null )\n-\t\t\t\t\tthrow new DMLRuntimeException(\"Mandatory \\\"order\\\" must be specified when fn=\\\"centralmoment\\\" in groupedAggregate.\");\n+\t\t\tif(fnStr.equalsIgnoreCase(\"centralmoment\")) {\n+\t\t\t\tif(paramsMap.get(\"order\") == null)\n+\t\t\t\t\tthrow new DMLRuntimeException(\n+\t\t\t\t\t\t\"Mandatory \\\"order\\\" must be specified when fn=\\\"centralmoment\\\" in groupedAggregate.\");\n \t\t\t}\n-\t\t\t\n+\n \t\t\tOperator op = InstructionUtils.parseGroupedAggOperator(fnStr, paramsMap.get(\"order\"));\n \t\t\treturn new ParameterizedBuiltinCPInstruction(op, paramsMap, out, opcode, str);\n-\t\t} else if (opcode.equalsIgnoreCase(\"rmempty\")\n-\t\t\t\t|| opcode.equalsIgnoreCase(\"replace\")\n-\t\t\t\t|| opcode.equalsIgnoreCase(\"rexpand\")\n-\t\t\t\t|| opcode.equalsIgnoreCase(\"lowertri\")\n-\t\t\t\t|| opcode.equalsIgnoreCase(\"uppertri\")) {\n+\t\t}\n+\t\telse if(opcode.equalsIgnoreCase(\"rmempty\") || opcode.equalsIgnoreCase(\"replace\") ||\n+\t\t\topcode.equalsIgnoreCase(\"rexpand\") || opcode.equalsIgnoreCase(\"lowertri\") ||\n+\t\t\topcode.equalsIgnoreCase(\"uppertri\")) {\n \t\t\tfunc = ParameterizedBuiltin.getParameterizedBuiltinFnObject(opcode);\n \t\t\treturn new ParameterizedBuiltinCPInstruction(new SimpleOperator(func), paramsMap, out, opcode, str);\n-\t\t} else if (opcode.equals(\"transformapply\")\n-\t\t\t\t|| opcode.equals(\"transformdecode\")\n-\t\t\t\t|| opcode.equals(\"transformcolmap\")\n-\t\t\t\t|| opcode.equals(\"transformmeta\")\n-\t\t\t\t|| opcode.equals(\"tokenize\")\n-\t\t\t\t|| opcode.equals(\"toString\")\n-\t\t\t\t|| opcode.equals(\"nvlist\")) {\n+\t\t}\n+\t\telse if(opcode.equals(\"transformapply\") || opcode.equals(\"transformdecode\") ||\n+\t\t\topcode.equals(\"transformcolmap\") || opcode.equals(\"transformmeta\") || opcode.equals(\"tokenize\") ||\n+\t\t\topcode.equals(\"toString\") || opcode.equals(\"nvlist\")) {\n \t\t\treturn new ParameterizedBuiltinCPInstruction(null, paramsMap, out, opcode, str);\n-\t\t} else if (\"paramserv\".equals(opcode)) {\n+\t\t}\n+\t\telse if(\"paramserv\".equals(opcode)) {\n \t\t\treturn new ParamservBuiltinCPInstruction(null, paramsMap, out, opcode, str);\n-\t\t} else {\n+\t\t}\n+\t\telse {\n \t\t\tthrow new DMLRuntimeException(\"Unknown opcode (\" + opcode + \") for ParameterizedBuiltin Instruction.\");\n \t\t}\n \n \t}\n \n-\t@Override \n+\t@Override\n \tpublic void processInstruction(ExecutionContext ec) {\n \t\tString opcode = getOpcode();\n \t\tScalarObject sores = null;\n-\t\tif ( opcode.equalsIgnoreCase(\"cdf\")) {\n+\t\tif(opcode.equalsIgnoreCase(\"cdf\")) {\n \t\t\tSimpleOperator op = (SimpleOperator) _optr;\n-\t\t\tdouble result =  op.fn.execute(params);\n+\t\t\tdouble result = op.fn.execute(params);\n \t\t\tsores = new DoubleObject(result);\n \t\t\tec.setScalarOutput(output.getName(), sores);\n-\t\t} \n-\t\telse if ( opcode.equalsIgnoreCase(\"invcdf\")) {\n+\t\t}\n+\t\telse if(opcode.equalsIgnoreCase(\"invcdf\")) {\n \t\t\tSimpleOperator op = (SimpleOperator) _optr;\n-\t\t\tdouble result =  op.fn.execute(params);\n+\t\t\tdouble result = op.fn.execute(params);\n \t\t\tsores = new DoubleObject(result);\n \t\t\tec.setScalarOutput(output.getName(), sores);\n-\t\t} \n-\t\telse if ( opcode.equalsIgnoreCase(\"groupedagg\") ) {\n+\t\t}\n+\t\telse if(opcode.equalsIgnoreCase(\"groupedagg\")) {\n \t\t\t// acquire locks\n \t\t\tMatrixBlock target = ec.getMatrixInput(params.get(Statement.GAGG_TARGET));\n \t\t\tMatrixBlock groups = ec.getMatrixInput(params.get(Statement.GAGG_GROUPS));\n-\t\t\tMatrixBlock weights= null;\n-\t\t\tif ( params.get(Statement.GAGG_WEIGHTS) != null )\n+\t\t\tMatrixBlock weights = null;\n+\t\t\tif(params.get(Statement.GAGG_WEIGHTS) != null)\n \t\t\t\tweights = ec.getMatrixInput(params.get(Statement.GAGG_WEIGHTS));\n-\t\t\t\n+\n \t\t\tint ngroups = -1;\n-\t\t\tif ( params.get(Statement.GAGG_NUM_GROUPS) != null) {\n+\t\t\tif(params.get(Statement.GAGG_NUM_GROUPS) != null) {\n \t\t\t\tngroups = (int) Double.parseDouble(params.get(Statement.GAGG_NUM_GROUPS));\n \t\t\t}\n-\t\t\t\n+\n \t\t\t// compute the result\n-\t\t\tint k = Integer.parseInt(params.get(\"k\")); //num threads\n+\t\t\tint k = Integer.parseInt(params.get(\"k\")); // num threads\n \t\t\tMatrixBlock soresBlock = groups.groupedAggOperations(target, weights, new MatrixBlock(), ngroups, _optr, k);\n-\t\t\t\n+\n \t\t\tec.setMatrixOutput(output.getName(), soresBlock);\n \t\t\t// release locks\n \t\t\ttarget = groups = weights = null;\n \t\t\tec.releaseMatrixInput(params.get(Statement.GAGG_TARGET));\n \t\t\tec.releaseMatrixInput(params.get(Statement.GAGG_GROUPS));\n-\t\t\tif ( params.get(Statement.GAGG_WEIGHTS) != null )\n+\t\t\tif(params.get(Statement.GAGG_WEIGHTS) != null)\n \t\t\t\tec.releaseMatrixInput(params.get(Statement.GAGG_WEIGHTS));\n-\t\t\t\n+\n \t\t}\n-\t\telse if ( opcode.equalsIgnoreCase(\"rmempty\") ) {\n+\t\telse if(opcode.equalsIgnoreCase(\"rmempty\")) {\n \t\t\tString margin = params.get(\"margin\");\n-\t\t\tif( !(margin.equals(\"rows\") || margin.equals(\"cols\")) )\n-\t\t\t\tthrow new DMLRuntimeException(\"Unspupported margin identifier '\"+margin+\"'.\");\n-\t\t\t\n+\t\t\tif(!(margin.equals(\"rows\") || margin.equals(\"cols\")))\n+\t\t\t\tthrow new DMLRuntimeException(\"Unspupported margin identifier '\" + margin + \"'.\");\n+\n \t\t\t// acquire locks\n \t\t\tMatrixBlock target = ec.getMatrixInput(params.get(\"target\"));\n-\t\t\tMatrixBlock select = params.containsKey(\"select\") ?\n-\t\t\t\tec.getMatrixInput(params.get(\"select\")):null;\n-\t\t\t\n+\t\t\tMatrixBlock select = params.containsKey(\"select\") ? ec.getMatrixInput(params.get(\"select\")) : null;\n+\n \t\t\t// compute the result\n \t\t\tboolean emptyReturn = Boolean.parseBoolean(params.get(\"empty.return\").toLowerCase());\n-\t\t\tMatrixBlock soresBlock = target.removeEmptyOperations(new MatrixBlock(),\n-\t\t\t\tmargin.equals(\"rows\"), emptyReturn, select);\n-\t\t\t\n-\t\t\t//release locks\n+\t\t\tMatrixBlock soresBlock = target\n+\t\t\t\t.removeEmptyOperations(new MatrixBlock(), margin.equals(\"rows\"), emptyReturn, select);\n+\n+\t\t\t// release locks\n \t\t\tec.setMatrixOutput(output.getName(), soresBlock);\n \t\t\tec.releaseMatrixInput(params.get(\"target\"));\n-\t\t\tif (params.containsKey(\"select\"))\n+\t\t\tif(params.containsKey(\"select\"))\n \t\t\t\tec.releaseMatrixInput(params.get(\"select\"));\n \t\t}\n-\t\telse if ( opcode.equalsIgnoreCase(\"replace\") ) {\n+\t\telse if(opcode.equalsIgnoreCase(\"replace\")) {\n \t\t\tMatrixBlock target = ec.getMatrixInput(params.get(\"target\"));\n \t\t\tdouble pattern = Double.parseDouble(params.get(\"pattern\"));\n \t\t\tdouble replacement = Double.parseDouble(params.get(\"replacement\"));\n \t\t\tMatrixBlock ret = target.replaceOperations(new MatrixBlock(), pattern, replacement);\n \t\t\tec.setMatrixOutput(output.getName(), ret);\n \t\t\tec.releaseMatrixInput(params.get(\"target\"));\n \t\t}\n-\t\telse if ( opcode.equals(\"lowertri\") || opcode.equals(\"uppertri\")) {\n+\t\telse if(opcode.equals(\"lowertri\") || opcode.equals(\"uppertri\")) {\n \t\t\tMatrixBlock target = ec.getMatrixInput(params.get(\"target\"));\n \t\t\tboolean lower = opcode.equals(\"lowertri\");\n \t\t\tboolean diag = Boolean.parseBoolean(params.get(\"diag\"));\n@@ -242,271 +240,277 @@ else if ( opcode.equals(\"lowertri\") || opcode.equals(\"uppertri\")) {\n \t\t\tec.setMatrixOutput(output.getName(), ret);\n \t\t\tec.releaseMatrixInput(params.get(\"target\"));\n \t\t}\n-\t\telse if ( opcode.equalsIgnoreCase(\"rexpand\") ) {\n+\t\telse if(opcode.equalsIgnoreCase(\"rexpand\")) {\n \t\t\t// acquire locks\n \t\t\tMatrixBlock target = ec.getMatrixInput(params.get(\"target\"));\n-\t\t\t\n+\n \t\t\t// compute the result\n \t\t\tdouble maxVal = Double.parseDouble(params.get(\"max\"));\n \t\t\tboolean dirVal = params.get(\"dir\").equals(\"rows\");\n \t\t\tboolean cast = Boolean.parseBoolean(params.get(\"cast\"));\n \t\t\tboolean ignore = Boolean.parseBoolean(params.get(\"ignore\"));\n \t\t\tint numThreads = Integer.parseInt(params.get(\"k\"));\n-\t\t\tMatrixBlock ret = target.rexpandOperations(\n-\t\t\t\tnew MatrixBlock(), maxVal, dirVal, cast, ignore, numThreads);\n-\t\t\t\n-\t\t\t//release locks\n+\t\t\tMatrixBlock ret = target.rexpandOperations(new MatrixBlock(), maxVal, dirVal, cast, ignore, numThreads);\n+\n+\t\t\t// release locks\n \t\t\tec.setMatrixOutput(output.getName(), ret);\n \t\t\tec.releaseMatrixInput(params.get(\"target\"));\n \t\t}\n-\t\telse if ( opcode.equalsIgnoreCase(\"tokenize\") ) {\n-\t\t\t//acquire locks\n+\t\telse if(opcode.equalsIgnoreCase(\"tokenize\")) {\n+\t\t\t// acquire locks\n \t\t\tFrameBlock data = ec.getFrameInput(params.get(\"target\"));\n \n \t\t\t// compute tokenizer\n-\t\t\tTokenizer tokenizer = TokenizerFactory.createTokenizer(\n-\t\t\t\t\tgetParameterMap().get(\"spec\"), Integer.parseInt(getParameterMap().get(\"max_tokens\")));\n+\t\t\tTokenizer tokenizer = TokenizerFactory.createTokenizer(getParameterMap().get(\"spec\"),\n+\t\t\t\tInteger.parseInt(getParameterMap().get(\"max_tokens\")));\n \t\t\tFrameBlock fbout = tokenizer.tokenize(data, new FrameBlock(tokenizer.getSchema()));\n \n-\t\t\t//release locks\n+\t\t\t// release locks\n \t\t\tec.setFrameOutput(output.getName(), fbout);\n \t\t\tec.releaseFrameInput(params.get(\"target\"));\n \t\t}\n-\t\telse if ( opcode.equalsIgnoreCase(\"transformapply\")) {\n-\t\t\t//acquire locks\n+\t\telse if(opcode.equalsIgnoreCase(\"transformapply\")) {\n+\t\t\t// acquire locks\n \t\t\tFrameBlock data = ec.getFrameInput(params.get(\"target\"));\n \t\t\tFrameBlock meta = ec.getFrameInput(params.get(\"meta\"));\n \t\t\tString[] colNames = data.getColumnNames();\n-\t\t\t\n-\t\t\t//compute transformapply\n-\t\t\tEncoder encoder = EncoderFactory.createEncoder(params.get(\"spec\"), colNames, data.getNumColumns(), meta);\n-\t\t\tMatrixBlock mbout = encoder.apply(data, new MatrixBlock(data.getNumRows(), data.getNumColumns(), false));\n-\t\t\t\n-\t\t\t//release locks\n+\n+\t\t\t// compute transformapply\n+\t\t\tMultiColumnEncoder encoder = EncoderFactory\n+\t\t\t\t.createEncoder(params.get(\"spec\"), colNames, data.getNumColumns(), meta);\n+\t\t\tMatrixBlock mbout = encoder.apply(data);\n+\n+\t\t\t// release locks\n \t\t\tec.setMatrixOutput(output.getName(), mbout);\n \t\t\tec.releaseFrameInput(params.get(\"target\"));\n \t\t\tec.releaseFrameInput(params.get(\"meta\"));\n \t\t}\n-\t\telse if ( opcode.equalsIgnoreCase(\"transformdecode\")) {\n-\t\t\t//acquire locks\n+\t\telse if(opcode.equalsIgnoreCase(\"transformdecode\")) {\n+\t\t\t// acquire locks\n \t\t\tMatrixBlock data = ec.getMatrixInput(params.get(\"target\"));\n \t\t\tFrameBlock meta = ec.getFrameInput(params.get(\"meta\"));\n \t\t\tString[] colnames = meta.getColumnNames();\n-\t\t\t\n-\t\t\t//compute transformdecode\n-\t\t\tDecoder decoder = DecoderFactory.createDecoder(\n-\t\t\t\tgetParameterMap().get(\"spec\"), colnames, null, meta, data.getNumColumns());\n+\n+\t\t\t// compute transformdecode\n+\t\t\tDecoder decoder = DecoderFactory\n+\t\t\t\t.createDecoder(getParameterMap().get(\"spec\"), colnames, null, meta, data.getNumColumns());\n \t\t\tFrameBlock fbout = decoder.decode(data, new FrameBlock(decoder.getSchema()));\n \t\t\tfbout.setColumnNames(Arrays.copyOfRange(colnames, 0, fbout.getNumColumns()));\n-\t\t\t\n-\t\t\t//release locks\n+\n+\t\t\t// release locks\n \t\t\tec.setFrameOutput(output.getName(), fbout);\n \t\t\tec.releaseMatrixInput(params.get(\"target\"));\n \t\t\tec.releaseFrameInput(params.get(\"meta\"));\n \t\t}\n-\t\telse if ( opcode.equalsIgnoreCase(\"transformcolmap\")) {\n-\t\t\t//acquire locks\n+\t\telse if(opcode.equalsIgnoreCase(\"transformcolmap\")) {\n+\t\t\t// acquire locks\n \t\t\tFrameBlock meta = ec.getFrameInput(params.get(\"target\"));\n \t\t\tString[] colNames = meta.getColumnNames();\n-\t\t\t\n-\t\t\t//compute transformapply\n-\t\t\tEncoder encoder = EncoderFactory.createEncoder(params.get(\"spec\"), colNames, meta.getNumColumns(), null);\n-\t\t\tMatrixBlock mbout = encoder.getColMapping(meta, new MatrixBlock(meta.getNumColumns(), 3, false));\n-\t\t\t\n-\t\t\t//release locks\n+\n+\t\t\t// compute transformapply\n+\t\t\tMultiColumnEncoder encoder = EncoderFactory\n+\t\t\t\t.createEncoder(params.get(\"spec\"), colNames, meta.getNumColumns(), null);\n+\t\t\tMatrixBlock mbout = encoder.getColMapping(meta);\n+\n+\t\t\t// release locks\n \t\t\tec.setMatrixOutput(output.getName(), mbout);\n \t\t\tec.releaseFrameInput(params.get(\"target\"));\n \t\t}\n-\t\telse if ( opcode.equalsIgnoreCase(\"transformmeta\")) {\n-\t\t\t//get input spec and path\n+\t\telse if(opcode.equalsIgnoreCase(\"transformmeta\")) {\n+\t\t\t// get input spec and path\n \t\t\tString spec = getParameterMap().get(\"spec\");\n \t\t\tString path = getParameterMap().get(ParameterizedBuiltinFunctionExpression.TF_FN_PARAM_MTD);\n \t\t\tString delim = getParameterMap().getOrDefault(\"sep\", TfUtils.TXMTD_SEP);\n-\t\t\t\n-\t\t\t//execute transform meta data read\n+\n+\t\t\t// execute transform meta data read\n \t\t\tFrameBlock meta = null;\n \t\t\ttry {\n \t\t\t\tmeta = TfMetaUtils.readTransformMetaDataFromFile(spec, path, delim);\n \t\t\t}\n \t\t\tcatch(Exception ex) {\n \t\t\t\tthrow new DMLRuntimeException(ex);\n \t\t\t}\n-\t\t\t\n-\t\t\t//release locks\n+\n+\t\t\t// release locks\n \t\t\tec.setFrameOutput(output.getName(), meta);\n \t\t}\n-\t\telse if ( opcode.equalsIgnoreCase(\"toString\")) {\n-\t\t\t//handle input parameters\n-\t\t\tint rows = (getParam(\"rows\")!=null) ? Integer.parseInt(getParam(\"rows\")) : TOSTRING_MAXROWS;\n+\t\telse if(opcode.equalsIgnoreCase(\"toString\")) {\n+\t\t\t// handle input parameters\n+\t\t\tint rows = (getParam(\"rows\") != null) ? Integer.parseInt(getParam(\"rows\")) : TOSTRING_MAXROWS;\n \t\t\tint cols = (getParam(\"cols\") != null) ? Integer.parseInt(getParam(\"cols\")) : TOSTRING_MAXCOLS;\n \t\t\tint decimal = (getParam(\"decimal\") != null) ? Integer.parseInt(getParam(\"decimal\")) : TOSTRING_DECIMAL;\n \t\t\tboolean sparse = (getParam(\"sparse\") != null) ? Boolean.parseBoolean(getParam(\"sparse\")) : TOSTRING_SPARSE;\n \t\t\tString separator = (getParam(\"sep\") != null) ? getParam(\"sep\") : TOSTRING_SEPARATOR;\n \t\t\tString lineSeparator = (getParam(\"linesep\") != null) ? getParam(\"linesep\") : TOSTRING_LINESEPARATOR;\n-\t\t\t\n-\t\t\t//get input matrix/frame and convert to string\n+\n+\t\t\t// get input matrix/frame and convert to string\n \t\t\tString out = null;\n \n \t\t\tData cacheData = ec.getVariable(getParam(\"target\"));\n-\t\t\tif( cacheData instanceof MatrixObject) {\n-\t\t\t\tMatrixBlock matrix = ((MatrixObject)cacheData).acquireRead();\n+\t\t\tif(cacheData instanceof MatrixObject) {\n+\t\t\t\tMatrixBlock matrix = ((MatrixObject) cacheData).acquireRead();\n \t\t\t\twarnOnTrunction(matrix, rows, cols);\n \t\t\t\tout = DataConverter.toString(matrix, sparse, separator, lineSeparator, rows, cols, decimal);\n \t\t\t}\n-\t\t\telse if( cacheData instanceof TensorObject ) {\n-\t\t\t\tTensorBlock tensor = ((TensorObject)cacheData).acquireRead();\n+\t\t\telse if(cacheData instanceof TensorObject) {\n+\t\t\t\tTensorBlock tensor = ((TensorObject) cacheData).acquireRead();\n \t\t\t\t// TODO improve truncation to check all dimensions\n \t\t\t\twarnOnTrunction(tensor, rows, cols);\n-\t\t\t\tout = DataConverter.toString(tensor, sparse, separator,\n-\t\t\t\t\tlineSeparator, \"[\", \"]\", rows, cols, decimal);\n+\t\t\t\tout = DataConverter.toString(tensor, sparse, separator, lineSeparator, \"[\", \"]\", rows, cols, decimal);\n \t\t\t}\n-\t\t\telse if( cacheData instanceof FrameObject ) {\n+\t\t\telse if(cacheData instanceof FrameObject) {\n \t\t\t\tFrameBlock frame = ((FrameObject) cacheData).acquireRead();\n \t\t\t\twarnOnTrunction(frame, rows, cols);\n \t\t\t\tout = DataConverter.toString(frame, sparse, separator, lineSeparator, rows, cols, decimal);\n \t\t\t}\n-\t\t\telse if (cacheData instanceof ListObject){\n-\t\t\t\tout = DataConverter.toString((ListObject) cacheData, rows, cols,\n-\t\t\t\t\tsparse, separator, lineSeparator, rows, cols, decimal);\n+\t\t\telse if(cacheData instanceof ListObject) {\n+\t\t\t\tout = DataConverter.toString((ListObject) cacheData,\n+\t\t\t\t\trows,\n+\t\t\t\t\tcols,\n+\t\t\t\t\tsparse,\n+\t\t\t\t\tseparator,\n+\t\t\t\t\tlineSeparator,\n+\t\t\t\t\trows,\n+\t\t\t\t\tcols,\n+\t\t\t\t\tdecimal);\n \t\t\t}\n \t\t\telse {\n \t\t\t\tthrow new DMLRuntimeException(\"toString only converts matrix, tensors, lists or frames to string\");\n \t\t\t}\n-\t\t\tif(!(cacheData instanceof ListObject)){\n+\t\t\tif(!(cacheData instanceof ListObject)) {\n \t\t\t\tec.releaseCacheableData(getParam(\"target\"));\n \t\t\t}\n \t\t\tec.setScalarOutput(output.getName(), new StringObject(out));\n \t\t}\n-\t\telse if( opcode.equals(\"nvlist\") ) {\n-\t\t\t//obtain all input data objects and names in insertion order\n-\t\t\tList<Data> data = params.values().stream().map(d -> ec.containsVariable(d) ?\n-\t\t\t\tec.getVariable(d) : new StringObject(d)).collect(Collectors.toList());\n+\t\telse if(opcode.equals(\"nvlist\")) {\n+\t\t\t// obtain all input data objects and names in insertion order\n+\t\t\tList<Data> data = params.values().stream()\n+\t\t\t\t.map(d -> ec.containsVariable(d) ? ec.getVariable(d) : new StringObject(d))\n+\t\t\t\t.collect(Collectors.toList());\n \t\t\tList<String> names = new ArrayList<>(params.keySet());\n-\t\t\t\n-\t\t\t//create list object over all inputs\n+\n+\t\t\t// create list object over all inputs\n \t\t\tListObject list = new ListObject(data, names);\n \t\t\tlist.deriveAndSetStatusFromData();\n-\t\t\t\n+\n \t\t\tec.setVariable(output.getName(), list);\n \t\t}\n \t\telse {\n \t\t\tthrow new DMLRuntimeException(\"Unknown opcode : \" + opcode);\n \t\t}\n \t}\n-\t\n+\n \tprivate void warnOnTrunction(CacheBlock data, int rows, int cols) {\n-\t\t//warn on truncation because users might not be aware and use toString for verification\n-\t\tif( (getParam(\"rows\")==null && data.getNumRows()>rows)\n-\t\t\t|| (getParam(\"cols\")==null && data.getNumColumns()>cols) )\n-\t\t{\n-\t\t\tLOG.warn(\"Truncating \"+data.getClass().getSimpleName()+\" of size \"\n-\t\t\t\t+ data.getNumRows()+\"x\"+data.getNumColumns()+\" to \"+rows+\"x\"+cols+\". \"\n+\t\t// warn on truncation because users might not be aware and use toString for verification\n+\t\tif((getParam(\"rows\") == null && data.getNumRows() > rows) ||\n+\t\t\t(getParam(\"cols\") == null && data.getNumColumns() > cols)) {\n+\t\t\tLOG.warn(\"Truncating \" + data.getClass().getSimpleName() + \" of size \" + data.getNumRows() + \"x\"\n+\t\t\t\t+ data.getNumColumns() + \" to \" + rows + \"x\" + cols + \". \"\n \t\t\t\t+ \"Use toString(X, rows=..., cols=...) if necessary.\");\n \t\t}\n \t}\n \n \tprivate void warnOnTrunction(TensorBlock data, int rows, int cols) {\n-\t\t//warn on truncation because users might not be aware and use toString for verification\n-\t\tif( (getParam(\"rows\")==null && data.getDim(0)>rows)\n-\t\t\t|| (getParam(\"cols\")==null && data.getDim(1)>cols) )\n-\t\t{\n+\t\t// warn on truncation because users might not be aware and use toString for verification\n+\t\tif((getParam(\"rows\") == null && data.getDim(0) > rows) || (getParam(\"cols\") == null && data.getDim(1) > cols)) {\n \t\t\tStringBuilder sb = new StringBuilder();\n \t\t\tIntStream.range(0, data.getNumDims()).forEach((i) -> {\n-\t\t\t\tif ((i == data.getNumDims() - 1))\n+\t\t\t\tif((i == data.getNumDims() - 1))\n \t\t\t\t\tsb.append(data.getDim(i));\n \t\t\t\telse\n \t\t\t\t\tsb.append(data.getDim(i)).append(\"x\");\n \t\t\t});\n-\t\t\tLOG.warn(\"Truncating \"+data.getClass().getSimpleName()+\" of size \"+sb.toString()+\" to \"+rows+\"x\"+cols+\". \"\n-\t\t\t\t\t+ \"Use toString(X, rows=..., cols=...) if necessary.\");\n+\t\t\tLOG.warn(\"Truncating \" + data.getClass().getSimpleName() + \" of size \" + sb.toString() + \" to \" + rows + \"x\"\n+\t\t\t\t+ cols + \". \" + \"Use toString(X, rows=..., cols=...) if necessary.\");\n \t\t}\n \t}\n \n \t@Override\n \tpublic Pair<String, LineageItem> getLineageItem(ExecutionContext ec) {\n \t\tString opcode = getOpcode();\n-\t\tif (opcode.equalsIgnoreCase(\"groupedagg\")) {\n+\t\tif(opcode.equalsIgnoreCase(\"groupedagg\")) {\n \t\t\tCPOperand target = getTargetOperand();\n \t\t\tCPOperand groups = new CPOperand(params.get(Statement.GAGG_GROUPS), ValueType.FP64, DataType.MATRIX);\n-\t\t\tString wt = params.containsKey(Statement.GAGG_WEIGHTS) ? params.get(Statement.GAGG_WEIGHTS) : String.valueOf(-1);\n+\t\t\tString wt = params.containsKey(Statement.GAGG_WEIGHTS) ? params.get(Statement.GAGG_WEIGHTS) : String\n+\t\t\t\t.valueOf(-1);\n \t\t\tCPOperand weights = new CPOperand(wt, ValueType.FP64, DataType.MATRIX);\n \t\t\tCPOperand fn = getStringLiteral(Statement.GAGG_FN);\n-\t\t\tString ng = params.containsKey(Statement.GAGG_NUM_GROUPS) ? params.get(Statement.GAGG_NUM_GROUPS) : String.valueOf(-1);\n-\t\t\tCPOperand ngroups = new CPOperand(ng , ValueType.INT64, DataType.SCALAR, true);\n-\t\t\treturn Pair.of(output.getName(), new LineageItem(getOpcode(),\n-\t\t\t\tLineageItemUtils.getLineage(ec, target, groups, weights, fn, ngroups)));\n+\t\t\tString ng = params.containsKey(Statement.GAGG_NUM_GROUPS) ? params.get(Statement.GAGG_NUM_GROUPS) : String\n+\t\t\t\t.valueOf(-1);\n+\t\t\tCPOperand ngroups = new CPOperand(ng, ValueType.INT64, DataType.SCALAR, true);\n+\t\t\treturn Pair.of(output.getName(),\n+\t\t\t\tnew LineageItem(getOpcode(), LineageItemUtils.getLineage(ec, target, groups, weights, fn, ngroups)));\n \t\t}\n-\t\telse if (opcode.equalsIgnoreCase(\"rmempty\")) {\n+\t\telse if(opcode.equalsIgnoreCase(\"rmempty\")) {\n \t\t\tCPOperand target = getTargetOperand();\n \t\t\tCPOperand margin = getStringLiteral(\"margin\");\n \t\t\tString sl = params.containsKey(\"select\") ? params.get(\"select\") : String.valueOf(-1);\n-\t\t\tCPOperand select = new CPOperand(sl, ValueType.FP64, DataType.MATRIX); \n-\t\t\treturn Pair.of(output.getName(), new LineageItem(getOpcode(),\n-\t\t\t\tLineageItemUtils.getLineage(ec, target, margin, select)));\n+\t\t\tCPOperand select = new CPOperand(sl, ValueType.FP64, DataType.MATRIX);\n+\t\t\treturn Pair.of(output.getName(),\n+\t\t\t\tnew LineageItem(getOpcode(), LineageItemUtils.getLineage(ec, target, margin, select)));\n \t\t}\n \t\telse if(opcode.equalsIgnoreCase(\"replace\")) {\n \t\t\tCPOperand target = getTargetOperand();\n \t\t\tCPOperand pattern = getFP64Literal(\"pattern\");\n \t\t\tCPOperand replace = getFP64Literal(\"replacement\");\n-\t\t\treturn Pair.of(output.getName(), new LineageItem(getOpcode(),\n-\t\t\t\tLineageItemUtils.getLineage(ec, target, pattern, replace)));\n+\t\t\treturn Pair.of(output.getName(),\n+\t\t\t\tnew LineageItem(getOpcode(), LineageItemUtils.getLineage(ec, target, pattern, replace)));\n \t\t}\n \t\telse if(opcode.equalsIgnoreCase(\"rexpand\")) {\n \t\t\tCPOperand target = getTargetOperand();\n \t\t\tCPOperand max = getFP64Literal(\"max\");\n \t\t\tCPOperand dir = getStringLiteral(\"dir\");\n \t\t\tCPOperand cast = getBoolLiteral(\"cast\");\n \t\t\tCPOperand ignore = getBoolLiteral(\"ignore\");\n-\t\t\treturn Pair.of(output.getName(), new LineageItem(getOpcode(),\n-\t\t\t\tLineageItemUtils.getLineage(ec, target, max, dir, cast, ignore)));\n+\t\t\treturn Pair.of(output.getName(),\n+\t\t\t\tnew LineageItem(getOpcode(), LineageItemUtils.getLineage(ec, target, max, dir, cast, ignore)));\n \t\t}\n-\t\telse if (opcode.equalsIgnoreCase(\"lowertri\") || opcode.equalsIgnoreCase(\"uppertri\")) {\n+\t\telse if(opcode.equalsIgnoreCase(\"lowertri\") || opcode.equalsIgnoreCase(\"uppertri\")) {\n \t\t\tCPOperand target = getTargetOperand();\n \t\t\tCPOperand lower = getBoolLiteral(\"lowertri\");\n \t\t\tCPOperand diag = getBoolLiteral(\"diag\");\n \t\t\tCPOperand values = getBoolLiteral(\"values\");\n-\t\t\treturn Pair.of(output.getName(), new LineageItem(getOpcode(),\n-\t\t\t\tLineageItemUtils.getLineage(ec, target, lower, diag, values)));\n+\t\t\treturn Pair.of(output.getName(),\n+\t\t\t\tnew LineageItem(getOpcode(), LineageItemUtils.getLineage(ec, target, lower, diag, values)));\n \t\t}\n-\t\telse if (opcode.equalsIgnoreCase(\"transformdecode\") ||\n-\t\t\t\topcode.equalsIgnoreCase(\"transformapply\")) {\n+\t\telse if(opcode.equalsIgnoreCase(\"transformdecode\") || opcode.equalsIgnoreCase(\"transformapply\")) {\n \t\t\tCPOperand target = new CPOperand(params.get(\"target\"), ValueType.FP64, DataType.FRAME);\n \t\t\tCPOperand meta = getLiteral(\"meta\", ValueType.UNKNOWN, DataType.FRAME);\n \t\t\tCPOperand spec = getStringLiteral(\"spec\");\n-\t\t\treturn Pair.of(output.getName(), new LineageItem(getOpcode(),\n-\t\t\t\tLineageItemUtils.getLineage(ec, target, meta, spec)));\n+\t\t\treturn Pair.of(output.getName(),\n+\t\t\t\tnew LineageItem(getOpcode(), LineageItemUtils.getLineage(ec, target, meta, spec)));\n \t\t}\n \t\telse {\n-\t\t\t//NOTE: for now, we cannot have a generic fall through path, because the \n-\t\t\t//data and value types of parmeters are not compiled into the instruction\n-\t\t\tthrow new DMLRuntimeException(\"Unsupported lineage tracing for: \"+opcode);\n+\t\t\t// NOTE: for now, we cannot have a generic fall through path, because the\n+\t\t\t// data and value types of parmeters are not compiled into the instruction\n+\t\t\tthrow new DMLRuntimeException(\"Unsupported lineage tracing for: \" + opcode);\n \t\t}\n \t}\n-\t\n+\n \tpublic CacheableData<?> getTarget(ExecutionContext ec) {\n \t\treturn ec.getCacheableData(params.get(\"target\"));\n \t}\n-\t\n+\n \tprivate CPOperand getTargetOperand() {\n \t\treturn new CPOperand(params.get(\"target\"), ValueType.FP64, DataType.MATRIX);\n \t}\n-\t\n+\n \tprivate CPOperand getFP64Literal(String name) {\n \t\treturn getLiteral(name, ValueType.FP64);\n \t}\n-\t\n+\n \tprivate CPOperand getStringLiteral(String name) {\n \t\treturn getLiteral(name, ValueType.STRING);\n \t}\n-\t\n+\n \tprivate CPOperand getBoolLiteral(String name) {\n \t\treturn getLiteral(name, ValueType.BOOLEAN);\n \t}\n-\t\n+\n \tprivate CPOperand getLiteral(String name, ValueType vt) {\n \t\treturn new CPOperand(params.get(name), vt, DataType.SCALAR, true);\n \t}\n-\t\n+\n \tprivate CPOperand getLiteral(String name, ValueType vt, DataType dt) {\n \t\treturn new CPOperand(params.get(name), vt, dt);\n \t}"
  },
  {
    "sha": "0b8111646dfeaa0f02a65c76c0e732fc0a395b56",
    "filename": "src/main/java/org/apache/sysds/runtime/instructions/fed/MultiReturnParameterizedBuiltinFEDInstruction.java",
    "status": "modified",
    "additions": 46,
    "deletions": 63,
    "changes": 109,
    "blob_url": "https://github.com/apache/systemds/blob/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/instructions/fed/MultiReturnParameterizedBuiltinFEDInstruction.java",
    "raw_url": "https://github.com/apache/systemds/raw/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/instructions/fed/MultiReturnParameterizedBuiltinFEDInstruction.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/instructions/fed/MultiReturnParameterizedBuiltinFEDInstruction.java?ref=cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd",
    "patch": "@@ -51,16 +51,9 @@\n import org.apache.sysds.runtime.matrix.data.FrameBlock;\n import org.apache.sysds.runtime.matrix.data.MatrixBlock;\n import org.apache.sysds.runtime.matrix.operators.Operator;\n-import org.apache.sysds.runtime.transform.encode.Encoder;\n-import org.apache.sysds.runtime.transform.encode.EncoderBin;\n-import org.apache.sysds.runtime.transform.encode.EncoderComposite;\n-import org.apache.sysds.runtime.transform.encode.EncoderDummycode;\n+import org.apache.sysds.runtime.transform.encode.ColumnEncoderRecode;\n import org.apache.sysds.runtime.transform.encode.EncoderFactory;\n-import org.apache.sysds.runtime.transform.encode.EncoderFeatureHash;\n-import org.apache.sysds.runtime.transform.encode.EncoderMVImpute;\n-import org.apache.sysds.runtime.transform.encode.EncoderOmit;\n-import org.apache.sysds.runtime.transform.encode.EncoderPassThrough;\n-import org.apache.sysds.runtime.transform.encode.EncoderRecode;\n+import org.apache.sysds.runtime.transform.encode.MultiColumnEncoder;\n import org.apache.sysds.runtime.util.IndexRange;\n \n public class MultiReturnParameterizedBuiltinFEDInstruction extends ComputationFEDInstruction {\n@@ -100,38 +93,29 @@ public void processInstruction(ExecutionContext ec) {\n \t\t// obtain and pin input frame\n \t\tFrameObject fin = ec.getFrameObject(input1.getName());\n \t\tString spec = ec.getScalarInput(input2).getStringValue();\n-\t\t\n+\n \t\tString[] colNames = new String[(int) fin.getNumColumns()];\n \t\tArrays.fill(colNames, \"\");\n \n \t\t// the encoder in which the complete encoding information will be aggregated\n-\t\tEncoderComposite globalEncoder = new EncoderComposite(\n-\t\t\t// IMPORTANT: Encoder order matters\n-\t\t\tArrays.asList(new EncoderRecode(),\n-\t\t\t\tnew EncoderFeatureHash(),\n-\t\t\t\tnew EncoderPassThrough(),\n-\t\t\t\tnew EncoderBin(),\n-\t\t\t\tnew EncoderDummycode(),\n-\t\t\t\tnew EncoderOmit(true),\n-\t\t\t\tnew EncoderMVImpute()));\n+\t\tMultiColumnEncoder globalEncoder = new MultiColumnEncoder(new ArrayList<>());\n \t\t// first create encoders at the federated workers, then collect them and aggregate them to a single large\n \t\t// encoder\n \t\tFederationMap fedMapping = fin.getFedMapping();\n \t\tfedMapping.forEachParallel((range, data) -> {\n-\t\t\tint columnOffset = (int) range.getBeginDims()[1] + 1;\n+\t\t\tint columnOffset = (int) range.getBeginDims()[1];\n \n-\t\t\t// create an encoder with the given spec. The columnOffset (which is 1 based) has to be used to\n+\t\t\t// create an encoder with the given spec. The columnOffset (which is 0 based) has to be used to\n \t\t\t// tell the federated worker how much the indexes in the spec have to be offset.\n-\t\t\tFuture<FederatedResponse> responseFuture = data.executeFederatedOperation(\n-\t\t\t\tnew FederatedRequest(RequestType.EXEC_UDF, -1,\n-\t\t\t\t\tnew CreateFrameEncoder(data.getVarID(), spec, columnOffset)));\n+\t\t\tFuture<FederatedResponse> responseFuture = data.executeFederatedOperation(new FederatedRequest(\n+\t\t\t\tRequestType.EXEC_UDF, -1, new CreateFrameEncoder(data.getVarID(), spec, columnOffset + 1)));\n \t\t\t// collect responses with encoders\n \t\t\ttry {\n \t\t\t\tFederatedResponse response = responseFuture.get();\n-\t\t\t\tEncoder encoder = (Encoder) response.getData()[0];\n+\t\t\t\tMultiColumnEncoder encoder = (MultiColumnEncoder) response.getData()[0];\n \t\t\t\t// merge this encoder into a composite encoder\n \t\t\t\tsynchronized(globalEncoder) {\n-\t\t\t\t\tglobalEncoder.mergeAt(encoder, (int) (range.getBeginDims()[0] + 1), columnOffset);\n+\t\t\t\t\tglobalEncoder.mergeAt(encoder, columnOffset, (int) (range.getBeginDims()[0] + 1));\n \t\t\t\t}\n \t\t\t\t// no synchronization necessary since names should anyway match\n \t\t\t\tString[] subRangeColNames = (String[]) response.getData()[1];\n@@ -142,39 +126,38 @@ public void processInstruction(ExecutionContext ec) {\n \t\t\t}\n \t\t\treturn null;\n \t\t});\n-\t\t\n-\t\t//sort for consistent encoding in local and federated\n-\t\tif( EncoderRecode.SORT_RECODE_MAP ) {\n-\t\t\tfor(Encoder encoder : globalEncoder.getEncoders())\n-\t\t\t\tif( encoder instanceof EncoderRecode )\n-\t\t\t\t\t((EncoderRecode)encoder).sortCPRecodeMaps();\n+\n+\t\t// sort for consistent encoding in local and federated\n+\t\tif(ColumnEncoderRecode.SORT_RECODE_MAP) {\n+\t\t\tglobalEncoder.applyToAll(ColumnEncoderRecode.class, ColumnEncoderRecode::sortCPRecodeMaps);\n \t\t}\n-\t\t\n+\n \t\tFrameBlock meta = new FrameBlock((int) fin.getNumColumns(), Types.ValueType.STRING);\n \t\tmeta.setColumnNames(colNames);\n \t\tglobalEncoder.getMetaData(meta);\n \t\tglobalEncoder.initMetaData(meta);\n \n \t\tencodeFederatedFrames(fedMapping, globalEncoder, ec.getMatrixObject(getOutput(0)));\n-\t\t\n+\n \t\t// release input and outputs\n \t\tec.setFrameOutput(getOutput(1).getName(), meta);\n \t}\n-\t\n-\tpublic static void encodeFederatedFrames(FederationMap fedMapping, Encoder globalEncoder,\n+\n+\tpublic static void encodeFederatedFrames(FederationMap fedMapping, MultiColumnEncoder globalencoder,\n \t\tMatrixObject transformedMat) {\n \t\tlong varID = FederationUtils.getNextFedDataID();\n \t\tFederationMap transformedFedMapping = fedMapping.mapParallel(varID, (range, data) -> {\n \t\t\t// copy because we reuse it\n \t\t\tlong[] beginDims = range.getBeginDims();\n \t\t\tlong[] endDims = range.getEndDims();\n-\t\t\tIndexRange ixRange = new IndexRange(beginDims[0], endDims[0], beginDims[1], endDims[1]).add(1);// make 1-based\n-\n-\t\t\t// update begin end dims (column part) considering columns added by dummycoding\n-\t\t\tglobalEncoder.updateIndexRanges(beginDims, endDims);\n+\t\t\tIndexRange ixRange = new IndexRange(beginDims[0], endDims[0], beginDims[1], endDims[1]).add(1);// make\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t// 1-based\n+\t\t\tIndexRange ixRangeInv = new IndexRange(0, beginDims[0], 0, beginDims[1]);\n \n \t\t\t// get the encoder segment that is relevant for this federated worker\n-\t\t\tEncoder encoder = globalEncoder.subRangeEncoder(ixRange);\n+\t\t\tMultiColumnEncoder encoder = globalencoder.subRangeEncoder(ixRange);\n+\t\t\t// update begin end dims (column part) considering columns added by dummycoding\n+\t\t\tencoder.updateIndexRanges(beginDims, endDims, globalencoder.getNumExtraCols(ixRangeInv));\n \n \t\t\ttry {\n \t\t\t\tFederatedResponse response = data.executeFederatedOperation(new FederatedRequest(RequestType.EXEC_UDF,\n@@ -189,18 +172,18 @@ public static void encodeFederatedFrames(FederationMap fedMapping, Encoder globa\n \t\t});\n \n \t\t// construct a federated matrix with the encoded data\n-\t\ttransformedMat.getDataCharacteristics().setDimension(\n-\t\t\ttransformedFedMapping.getMaxIndexInRange(0), transformedFedMapping.getMaxIndexInRange(1));\n+\t\ttransformedMat.getDataCharacteristics().setDimension(transformedFedMapping.getMaxIndexInRange(0),\n+\t\t\ttransformedFedMapping.getMaxIndexInRange(1));\n \t\ttransformedMat.setFedMapping(transformedFedMapping);\n \t}\n-\t\n+\n \tpublic static class CreateFrameEncoder extends FederatedUDF {\n \t\tprivate static final long serialVersionUID = 2376756757742169692L;\n \t\tprivate final String _spec;\n \t\tprivate final int _offset;\n-\t\t\n+\n \t\tpublic CreateFrameEncoder(long input, String spec, int offset) {\n-\t\t\tsuper(new long[]{input});\n+\t\t\tsuper(new long[] {input});\n \t\t\t_spec = spec;\n \t\t\t_offset = offset;\n \t\t}\n@@ -212,9 +195,9 @@ public FederatedResponse execute(ExecutionContext ec, Data... data) {\n \t\t\tString[] colNames = fb.getColumnNames();\n \n \t\t\t// create the encoder\n-\t\t\tEncoder encoder = EncoderFactory.createEncoder(_spec, colNames,\n-\t\t\t\tfb.getNumColumns(), null, _offset, _offset + fb.getNumColumns());\n-\t\t\t\n+\t\t\tMultiColumnEncoder encoder = EncoderFactory\n+\t\t\t\t.createEncoder(_spec, colNames, fb.getNumColumns(), null, _offset, _offset + fb.getNumColumns());\n+\n \t\t\t// build necessary structures for encoding\n \t\t\tencoder.build(fb);\n \t\t\tfo.release();\n@@ -232,28 +215,29 @@ public FederatedResponse execute(ExecutionContext ec, Data... data) {\n \tpublic static class ExecuteFrameEncoder extends FederatedUDF {\n \t\tprivate static final long serialVersionUID = 6034440964680578276L;\n \t\tprivate final long _outputID;\n-\t\tprivate final Encoder _encoder;\n-\t\t\n-\t\tpublic ExecuteFrameEncoder(long input, long output, Encoder encoder) {\n+\t\tprivate final MultiColumnEncoder _encoder;\n+\n+\t\tpublic ExecuteFrameEncoder(long input, long output, MultiColumnEncoder encoder) {\n \t\t\tsuper(new long[] {input});\n \t\t\t_outputID = output;\n \t\t\t_encoder = encoder;\n \t\t}\n \n \t\t@Override\n \t\tpublic FederatedResponse execute(ExecutionContext ec, Data... data) {\n-\t\t\tFrameBlock fb = ((FrameObject)data[0]).acquireReadAndRelease();\n+\t\t\tFrameBlock fb = ((FrameObject) data[0]).acquireReadAndRelease();\n \n+\t\t\t// offset is applied on the Worker to shift the local encoders to their respective column\n+\t\t\t_encoder.applyColumnOffset();\n \t\t\t// apply transformation\n-\t\t\tMatrixBlock mbout = _encoder.apply(fb,\n-\t\t\t\tnew MatrixBlock(fb.getNumRows(), fb.getNumColumns(), false));\n+\t\t\tMatrixBlock mbout = _encoder.apply(fb);\n \n \t\t\t// create output matrix object\n \t\t\tMatrixObject mo = ExecutionContext.createMatrixObject(mbout);\n \n \t\t\t// add it to the list of variables\n \t\t\tec.setVariable(String.valueOf(_outputID), mo);\n-\t\t\n+\n \t\t\t// return id handle\n \t\t\treturn new FederatedResponse(ResponseType.SUCCESS_EMPTY);\n \t\t}\n@@ -266,18 +250,17 @@ public FederatedResponse execute(ExecutionContext ec, Data... data) {\n \t\t@Override\n \t\tpublic Pair<String, LineageItem> getLineageItem(ExecutionContext ec) {\n \t\t\tLineageItem[] liUdfInputs = Arrays.stream(getInputIDs())\n-\t\t\t\t\t.mapToObj(id -> ec.getLineage().get(String.valueOf(id))).toArray(LineageItem[]::new);\n+\t\t\t\t.mapToObj(id -> ec.getLineage().get(String.valueOf(id))).toArray(LineageItem[]::new);\n \t\t\t// calculate checksum for the encoder\n \t\t\tChecksum checksum = new Adler32();\n-\t\t\tbyte bytes[] = SerializationUtils.serialize(_encoder);\n+\t\t\tbyte[] bytes = SerializationUtils.serialize(_encoder);\n \t\t\tchecksum.update(bytes, 0, bytes.length);\n-\t\t\tCPOperand encoder = new CPOperand(String.valueOf(checksum.getValue()), \n-\t\t\t\t\tValueType.INT64, DataType.SCALAR, true);\n+\t\t\tCPOperand encoder = new CPOperand(String.valueOf(checksum.getValue()), ValueType.INT64, DataType.SCALAR,\n+\t\t\t\ttrue);\n \t\t\tLineageItem[] otherInputs = LineageItemUtils.getLineage(ec, encoder);\n \t\t\tLineageItem[] liInputs = Stream.concat(Arrays.stream(liUdfInputs), Arrays.stream(otherInputs))\n-\t\t\t\t\t.toArray(LineageItem[]::new);\n-\t\t\treturn Pair.of(String.valueOf(_outputID), \n-\t\t\t\t\tnew LineageItem(getClass().getSimpleName(), liInputs));\n+\t\t\t\t.toArray(LineageItem[]::new);\n+\t\t\treturn Pair.of(String.valueOf(_outputID), new LineageItem(getClass().getSimpleName(), liInputs));\n \t\t}\n \t}\n }"
  },
  {
    "sha": "51b8d9ba1c07abbe344d98fd4effe900d5c23ac6",
    "filename": "src/main/java/org/apache/sysds/runtime/instructions/fed/ParameterizedBuiltinFEDInstruction.java",
    "status": "modified",
    "additions": 90,
    "deletions": 87,
    "changes": 177,
    "blob_url": "https://github.com/apache/systemds/blob/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/instructions/fed/ParameterizedBuiltinFEDInstruction.java",
    "raw_url": "https://github.com/apache/systemds/raw/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/instructions/fed/ParameterizedBuiltinFEDInstruction.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/instructions/fed/ParameterizedBuiltinFEDInstruction.java?ref=cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd",
    "patch": "@@ -68,10 +68,9 @@\n import org.apache.sysds.runtime.meta.MetaDataFormat;\n import org.apache.sysds.runtime.transform.decode.Decoder;\n import org.apache.sysds.runtime.transform.decode.DecoderFactory;\n-import org.apache.sysds.runtime.transform.encode.Encoder;\n-import org.apache.sysds.runtime.transform.encode.EncoderComposite;\n import org.apache.sysds.runtime.transform.encode.EncoderFactory;\n import org.apache.sysds.runtime.transform.encode.EncoderOmit;\n+import org.apache.sysds.runtime.transform.encode.MultiColumnEncoder;\n \n public class ParameterizedBuiltinFEDInstruction extends ComputationFEDInstruction {\n \tprotected final LinkedHashMap<String, String> params;\n@@ -115,8 +114,8 @@ public static ParameterizedBuiltinFEDInstruction parseInstruction(String str) {\n \t\tLinkedHashMap<String, String> paramsMap = constructParameterMap(parts);\n \n \t\t// determine the appropriate value function\n-\t\tif(opcode.equalsIgnoreCase(\"replace\") || opcode.equalsIgnoreCase(\"rmempty\")\n-\t\t\t|| opcode.equalsIgnoreCase(\"lowertri\") || opcode.equalsIgnoreCase(\"uppertri\")) {\n+\t\tif(opcode.equalsIgnoreCase(\"replace\") || opcode.equalsIgnoreCase(\"rmempty\") ||\n+\t\t\topcode.equalsIgnoreCase(\"lowertri\") || opcode.equalsIgnoreCase(\"uppertri\")) {\n \t\t\tValueFunction func = ParameterizedBuiltin.getParameterizedBuiltinFnObject(opcode);\n \t\t\treturn new ParameterizedBuiltinFEDInstruction(new SimpleOperator(func), paramsMap, out, opcode, str);\n \t\t}\n@@ -159,7 +158,7 @@ else if(opcode.equalsIgnoreCase(\"transformapply\"))\n \t\t\tthrow new DMLRuntimeException(\"Unknown opcode : \" + opcode);\n \t\t}\n \t}\n-\t\n+\n \tprivate void triangle(ExecutionContext ec, String opcode) {\n \t\tboolean lower = opcode.equals(\"lowertri\");\n \t\tboolean diag = Boolean.parseBoolean(params.get(\"diag\"));\n@@ -175,12 +174,13 @@ private void triangle(ExecutionContext ec, String opcode) {\n \n \t\tdiagFedMap = fedMap.mapParallel(varID, (range, data) -> {\n \t\t\ttry {\n-\t\t\t\tFederatedResponse response = data.executeFederatedOperation(new FederatedRequest(\n-\t\t\t\t\tFederatedRequest.RequestType.EXEC_UDF, -1,\n-\t\t\t\t\tnew ParameterizedBuiltinFEDInstruction.Tri(data.getVarID(), varID,\n-\t\t\t\t\t\trowFed ? (new int[] {range.getBeginDimsInt()[0], range.getEndDimsInt()[0]}) :\n-\t\t\t\t\t\t\tnew int[] {range.getBeginDimsInt()[1], range.getEndDimsInt()[1]},\n-\t\t\t\t\t\trowFed, lower, diag, values))).get();\n+\t\t\t\tFederatedResponse response = data\n+\t\t\t\t\t.executeFederatedOperation(new FederatedRequest(FederatedRequest.RequestType.EXEC_UDF, -1,\n+\t\t\t\t\t\tnew ParameterizedBuiltinFEDInstruction.Tri(data.getVarID(), varID,\n+\t\t\t\t\t\t\trowFed ? (new int[] {range.getBeginDimsInt()[0], range.getEndDimsInt()[0]}) : new int[] {\n+\t\t\t\t\t\t\t\trange.getBeginDimsInt()[1], range.getEndDimsInt()[1]},\n+\t\t\t\t\t\t\trowFed, lower, diag, values)))\n+\t\t\t\t\t.get();\n \t\t\t\tif(!response.isSuccessful())\n \t\t\t\t\tresponse.throwExceptionFromResponse();\n \t\t\t\treturn null;\n@@ -203,7 +203,8 @@ private void triangle(ExecutionContext ec, String opcode) {\n \t\tprivate final boolean _diag;\n \t\tprivate final boolean _values;\n \n-\t\tprivate Tri(long input, long outputID, int[] slice, boolean rowFed, boolean lower, boolean diag, boolean values) {\n+\t\tprivate Tri(long input, long outputID, int[] slice, boolean rowFed, boolean lower, boolean diag,\n+\t\t\tboolean values) {\n \t\t\tsuper(new long[] {input});\n \t\t\t_outputID = outputID;\n \t\t\t_slice = slice;\n@@ -219,33 +220,36 @@ public FederatedResponse execute(ExecutionContext ec, Data... data) {\n \t\t\tMatrixBlock soresBlock, addBlock;\n \t\t\tMatrixBlock ret;\n \n-\t\t\t//slice\n-\t\t\tsoresBlock = _rowFed ?\n-\t\t\t\tmb.slice(0, mb.getNumRows()-1, _slice[0], _slice[1]-1, new MatrixBlock()) :\n-\t\t\t\tmb.slice(_slice[0], _slice[1]-1);\n+\t\t\t// slice\n+\t\t\tsoresBlock = _rowFed ? mb.slice(0, mb.getNumRows() - 1, _slice[0], _slice[1] - 1, new MatrixBlock()) : mb\n+\t\t\t\t.slice(_slice[0], _slice[1] - 1);\n \n-\t\t\t//triangle\n+\t\t\t// triangle\n \t\t\tMatrixBlock tri = soresBlock.extractTriangular(new MatrixBlock(), _lower, _diag, _values);\n-\t\t\t// todo: optimize to not allocate and slice all these matrix blocks, but leveraging underlying dense or sparse blocks.\n+\t\t\t// todo: optimize to not allocate and slice all these matrix blocks, but leveraging underlying dense or\n+\t\t\t// sparse blocks.\n \t\t\tif(_rowFed) {\n \t\t\t\tret = new MatrixBlock(mb.getNumRows(), mb.getNumColumns(), 0.0);\n-\t\t\t\tret.copy(0, ret.getNumRows()-1, _slice[0], _slice[1]-1, tri, false);\n-\t\t\t\tif(_slice[1] <= mb.getNumColumns()-1 && !_lower) {\n-\t\t\t\t\taddBlock = mb.slice(0, mb.getNumRows()-1, _slice[1], mb.getNumColumns()-1, new MatrixBlock());\n-\t\t\t\t\tret.copy(0, ret.getNumRows()-1, _slice[1], ret.getNumColumns() - 1, addBlock, false);\n-\t\t\t\t} else if(_slice[0] > 0 && _lower) {\n-\t\t\t\t\taddBlock = mb.slice(0, mb.getNumRows()-1, 0, _slice[0]-1, new MatrixBlock());\n-\t\t\t\t\tret.copy(0, ret.getNumRows()-1, 0,  _slice[0]-1, addBlock, false);\n+\t\t\t\tret.copy(0, ret.getNumRows() - 1, _slice[0], _slice[1] - 1, tri, false);\n+\t\t\t\tif(_slice[1] <= mb.getNumColumns() - 1 && !_lower) {\n+\t\t\t\t\taddBlock = mb.slice(0, mb.getNumRows() - 1, _slice[1], mb.getNumColumns() - 1, new MatrixBlock());\n+\t\t\t\t\tret.copy(0, ret.getNumRows() - 1, _slice[1], ret.getNumColumns() - 1, addBlock, false);\n+\t\t\t\t}\n+\t\t\t\telse if(_slice[0] > 0 && _lower) {\n+\t\t\t\t\taddBlock = mb.slice(0, mb.getNumRows() - 1, 0, _slice[0] - 1, new MatrixBlock());\n+\t\t\t\t\tret.copy(0, ret.getNumRows() - 1, 0, _slice[0] - 1, addBlock, false);\n \t\t\t\t}\n-\t\t\t} else {\n+\t\t\t}\n+\t\t\telse {\n \t\t\t\tret = new MatrixBlock(mb.getNumRows(), mb.getNumColumns(), 0.0);\n-\t\t\t\tret.copy(_slice[0], _slice[1]-1, 0, mb.getNumColumns() - 1, tri, false);\n+\t\t\t\tret.copy(_slice[0], _slice[1] - 1, 0, mb.getNumColumns() - 1, tri, false);\n \t\t\t\tif(_slice[0] > 0 && !_lower) {\n-\t\t\t\t\taddBlock = mb.slice(0, _slice[0]-1,0, mb.getNumColumns()-1, new MatrixBlock());\n+\t\t\t\t\taddBlock = mb.slice(0, _slice[0] - 1, 0, mb.getNumColumns() - 1, new MatrixBlock());\n \t\t\t\t\tret.copy(0, ret.getNumRows() - 1, _slice[1], ret.getNumColumns() - 1, addBlock, false);\n-\t\t\t\t} else if(_slice[1] <= mb.getNumRows() &&_lower) {\n-\t\t\t\t\taddBlock = mb.slice(_slice[1], ret.getNumRows()-1,0, mb.getNumColumns()-1, new MatrixBlock());\n-\t\t\t\t\tret.copy(_slice[1], ret.getNumRows() - 1, 0, mb.getNumColumns()-1, addBlock, false);\n+\t\t\t\t}\n+\t\t\t\telse if(_slice[1] <= mb.getNumRows() && _lower) {\n+\t\t\t\t\taddBlock = mb.slice(_slice[1], ret.getNumRows() - 1, 0, mb.getNumColumns() - 1, new MatrixBlock());\n+\t\t\t\t\tret.copy(_slice[1], ret.getNumRows() - 1, 0, mb.getNumColumns() - 1, addBlock, false);\n \t\t\t\t}\n \t\t\t}\n \t\t\tMatrixObject mout = ExecutionContext.createMatrixObject(ret);\n@@ -262,24 +266,23 @@ public FederatedResponse execute(ExecutionContext ec, Data... data) {\n \t\t@Override\n \t\tpublic Pair<String, LineageItem> getLineageItem(ExecutionContext ec) {\n \t\t\tLineageItem[] liUdfInputs = Arrays.stream(getInputIDs())\n-\t\t\t\t\t.mapToObj(id -> ec.getLineage().get(String.valueOf(id))).toArray(LineageItem[]::new);\n+\t\t\t\t.mapToObj(id -> ec.getLineage().get(String.valueOf(id))).toArray(LineageItem[]::new);\n \t\t\tCPOperand slice = new CPOperand(Arrays.toString(_slice), ValueType.STRING, DataType.SCALAR, true);\n \t\t\tCPOperand rowFed = new CPOperand(String.valueOf(_rowFed), ValueType.BOOLEAN, DataType.SCALAR, true);\n \t\t\tCPOperand lower = new CPOperand(String.valueOf(_lower), ValueType.BOOLEAN, DataType.SCALAR, true);\n \t\t\tCPOperand diag = new CPOperand(String.valueOf(_diag), ValueType.BOOLEAN, DataType.SCALAR, true);\n \t\t\tCPOperand values = new CPOperand(String.valueOf(_values), ValueType.BOOLEAN, DataType.SCALAR, true);\n \t\t\tLineageItem[] otherInputs = LineageItemUtils.getLineage(ec, slice, rowFed, lower, diag, values);\n \t\t\tLineageItem[] liInputs = Stream.concat(Arrays.stream(liUdfInputs), Arrays.stream(otherInputs))\n-\t\t\t\t\t.toArray(LineageItem[]::new);\n-\t\t\treturn Pair.of(String.valueOf(_outputID), \n-\t\t\t\t\tnew LineageItem(getClass().getSimpleName(), liInputs));\n+\t\t\t\t.toArray(LineageItem[]::new);\n+\t\t\treturn Pair.of(String.valueOf(_outputID), new LineageItem(getClass().getSimpleName(), liInputs));\n \t\t}\n \t}\n \n \tprivate void rmempty(ExecutionContext ec) {\n \t\tString margin = params.get(\"margin\");\n-\t\tif( !(margin.equals(\"rows\") || margin.equals(\"cols\")) )\n-\t\t\tthrow new DMLRuntimeException(\"Unsupported margin identifier '\"+margin+\"'.\");\n+\t\tif(!(margin.equals(\"rows\") || margin.equals(\"cols\")))\n+\t\t\tthrow new DMLRuntimeException(\"Unsupported margin identifier '\" + margin + \"'.\");\n \n \t\tMatrixObject mo = (MatrixObject) getTarget(ec);\n \t\tMatrixObject select = params.containsKey(\"select\") ? ec.getMatrixObject(params.get(\"select\")) : null;\n@@ -325,42 +328,47 @@ private void rmempty(ExecutionContext ec) {\n \t\t\tparams.put(\"select\", String.valueOf(varID));\n \t\t\t// construct new string\n \t\t\tString[] oldString = InstructionUtils.getInstructionParts(instString);\n-\t\t\tString[] newString = new String[oldString.length+1];\n-\t\t\tnewString[2] = \"select=\"+varID;\n-\t\t\tSystem.arraycopy(oldString, 0, newString, 0,2);\n-\t\t\tSystem.arraycopy(oldString,2, newString, 3, newString.length-3);\n-\t\t\tinstString = instString.replace(InstructionUtils.concatOperands(oldString), InstructionUtils.concatOperands(newString));\n+\t\t\tString[] newString = new String[oldString.length + 1];\n+\t\t\tnewString[2] = \"select=\" + varID;\n+\t\t\tSystem.arraycopy(oldString, 0, newString, 0, 2);\n+\t\t\tSystem.arraycopy(oldString, 2, newString, 3, newString.length - 3);\n+\t\t\tinstString = instString.replace(InstructionUtils.concatOperands(oldString),\n+\t\t\t\tInstructionUtils.concatOperands(newString));\n \t\t}\n \n-\t\tif (select == null) {\n-\t\t\tFederatedRequest fr1 = FederationUtils.callInstruction(instString, output,\n+\t\tif(select == null) {\n+\t\t\tFederatedRequest fr1 = FederationUtils.callInstruction(instString,\n+\t\t\t\toutput,\n \t\t\t\tnew CPOperand[] {getTargetOperand()},\n \t\t\t\tnew long[] {mo.getFedMapping().getID()});\n \t\t\tmo.getFedMapping().execute(getTID(), true, fr1);\n \t\t\tout.setFedMapping(mo.getFedMapping().copyWithNewID(fr1.getID()));\n \t\t}\n-\t\telse if (!isNotAligned) {\n-\t\t\t//construct commands: broadcast , fed rmempty, clean broadcast\n+\t\telse if(!isNotAligned) {\n+\t\t\t// construct commands: broadcast , fed rmempty, clean broadcast\n \t\t\tFederatedRequest[] fr1 = mo.getFedMapping().broadcastSliced(select, !marginRow);\n \t\t\tFederatedRequest fr2 = FederationUtils.callInstruction(instString,\n \t\t\t\toutput,\n-\t\t\t\tnew CPOperand[] {getTargetOperand(), new CPOperand(params.get(\"select\"), ValueType.FP64, DataType.MATRIX)},\n+\t\t\t\tnew CPOperand[] {getTargetOperand(),\n+\t\t\t\t\tnew CPOperand(params.get(\"select\"), ValueType.FP64, DataType.MATRIX)},\n \t\t\t\tnew long[] {mo.getFedMapping().getID(), fr1[0].getID()});\n \t\t\tFederatedRequest fr3 = mo.getFedMapping().cleanup(getTID(), fr1[0].getID());\n \n-\t\t\t//execute federated operations and set output\n+\t\t\t// execute federated operations and set output\n \t\t\tmo.getFedMapping().execute(getTID(), true, fr1, fr2, fr3);\n \t\t\tout.setFedMapping(mo.getFedMapping().copyWithNewID(fr2.getID()));\n-\t\t} else {\n-\t\t\t//construct commands: broadcast , fed rmempty, clean broadcast\n+\t\t}\n+\t\telse {\n+\t\t\t// construct commands: broadcast , fed rmempty, clean broadcast\n \t\t\tFederatedRequest fr1 = mo.getFedMapping().broadcast(select);\n \t\t\tFederatedRequest fr2 = FederationUtils.callInstruction(instString,\n \t\t\t\toutput,\n-\t\t\t\tnew CPOperand[] {getTargetOperand(), new CPOperand(params.get(\"select\"), ValueType.FP64, DataType.MATRIX)},\n+\t\t\t\tnew CPOperand[] {getTargetOperand(),\n+\t\t\t\t\tnew CPOperand(params.get(\"select\"), ValueType.FP64, DataType.MATRIX)},\n \t\t\t\tnew long[] {mo.getFedMapping().getID(), fr1.getID()});\n \t\t\tFederatedRequest fr3 = mo.getFedMapping().cleanup(getTID(), fr1.getID());\n \n-\t\t\t//execute federated operations and set output\n+\t\t\t// execute federated operations and set output\n \t\t\tmo.getFedMapping().execute(getTID(), true, fr1, fr2, fr3);\n \t\t\tout.setFedMapping(mo.getFedMapping().copyWithNewID(fr2.getID()));\n \t\t}\n@@ -408,7 +416,8 @@ else if (!isNotAligned) {\n \t\t}\n \n \t\tout.getDataCharacteristics().set(out.getFedMapping().getMaxIndexInRange(0),\n-\t\t\tout.getFedMapping().getMaxIndexInRange(1), (int) mo.getBlocksize());\n+\t\t\tout.getFedMapping().getMaxIndexInRange(1),\n+\t\t\t(int) mo.getBlocksize());\n \t}\n \n \tprivate void transformDecode(ExecutionContext ec) {\n@@ -502,21 +511,13 @@ private void transformApply(ExecutionContext ec) {\n \t\t\treturn null;\n \t\t});\n \n-\t\tEncoder globalEncoder = EncoderFactory.createEncoder(spec, colNames, colNames.length, meta);\n+\t\tMultiColumnEncoder globalEncoder = EncoderFactory.createEncoder(spec, colNames, colNames.length, meta);\n \n-\t\t// check if EncoderOmit exists\n-\t\tList<Encoder> encoders = ((EncoderComposite) globalEncoder).getEncoders();\n-\t\tint omitIx = -1;\n-\t\tfor(int i = 0; i < encoders.size(); i++) {\n-\t\t\tif(encoders.get(i) instanceof EncoderOmit) {\n-\t\t\t\tomitIx = i;\n-\t\t\t\tbreak;\n-\t\t\t}\n-\t\t}\n-\t\tif(omitIx != -1) {\n+\t\tif(globalEncoder.hasLegacyEncoder(EncoderOmit.class)) {\n \t\t\t// extra step, build the omit encoder: we need information about all the rows to omit, if our federated\n \t\t\t// ranges are split up row-wise we need to build the encoder separately and combine it\n-\t\t\tbuildOmitEncoder(fedMapping, encoders, omitIx);\n+\t\t\tglobalEncoder.addReplaceLegacyEncoder(\n+\t\t\t\tbuildOmitEncoder(fedMapping, globalEncoder.getLegacyEncoder(EncoderOmit.class)));\n \t\t}\n \n \t\tMultiReturnParameterizedBuiltinFEDInstruction\n@@ -526,28 +527,29 @@ private void transformApply(ExecutionContext ec) {\n \t\tec.releaseFrameInput(params.get(\"meta\"));\n \t}\n \n-\tprivate static void buildOmitEncoder(FederationMap fedMapping, List<Encoder> encoders, int omitIx) {\n-\t\tEncoder omitEncoder = encoders.get(omitIx);\n+\tprivate static EncoderOmit buildOmitEncoder(FederationMap fedMapping, EncoderOmit omitEncoder) {\n \t\tEncoderOmit newOmit = new EncoderOmit(true);\n \t\tfedMapping.forEachParallel((range, data) -> {\n \t\t\ttry {\n-\t\t\t\tEncoderOmit subRangeEncoder = (EncoderOmit) omitEncoder.subRangeEncoder(range.asIndexRange().add(1));\n+\t\t\t\tint colOffset = (int) range.getBeginDims()[1];\n+\t\t\t\tEncoderOmit subRangeEncoder = omitEncoder.subRangeEncoder(range.asIndexRange().add(1));\n \t\t\t\tFederatedResponse response = data\n \t\t\t\t\t.executeFederatedOperation(new FederatedRequest(FederatedRequest.RequestType.EXEC_UDF, -1,\n-\t\t\t\t\t\tnew InitRowsToRemoveOmit(data.getVarID(), subRangeEncoder)))\n+\t\t\t\t\t\tnew InitRowsToRemoveOmit(data.getVarID(), subRangeEncoder, colOffset)))\n \t\t\t\t\t.get();\n \n \t\t\t\t// no synchronization necessary since names should anyway match\n-\t\t\t\tEncoder builtEncoder = (Encoder) response.getData()[0];\n-\t\t\t\tnewOmit.mergeAt(builtEncoder, (int) (range.getBeginDims()[0] + 1), (int) (range.getBeginDims()[1] + 1));\n+\t\t\t\tEncoderOmit builtEncoder = (EncoderOmit) response.getData()[0];\n+\t\t\t\tnewOmit.mergeAt(builtEncoder,\n+\t\t\t\t\t(int) (range.getBeginDims()[0] + 1),\n+\t\t\t\t\t(int) (range.getBeginDims()[1] + 1));\n \t\t\t}\n \t\t\tcatch(Exception e) {\n \t\t\t\tthrow new DMLRuntimeException(e);\n \t\t\t}\n \t\t\treturn null;\n \t\t});\n-\t\tencoders.remove(omitIx);\n-\t\tencoders.add(omitIx, newOmit);\n+\t\treturn newOmit;\n \t}\n \n \tpublic CacheableData<?> getTarget(ExecutionContext ec) {\n@@ -602,31 +604,29 @@ public FederatedResponse execute(ExecutionContext ec, Data... data) {\n \t\t@Override\n \t\tpublic Pair<String, LineageItem> getLineageItem(ExecutionContext ec) {\n \t\t\tLineageItem[] liUdfInputs = Arrays.stream(getInputIDs())\n-\t\t\t\t\t.mapToObj(id -> ec.getLineage().get(String.valueOf(id))).toArray(LineageItem[]::new);\n+\t\t\t\t.mapToObj(id -> ec.getLineage().get(String.valueOf(id))).toArray(LineageItem[]::new);\n \t\t\t// calculate checksums for meta and decoder\n \t\t\tChecksum checksum = new Adler32();\n \t\t\ttry {\n \t\t\t\tlong cbsize = LazyWriteBuffer.getCacheBlockSize(_meta);\n-\t\t\t\tDataOutput fout = new CacheDataOutput(new byte[(int)cbsize]);\n+\t\t\t\tDataOutput fout = new CacheDataOutput(new byte[(int) cbsize]);\n \t\t\t\t_meta.write(fout);\n-\t\t\t\tbyte bytes[] = ((CacheDataOutput) fout).getBytes();\n+\t\t\t\tbyte[] bytes = ((CacheDataOutput) fout).getBytes();\n \t\t\t\tchecksum.update(bytes, 0, bytes.length);\n \t\t\t}\n-\t\t\tcatch (IOException e) {\n+\t\t\tcatch(IOException e) {\n \t\t\t\tthrow new DMLRuntimeException(\"Failed to serialize cache block.\");\n \t\t\t}\n-\t\t\tCPOperand meta = new CPOperand(String.valueOf(checksum.getValue()), \n-\t\t\t\t\tValueType.INT64, DataType.SCALAR, true);\n+\t\t\tCPOperand meta = new CPOperand(String.valueOf(checksum.getValue()), ValueType.INT64, DataType.SCALAR, true);\n \t\t\tchecksum.reset();\n-\t\t\tbyte bytes[] = SerializationUtils.serialize(_decoder);\n+\t\t\tbyte[] bytes = SerializationUtils.serialize(_decoder);\n \t\t\tchecksum.update(bytes, 0, bytes.length);\n-\t\t\tCPOperand decoder = new CPOperand(String.valueOf(checksum.getValue()), \n-\t\t\t\t\tValueType.INT64, DataType.SCALAR, true);\n+\t\t\tCPOperand decoder = new CPOperand(String.valueOf(checksum.getValue()), ValueType.INT64, DataType.SCALAR,\n+\t\t\t\ttrue);\n \t\t\tLineageItem[] otherInputs = LineageItemUtils.getLineage(ec, meta, decoder);\n \t\t\tLineageItem[] liInputs = Stream.concat(Arrays.stream(liUdfInputs), Arrays.stream(otherInputs))\n-\t\t\t\t\t.toArray(LineageItem[]::new);\n-\t\t\treturn Pair.of(String.valueOf(_outputID), \n-\t\t\t\t\tnew LineageItem(getClass().getSimpleName(), liInputs));\n+\t\t\t\t.toArray(LineageItem[]::new);\n+\t\t\treturn Pair.of(String.valueOf(_outputID), new LineageItem(getClass().getSimpleName(), liInputs));\n \t\t}\n \t}\n \n@@ -654,15 +654,18 @@ public FederatedResponse execute(ExecutionContext ec, Data... data) {\n \t\tprivate static final long serialVersionUID = -8196730717390438411L;\n \n \t\tEncoderOmit _encoder;\n+\t\tint _offset;\n \n-\t\tpublic InitRowsToRemoveOmit(long varID, EncoderOmit encoder) {\n+\t\tpublic InitRowsToRemoveOmit(long varID, EncoderOmit encoder, int offset) {\n \t\t\tsuper(new long[] {varID});\n \t\t\t_encoder = encoder;\n+\t\t\t_offset = offset;\n \t\t}\n \n \t\t@Override\n \t\tpublic FederatedResponse execute(ExecutionContext ec, Data... data) {\n \t\t\tFrameBlock fb = ((FrameObject) data[0]).acquireReadAndRelease();\n+\t\t\t_encoder.shiftCols(-_offset);\n \t\t\t_encoder.build(fb);\n \t\t\treturn new FederatedResponse(ResponseType.SUCCESS, new Object[] {_encoder});\n \t\t}\n@@ -685,7 +688,7 @@ public GetDataCharacteristics(long varID) {\n \t\tpublic FederatedResponse execute(ExecutionContext ec, Data... data) {\n \t\t\tMatrixBlock mb = ((MatrixObject) data[0]).acquireReadAndRelease();\n \t\t\tint r = mb.getDenseBlockValues() != null ? mb.getNumRows() : 0;\n-\t\t\tint c = mb.getDenseBlockValues() != null ? mb.getNumColumns(): 0;\n+\t\t\tint c = mb.getDenseBlockValues() != null ? mb.getNumColumns() : 0;\n \t\t\treturn new FederatedResponse(ResponseType.SUCCESS, new int[] {r, c});\n \t\t}\n "
  },
  {
    "sha": "9c8c08bcfa59a9d17d90465042ee5d3329560822",
    "filename": "src/main/java/org/apache/sysds/runtime/instructions/spark/MultiReturnParameterizedBuiltinSPInstruction.java",
    "status": "modified",
    "additions": 187,
    "deletions": 214,
    "changes": 401,
    "blob_url": "https://github.com/apache/systemds/blob/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/instructions/spark/MultiReturnParameterizedBuiltinSPInstruction.java",
    "raw_url": "https://github.com/apache/systemds/raw/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/instructions/spark/MultiReturnParameterizedBuiltinSPInstruction.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/instructions/spark/MultiReturnParameterizedBuiltinSPInstruction.java?ref=cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd",
    "patch": "@@ -19,6 +19,16 @@\n \n package org.apache.sysds.runtime.instructions.spark;\n \n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.stream.Collectors;\n+\n import org.apache.spark.api.java.JavaPairRDD;\n import org.apache.spark.api.java.JavaRDD;\n import org.apache.spark.api.java.JavaSparkContext;\n@@ -51,46 +61,33 @@\n import org.apache.sysds.runtime.matrix.operators.Operator;\n import org.apache.sysds.runtime.meta.DataCharacteristics;\n import org.apache.sysds.runtime.transform.TfUtils;\n-import org.apache.sysds.runtime.transform.encode.Encoder;\n-import org.apache.sysds.runtime.transform.encode.EncoderBin;\n-import org.apache.sysds.runtime.transform.encode.EncoderComposite;\n-import org.apache.sysds.runtime.transform.encode.EncoderFactory;\n-import org.apache.sysds.runtime.transform.encode.EncoderMVImpute;\n+import org.apache.sysds.runtime.transform.encode.*;\n import org.apache.sysds.runtime.transform.encode.EncoderMVImpute.MVMethod;\n-import org.apache.sysds.runtime.transform.encode.EncoderRecode;\n import org.apache.sysds.runtime.transform.meta.TfMetaUtils;\n import org.apache.sysds.runtime.transform.meta.TfOffsetMap;\n \n import scala.Tuple2;\n \n-import java.io.IOException;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.Iterator;\n-import java.util.Map.Entry;\n-\n public class MultiReturnParameterizedBuiltinSPInstruction extends ComputationSPInstruction {\n \tprotected ArrayList<CPOperand> _outputs;\n \n \tprivate MultiReturnParameterizedBuiltinSPInstruction(Operator op, CPOperand input1, CPOperand input2,\n-\t\t\tArrayList<CPOperand> outputs, String opcode, String istr) {\n+\t\tArrayList<CPOperand> outputs, String opcode, String istr) {\n \t\tsuper(SPType.MultiReturnBuiltin, op, input1, input2, outputs.get(0), opcode, istr);\n \t\t_outputs = outputs;\n \t}\n \n-\tpublic static MultiReturnParameterizedBuiltinSPInstruction parseInstruction( String str ) {\n+\tpublic static MultiReturnParameterizedBuiltinSPInstruction parseInstruction(String str) {\n \t\tString[] parts = InstructionUtils.getInstructionPartsWithValueType(str);\n \t\tArrayList<CPOperand> outputs = new ArrayList<>();\n \t\tString opcode = parts[0];\n-\t\t\n-\t\tif ( opcode.equalsIgnoreCase(\"transformencode\") ) {\n+\n+\t\tif(opcode.equalsIgnoreCase(\"transformencode\")) {\n \t\t\t// one input and two outputs\n \t\t\tCPOperand in1 = new CPOperand(parts[1]);\n \t\t\tCPOperand in2 = new CPOperand(parts[2]);\n-\t\t\toutputs.add ( new CPOperand(parts[3], ValueType.FP64, DataType.MATRIX) );\n-\t\t\toutputs.add ( new CPOperand(parts[4], ValueType.STRING, DataType.FRAME) );\n+\t\t\toutputs.add(new CPOperand(parts[3], ValueType.FP64, DataType.MATRIX));\n+\t\t\toutputs.add(new CPOperand(parts[4], ValueType.STRING, DataType.FRAME));\n \t\t\treturn new MultiReturnParameterizedBuiltinSPInstruction(null, in1, in2, outputs, opcode, str);\n \t\t}\n \t\telse {\n@@ -99,71 +96,65 @@ public static MultiReturnParameterizedBuiltinSPInstruction parseInstruction( Str\n \n \t}\n \n-\t@Override \n+\t@Override\n \t@SuppressWarnings(\"unchecked\")\n \tpublic void processInstruction(ExecutionContext ec) {\n \t\tSparkExecutionContext sec = (SparkExecutionContext) ec;\n-\t\t\n-\t\ttry\n-\t\t{\n-\t\t\t//get input RDD and meta data\n+\n+\t\ttry {\n+\t\t\t// get input RDD and meta data\n \t\t\tFrameObject fo = sec.getFrameObject(input1.getName());\n \t\t\tFrameObject fometa = sec.getFrameObject(_outputs.get(1).getName());\n-\t\t\tJavaPairRDD<Long,FrameBlock> in = (JavaPairRDD<Long,FrameBlock>)\n-\t\t\t\tsec.getRDDHandleForFrameObject(fo, FileFormat.BINARY);\n+\t\t\tJavaPairRDD<Long, FrameBlock> in = (JavaPairRDD<Long, FrameBlock>) sec.getRDDHandleForFrameObject(fo,\n+\t\t\t\tFileFormat.BINARY);\n \t\t\tString spec = ec.getScalarInput(input2).getStringValue();\n \t\t\tDataCharacteristics mcIn = sec.getDataCharacteristics(input1.getName());\n \t\t\tDataCharacteristics mcOut = sec.getDataCharacteristics(output.getName());\n-\t\t\tString[] colnames = !TfMetaUtils.isIDSpec(spec) ?\n-\t\t\t\tin.lookup(1L).get(0).getColumnNames() : null; \n-\t\t\t\n-\t\t\t//step 1: build transform meta data\n-\t\t\tEncoderComposite encoderBuild = (EncoderComposite) EncoderFactory\n-\t\t\t\t.createEncoder(spec, colnames, fo.getSchema(), (int)fo.getNumColumns(), null);\n-\t\t\t\n+\t\t\tString[] colnames = !TfMetaUtils.isIDSpec(spec) ? in.lookup(1L).get(0).getColumnNames() : null;\n+\n+\t\t\t// step 1: build transform meta data\n+\t\t\tMultiColumnEncoder encoderBuild = EncoderFactory\n+\t\t\t\t.createEncoder(spec, colnames, fo.getSchema(), (int) fo.getNumColumns(), null);\n+\n \t\t\tMaxLongAccumulator accMax = registerMaxLongAccumulator(sec.getSparkContext());\n-\t\t\tJavaRDD<String> rcMaps = in\n-\t\t\t\t.mapPartitionsToPair(new TransformEncodeBuildFunction(encoderBuild))\n-\t\t\t\t.distinct().groupByKey()\n-\t\t\t\t.flatMap(new TransformEncodeGroupFunction(encoderBuild, accMax));\n-\t\t\tif( containsMVImputeEncoder(encoderBuild) ) {\n-\t\t\t\tEncoderMVImpute mva = getMVImputeEncoder(encoderBuild);\n-\t\t\t\trcMaps = rcMaps.union(\n-\t\t\t\t\tin.mapPartitionsToPair(new TransformEncodeBuild2Function(mva))\n-\t\t\t\t\t  .groupByKey().flatMap(new TransformEncodeGroup2Function(mva)) );\n+\t\t\tJavaRDD<String> rcMaps = in.mapPartitionsToPair(new TransformEncodeBuildFunction(encoderBuild)).distinct()\n+\t\t\t\t.groupByKey().flatMap(new TransformEncodeGroupFunction(encoderBuild, accMax));\n+\t\t\tif(containsMVImputeEncoder(encoderBuild)) {\n+\t\t\t\tEncoderMVImpute mva = encoderBuild.getLegacyEncoder(EncoderMVImpute.class);\n+\t\t\t\trcMaps = rcMaps.union(in.mapPartitionsToPair(new TransformEncodeBuild2Function(mva)).groupByKey()\n+\t\t\t\t\t.flatMap(new TransformEncodeGroup2Function(mva)));\n \t\t\t}\n-\t\t\trcMaps.saveAsTextFile(fometa.getFileName()); //trigger eval\n-\t\t\t\n-\t\t\t//consolidate meta data frame (reuse multi-threaded reader, special handling missing values) \n+\t\t\trcMaps.saveAsTextFile(fometa.getFileName()); // trigger eval\n+\n+\t\t\t// consolidate meta data frame (reuse multi-threaded reader, special handling missing values)\n \t\t\tFrameReader reader = FrameReaderFactory.createFrameReader(FileFormat.TEXT);\n \t\t\tFrameBlock meta = reader.readFrameFromHDFS(fometa.getFileName(), accMax.value(), fo.getNumColumns());\n-\t\t\tmeta.recomputeColumnCardinality(); //recompute num distinct items per column\n-\t\t\tmeta.setColumnNames((colnames!=null)?colnames:meta.getColumnNames());\n-\t\t\tmeta.mapInplace(v -> TfUtils.desanitizeSpaces(v)); //due to format TEXT\n-\t\t\t\n-\t\t\t//step 2: transform apply (similar to spark transformapply)\n-\t\t\t//compute omit offset map for block shifts\n+\t\t\tmeta.recomputeColumnCardinality(); // recompute num distinct items per column\n+\t\t\tmeta.setColumnNames((colnames != null) ? colnames : meta.getColumnNames());\n+\t\t\tmeta.mapInplace(TfUtils::desanitizeSpaces); // due to format TEXT\n+\n+\t\t\t// step 2: transform apply (similar to spark transformapply)\n+\t\t\t// compute omit offset map for block shifts\n \t\t\tTfOffsetMap omap = null;\n-\t\t\tif( TfMetaUtils.containsOmitSpec(spec, colnames) ) {\n-\t\t\t\tomap = new TfOffsetMap(SparkUtils.toIndexedLong(in.mapToPair(\n-\t\t\t\t\tnew RDDTransformApplyOffsetFunction(spec, colnames)).collect()));\n+\t\t\tif(TfMetaUtils.containsOmitSpec(spec, colnames)) {\n+\t\t\t\tomap = new TfOffsetMap(SparkUtils\n+\t\t\t\t\t.toIndexedLong(in.mapToPair(new RDDTransformApplyOffsetFunction(spec, colnames)).collect()));\n \t\t\t}\n-\t\t\t\n-\t\t\t//create encoder broadcast (avoiding replication per task) \n-\t\t\tEncoder encoder = EncoderFactory.createEncoder(spec, colnames,\n-\t\t\t\tfo.getSchema(), (int)fo.getNumColumns(), meta);\n-\t\t\tmcOut.setDimension(mcIn.getRows()-((omap!=null)?omap.getNumRmRows():0), encoder.getNumCols()); \n-\t\t\tBroadcast<Encoder> bmeta = sec.getSparkContext().broadcast(encoder);\n-\t\t\tBroadcast<TfOffsetMap> bomap = (omap!=null) ? sec.getSparkContext().broadcast(omap) : null;\n-\t\t\t\n-\t\t\t//execute transform apply\n-\t\t\tJavaPairRDD<Long,FrameBlock> tmp = in\n-\t\t\t\t.mapToPair(new RDDTransformApplyFunction(bmeta, bomap));\n-\t\t\tJavaPairRDD<MatrixIndexes,MatrixBlock> out = FrameRDDConverterUtils\n-\t\t\t\t.binaryBlockToMatrixBlock(tmp, mcOut, mcOut)\n-\t\t\t\t.cache(); //best effort cache as reblock not at hop level\n-\t\t\t\n-\t\t\t//set output and maintain lineage/output characteristics\n+\n+\t\t\t// create encoder broadcast (avoiding replication per task)\n+\t\t\tMultiColumnEncoder encoder = EncoderFactory\n+\t\t\t\t.createEncoder(spec, colnames, fo.getSchema(), (int) fo.getNumColumns(), meta);\n+\t\t\tmcOut.setDimension(mcIn.getRows() - ((omap != null) ? omap.getNumRmRows() : 0),\n+\t\t\t\t(int) fo.getNumColumns() + encoder.getNumExtraCols());\n+\t\t\tBroadcast<MultiColumnEncoder> bmeta = sec.getSparkContext().broadcast(encoder);\n+\t\t\tBroadcast<TfOffsetMap> bomap = (omap != null) ? sec.getSparkContext().broadcast(omap) : null;\n+\n+\t\t\t// execute transform apply\n+\t\t\tJavaPairRDD<Long, FrameBlock> tmp = in.mapToPair(new RDDTransformApplyFunction(bmeta, bomap));\n+\t\t\tJavaPairRDD<MatrixIndexes, MatrixBlock> out = FrameRDDConverterUtils\n+\t\t\t\t.binaryBlockToMatrixBlock(tmp, mcOut, mcOut).cache(); // best effort cache as reblock not at hop level\n+\n+\t\t\t// set output and maintain lineage/output characteristics\n \t\t\tsec.setRDDHandleForVariable(_outputs.get(0).getName(), out);\n \t\t\tsec.addLineageRDD(_outputs.get(0).getName(), input1.getName());\n \t\t\tsec.setFrameOutput(_outputs.get(1).getName(), meta);\n@@ -174,34 +165,26 @@ public void processInstruction(ExecutionContext ec) {\n \t}\n \n \tprivate static boolean containsMVImputeEncoder(Encoder encoder) {\n-\t\tif( encoder instanceof EncoderComposite )\n-\t\t\tfor( Encoder cencoder : ((EncoderComposite)encoder).getEncoders() )\n-\t\t\t\tif( cencoder instanceof EncoderMVImpute )\n-\t\t\t\t\treturn true;\n+\t\tif(encoder instanceof ColumnEncoderComposite) {\n+\t\t\tthrow new DMLRuntimeException(\"CompositeEncoders cannot contain legacy encoder MVImpute\");\n+\t\t}\n+\t\telse if(encoder instanceof MultiColumnEncoder) {\n+\t\t\treturn ((MultiColumnEncoder) encoder).hasLegacyEncoder(EncoderMVImpute.class);\n+\t\t}\n \t\treturn false;\n \t}\n \n-\tprivate static EncoderMVImpute getMVImputeEncoder(Encoder encoder) {\n-\t\tif( encoder instanceof EncoderComposite )\n-\t\t\tfor( Encoder cencoder : ((EncoderComposite)encoder).getEncoders() )\n-\t\t\t\tif( cencoder instanceof EncoderMVImpute )\n-\t\t\t\t\treturn (EncoderMVImpute) cencoder;\n-\t\treturn null;\n-\t}\n-\t\n \tprivate static MaxLongAccumulator registerMaxLongAccumulator(JavaSparkContext sc) {\n \t\tMaxLongAccumulator acc = new MaxLongAccumulator(Long.MIN_VALUE);\n \t\tsc.sc().register(acc, \"max\");\n \t\treturn acc;\n \t}\n-\t\n \n-\tprivate static class MaxLongAccumulator extends AccumulatorV2<Long,Long>\n-\t{\n+\tprivate static class MaxLongAccumulator extends AccumulatorV2<Long, Long> {\n \t\tprivate static final long serialVersionUID = -3739727823287550826L;\n \n \t\tprivate long _value = Long.MIN_VALUE;\n-\t\t\n+\n \t\tpublic MaxLongAccumulator(long value) {\n \t\t\t_value = value;\n \t\t}\n@@ -236,121 +219,115 @@ public Long value() {\n \t\t\treturn _value;\n \t\t}\n \t}\n-\t\n+\n \t/**\n-\t * This function pre-aggregates distinct values of recoded columns per partition\n-\t * (part of distributed recode map construction, used for recoding, binning and \n-\t * dummy coding). We operate directly over schema-specific objects to avoid \n-\t * unnecessary string conversion, as well as reduce memory overhead and shuffle.\n+\t * This function pre-aggregates distinct values of recoded columns per partition (part of distributed recode map\n+\t * construction, used for recoding, binning and dummy coding). We operate directly over schema-specific objects to\n+\t * avoid unnecessary string conversion, as well as reduce memory overhead and shuffle.\n \t */\n-\tpublic static class TransformEncodeBuildFunction \n-\t\timplements PairFlatMapFunction<Iterator<Tuple2<Long, FrameBlock>>, Integer, Object>\n-\t{\n+\tpublic static class TransformEncodeBuildFunction\n+\t\timplements PairFlatMapFunction<Iterator<Tuple2<Long, FrameBlock>>, Integer, Object> {\n \t\tprivate static final long serialVersionUID = 6336375833412029279L;\n \n-\t\tprivate EncoderComposite _encoder = null;\n-\t\t\n-\t\tpublic TransformEncodeBuildFunction(EncoderComposite encoder) {\n+\t\tprivate MultiColumnEncoder _encoder;\n+\n+\t\tpublic TransformEncodeBuildFunction(MultiColumnEncoder encoder) {\n \t\t\t_encoder = encoder;\n \t\t}\n-\t\t\n+\n \t\t@Override\n-\t\tpublic Iterator<Tuple2<Integer, Object>> call(Iterator<Tuple2<Long, FrameBlock>> iter)\n-\t\t\tthrows Exception \n-\t\t{\n-\t\t\t//build meta data (e.g., recoding recode maps and binning min/max)\n+\t\tpublic Iterator<Tuple2<Integer, Object>> call(Iterator<Tuple2<Long, FrameBlock>> iter) throws Exception {\n+\t\t\t// build meta data (e.g., recoding recode maps and binning min/max)\n \t\t\t_encoder.prepareBuildPartial();\n-\t\t\twhile( iter.hasNext() )\n+\t\t\twhile(iter.hasNext())\n \t\t\t\t_encoder.buildPartial(iter.next()._2());\n-\t\t\t\n-\t\t\t//encoder-specific outputs\n-\t\t\tEncoderRecode raEncoder = (EncoderRecode)_encoder.getEncoder(EncoderRecode.class);\n-\t\t\tEncoderBin baEncoder = (EncoderBin)_encoder.getEncoder(EncoderBin.class);\n-\t\t\tArrayList<Tuple2<Integer,Object>> ret = new ArrayList<>();\n-\t\t\t\n-\t\t\t//output recode maps as columnID - token pairs\n-\t\t\tif( raEncoder != null ) {\n-\t\t\t\tHashMap<Integer,HashSet<Object>> tmp = raEncoder.getCPRecodeMapsPartial();\n-\t\t\t\tfor( Entry<Integer,HashSet<Object>> e1 : tmp.entrySet() )\n-\t\t\t\t\tfor( Object token : e1.getValue() )\n+\n+\t\t\t// encoder-specific outputs\n+\t\t\tList<ColumnEncoderRecode> raEncoders = _encoder.getColumnEncoders(ColumnEncoderRecode.class);\n+\t\t\tList<ColumnEncoderBin> baEncoders = _encoder.getColumnEncoders(ColumnEncoderBin.class);\n+\t\t\tArrayList<Tuple2<Integer, Object>> ret = new ArrayList<>();\n+\n+\t\t\t// output recode maps as columnID - token pairs\n+\t\t\tif(!raEncoders.isEmpty()) {\n+\t\t\t\t// TODO check in debbuger if correct\n+\t\t\t\tMap<Integer, HashSet<Object>> tmp = raEncoders.stream()\n+\t\t\t\t\t.collect(Collectors.toMap(ColumnEncoder::getColID, ColumnEncoderRecode::getCPRecodeMapsPartial));\n+\t\t\t\tfor(Entry<Integer, HashSet<Object>> e1 : tmp.entrySet())\n+\t\t\t\t\tfor(Object token : e1.getValue())\n \t\t\t\t\t\tret.add(new Tuple2<>(e1.getKey(), token));\n-\t\t\t\tif( raEncoder != null )\n-\t\t\t\t\traEncoder.getCPRecodeMapsPartial().clear();\n+\t\t\t\tif(!raEncoders.isEmpty())\n+\t\t\t\t\traEncoders.forEach(columnEncoderRecode -> columnEncoderRecode.getCPRecodeMapsPartial().clear());\n \t\t\t}\n-\t\t\t\n-\t\t\t//output binning column min/max as columnID - min/max pairs\n-\t\t\tif( baEncoder != null ) {\n-\t\t\t\tint[] colIDs = baEncoder.getColList();\n-\t\t\t\tdouble[] colMins = baEncoder.getColMins();\n-\t\t\t\tdouble[] colMaxs = baEncoder.getColMaxs();\n-\t\t\t\tfor(int j=0; j<colIDs.length; j++) {\n+\n+\t\t\t// output binning column min/max as columnID - min/max pairs\n+\t\t\tif(!baEncoders.isEmpty()) {\n+\t\t\t\tint[] colIDs = _encoder.getFromAllIntArray(ColumnEncoderBin.class, ColumnEncoder::getColID);\n+\t\t\t\tdouble[] colMins = _encoder.getFromAllDoubleArray(ColumnEncoderBin.class, ColumnEncoderBin::getColMins);\n+\t\t\t\tdouble[] colMaxs = _encoder.getFromAllDoubleArray(ColumnEncoderBin.class, ColumnEncoderBin::getColMaxs);\n+\t\t\t\tfor(int j = 0; j < colIDs.length; j++) {\n \t\t\t\t\tret.add(new Tuple2<>(colIDs[j], String.valueOf(colMins[j])));\n \t\t\t\t\tret.add(new Tuple2<>(colIDs[j], String.valueOf(colMaxs[j])));\n \t\t\t\t}\n \t\t\t}\n-\t\t\t\n+\n \t\t\treturn ret.iterator();\n \t\t}\n \t}\n-\t\n+\n \t/**\n-\t * This function assigns codes to globally distinct values of recoded columns \n-\t * and writes the resulting column map in textcell (IJV) format to the output. \n-\t * (part of distributed recode map construction, used for recoding, binning and \n-\t * dummy coding). We operate directly over schema-specific objects to avoid \n-\t * unnecessary string conversion, as well as reduce memory overhead and shuffle.\n+\t * This function assigns codes to globally distinct values of recoded columns and writes the resulting column map in\n+\t * textcell (IJV) format to the output. (part of distributed recode map construction, used for recoding, binning and\n+\t * dummy coding). We operate directly over schema-specific objects to avoid unnecessary string conversion, as well\n+\t * as reduce memory overhead and shuffle.\n \t */\n-\tpublic static class TransformEncodeGroupFunction \n-\t\timplements FlatMapFunction<Tuple2<Integer, Iterable<Object>>, String>\n-\t{\n+\tpublic static class TransformEncodeGroupFunction\n+\t\timplements FlatMapFunction<Tuple2<Integer, Iterable<Object>>, String> {\n \t\tprivate static final long serialVersionUID = -1034187226023517119L;\n \n-\t\tprivate final EncoderComposite _encoder;\n+\t\tprivate final MultiColumnEncoder _encoder;\n \t\tprivate final MaxLongAccumulator _accMax;\n-\t\t\n-\t\tpublic TransformEncodeGroupFunction(EncoderComposite encoder, MaxLongAccumulator accMax) {\n+\n+\t\tpublic TransformEncodeGroupFunction(MultiColumnEncoder encoder, MaxLongAccumulator accMax) {\n \t\t\t_encoder = encoder;\n \t\t\t_accMax = accMax;\n \t\t}\n-\t\t\n+\n \t\t@Override\n-\t\tpublic Iterator<String> call(Tuple2<Integer, Iterable<Object>> arg0)\n-\t\t\tthrows Exception \n-\t\t{\n+\t\tpublic Iterator<String> call(Tuple2<Integer, Iterable<Object>> arg0) throws Exception {\n \t\t\tString scolID = String.valueOf(arg0._1());\n \t\t\tint colID = Integer.parseInt(scolID);\n \t\t\tIterator<Object> iter = arg0._2().iterator();\n \t\t\tArrayList<String> ret = new ArrayList<>();\n-\t\t\t\n+\n \t\t\tlong rowID = 1;\n \t\t\tStringBuilder sb = new StringBuilder();\n-\t\t\t\n-\t\t\t//handle recode maps\n-\t\t\tif( _encoder.isEncoder(colID, EncoderRecode.class) ) {\n-\t\t\t\twhile( iter.hasNext() ) {\n+\n+\t\t\t// handle recode maps\n+\t\t\tif(_encoder.containsEncoderForID(colID, ColumnEncoderRecode.class)) {\n+\t\t\t\twhile(iter.hasNext()) {\n \t\t\t\t\tString token = TfUtils.sanitizeSpaces(iter.next().toString());\n \t\t\t\t\tsb.append(rowID).append(' ').append(scolID).append(' ');\n-\t\t\t\t\tsb.append(EncoderRecode.constructRecodeMapEntry(token, rowID));\n+\t\t\t\t\tsb.append(ColumnEncoderRecode.constructRecodeMapEntry(token, rowID));\n \t\t\t\t\tret.add(sb.toString());\n-\t\t\t\t\tsb.setLength(0); \n+\t\t\t\t\tsb.setLength(0);\n \t\t\t\t\trowID++;\n \t\t\t\t}\n \t\t\t}\n-\t\t\t//handle bin boundaries\n-\t\t\telse if( _encoder.isEncoder(colID, EncoderBin.class) ) {\n-\t\t\t\tEncoderBin baEncoder = (EncoderBin)_encoder.getEncoder(EncoderBin.class);\n+\t\t\t// handle bin boundaries\n+\t\t\telse if(_encoder.containsEncoderForID(colID, ColumnEncoderBin.class)) {\n \t\t\t\tdouble min = Double.MAX_VALUE;\n \t\t\t\tdouble max = -Double.MAX_VALUE;\n-\t\t\t\twhile( iter.hasNext() ) {\n+\t\t\t\twhile(iter.hasNext()) {\n \t\t\t\t\tdouble value = Double.parseDouble(iter.next().toString());\n \t\t\t\t\tmin = Math.min(min, value);\n \t\t\t\t\tmax = Math.max(max, value);\n \t\t\t\t}\n-\t\t\t\tint j = Arrays.binarySearch(baEncoder.getColList(), colID);\n-\t\t\t\tbaEncoder.computeBins(j, min, max);\n-\t\t\t\tdouble[] binMins = baEncoder.getBinMins(j);\n-\t\t\t\tdouble[] binMaxs = baEncoder.getBinMaxs(j);\n-\t\t\t\tfor(int i=0; i<binMins.length; i++) {\n+\t\t\t\tColumnEncoderBin baEncoder = _encoder.getColumnEncoder(colID, ColumnEncoderBin.class);\n+\t\t\t\tassert baEncoder != null;\n+\t\t\t\tbaEncoder.computeBins(min, max);\n+\t\t\t\tdouble[] binMins = baEncoder.getBinMins();\n+\t\t\t\tdouble[] binMaxs = baEncoder.getBinMaxs();\n+\t\t\t\tfor(int i = 0; i < binMins.length; i++) {\n \t\t\t\t\tsb.append(rowID).append(' ').append(scolID).append(' ');\n \t\t\t\t\tsb.append(binMins[i]).append(Lop.DATATYPE_PREFIX).append(binMaxs[i]);\n \t\t\t\t\tret.add(sb.toString());\n@@ -359,111 +336,107 @@ else if( _encoder.isEncoder(colID, EncoderBin.class) ) {\n \t\t\t\t}\n \t\t\t}\n \t\t\telse {\n-\t\t\t\tthrow new DMLRuntimeException(\"Unsupported metadata output for encoder: \\n\"+_encoder);\n+\t\t\t\tthrow new DMLRuntimeException(\"Unsupported metadata output for encoder: \\n\" + _encoder);\n \t\t\t}\n-\t\t\t_accMax.add(rowID-1);\n-\t\t\t\n+\t\t\t_accMax.add(rowID - 1);\n+\n \t\t\treturn ret.iterator();\n \t\t}\n \t}\n \n-\tpublic static class TransformEncodeBuild2Function implements PairFlatMapFunction<Iterator<Tuple2<Long, FrameBlock>>, Integer, ColumnMetadata>\n-\t{\n+\tpublic static class TransformEncodeBuild2Function\n+\t\timplements PairFlatMapFunction<Iterator<Tuple2<Long, FrameBlock>>, Integer, ColumnMetadata> {\n \t\tprivate static final long serialVersionUID = 6336375833412029279L;\n \n-\t\tprivate EncoderMVImpute _encoder = null;\n-\t\t\n+\t\tprivate EncoderMVImpute _encoder;\n+\n \t\tpublic TransformEncodeBuild2Function(EncoderMVImpute encoder) {\n \t\t\t_encoder = encoder;\n \t\t}\n-\t\t\n+\n \t\t@Override\n \t\tpublic Iterator<Tuple2<Integer, ColumnMetadata>> call(Iterator<Tuple2<Long, FrameBlock>> iter)\n-\t\t\tthrows Exception \n-\t\t{\n-\t\t\t//build meta data (e.g., histograms and means)\n-\t\t\twhile( iter.hasNext() ) {\n+\t\t\tthrows Exception {\n+\t\t\t// build meta data (e.g., histograms and means)\n+\t\t\twhile(iter.hasNext()) {\n \t\t\t\tFrameBlock block = iter.next()._2();\n-\t\t\t\t_encoder.build(block);\t\n+\t\t\t\t_encoder.build(block);\n \t\t\t}\n-\t\t\t\n-\t\t\t//extract meta data\n-\t\t\tArrayList<Tuple2<Integer,ColumnMetadata>> ret = new ArrayList<>();\n+\n+\t\t\t// extract meta data\n+\t\t\tArrayList<Tuple2<Integer, ColumnMetadata>> ret = new ArrayList<>();\n \t\t\tint[] collist = _encoder.getColList();\n-\t\t\tfor( int j=0; j<collist.length; j++ ) {\n-\t\t\t\tif( _encoder.getMethod(collist[j]) == MVMethod.GLOBAL_MODE ) {\n-\t\t\t\t\tHashMap<String,Long> hist = _encoder.getHistogram(collist[j]);\n-\t\t\t\t\tfor( Entry<String,Long> e : hist.entrySet() )\n-\t\t\t\t\t\tret.add(new Tuple2<>(collist[j], \n-\t\t\t\t\t\t\t\tnew ColumnMetadata(e.getValue(), e.getKey())));\n+\t\t\tfor(int j = 0; j < collist.length; j++) {\n+\t\t\t\tif(_encoder.getMethod(collist[j]) == MVMethod.GLOBAL_MODE) {\n+\t\t\t\t\tHashMap<String, Long> hist = _encoder.getHistogram(collist[j]);\n+\t\t\t\t\tfor(Entry<String, Long> e : hist.entrySet())\n+\t\t\t\t\t\tret.add(new Tuple2<>(collist[j], new ColumnMetadata(e.getValue(), e.getKey())));\n \t\t\t\t}\n-\t\t\t\telse if( _encoder.getMethod(collist[j]) == MVMethod.GLOBAL_MEAN ) {\n-\t\t\t\t\tret.add(new Tuple2<>(collist[j], \n-\t\t\t\t\t\t\tnew ColumnMetadata(_encoder.getNonMVCount(collist[j]), String.valueOf(_encoder.getMeans()[j]._sum))));\n+\t\t\t\telse if(_encoder.getMethod(collist[j]) == MVMethod.GLOBAL_MEAN) {\n+\t\t\t\t\tret.add(new Tuple2<>(collist[j], new ColumnMetadata(_encoder.getNonMVCount(collist[j]),\n+\t\t\t\t\t\tString.valueOf(_encoder.getMeans()[j]._sum))));\n \t\t\t\t}\n-\t\t\t\telse if( _encoder.getMethod(collist[j]) == MVMethod.CONSTANT ) {\n-\t\t\t\t\tret.add(new Tuple2<>(collist[j],\n-\t\t\t\t\t\t\tnew ColumnMetadata(0, _encoder.getReplacement(collist[j]))));\n+\t\t\t\telse if(_encoder.getMethod(collist[j]) == MVMethod.CONSTANT) {\n+\t\t\t\t\tret.add(new Tuple2<>(collist[j], new ColumnMetadata(0, _encoder.getReplacement(collist[j]))));\n \t\t\t\t}\n \t\t\t}\n-\t\t\t\n+\n \t\t\treturn ret.iterator();\n \t\t}\n \t}\n \n-\tpublic static class TransformEncodeGroup2Function implements FlatMapFunction<Tuple2<Integer, Iterable<ColumnMetadata>>, String>\n-\t{\n+\tpublic static class TransformEncodeGroup2Function\n+\t\timplements FlatMapFunction<Tuple2<Integer, Iterable<ColumnMetadata>>, String> {\n \t\tprivate static final long serialVersionUID = 702100641492347459L;\n-\t\t\n-\t\tprivate EncoderMVImpute _encoder = null;\n-\t\t\n-\t\tpublic TransformEncodeGroup2Function(EncoderMVImpute encoder) {\t\n+\n+\t\tprivate EncoderMVImpute _encoder;\n+\n+\t\tpublic TransformEncodeGroup2Function(EncoderMVImpute encoder) {\n \t\t\t_encoder = encoder;\n \t\t}\n \n \t\t@Override\n-\t\tpublic Iterator<String> call(Tuple2<Integer, Iterable<ColumnMetadata>> arg0)\n-\t\t\t\tthrows Exception \n-\t\t{\n+\t\tpublic Iterator<String> call(Tuple2<Integer, Iterable<ColumnMetadata>> arg0) throws Exception {\n \t\t\tint colix = arg0._1();\n \t\t\tIterator<ColumnMetadata> iter = arg0._2().iterator();\n \t\t\tArrayList<String> ret = new ArrayList<>();\n-\t\t\t\n-\t\t\t//compute global mode of categorical feature, i.e., value with highest frequency\n-\t\t\tif( _encoder.getMethod(colix) == MVMethod.GLOBAL_MODE ) {\n+\n+\t\t\t// compute global mode of categorical feature, i.e., value with highest frequency\n+\t\t\tif(_encoder.getMethod(colix) == MVMethod.GLOBAL_MODE) {\n \t\t\t\tHashMap<String, Long> hist = new HashMap<>();\n-\t\t\t\twhile( iter.hasNext() ) {\n-\t\t\t\t\tColumnMetadata cmeta = iter.next(); \n+\t\t\t\twhile(iter.hasNext()) {\n+\t\t\t\t\tColumnMetadata cmeta = iter.next();\n \t\t\t\t\tLong tmp = hist.get(cmeta.getMvValue());\n-\t\t\t\t\thist.put(cmeta.getMvValue(), cmeta.getNumDistinct() + ((tmp!=null)?tmp:0));\n+\t\t\t\t\thist.put(cmeta.getMvValue(), cmeta.getNumDistinct() + ((tmp != null) ? tmp : 0));\n \t\t\t\t}\n-\t\t\t\tlong max = Long.MIN_VALUE; String mode = null;\n-\t\t\t\tfor( Entry<String, Long> e : hist.entrySet() ) \n-\t\t\t\t\tif( e.getValue() > max  ) {\n+\t\t\t\tlong max = Long.MIN_VALUE;\n+\t\t\t\tString mode = null;\n+\t\t\t\tfor(Entry<String, Long> e : hist.entrySet())\n+\t\t\t\t\tif(e.getValue() > max) {\n \t\t\t\t\t\tmode = e.getKey();\n \t\t\t\t\t\tmax = e.getValue();\n \t\t\t\t\t}\n \t\t\t\tret.add(\"-2 \" + colix + \" \" + TfUtils.sanitizeSpaces(mode));\n \t\t\t}\n-\t\t\t//compute global mean of categorical feature\n-\t\t\telse if( _encoder.getMethod(colix) == MVMethod.GLOBAL_MEAN ) {\n+\t\t\t// compute global mean of categorical feature\n+\t\t\telse if(_encoder.getMethod(colix) == MVMethod.GLOBAL_MEAN) {\n \t\t\t\tKahanObject kbuff = new KahanObject(0, 0);\n \t\t\t\tKahanPlus kplus = KahanPlus.getKahanPlusFnObject();\n \t\t\t\tint count = 0;\n-\t\t\t\twhile( iter.hasNext() ) {\n-\t\t\t\t\tColumnMetadata cmeta = iter.next(); \n+\t\t\t\twhile(iter.hasNext()) {\n+\t\t\t\t\tColumnMetadata cmeta = iter.next();\n \t\t\t\t\tkplus.execute2(kbuff, Double.parseDouble(cmeta.getMvValue()));\n \t\t\t\t\tcount += cmeta.getNumDistinct();\n \t\t\t\t}\n-\t\t\t\tif( count > 0 )\n-\t\t\t\t\tret.add(\"-2 \" + colix + \" \" + String.valueOf(kbuff._sum/count));\n+\t\t\t\tif(count > 0)\n+\t\t\t\t\tret.add(\"-2 \" + colix + \" \" + kbuff._sum / count);\n \t\t\t}\n-\t\t\t//pass-through constant label\n-\t\t\telse if( _encoder.getMethod(colix) == MVMethod.CONSTANT ) {\n-\t\t\t\tif( iter.hasNext() )\n+\t\t\t// pass-through constant label\n+\t\t\telse if(_encoder.getMethod(colix) == MVMethod.CONSTANT) {\n+\t\t\t\tif(iter.hasNext())\n \t\t\t\t\tret.add(\"-2 \" + colix + \" \" + TfUtils.sanitizeSpaces(iter.next().getMvValue()));\n \t\t\t}\n-\t\t\t\n+\n \t\t\treturn ret.iterator();\n \t\t}\n \t}"
  },
  {
    "sha": "a214f94f8c8c7938f0178e14abb69f2f40ab6c99",
    "filename": "src/main/java/org/apache/sysds/runtime/instructions/spark/ParameterizedBuiltinSPInstruction.java",
    "status": "modified",
    "additions": 422,
    "deletions": 458,
    "changes": 880,
    "blob_url": "https://github.com/apache/systemds/blob/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/instructions/spark/ParameterizedBuiltinSPInstruction.java",
    "raw_url": "https://github.com/apache/systemds/raw/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/instructions/spark/ParameterizedBuiltinSPInstruction.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/instructions/spark/ParameterizedBuiltinSPInstruction.java?ref=cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd",
    "patch": "@@ -19,6 +19,11 @@\n \n package org.apache.sysds.runtime.instructions.spark;\n \n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+\n import org.apache.spark.api.java.JavaPairRDD;\n import org.apache.spark.api.java.JavaRDD;\n import org.apache.spark.api.java.function.Function;\n@@ -69,98 +74,92 @@\n import org.apache.sysds.runtime.transform.TfUtils.TfMethod;\n import org.apache.sysds.runtime.transform.decode.Decoder;\n import org.apache.sysds.runtime.transform.decode.DecoderFactory;\n-import org.apache.sysds.runtime.transform.encode.Encoder;\n import org.apache.sysds.runtime.transform.encode.EncoderFactory;\n+import org.apache.sysds.runtime.transform.encode.MultiColumnEncoder;\n import org.apache.sysds.runtime.transform.meta.TfMetaUtils;\n import org.apache.sysds.runtime.transform.meta.TfOffsetMap;\n import org.apache.sysds.runtime.transform.tokenize.Tokenizer;\n import org.apache.sysds.runtime.transform.tokenize.TokenizerFactory;\n import org.apache.sysds.runtime.util.DataConverter;\n import org.apache.sysds.runtime.util.UtilFunctions;\n-import scala.Tuple2;\n \n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.HashMap;\n-import java.util.Iterator;\n+import scala.Tuple2;\n \n public class ParameterizedBuiltinSPInstruction extends ComputationSPInstruction {\n \tprotected HashMap<String, String> params;\n \n-\tParameterizedBuiltinSPInstruction(Operator op, HashMap<String, String> paramsMap, CPOperand out, String opcode, String istr) {\n+\tParameterizedBuiltinSPInstruction(Operator op, HashMap<String, String> paramsMap, CPOperand out, String opcode,\n+\t\tString istr) {\n \t\tsuper(SPType.ParameterizedBuiltin, op, null, null, out, opcode, istr);\n \t\tparams = paramsMap;\n \t}\n \n-\tpublic HashMap<String,String> getParams() { return params; }\n-\t\n+\tpublic HashMap<String, String> getParams() {\n+\t\treturn params;\n+\t}\n+\n \tpublic static HashMap<String, String> constructParameterMap(String[] params) {\n \t\t// process all elements in \"params\" except first(opcode) and last(output)\n-\t\tHashMap<String,String> paramMap = new HashMap<>();\n-\t\t\n+\t\tHashMap<String, String> paramMap = new HashMap<>();\n+\n \t\t// all parameters are of form <name=value>\n \t\tString[] parts;\n-\t\tfor ( int i=1; i <= params.length-2; i++ ) {\n+\t\tfor(int i = 1; i <= params.length - 2; i++) {\n \t\t\tparts = params[i].split(Lop.NAME_VALUE_SEPARATOR);\n \t\t\tparamMap.put(parts[0], parts[1]);\n \t\t}\n-\t\t\n+\n \t\treturn paramMap;\n \t}\n-\t\n-\tpublic static ParameterizedBuiltinSPInstruction parseInstruction ( String str ) {\n+\n+\tpublic static ParameterizedBuiltinSPInstruction parseInstruction(String str) {\n \t\tString[] parts = InstructionUtils.getInstructionPartsWithValueType(str);\n \t\t// first part is always the opcode\n \t\tString opcode = parts[0];\n \n-\t\tif( opcode.equalsIgnoreCase(\"mapgroupedagg\") )\n-\t\t{\n-\t\t\tCPOperand target = new CPOperand( parts[1] ); \n-\t\t\tCPOperand groups = new CPOperand( parts[2] );\n-\t\t\tCPOperand out = new CPOperand( parts[3] );\n+\t\tif(opcode.equalsIgnoreCase(\"mapgroupedagg\")) {\n+\t\t\tCPOperand target = new CPOperand(parts[1]);\n+\t\t\tCPOperand groups = new CPOperand(parts[2]);\n+\t\t\tCPOperand out = new CPOperand(parts[3]);\n \n-\t\t\tHashMap<String,String> paramsMap = new HashMap<>();\n+\t\t\tHashMap<String, String> paramsMap = new HashMap<>();\n \t\t\tparamsMap.put(Statement.GAGG_TARGET, target.getName());\n \t\t\tparamsMap.put(Statement.GAGG_GROUPS, groups.getName());\n \t\t\tparamsMap.put(Statement.GAGG_NUM_GROUPS, parts[4]);\n-\t\t\t\n+\n \t\t\tOperator op = new AggregateOperator(0, KahanPlus.getKahanPlusFnObject(), CorrectionLocationType.LASTCOLUMN);\n \t\t\treturn new ParameterizedBuiltinSPInstruction(op, paramsMap, out, opcode, str);\n \t\t}\n-\t\telse\n-\t\t{\n+\t\telse {\n \t\t\t// last part is always the output\n-\t\t\tCPOperand out = new CPOperand( parts[parts.length-1] ); \n+\t\t\tCPOperand out = new CPOperand(parts[parts.length - 1]);\n \n \t\t\t// process remaining parts and build a hash map\n-\t\t\tHashMap<String,String> paramsMap = constructParameterMap(parts);\n+\t\t\tHashMap<String, String> paramsMap = constructParameterMap(parts);\n \n \t\t\t// determine the appropriate value function\n \t\t\tValueFunction func = null;\n-\t\t\tif ( opcode.equalsIgnoreCase(\"groupedagg\")) {\n+\t\t\tif(opcode.equalsIgnoreCase(\"groupedagg\")) {\n \t\t\t\t// check for mandatory arguments\n \t\t\t\tString fnStr = paramsMap.get(\"fn\");\n-\t\t\t\tif ( fnStr == null ) \n+\t\t\t\tif(fnStr == null)\n \t\t\t\t\tthrow new DMLRuntimeException(\"Function parameter is missing in groupedAggregate.\");\n-\t\t\t\tif ( fnStr.equalsIgnoreCase(\"centralmoment\") ) {\n-\t\t\t\t\tif ( paramsMap.get(\"order\") == null )\n-\t\t\t\t\t\tthrow new DMLRuntimeException(\"Mandatory \\\"order\\\" must be specified when fn=\\\"centralmoment\\\" in groupedAggregate.\");\n+\t\t\t\tif(fnStr.equalsIgnoreCase(\"centralmoment\")) {\n+\t\t\t\t\tif(paramsMap.get(\"order\") == null)\n+\t\t\t\t\t\tthrow new DMLRuntimeException(\n+\t\t\t\t\t\t\t\"Mandatory \\\"order\\\" must be specified when fn=\\\"centralmoment\\\" in groupedAggregate.\");\n \t\t\t\t}\n \t\t\t\tOperator op = InstructionUtils.parseGroupedAggOperator(fnStr, paramsMap.get(\"order\"));\n \t\t\t\treturn new ParameterizedBuiltinSPInstruction(op, paramsMap, out, opcode, str);\n-\t\t\t} \n-\t\t\telse if (opcode.equalsIgnoreCase(\"rmempty\")) {\n+\t\t\t}\n+\t\t\telse if(opcode.equalsIgnoreCase(\"rmempty\")) {\n \t\t\t\tfunc = ParameterizedBuiltin.getParameterizedBuiltinFnObject(opcode);\n-\t\t\t\treturn new ParameterizedBuiltinSPInstruction(\n-\t\t\t\t\tnew SimpleOperator(func), paramsMap, out, opcode, str);\n+\t\t\t\treturn new ParameterizedBuiltinSPInstruction(new SimpleOperator(func), paramsMap, out, opcode, str);\n \t\t\t}\n-\t\t\telse if (opcode.equalsIgnoreCase(\"rexpand\")\n-\t\t\t\t|| opcode.equalsIgnoreCase(\"replace\")\n-\t\t\t\t|| opcode.equalsIgnoreCase(\"lowertri\")\n-\t\t\t\t|| opcode.equalsIgnoreCase(\"uppertri\")\n-\t\t\t\t|| opcode.equalsIgnoreCase(\"tokenize\")\n-\t\t\t\t|| opcode.equalsIgnoreCase(\"transformapply\")\n-\t\t\t\t|| opcode.equalsIgnoreCase(\"transformdecode\")) {\n+\t\t\telse if(opcode.equalsIgnoreCase(\"rexpand\") || opcode.equalsIgnoreCase(\"replace\") ||\n+\t\t\t\topcode.equalsIgnoreCase(\"lowertri\") || opcode.equalsIgnoreCase(\"uppertri\") ||\n+\t\t\t\topcode.equalsIgnoreCase(\"tokenize\") || opcode.equalsIgnoreCase(\"transformapply\") ||\n+\t\t\t\topcode.equalsIgnoreCase(\"transformdecode\")) {\n \t\t\t\tfunc = ParameterizedBuiltin.getParameterizedBuiltinFnObject(opcode);\n \t\t\t\treturn new ParameterizedBuiltinSPInstruction(new SimpleOperator(func), paramsMap, out, opcode, str);\n \t\t\t}\n@@ -169,402 +168,396 @@ else if (opcode.equalsIgnoreCase(\"rexpand\")\n \t\t\t}\n \t\t}\n \t}\n-\t\n \n-\t@Override \n+\t@Override\n \t@SuppressWarnings(\"unchecked\")\n \tpublic void processInstruction(ExecutionContext ec) {\n-\t\tSparkExecutionContext sec = (SparkExecutionContext)ec;\n+\t\tSparkExecutionContext sec = (SparkExecutionContext) ec;\n \t\tString opcode = getOpcode();\n-\t\t\n-\t\t//opcode guaranteed to be a valid opcode (see parsing)\n-\t\tif( opcode.equalsIgnoreCase(\"mapgroupedagg\") )\n-\t\t{\n-\t\t\t//get input rdd handle\n+\n+\t\t// opcode guaranteed to be a valid opcode (see parsing)\n+\t\tif(opcode.equalsIgnoreCase(\"mapgroupedagg\")) {\n+\t\t\t// get input rdd handle\n \t\t\tString targetVar = params.get(Statement.GAGG_TARGET);\n \t\t\tString groupsVar = params.get(Statement.GAGG_GROUPS);\n-\t\t\tJavaPairRDD<MatrixIndexes,MatrixBlock> target = sec.getBinaryMatrixBlockRDDHandleForVariable(targetVar);\n+\t\t\tJavaPairRDD<MatrixIndexes, MatrixBlock> target = sec.getBinaryMatrixBlockRDDHandleForVariable(targetVar);\n \t\t\tPartitionedBroadcast<MatrixBlock> groups = sec.getBroadcastForVariable(groupsVar);\n-\t\t\tDataCharacteristics mc1 = sec.getDataCharacteristics( targetVar );\n+\t\t\tDataCharacteristics mc1 = sec.getDataCharacteristics(targetVar);\n \t\t\tDataCharacteristics mcOut = sec.getDataCharacteristics(output.getName());\n \t\t\tCPOperand ngrpOp = new CPOperand(params.get(Statement.GAGG_NUM_GROUPS));\n-\t\t\tint ngroups = (int)sec.getScalarInput(ngrpOp).getLongValue();\n-\t\t\t\n-\t\t\t//single-block aggregation\n-\t\t\tif( ngroups <= mc1.getBlocksize() && mc1.getCols() <= mc1.getBlocksize() ) {\n-\t\t\t\t//execute map grouped aggregate\n+\t\t\tint ngroups = (int) sec.getScalarInput(ngrpOp).getLongValue();\n+\n+\t\t\t// single-block aggregation\n+\t\t\tif(ngroups <= mc1.getBlocksize() && mc1.getCols() <= mc1.getBlocksize()) {\n+\t\t\t\t// execute map grouped aggregate\n \t\t\t\tJavaRDD<MatrixBlock> out = target.map(new RDDMapGroupedAggFunction2(groups, _optr, ngroups));\n \t\t\t\tMatrixBlock out2 = RDDAggregateUtils.sumStable(out);\n-\t\t\t\t\n-\t\t\t\t//put output block into symbol table (no lineage because single block)\n-\t\t\t\t//this also includes implicit maintenance of matrix characteristics\n+\n+\t\t\t\t// put output block into symbol table (no lineage because single block)\n+\t\t\t\t// this also includes implicit maintenance of matrix characteristics\n \t\t\t\tsec.setMatrixOutput(output.getName(), out2);\n \t\t\t}\n-\t\t\t//multi-block aggregation\n+\t\t\t// multi-block aggregation\n \t\t\telse {\n-\t\t\t\t//execute map grouped aggregate\n+\t\t\t\t// execute map grouped aggregate\n \t\t\t\tJavaPairRDD<MatrixIndexes, MatrixBlock> out = target\n \t\t\t\t\t.flatMapToPair(new RDDMapGroupedAggFunction(groups, _optr, ngroups, mc1.getBlocksize()));\n-\t\t\t\t\n+\n \t\t\t\tout = RDDAggregateUtils.sumByKeyStable(out, false);\n-\t\t\t\t\n-\t\t\t\t//updated characteristics and handle outputs\n+\n+\t\t\t\t// updated characteristics and handle outputs\n \t\t\t\tmcOut.set(ngroups, mc1.getCols(), mc1.getBlocksize(), -1);\n \t\t\t\tsec.setRDDHandleForVariable(output.getName(), out);\n-\t\t\t\tsec.addLineageRDD( output.getName(), targetVar );\n-\t\t\t\tsec.addLineageBroadcast( output.getName(), groupsVar );\n+\t\t\t\tsec.addLineageRDD(output.getName(), targetVar);\n+\t\t\t\tsec.addLineageBroadcast(output.getName(), groupsVar);\n \t\t\t}\n \t\t}\n-\t\telse if ( opcode.equalsIgnoreCase(\"groupedagg\") )\n-\t\t{\n+\t\telse if(opcode.equalsIgnoreCase(\"groupedagg\")) {\n \t\t\tboolean broadcastGroups = Boolean.parseBoolean(params.get(\"broadcast\"));\n-\t\t\t\n-\t\t\t//get input rdd handle\n+\n+\t\t\t// get input rdd handle\n \t\t\tString groupsVar = params.get(Statement.GAGG_GROUPS);\n-\t\t\tJavaPairRDD<MatrixIndexes,MatrixBlock> target = sec.getBinaryMatrixBlockRDDHandleForVariable( params.get(Statement.GAGG_TARGET) );\n-\t\t\tJavaPairRDD<MatrixIndexes,MatrixBlock> groups = broadcastGroups ? null : sec.getBinaryMatrixBlockRDDHandleForVariable( groupsVar );\n-\t\t\tJavaPairRDD<MatrixIndexes,MatrixBlock> weights = null;\n-\t\t\t\n-\t\t\tDataCharacteristics mc1 = sec.getDataCharacteristics( params.get(Statement.GAGG_TARGET) );\n-\t\t\tDataCharacteristics mc2 = sec.getDataCharacteristics( groupsVar );\n-\t\t\tif(mc1.dimsKnown() && mc2.dimsKnown() && (mc1.getRows() != mc2.getRows() || mc2.getCols() !=1)) {\n+\t\t\tJavaPairRDD<MatrixIndexes, MatrixBlock> target = sec\n+\t\t\t\t.getBinaryMatrixBlockRDDHandleForVariable(params.get(Statement.GAGG_TARGET));\n+\t\t\tJavaPairRDD<MatrixIndexes, MatrixBlock> groups = broadcastGroups ? null : sec\n+\t\t\t\t.getBinaryMatrixBlockRDDHandleForVariable(groupsVar);\n+\t\t\tJavaPairRDD<MatrixIndexes, MatrixBlock> weights = null;\n+\n+\t\t\tDataCharacteristics mc1 = sec.getDataCharacteristics(params.get(Statement.GAGG_TARGET));\n+\t\t\tDataCharacteristics mc2 = sec.getDataCharacteristics(groupsVar);\n+\t\t\tif(mc1.dimsKnown() && mc2.dimsKnown() && (mc1.getRows() != mc2.getRows() || mc2.getCols() != 1)) {\n \t\t\t\tthrow new DMLRuntimeException(\"Grouped Aggregate dimension mismatch between target and groups.\");\n \t\t\t}\n \t\t\tDataCharacteristics mcOut = sec.getDataCharacteristics(output.getName());\n-\t\t\t\n+\n \t\t\tJavaPairRDD<MatrixIndexes, WeightedCell> groupWeightedCells = null;\n-\t\t\t\n+\n \t\t\t// Step 1: First extract groupWeightedCells from group, target and weights\n-\t\t\tif ( params.get(Statement.GAGG_WEIGHTS) != null ) {\n-\t\t\t\tweights = sec.getBinaryMatrixBlockRDDHandleForVariable( params.get(Statement.GAGG_WEIGHTS) );\n-\t\t\t\t\n-\t\t\t\tDataCharacteristics mc3 = sec.getDataCharacteristics( params.get(Statement.GAGG_WEIGHTS) );\n-\t\t\t\tif(mc1.dimsKnown() && mc3.dimsKnown() && (mc1.getRows() != mc3.getRows() || mc1.getCols() != mc3.getCols())) {\n-\t\t\t\t\tthrow new DMLRuntimeException(\"Grouped Aggregate dimension mismatch between target, groups, and weights.\");\n+\t\t\tif(params.get(Statement.GAGG_WEIGHTS) != null) {\n+\t\t\t\tweights = sec.getBinaryMatrixBlockRDDHandleForVariable(params.get(Statement.GAGG_WEIGHTS));\n+\n+\t\t\t\tDataCharacteristics mc3 = sec.getDataCharacteristics(params.get(Statement.GAGG_WEIGHTS));\n+\t\t\t\tif(mc1.dimsKnown() && mc3.dimsKnown() &&\n+\t\t\t\t\t(mc1.getRows() != mc3.getRows() || mc1.getCols() != mc3.getCols())) {\n+\t\t\t\t\tthrow new DMLRuntimeException(\n+\t\t\t\t\t\t\"Grouped Aggregate dimension mismatch between target, groups, and weights.\");\n \t\t\t\t}\n-\t\t\t\t\n-\t\t\t\tgroupWeightedCells = groups.join(target).join(weights)\n-\t\t\t\t\t.flatMapToPair(new ExtractGroupNWeights());\n+\n+\t\t\t\tgroupWeightedCells = groups.join(target).join(weights).flatMapToPair(new ExtractGroupNWeights());\n \t\t\t}\n-\t\t\telse //input vector or matrix\n+\t\t\telse // input vector or matrix\n \t\t\t{\n \t\t\t\tString ngroupsStr = params.get(Statement.GAGG_NUM_GROUPS);\n \t\t\t\tlong ngroups = (ngroupsStr != null) ? (long) Double.parseDouble(ngroupsStr) : -1;\n-\t\t\t\t\n-\t\t\t\t//execute basic grouped aggregate (extract and preagg)\n-\t\t\t\tif( broadcastGroups ) {\n+\n+\t\t\t\t// execute basic grouped aggregate (extract and preagg)\n+\t\t\t\tif(broadcastGroups) {\n \t\t\t\t\tPartitionedBroadcast<MatrixBlock> pbm = sec.getBroadcastForVariable(groupsVar);\n \t\t\t\t\tgroupWeightedCells = target\n \t\t\t\t\t\t.flatMapToPair(new ExtractGroupBroadcast(pbm, mc1.getBlocksize(), ngroups, _optr));\n \t\t\t\t}\n-\t\t\t\telse { //general case\n-\t\t\t\t\t\n-\t\t\t\t\t//replicate groups if necessary\n-\t\t\t\t\tif( mc1.getNumColBlocks() > 1 ) {\n-\t\t\t\t\t\tgroups = groups.flatMapToPair(\n-\t\t\t\t\t\t\tnew ReplicateVectorFunction(false, mc1.getNumColBlocks() ));\n+\t\t\t\telse { // general case\n+\n+\t\t\t\t\t// replicate groups if necessary\n+\t\t\t\t\tif(mc1.getNumColBlocks() > 1) {\n+\t\t\t\t\t\tgroups = groups.flatMapToPair(new ReplicateVectorFunction(false, mc1.getNumColBlocks()));\n \t\t\t\t\t}\n-\t\t\t\t\t\n+\n \t\t\t\t\tgroupWeightedCells = groups.join(target)\n \t\t\t\t\t\t.flatMapToPair(new ExtractGroupJoin(mc1.getBlocksize(), ngroups, _optr));\n \t\t\t\t}\n \t\t\t}\n-\t\t\t\n-\t\t\t// Step 2: Make sure we have blen required while creating <MatrixIndexes, MatrixCell> \n+\n+\t\t\t// Step 2: Make sure we have blen required while creating <MatrixIndexes, MatrixCell>\n \t\t\tif(mc1.getBlocksize() == -1) {\n \t\t\t\tthrow new DMLRuntimeException(\"The block sizes are not specified for grouped aggregate\");\n \t\t\t}\n \t\t\tint blen = mc1.getBlocksize();\n-\t\t\t\n+\n \t\t\t// Step 3: Now perform grouped aggregate operation (either on combiner side or reducer side)\n \t\t\tJavaPairRDD<MatrixIndexes, MatrixCell> out = null;\n-\t\t\tif(_optr instanceof CMOperator && ((CMOperator) _optr).isPartialAggregateOperator() \n-\t\t\t\t|| _optr instanceof AggregateOperator ) {\n+\t\t\tif(_optr instanceof CMOperator && ((CMOperator) _optr).isPartialAggregateOperator() ||\n+\t\t\t\t_optr instanceof AggregateOperator) {\n \t\t\t\tout = groupWeightedCells.reduceByKey(new PerformGroupByAggInCombiner(_optr))\n \t\t\t\t\t.mapValues(new CreateMatrixCell(blen, _optr));\n \t\t\t}\n \t\t\telse {\n \t\t\t\t// Use groupby key because partial aggregation is not supported\n-\t\t\t\tout = groupWeightedCells.groupByKey()\n-\t\t\t\t\t.mapValues(new PerformGroupByAggInReducer(_optr))\n+\t\t\t\tout = groupWeightedCells.groupByKey().mapValues(new PerformGroupByAggInReducer(_optr))\n \t\t\t\t\t.mapValues(new CreateMatrixCell(blen, _optr));\n \t\t\t}\n-\t\t\t\n-\t\t\t// Step 4: Set output characteristics and rdd handle \n+\n+\t\t\t// Step 4: Set output characteristics and rdd handle\n \t\t\tsetOutputCharacteristicsForGroupedAgg(mc1, mcOut, out);\n-\t\t\t\n-\t\t\t//store output rdd handle\n+\n+\t\t\t// store output rdd handle\n \t\t\tsec.setRDDHandleForVariable(output.getName(), out);\n-\t\t\tsec.addLineageRDD( output.getName(), params.get(Statement.GAGG_TARGET) );\n-\t\t\tsec.addLineage( output.getName(), groupsVar, broadcastGroups );\n-\t\t\tif ( params.get(Statement.GAGG_WEIGHTS) != null ) {\n-\t\t\t\tsec.addLineageRDD(output.getName(), params.get(Statement.GAGG_WEIGHTS) );\n+\t\t\tsec.addLineageRDD(output.getName(), params.get(Statement.GAGG_TARGET));\n+\t\t\tsec.addLineage(output.getName(), groupsVar, broadcastGroups);\n+\t\t\tif(params.get(Statement.GAGG_WEIGHTS) != null) {\n+\t\t\t\tsec.addLineageRDD(output.getName(), params.get(Statement.GAGG_WEIGHTS));\n \t\t\t}\n \t\t}\n-\t\telse if ( opcode.equalsIgnoreCase(\"rmempty\") ) \n-\t\t{\n+\t\telse if(opcode.equalsIgnoreCase(\"rmempty\")) {\n \t\t\tString rddInVar = params.get(\"target\");\n \t\t\tString rddOffVar = params.get(\"offset\");\n-\t\t\t\n-\t\t\tboolean rows = sec.getScalarInput(params.get(\"margin\"), ValueType.STRING, true).getStringValue().equals(\"rows\");\n+\n+\t\t\tboolean rows = sec.getScalarInput(params.get(\"margin\"), ValueType.STRING, true).getStringValue()\n+\t\t\t\t.equals(\"rows\");\n \t\t\tboolean emptyReturn = Boolean.parseBoolean(params.get(\"empty.return\").toLowerCase());\n \t\t\tlong maxDim = sec.getScalarInput(params.get(\"maxdim\"), ValueType.FP64, false).getLongValue();\n \t\t\tboolean bRmEmptyBC = Boolean.parseBoolean(params.get(\"bRmEmptyBC\"));\n \t\t\tDataCharacteristics mcIn = sec.getDataCharacteristics(rddInVar);\n-\t\t\t\n-\t\t\tif( maxDim > 0 ) //default case\n+\n+\t\t\tif(maxDim > 0) // default case\n \t\t\t{\n-\t\t\t\t//get input rdd handle\n-\t\t\t\tJavaPairRDD<MatrixIndexes,MatrixBlock> in = sec.getBinaryMatrixBlockRDDHandleForVariable( rddInVar );\n-\t\t\t\tJavaPairRDD<MatrixIndexes,MatrixBlock> off;\n+\t\t\t\t// get input rdd handle\n+\t\t\t\tJavaPairRDD<MatrixIndexes, MatrixBlock> in = sec.getBinaryMatrixBlockRDDHandleForVariable(rddInVar);\n+\t\t\t\tJavaPairRDD<MatrixIndexes, MatrixBlock> off;\n \t\t\t\tPartitionedBroadcast<MatrixBlock> broadcastOff;\n \t\t\t\tlong blen = mcIn.getBlocksize();\n-\t\t\t\tlong numRep = (long)Math.ceil( rows ? (double)mcIn.getCols()/blen : (double)mcIn.getRows()/blen);\n-\t\t\t\t\n-\t\t\t\t//execute remove empty rows/cols operation\n-\t\t\t\tJavaPairRDD<MatrixIndexes,MatrixBlock> out;\n-\t\n-\t\t\t\tif(bRmEmptyBC){\n-\t\t\t\t\tbroadcastOff = sec.getBroadcastForVariable( rddOffVar );\n+\t\t\t\tlong numRep = (long) Math.ceil(rows ? (double) mcIn.getCols() / blen : (double) mcIn.getRows() / blen);\n+\n+\t\t\t\t// execute remove empty rows/cols operation\n+\t\t\t\tJavaPairRDD<MatrixIndexes, MatrixBlock> out;\n+\n+\t\t\t\tif(bRmEmptyBC) {\n+\t\t\t\t\tbroadcastOff = sec.getBroadcastForVariable(rddOffVar);\n \t\t\t\t\t// Broadcast offset vector\n-\t\t\t\t\tout = in\n-\t\t\t\t\t\t.flatMapToPair(new RDDRemoveEmptyFunctionInMem(rows, maxDim, blen, broadcastOff));\n+\t\t\t\t\tout = in.flatMapToPair(new RDDRemoveEmptyFunctionInMem(rows, maxDim, blen, broadcastOff));\n \t\t\t\t}\n \t\t\t\telse {\n-\t\t\t\t\toff = sec.getBinaryMatrixBlockRDDHandleForVariable( rddOffVar );\n-\t\t\t\t\tout = in\n-\t\t\t\t\t\t.join( off.flatMapToPair(new ReplicateVectorFunction(!rows,numRep)) )\n+\t\t\t\t\toff = sec.getBinaryMatrixBlockRDDHandleForVariable(rddOffVar);\n+\t\t\t\t\tout = in.join(off.flatMapToPair(new ReplicateVectorFunction(!rows, numRep)))\n \t\t\t\t\t\t.flatMapToPair(new RDDRemoveEmptyFunction(rows, maxDim, blen));\n \t\t\t\t}\n-\t\n+\n \t\t\t\tout = RDDAggregateUtils.mergeByKey(out, false);\n-\t\t\t\t\n-\t\t\t\t//store output rdd handle\n+\n+\t\t\t\t// store output rdd handle\n \t\t\t\tsec.setRDDHandleForVariable(output.getName(), out);\n \t\t\t\tsec.addLineageRDD(output.getName(), rddInVar);\n \t\t\t\tif(bRmEmptyBC)\n \t\t\t\t\tsec.addLineageBroadcast(output.getName(), rddOffVar);\n \t\t\t\telse\n \t\t\t\t\tsec.addLineageRDD(output.getName(), rddOffVar);\n-\t\t\t\t\n-\t\t\t\t//update output statistics (required for correctness)\n+\n+\t\t\t\t// update output statistics (required for correctness)\n \t\t\t\tDataCharacteristics mcOut = sec.getDataCharacteristics(output.getName());\n-\t\t\t\tmcOut.set(rows?maxDim:mcIn.getRows(), rows?mcIn.getCols():maxDim, (int)blen, mcIn.getNonZeros());\n+\t\t\t\tmcOut.set(rows ? maxDim : mcIn.getRows(),\n+\t\t\t\t\trows ? mcIn.getCols() : maxDim,\n+\t\t\t\t\t(int) blen,\n+\t\t\t\t\tmcIn.getNonZeros());\n \t\t\t}\n-\t\t\telse //special case: empty output (ensure valid dims)\n+\t\t\telse // special case: empty output (ensure valid dims)\n \t\t\t{\n \t\t\t\tint n = emptyReturn ? 1 : 0;\n-\t\t\t\tMatrixBlock out = new MatrixBlock(rows?n:(int)mcIn.getRows(), rows?(int)mcIn.getCols():n, true); \n+\t\t\t\tMatrixBlock out = new MatrixBlock(rows ? n : (int) mcIn.getRows(), rows ? (int) mcIn.getCols() : n,\n+\t\t\t\t\ttrue);\n \t\t\t\tsec.setMatrixOutput(output.getName(), out);\n \t\t\t}\n \t\t}\n-\t\telse if ( opcode.equalsIgnoreCase(\"replace\") ) \n-\t\t{\n-\t\t\tJavaPairRDD<MatrixIndexes,MatrixBlock> in1 = sec.getBinaryMatrixBlockRDDHandleForVariable(params.get(\"target\"));\n+\t\telse if(opcode.equalsIgnoreCase(\"replace\")) {\n+\t\t\tJavaPairRDD<MatrixIndexes, MatrixBlock> in1 = sec\n+\t\t\t\t.getBinaryMatrixBlockRDDHandleForVariable(params.get(\"target\"));\n \t\t\tDataCharacteristics mcIn = sec.getDataCharacteristics(params.get(\"target\"));\n-\t\t\t\n-\t\t\t//execute replace operation\n-\t\t\tdouble pattern = Double.parseDouble( params.get(\"pattern\") );\n-\t\t\tdouble replacement = Double.parseDouble( params.get(\"replacement\") );\n-\t\t\tJavaPairRDD<MatrixIndexes,MatrixBlock> out = \n-\t\t\t\tin1.mapValues(new RDDReplaceFunction(pattern, replacement));\n-\t\t\t\n-\t\t\t//store output rdd handle\n+\n+\t\t\t// execute replace operation\n+\t\t\tdouble pattern = Double.parseDouble(params.get(\"pattern\"));\n+\t\t\tdouble replacement = Double.parseDouble(params.get(\"replacement\"));\n+\t\t\tJavaPairRDD<MatrixIndexes, MatrixBlock> out = in1.mapValues(new RDDReplaceFunction(pattern, replacement));\n+\n+\t\t\t// store output rdd handle\n \t\t\tsec.setRDDHandleForVariable(output.getName(), out);\n \t\t\tsec.addLineageRDD(output.getName(), params.get(\"target\"));\n-\t\t\t\n-\t\t\t//update output statistics (required for correctness)\n+\n+\t\t\t// update output statistics (required for correctness)\n \t\t\tDataCharacteristics mcOut = sec.getDataCharacteristics(output.getName());\n-\t\t\tmcOut.set(mcIn.getRows(), mcIn.getCols(), mcIn.getBlocksize(),\n-\t\t\t\t(pattern!=0 && replacement!=0)?mcIn.getNonZeros():-1);\n-\t\t}\n-\t\telse if ( opcode.equalsIgnoreCase(\"lowertri\") || opcode.equalsIgnoreCase(\"uppertri\") ) \n-\t\t{\n-\t\t\tJavaPairRDD<MatrixIndexes,MatrixBlock> in1 = sec.getBinaryMatrixBlockRDDHandleForVariable(params.get(\"target\"));\n+\t\t\tmcOut.set(mcIn.getRows(),\n+\t\t\t\tmcIn.getCols(),\n+\t\t\t\tmcIn.getBlocksize(),\n+\t\t\t\t(pattern != 0 && replacement != 0) ? mcIn.getNonZeros() : -1);\n+\t\t}\n+\t\telse if(opcode.equalsIgnoreCase(\"lowertri\") || opcode.equalsIgnoreCase(\"uppertri\")) {\n+\t\t\tJavaPairRDD<MatrixIndexes, MatrixBlock> in1 = sec\n+\t\t\t\t.getBinaryMatrixBlockRDDHandleForVariable(params.get(\"target\"));\n \t\t\tDataCharacteristics mcIn = sec.getDataCharacteristics(params.get(\"target\"));\n \t\t\tboolean lower = opcode.equalsIgnoreCase(\"lowertri\");\n \t\t\tboolean diag = Boolean.parseBoolean(params.get(\"diag\"));\n \t\t\tboolean values = Boolean.parseBoolean(params.get(\"values\"));\n-\t\t\t\n-\t\t\tJavaPairRDD<MatrixIndexes,MatrixBlock> out = in1.mapPartitionsToPair(\n-\t\t\t\tnew RDDExtractTriangularFunction(lower, diag, values), true);\n-\t\t\t\n-\t\t\t//store output rdd handle\n+\n+\t\t\tJavaPairRDD<MatrixIndexes, MatrixBlock> out = in1\n+\t\t\t\t.mapPartitionsToPair(new RDDExtractTriangularFunction(lower, diag, values), true);\n+\n+\t\t\t// store output rdd handle\n \t\t\tsec.setRDDHandleForVariable(output.getName(), out);\n \t\t\tsec.addLineageRDD(output.getName(), params.get(\"target\"));\n-\t\t\t\n-\t\t\t//update output statistics (required for correctness)\n+\n+\t\t\t// update output statistics (required for correctness)\n \t\t\tsec.getDataCharacteristics(output.getName()).setDimension(mcIn.getRows(), mcIn.getCols());\n \t\t}\n-\t\telse if ( opcode.equalsIgnoreCase(\"rexpand\") ) \n-\t\t{\n+\t\telse if(opcode.equalsIgnoreCase(\"rexpand\")) {\n \t\t\tString rddInVar = params.get(\"target\");\n-\t\t\t\n-\t\t\t//get input rdd handle\n-\t\t\tJavaPairRDD<MatrixIndexes,MatrixBlock> in = sec.getBinaryMatrixBlockRDDHandleForVariable( rddInVar );\n+\n+\t\t\t// get input rdd handle\n+\t\t\tJavaPairRDD<MatrixIndexes, MatrixBlock> in = sec.getBinaryMatrixBlockRDDHandleForVariable(rddInVar);\n \t\t\tDataCharacteristics mcIn = sec.getDataCharacteristics(rddInVar);\n-\t\t\tdouble maxVal = Double.parseDouble( params.get(\"max\") );\n+\t\t\tdouble maxVal = Double.parseDouble(params.get(\"max\"));\n \t\t\tlong lmaxVal = UtilFunctions.toLong(maxVal);\n \t\t\tboolean dirRows = params.get(\"dir\").equals(\"rows\");\n \t\t\tboolean cast = Boolean.parseBoolean(params.get(\"cast\"));\n \t\t\tboolean ignore = Boolean.parseBoolean(params.get(\"ignore\"));\n \t\t\tlong blen = mcIn.getBlocksize();\n-\t\t\t\n-\t\t\t//repartition input vector for higher degree of parallelism \n-\t\t\t//(avoid scenarios where few input partitions create huge outputs)\n-\t\t\tDataCharacteristics mcTmp = new MatrixCharacteristics(dirRows?lmaxVal:mcIn.getRows(),\n-\t\t\t\t\tdirRows?mcIn.getRows():lmaxVal, (int)blen, mcIn.getRows());\n-\t\t\tint numParts = (int)Math.min(SparkUtils.getNumPreferredPartitions(mcTmp, in), mcIn.getNumBlocks());\n-\t\t\tif( numParts > in.getNumPartitions()*2 )\n+\n+\t\t\t// repartition input vector for higher degree of parallelism\n+\t\t\t// (avoid scenarios where few input partitions create huge outputs)\n+\t\t\tDataCharacteristics mcTmp = new MatrixCharacteristics(dirRows ? lmaxVal : mcIn.getRows(),\n+\t\t\t\tdirRows ? mcIn.getRows() : lmaxVal, (int) blen, mcIn.getRows());\n+\t\t\tint numParts = (int) Math.min(SparkUtils.getNumPreferredPartitions(mcTmp, in), mcIn.getNumBlocks());\n+\t\t\tif(numParts > in.getNumPartitions() * 2)\n \t\t\t\tin = in.repartition(numParts);\n-\t\t\t\n-\t\t\t//execute rexpand rows/cols operation (no shuffle required because outputs are\n-\t\t\t//block-aligned with the input, i.e., one input block generates n output blocks)\n-\t\t\tJavaPairRDD<MatrixIndexes,MatrixBlock> out = in\n+\n+\t\t\t// execute rexpand rows/cols operation (no shuffle required because outputs are\n+\t\t\t// block-aligned with the input, i.e., one input block generates n output blocks)\n+\t\t\tJavaPairRDD<MatrixIndexes, MatrixBlock> out = in\n \t\t\t\t.flatMapToPair(new RDDRExpandFunction(maxVal, dirRows, cast, ignore, blen));\n-\t\t\t\n-\t\t\t//store output rdd handle\n+\n+\t\t\t// store output rdd handle\n \t\t\tsec.setRDDHandleForVariable(output.getName(), out);\n \t\t\tsec.addLineageRDD(output.getName(), rddInVar);\n-\t\t\t\n-\t\t\t//update output statistics (required for correctness, nnz unknown due to cut-off)\n+\n+\t\t\t// update output statistics (required for correctness, nnz unknown due to cut-off)\n \t\t\tDataCharacteristics mcOut = sec.getDataCharacteristics(output.getName());\n-\t\t\tmcOut.set(dirRows?lmaxVal:mcIn.getRows(), dirRows?mcIn.getRows():lmaxVal, (int)blen, -1);\n+\t\t\tmcOut.set(dirRows ? lmaxVal : mcIn.getRows(), dirRows ? mcIn.getRows() : lmaxVal, (int) blen, -1);\n \t\t\tmcOut.setNonZerosBound(mcIn.getRows());\n-\t\t\t\n-\t\t\t//post-processing to obtain sparsity of ultra-sparse outputs\n+\n+\t\t\t// post-processing to obtain sparsity of ultra-sparse outputs\n \t\t\tSparkUtils.postprocessUltraSparseOutput(sec.getMatrixObject(output), mcOut);\n \t\t}\n-\t\telse if ( opcode.equalsIgnoreCase(\"tokenize\") )\n-\t\t{\n-\t\t\t//get input RDD data\n+\t\telse if(opcode.equalsIgnoreCase(\"tokenize\")) {\n+\t\t\t// get input RDD data\n \t\t\tFrameObject fo = sec.getFrameObject(params.get(\"target\"));\n-\t\t\tJavaPairRDD<Long,FrameBlock> in = (JavaPairRDD<Long,FrameBlock>)\n-\t\t\t\t\tsec.getRDDHandleForFrameObject(fo, FileFormat.BINARY);\n+\t\t\tJavaPairRDD<Long, FrameBlock> in = (JavaPairRDD<Long, FrameBlock>) sec.getRDDHandleForFrameObject(fo,\n+\t\t\t\tFileFormat.BINARY);\n \t\t\tDataCharacteristics mc = sec.getDataCharacteristics(params.get(\"target\"));\n \n-\t\t\t//construct tokenizer and tokenize text\n+\t\t\t// construct tokenizer and tokenize text\n \t\t\tTokenizer tokenizer = TokenizerFactory.createTokenizer(params.get(\"spec\"),\n-\t\t\t\t\tInteger.parseInt(params.get(\"max_tokens\")));\n-\t\t\tJavaPairRDD<Long,FrameBlock> out = in.mapToPair(\n-\t\t\t\t\tnew RDDTokenizeFunction(tokenizer, mc.getBlocksize()));\n+\t\t\t\tInteger.parseInt(params.get(\"max_tokens\")));\n+\t\t\tJavaPairRDD<Long, FrameBlock> out = in.mapToPair(new RDDTokenizeFunction(tokenizer, mc.getBlocksize()));\n \n-\t\t\t//set output and maintain lineage/output characteristics\n+\t\t\t// set output and maintain lineage/output characteristics\n \t\t\tsec.setRDDHandleForVariable(output.getName(), out);\n \t\t\tsec.addLineageRDD(output.getName(), params.get(\"target\"));\n \n \t\t\t// get max tokens for row upper bound\n \t\t\tlong numRows = tokenizer.getNumRows(mc.getRows());\n \t\t\tlong numCols = tokenizer.getNumCols();\n \n-\t\t\tsec.getDataCharacteristics(output.getName()).set(\n-\t\t\t\t\tnumRows, numCols, mc.getBlocksize());\n+\t\t\tsec.getDataCharacteristics(output.getName()).set(numRows, numCols, mc.getBlocksize());\n \t\t\tsec.getFrameObject(output.getName()).setSchema(tokenizer.getSchema());\n \t\t}\n-\t\telse if ( opcode.equalsIgnoreCase(\"transformapply\") )\n-\t\t{\n-\t\t\t//get input RDD and meta data\n+\t\telse if(opcode.equalsIgnoreCase(\"transformapply\")) {\n+\t\t\t// get input RDD and meta data\n \t\t\tFrameObject fo = sec.getFrameObject(params.get(\"target\"));\n-\t\t\tJavaPairRDD<Long,FrameBlock> in = (JavaPairRDD<Long,FrameBlock>)\n-\t\t\t\tsec.getRDDHandleForFrameObject(fo, FileFormat.BINARY);\n+\t\t\tJavaPairRDD<Long, FrameBlock> in = (JavaPairRDD<Long, FrameBlock>) sec.getRDDHandleForFrameObject(fo,\n+\t\t\t\tFileFormat.BINARY);\n \t\t\tFrameBlock meta = sec.getFrameInput(params.get(\"meta\"));\n \t\t\tDataCharacteristics mcIn = sec.getDataCharacteristics(params.get(\"target\"));\n \t\t\tDataCharacteristics mcOut = sec.getDataCharacteristics(output.getName());\n-\t\t\tString[] colnames = !TfMetaUtils.isIDSpec(params.get(\"spec\")) ?\n-\t\t\t\tin.lookup(1L).get(0).getColumnNames() : null; \n-\t\t\t\n-\t\t\t//compute omit offset map for block shifts\n+\t\t\tString[] colnames = !TfMetaUtils.isIDSpec(params.get(\"spec\")) ? in.lookup(1L).get(0)\n+\t\t\t\t.getColumnNames() : null;\n+\n+\t\t\t// compute omit offset map for block shifts\n \t\t\tTfOffsetMap omap = null;\n-\t\t\tif( TfMetaUtils.containsOmitSpec(params.get(\"spec\"), colnames) ) {\n-\t\t\t\tomap = new TfOffsetMap(SparkUtils.toIndexedLong(in.mapToPair(\n-\t\t\t\t\tnew RDDTransformApplyOffsetFunction(params.get(\"spec\"), colnames)).collect()));\n+\t\t\tif(TfMetaUtils.containsOmitSpec(params.get(\"spec\"), colnames)) {\n+\t\t\t\tomap = new TfOffsetMap(SparkUtils.toIndexedLong(\n+\t\t\t\t\tin.mapToPair(new RDDTransformApplyOffsetFunction(params.get(\"spec\"), colnames)).collect()));\n \t\t\t}\n-\t\t\t\n-\t\t\t//create encoder broadcast (avoiding replication per task) \n-\t\t\tEncoder encoder = EncoderFactory.createEncoder(params.get(\"spec\"), colnames,\n-\t\t\t\tfo.getSchema(), (int)fo.getNumColumns(), meta);\n-\t\t\tmcOut.setDimension(mcIn.getRows()-((omap!=null)?omap.getNumRmRows():0), encoder.getNumCols()); \n-\t\t\tBroadcast<Encoder> bmeta = sec.getSparkContext().broadcast(encoder);\n-\t\t\tBroadcast<TfOffsetMap> bomap = (omap!=null) ? sec.getSparkContext().broadcast(omap) : null;\n-\t\t\t\n-\t\t\t//execute transform apply\n-\t\t\tJavaPairRDD<Long,FrameBlock> tmp = in\n-\t\t\t\t.mapToPair(new RDDTransformApplyFunction(bmeta, bomap));\n-\t\t\tJavaPairRDD<MatrixIndexes,MatrixBlock> out = FrameRDDConverterUtils\n+\n+\t\t\t// create encoder broadcast (avoiding replication per task)\n+\t\t\tMultiColumnEncoder encoder = EncoderFactory\n+\t\t\t\t.createEncoder(params.get(\"spec\"), colnames, fo.getSchema(), (int) fo.getNumColumns(), meta);\n+\t\t\tmcOut.setDimension(mcIn.getRows() - ((omap != null) ? omap.getNumRmRows() : 0),\n+\t\t\t\t(int) fo.getNumColumns() + encoder.getNumExtraCols());\n+\t\t\tBroadcast<MultiColumnEncoder> bmeta = sec.getSparkContext().broadcast(encoder);\n+\t\t\tBroadcast<TfOffsetMap> bomap = (omap != null) ? sec.getSparkContext().broadcast(omap) : null;\n+\n+\t\t\t// execute transform apply\n+\t\t\tJavaPairRDD<Long, FrameBlock> tmp = in.mapToPair(new RDDTransformApplyFunction(bmeta, bomap));\n+\t\t\tJavaPairRDD<MatrixIndexes, MatrixBlock> out = FrameRDDConverterUtils\n \t\t\t\t.binaryBlockToMatrixBlock(tmp, mcOut, mcOut);\n-\t\t\t\n-\t\t\t//set output and maintain lineage/output characteristics\n+\n+\t\t\t// set output and maintain lineage/output characteristics\n \t\t\tsec.setRDDHandleForVariable(output.getName(), out);\n \t\t\tsec.addLineageRDD(output.getName(), params.get(\"target\"));\n \t\t\tec.releaseFrameInput(params.get(\"meta\"));\n \t\t}\n-\t\telse if ( opcode.equalsIgnoreCase(\"transformdecode\") ) \n-\t\t{\n-\t\t\t//get input RDD and meta data\n-\t\t\tJavaPairRDD<MatrixIndexes,MatrixBlock> in = sec.getBinaryMatrixBlockRDDHandleForVariable(params.get(\"target\"));\n+\t\telse if(opcode.equalsIgnoreCase(\"transformdecode\")) {\n+\t\t\t// get input RDD and meta data\n+\t\t\tJavaPairRDD<MatrixIndexes, MatrixBlock> in = sec\n+\t\t\t\t.getBinaryMatrixBlockRDDHandleForVariable(params.get(\"target\"));\n \t\t\tDataCharacteristics mc = sec.getDataCharacteristics(params.get(\"target\"));\n \t\t\tFrameBlock meta = sec.getFrameInput(params.get(\"meta\"));\n \t\t\tString[] colnames = meta.getColumnNames();\n-\t\t\t\n-\t\t\t//reblock if necessary (clen > blen)\n-\t\t\tif( mc.getCols() > mc.getNumColBlocks() ) {\n-\t\t\t\tin = in.mapToPair(new RDDTransformDecodeExpandFunction(\n-\t\t\t\t\t\t(int)mc.getCols(), mc.getBlocksize()));\n+\n+\t\t\t// reblock if necessary (clen > blen)\n+\t\t\tif(mc.getCols() > mc.getNumColBlocks()) {\n+\t\t\t\tin = in.mapToPair(new RDDTransformDecodeExpandFunction((int) mc.getCols(), mc.getBlocksize()));\n \t\t\t\tin = RDDAggregateUtils.mergeByKey(in, false);\n \t\t\t}\n-\t\t\t\n-\t\t\t//construct decoder and decode individual matrix blocks\n+\n+\t\t\t// construct decoder and decode individual matrix blocks\n \t\t\tDecoder decoder = DecoderFactory.createDecoder(params.get(\"spec\"), colnames, null, meta);\n-\t\t\tJavaPairRDD<Long,FrameBlock> out = in.mapToPair(\n-\t\t\t\t\tnew RDDTransformDecodeFunction(decoder, mc.getBlocksize()));\n-\t\t\t\n-\t\t\t//set output and maintain lineage/output characteristics\n+\t\t\tJavaPairRDD<Long, FrameBlock> out = in\n+\t\t\t\t.mapToPair(new RDDTransformDecodeFunction(decoder, mc.getBlocksize()));\n+\n+\t\t\t// set output and maintain lineage/output characteristics\n \t\t\tsec.setRDDHandleForVariable(output.getName(), out);\n \t\t\tsec.addLineageRDD(output.getName(), params.get(\"target\"));\n \t\t\tec.releaseFrameInput(params.get(\"meta\"));\n-\t\t\tsec.getDataCharacteristics(output.getName()).set(\n-\t\t\t\tmc.getRows(), meta.getNumColumns(), mc.getBlocksize(), -1);\n+\t\t\tsec.getDataCharacteristics(output.getName()).set(mc.getRows(), meta.getNumColumns(), mc.getBlocksize(), -1);\n \t\t\tsec.getFrameObject(output.getName()).setSchema(decoder.getSchema());\n \t\t}\n \t\telse {\n-\t\t\tthrow new DMLRuntimeException(\"Unknown parameterized builtin opcode: \"+opcode);\n+\t\t\tthrow new DMLRuntimeException(\"Unknown parameterized builtin opcode: \" + opcode);\n \t\t}\n \t}\n \n \tpublic static class RDDReplaceFunction implements Function<MatrixBlock, MatrixBlock> {\n \t\tprivate static final long serialVersionUID = 6576713401901671659L;\n-\t\tprivate double _pattern; \n-\t\tprivate double _replacement;\n-\t\t\n+\t\tprivate final double _pattern;\n+\t\tprivate final double _replacement;\n+\n \t\tpublic RDDReplaceFunction(double pattern, double replacement) {\n \t\t\t_pattern = pattern;\n \t\t\t_replacement = replacement;\n \t\t}\n-\t\t\n+\n \t\t@Override\n \t\tpublic MatrixBlock call(MatrixBlock arg0) {\n \t\t\treturn arg0.replaceOperations(new MatrixBlock(), _pattern, _replacement);\n \t\t}\n \t}\n-\t\n-\tprivate static class RDDExtractTriangularFunction implements PairFlatMapFunction<Iterator<Tuple2<MatrixIndexes, MatrixBlock>>, MatrixIndexes, MatrixBlock> \n-\t{\n+\n+\tprivate static class RDDExtractTriangularFunction\n+\t\timplements PairFlatMapFunction<Iterator<Tuple2<MatrixIndexes, MatrixBlock>>, MatrixIndexes, MatrixBlock> {\n \t\tprivate static final long serialVersionUID = 2754868819184155702L;\n \t\tprivate final boolean _lower, _diag, _values;\n-\t\t\n+\n \t\tpublic RDDExtractTriangularFunction(boolean lower, boolean diag, boolean values) {\n \t\t\t_lower = lower;\n \t\t\t_diag = diag;\n \t\t\t_values = values;\n \t\t}\n-\t\t\n+\n \t\t@Override\n-\t\tpublic LazyIterableIterator<Tuple2<MatrixIndexes, MatrixBlock>> call(Iterator<Tuple2<MatrixIndexes, MatrixBlock>> arg0) {\n+\t\tpublic LazyIterableIterator<Tuple2<MatrixIndexes, MatrixBlock>> call(\n+\t\t\tIterator<Tuple2<MatrixIndexes, MatrixBlock>> arg0) {\n \t\t\treturn new ExtractTriangularIterator(arg0);\n \t\t}\n-\t\t\n-\t\tprivate class ExtractTriangularIterator extends LazyIterableIterator<Tuple2<MatrixIndexes, MatrixBlock>>\n-\t\t{\n+\n+\t\tprivate class ExtractTriangularIterator extends LazyIterableIterator<Tuple2<MatrixIndexes, MatrixBlock>> {\n \t\t\tpublic ExtractTriangularIterator(Iterator<Tuple2<MatrixIndexes, MatrixBlock>> in) {\n \t\t\t\tsuper(in);\n \t\t\t}\n@@ -573,70 +566,66 @@ public ExtractTriangularIterator(Iterator<Tuple2<MatrixIndexes, MatrixBlock>> in\n \t\t\tprotected Tuple2<MatrixIndexes, MatrixBlock> computeNext(Tuple2<MatrixIndexes, MatrixBlock> arg) {\n \t\t\t\tMatrixIndexes ix = arg._1();\n \t\t\t\tMatrixBlock mb = arg._2();\n-\t\t\t\t\n-\t\t\t\t//handle cases of pass-through and reset block\n-\t\t\t\tif( (_lower && ix.getRowIndex() > ix.getColumnIndex())\n-\t\t\t\t\t|| (!_lower && ix.getRowIndex() < ix.getColumnIndex()) ) {\n-\t\t\t\t\treturn _values ? arg : new Tuple2<>(\n-\t\t\t\t\t\tix, new MatrixBlock(mb.getNumRows(), mb.getNumColumns(), 1d));\n+\n+\t\t\t\t// handle cases of pass-through and reset block\n+\t\t\t\tif((_lower && ix.getRowIndex() > ix.getColumnIndex()) ||\n+\t\t\t\t\t(!_lower && ix.getRowIndex() < ix.getColumnIndex())) {\n+\t\t\t\t\treturn _values ? arg : new Tuple2<>(ix, new MatrixBlock(mb.getNumRows(), mb.getNumColumns(), 1d));\n \t\t\t\t}\n-\t\t\t\t\n-\t\t\t\t//handle cases of empty blocks\n-\t\t\t\tif( (_lower && ix.getRowIndex() < ix.getColumnIndex())\n-\t\t\t\t\t|| (!_lower && ix.getRowIndex() > ix.getColumnIndex()) ) {\n-\t\t\t\t\treturn new Tuple2<>(ix,\n-\t\t\t\t\t\tnew MatrixBlock(mb.getNumRows(), mb.getNumColumns(), true));\n+\n+\t\t\t\t// handle cases of empty blocks\n+\t\t\t\tif((_lower && ix.getRowIndex() < ix.getColumnIndex()) ||\n+\t\t\t\t\t(!_lower && ix.getRowIndex() > ix.getColumnIndex())) {\n+\t\t\t\t\treturn new Tuple2<>(ix, new MatrixBlock(mb.getNumRows(), mb.getNumColumns(), true));\n \t\t\t\t}\n-\t\t\t\t\n-\t\t\t\t//extract triangular blocks for blocks on diagonal\n-\t\t\t\tassert(ix.getRowIndex() == ix.getColumnIndex());\n-\t\t\t\treturn new Tuple2<>(ix,\n-\t\t\t\t\tmb.extractTriangular(new MatrixBlock(), _lower, _diag, _values));\n+\n+\t\t\t\t// extract triangular blocks for blocks on diagonal\n+\t\t\t\tassert (ix.getRowIndex() == ix.getColumnIndex());\n+\t\t\t\treturn new Tuple2<>(ix, mb.extractTriangular(new MatrixBlock(), _lower, _diag, _values));\n \t\t\t}\n \t\t}\n \t}\n \n-\tpublic static class RDDRemoveEmptyFunction implements PairFlatMapFunction<Tuple2<MatrixIndexes,Tuple2<MatrixBlock, MatrixBlock>>,MatrixIndexes,MatrixBlock> \n-\t{\n+\tpublic static class RDDRemoveEmptyFunction implements\n+\t\tPairFlatMapFunction<Tuple2<MatrixIndexes, Tuple2<MatrixBlock, MatrixBlock>>, MatrixIndexes, MatrixBlock> {\n \t\tprivate static final long serialVersionUID = 4906304771183325289L;\n \n \t\tprivate final boolean _rmRows;\n \t\tprivate final long _len;\n \t\tprivate final long _blen;\n-\t\t\n+\n \t\tpublic RDDRemoveEmptyFunction(boolean rmRows, long len, long blen) {\n \t\t\t_rmRows = rmRows;\n \t\t\t_len = len;\n \t\t\t_blen = blen;\n \t\t}\n \n \t\t@Override\n-\t\tpublic Iterator<Tuple2<MatrixIndexes, MatrixBlock>> call(Tuple2<MatrixIndexes, Tuple2<MatrixBlock, MatrixBlock>> arg0)\n-\t\t\tthrows Exception \n-\t\t{\n-\t\t\t//prepare inputs (for internal api compatibility)\n-\t\t\tIndexedMatrixValue data = SparkUtils.toIndexedMatrixBlock(arg0._1(),arg0._2()._1());\n-\t\t\tIndexedMatrixValue offsets = SparkUtils.toIndexedMatrixBlock(arg0._1(),arg0._2()._2());\n-\t\t\t\n-\t\t\t//execute remove empty operations\n+\t\tpublic Iterator<Tuple2<MatrixIndexes, MatrixBlock>> call(\n+\t\t\tTuple2<MatrixIndexes, Tuple2<MatrixBlock, MatrixBlock>> arg0) throws Exception {\n+\t\t\t// prepare inputs (for internal api compatibility)\n+\t\t\tIndexedMatrixValue data = SparkUtils.toIndexedMatrixBlock(arg0._1(), arg0._2()._1());\n+\t\t\tIndexedMatrixValue offsets = SparkUtils.toIndexedMatrixBlock(arg0._1(), arg0._2()._2());\n+\n+\t\t\t// execute remove empty operations\n \t\t\tArrayList<IndexedMatrixValue> out = new ArrayList<>();\n \t\t\tLibMatrixReorg.rmempty(data, offsets, _rmRows, _len, _blen, out);\n-\t\t\t\n-\t\t\t//prepare and return outputs\n+\n+\t\t\t// prepare and return outputs\n \t\t\treturn SparkUtils.fromIndexedMatrixBlock(out).iterator();\n \t\t}\n \t}\n \n-\tpublic static class RDDRemoveEmptyFunctionInMem implements PairFlatMapFunction<Tuple2<MatrixIndexes,MatrixBlock>,MatrixIndexes,MatrixBlock> \n-\t{\n+\tpublic static class RDDRemoveEmptyFunctionInMem\n+\t\timplements PairFlatMapFunction<Tuple2<MatrixIndexes, MatrixBlock>, MatrixIndexes, MatrixBlock> {\n \t\tprivate static final long serialVersionUID = 4906304771183325289L;\n \n \t\tprivate final boolean _rmRows;\n \t\tprivate final long _len;\n \t\tprivate final long _blen;\n-\t\t\n+\n \t\tprivate PartitionedBroadcast<MatrixBlock> _off = null;\n-\t\t\n+\n \t\tpublic RDDRemoveEmptyFunctionInMem(boolean rmRows, long len, long blen, PartitionedBroadcast<MatrixBlock> off) {\n \t\t\t_rmRows = rmRows;\n \t\t\t_len = len;\n@@ -646,35 +635,33 @@ public RDDRemoveEmptyFunctionInMem(boolean rmRows, long len, long blen, Partitio\n \n \t\t@Override\n \t\tpublic Iterator<Tuple2<MatrixIndexes, MatrixBlock>> call(Tuple2<MatrixIndexes, MatrixBlock> arg0)\n-\t\t\tthrows Exception \n-\t\t{\n-\t\t\t//prepare inputs (for internal api compatibility)\n-\t\t\tIndexedMatrixValue data = SparkUtils.toIndexedMatrixBlock(arg0._1(),arg0._2());\n-\t\t\tIndexedMatrixValue offsets = _rmRows ?\n-\t\t\t\tSparkUtils.toIndexedMatrixBlock(arg0._1(), _off.getBlock((int)arg0._1().getRowIndex(), 1)) :\n-\t\t\t\tSparkUtils.toIndexedMatrixBlock(arg0._1(), _off.getBlock(1, (int)arg0._1().getColumnIndex()));\n-\t\t\t\n-\t\t\t//execute remove empty operations\n+\t\t\tthrows Exception {\n+\t\t\t// prepare inputs (for internal api compatibility)\n+\t\t\tIndexedMatrixValue data = SparkUtils.toIndexedMatrixBlock(arg0._1(), arg0._2());\n+\t\t\tIndexedMatrixValue offsets = _rmRows ? SparkUtils.toIndexedMatrixBlock(arg0._1(),\n+\t\t\t\t_off.getBlock((int) arg0._1().getRowIndex(), 1)) : SparkUtils.toIndexedMatrixBlock(arg0._1(),\n+\t\t\t\t\t_off.getBlock(1, (int) arg0._1().getColumnIndex()));\n+\n+\t\t\t// execute remove empty operations\n \t\t\tArrayList<IndexedMatrixValue> out = new ArrayList<>();\n \t\t\tLibMatrixReorg.rmempty(data, offsets, _rmRows, _len, _blen, out);\n \n-\t\t\t//prepare and return outputs\n+\t\t\t// prepare and return outputs\n \t\t\treturn SparkUtils.fromIndexedMatrixBlock(out).iterator();\n \t\t}\n \t}\n \n-\tpublic static class RDDRExpandFunction implements PairFlatMapFunction<Tuple2<MatrixIndexes,MatrixBlock>,MatrixIndexes,MatrixBlock> \n-\t{\n+\tpublic static class RDDRExpandFunction\n+\t\timplements PairFlatMapFunction<Tuple2<MatrixIndexes, MatrixBlock>, MatrixIndexes, MatrixBlock> {\n \t\tprivate static final long serialVersionUID = -6153643261956222601L;\n-\t\t\n-\t\tprivate double _maxVal;\n-\t\tprivate boolean _dirRows;\n-\t\tprivate boolean _cast;\n-\t\tprivate boolean _ignore;\n-\t\tprivate long _blen;\n-\t\t\n-\t\tpublic RDDRExpandFunction(double maxVal, boolean dirRows, boolean cast, boolean ignore, long blen) \n-\t\t{\n+\n+\t\tprivate final double _maxVal;\n+\t\tprivate final boolean _dirRows;\n+\t\tprivate final boolean _cast;\n+\t\tprivate final boolean _ignore;\n+\t\tprivate final long _blen;\n+\n+\t\tpublic RDDRExpandFunction(double maxVal, boolean dirRows, boolean cast, boolean ignore, long blen) {\n \t\t\t_maxVal = maxVal;\n \t\t\t_dirRows = dirRows;\n \t\t\t_cast = cast;\n@@ -684,29 +671,28 @@ public RDDRExpandFunction(double maxVal, boolean dirRows, boolean cast, boolean\n \n \t\t@Override\n \t\tpublic Iterator<Tuple2<MatrixIndexes, MatrixBlock>> call(Tuple2<MatrixIndexes, MatrixBlock> arg0)\n-\t\t\tthrows Exception \n-\t\t{\n-\t\t\t//prepare inputs (for internal api compatibility)\n-\t\t\tIndexedMatrixValue data = SparkUtils.toIndexedMatrixBlock(arg0._1(),arg0._2());\n-\t\t\t\n-\t\t\t//execute rexpand operations\n+\t\t\tthrows Exception {\n+\t\t\t// prepare inputs (for internal api compatibility)\n+\t\t\tIndexedMatrixValue data = SparkUtils.toIndexedMatrixBlock(arg0._1(), arg0._2());\n+\n+\t\t\t// execute rexpand operations\n \t\t\tArrayList<IndexedMatrixValue> out = new ArrayList<>();\n \t\t\tLibMatrixReorg.rexpand(data, _maxVal, _dirRows, _cast, _ignore, _blen, out);\n-\t\t\t\n-\t\t\t//prepare and return outputs\n+\n+\t\t\t// prepare and return outputs\n \t\t\treturn SparkUtils.fromIndexedMatrixBlock(out).iterator();\n \t\t}\n \t}\n-\t\n-\tpublic static class RDDMapGroupedAggFunction implements PairFlatMapFunction<Tuple2<MatrixIndexes,MatrixBlock>,MatrixIndexes,MatrixBlock> \n-\t{\n+\n+\tpublic static class RDDMapGroupedAggFunction\n+\t\timplements PairFlatMapFunction<Tuple2<MatrixIndexes, MatrixBlock>, MatrixIndexes, MatrixBlock> {\n \t\tprivate static final long serialVersionUID = 6795402640178679851L;\n-\t\t\n+\n \t\tprivate PartitionedBroadcast<MatrixBlock> _pbm = null;\n \t\tprivate Operator _op = null;\n \t\tprivate int _ngroups = -1;\n \t\tprivate int _blen = -1;\n-\t\t\n+\n \t\tpublic RDDMapGroupedAggFunction(PartitionedBroadcast<MatrixBlock> pbm, Operator op, int ngroups, int blen) {\n \t\t\t_pbm = pbm;\n \t\t\t_op = op;\n@@ -716,108 +702,99 @@ public RDDMapGroupedAggFunction(PartitionedBroadcast<MatrixBlock> pbm, Operator\n \n \t\t@Override\n \t\tpublic Iterator<Tuple2<MatrixIndexes, MatrixBlock>> call(Tuple2<MatrixIndexes, MatrixBlock> arg0)\n-\t\t\tthrows Exception \n-\t\t{\n-\t\t\t//get all inputs\n+\t\t\tthrows Exception {\n+\t\t\t// get all inputs\n \t\t\tMatrixIndexes ix = arg0._1();\n-\t\t\tMatrixBlock target = arg0._2();\t\t\n-\t\t\tMatrixBlock groups = _pbm.getBlock((int)ix.getRowIndex(), 1);\n-\t\t\t\n-\t\t\t//execute map grouped aggregate operations\n+\t\t\tMatrixBlock target = arg0._2();\n+\t\t\tMatrixBlock groups = _pbm.getBlock((int) ix.getRowIndex(), 1);\n+\n+\t\t\t// execute map grouped aggregate operations\n \t\t\tIndexedMatrixValue in1 = SparkUtils.toIndexedMatrixBlock(ix, target);\n \t\t\tArrayList<IndexedMatrixValue> outlist = new ArrayList<>();\n \t\t\tOperationsOnMatrixValues.performMapGroupedAggregate(_op, in1, groups, _ngroups, _blen, outlist);\n-\t\t\t\n-\t\t\t//output all result blocks\n+\n+\t\t\t// output all result blocks\n \t\t\treturn SparkUtils.fromIndexedMatrixBlock(outlist).iterator();\n \t\t}\n \t}\n \n \t/**\n \t * Similar to RDDMapGroupedAggFunction but single output block.\n \t */\n-\tpublic static class RDDMapGroupedAggFunction2 implements Function<Tuple2<MatrixIndexes,MatrixBlock>,MatrixBlock> \n-\t{\n+\tpublic static class RDDMapGroupedAggFunction2 implements Function<Tuple2<MatrixIndexes, MatrixBlock>, MatrixBlock> {\n \t\tprivate static final long serialVersionUID = -6820599604299797661L;\n-\t\t\n+\n \t\tprivate PartitionedBroadcast<MatrixBlock> _pbm = null;\n \t\tprivate Operator _op = null;\n \t\tprivate int _ngroups = -1;\n-\t\t\n+\n \t\tpublic RDDMapGroupedAggFunction2(PartitionedBroadcast<MatrixBlock> pbm, Operator op, int ngroups) {\n \t\t\t_pbm = pbm;\n \t\t\t_op = op;\n \t\t\t_ngroups = ngroups;\n \t\t}\n \n \t\t@Override\n-\t\tpublic MatrixBlock call(Tuple2<MatrixIndexes, MatrixBlock> arg0)\n-\t\t\tthrows Exception \n-\t\t{\n-\t\t\t//get all inputs\n+\t\tpublic MatrixBlock call(Tuple2<MatrixIndexes, MatrixBlock> arg0) throws Exception {\n+\t\t\t// get all inputs\n \t\t\tMatrixIndexes ix = arg0._1();\n \t\t\tMatrixBlock target = arg0._2();\n-\t\t\tMatrixBlock groups = _pbm.getBlock((int)ix.getRowIndex(), 1);\n-\t\t\t\n-\t\t\t//execute map grouped aggregate operations\n+\t\t\tMatrixBlock groups = _pbm.getBlock((int) ix.getRowIndex(), 1);\n+\n+\t\t\t// execute map grouped aggregate operations\n \t\t\treturn groups.groupedAggOperations(target, null, new MatrixBlock(), _ngroups, _op);\n \t\t}\n \t}\n \n-\tpublic static class CreateMatrixCell implements Function<WeightedCell, MatrixCell> \n-\t{\n+\tpublic static class CreateMatrixCell implements Function<WeightedCell, MatrixCell> {\n \t\tprivate static final long serialVersionUID = -5783727852453040737L;\n-\t\t\n-\t\tint blen; Operator op;\n+\n+\t\tint blen;\n+\t\tOperator op;\n+\n \t\tpublic CreateMatrixCell(int blen, Operator op) {\n \t\t\tthis.blen = blen;\n \t\t\tthis.op = op;\n \t\t}\n \n \t\t@Override\n-\t\tpublic MatrixCell call(WeightedCell kv) \n-\t\t\tthrows Exception \n-\t\t{\n+\t\tpublic MatrixCell call(WeightedCell kv) throws Exception {\n \t\t\tdouble val = -1;\n-\t\t\tif(op instanceof CMOperator)\n-\t\t\t{\n-\t\t\t\tAggregateOperationTypes agg=((CMOperator)op).aggOpType;\n-\t\t\t\tswitch(agg)\n-\t\t\t\t{\n-\t\t\t\tcase COUNT:\n-\t\t\t\t\tval = kv.getWeight();\n-\t\t\t\t\tbreak;\n-\t\t\t\tcase MEAN:\n-\t\t\t\t\tval = kv.getValue();\n-\t\t\t\t\tbreak;\n-\t\t\t\tcase CM2:\n-\t\t\t\t\tval = kv.getValue()/ kv.getWeight();\n-\t\t\t\t\tbreak;\n-\t\t\t\tcase CM3:\n-\t\t\t\t\tval = kv.getValue()/ kv.getWeight();\n-\t\t\t\t\tbreak;\n-\t\t\t\tcase CM4:\n-\t\t\t\t\tval = kv.getValue()/ kv.getWeight();\n-\t\t\t\t\tbreak;\n-\t\t\t\tcase VARIANCE:\n-\t\t\t\t\tval = kv.getValue()/kv.getWeight();\n-\t\t\t\t\tbreak;\n-\t\t\t\tdefault:\n-\t\t\t\t\tthrow new DMLRuntimeException(\"Invalid aggreagte in CM_CV_Object: \" + agg);\n+\t\t\tif(op instanceof CMOperator) {\n+\t\t\t\tAggregateOperationTypes agg = ((CMOperator) op).aggOpType;\n+\t\t\t\tswitch(agg) {\n+\t\t\t\t\tcase COUNT:\n+\t\t\t\t\t\tval = kv.getWeight();\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\tcase MEAN:\n+\t\t\t\t\t\tval = kv.getValue();\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\tcase CM2:\n+\t\t\t\t\t\tval = kv.getValue() / kv.getWeight();\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\tcase CM3:\n+\t\t\t\t\t\tval = kv.getValue() / kv.getWeight();\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\tcase CM4:\n+\t\t\t\t\t\tval = kv.getValue() / kv.getWeight();\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\tcase VARIANCE:\n+\t\t\t\t\t\tval = kv.getValue() / kv.getWeight();\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\tdefault:\n+\t\t\t\t\t\tthrow new DMLRuntimeException(\"Invalid aggreagte in CM_CV_Object: \" + agg);\n \t\t\t\t}\n \t\t\t}\n-\t\t\telse\n-\t\t\t{\n-\t\t\t\t//avoid division by 0\n-\t\t\t\tval = kv.getValue()/kv.getWeight();\n+\t\t\telse {\n+\t\t\t\t// avoid division by 0\n+\t\t\t\tval = kv.getValue() / kv.getWeight();\n \t\t\t}\n-\t\t\t\n+\n \t\t\treturn new MatrixCell(val);\n \t\t}\n \t}\n \n-\tpublic static class RDDTokenizeFunction implements PairFunction<Tuple2<Long, FrameBlock>, Long, FrameBlock>\n-\t{\n+\tpublic static class RDDTokenizeFunction implements PairFunction<Tuple2<Long, FrameBlock>, Long, FrameBlock> {\n \t\tprivate static final long serialVersionUID = -8788298032616522019L;\n \n \t\tprivate Tokenizer _tokenizer = null;\n@@ -829,9 +806,7 @@ public RDDTokenizeFunction(Tokenizer tokenizer, int blen) {\n \t\t}\n \n \t\t@Override\n-\t\tpublic Tuple2<Long,FrameBlock> call(Tuple2<Long, FrameBlock> in)\n-\t\t\t\tthrows Exception\n-\t\t{\n+\t\tpublic Tuple2<Long, FrameBlock> call(Tuple2<Long, FrameBlock> in) throws Exception {\n \t\t\tlong key = in._1();\n \t\t\tFrameBlock blk = in._2();\n \n@@ -840,146 +815,135 @@ public RDDTokenizeFunction(Tokenizer tokenizer, int blen) {\n \t\t}\n \t}\n \n-\tpublic static class RDDTransformApplyFunction implements PairFunction<Tuple2<Long,FrameBlock>,Long,FrameBlock> \n-\t{\n+\tpublic static class RDDTransformApplyFunction implements PairFunction<Tuple2<Long, FrameBlock>, Long, FrameBlock> {\n \t\tprivate static final long serialVersionUID = 5759813006068230916L;\n-\t\t\n-\t\tprivate Broadcast<Encoder> _bencoder = null;\n+\n+\t\tprivate Broadcast<MultiColumnEncoder> _bencoder = null;\n \t\tprivate Broadcast<TfOffsetMap> _omap = null;\n-\t\t\n-\t\tpublic RDDTransformApplyFunction(Broadcast<Encoder> bencoder, Broadcast<TfOffsetMap> omap) {\n+\n+\t\tpublic RDDTransformApplyFunction(Broadcast<MultiColumnEncoder> bencoder, Broadcast<TfOffsetMap> omap) {\n \t\t\t_bencoder = bencoder;\n \t\t\t_omap = omap;\n \t\t}\n \n \t\t@Override\n-\t\tpublic Tuple2<Long,FrameBlock> call(Tuple2<Long, FrameBlock> in) \n-\t\t\tthrows Exception \n-\t\t{\n+\t\tpublic Tuple2<Long, FrameBlock> call(Tuple2<Long, FrameBlock> in) throws Exception {\n \t\t\tlong key = in._1();\n \t\t\tFrameBlock blk = in._2();\n-\t\t\t\n-\t\t\t//execute block transform apply\n-\t\t\tEncoder encoder = _bencoder.getValue();\n-\t\t\tMatrixBlock tmp = encoder.apply(blk, new MatrixBlock(blk.getNumRows(), blk.getNumColumns(), false));\n-\t\t\t\n-\t\t\t//remap keys\n-\t\t\tif( _omap != null ) {\n+\n+\t\t\t// execute block transform apply\n+\t\t\tMultiColumnEncoder encoder = _bencoder.getValue();\n+\t\t\tMatrixBlock tmp = encoder.apply(blk);\n+\n+\t\t\t// remap keys\n+\t\t\tif(_omap != null) {\n \t\t\t\tkey = _omap.getValue().getOffset(key);\n \t\t\t}\n-\t\t\t\n-\t\t\t//convert to frameblock to reuse frame-matrix reblock\n-\t\t\treturn new Tuple2<>(key, \n-\t\t\t\t\tDataConverter.convertToFrameBlock(tmp));\n+\n+\t\t\t// convert to frameblock to reuse frame-matrix reblock\n+\t\t\treturn new Tuple2<>(key, DataConverter.convertToFrameBlock(tmp));\n \t\t}\n \t}\n \n-\tpublic static class RDDTransformApplyOffsetFunction implements PairFunction<Tuple2<Long,FrameBlock>,Long,Long> \n-\t{\n+\tpublic static class RDDTransformApplyOffsetFunction implements PairFunction<Tuple2<Long, FrameBlock>, Long, Long> {\n \t\tprivate static final long serialVersionUID = 3450977356721057440L;\n-\t\t\n+\n \t\tprivate int[] _omitColList = null;\n-\t\t\n+\n \t\tpublic RDDTransformApplyOffsetFunction(String spec, String[] colnames) {\n \t\t\ttry {\n-\t\t\t\t_omitColList = TfMetaUtils.parseJsonIDList(\n-\t\t\t\t\tspec, colnames, TfMethod.OMIT.toString());\n-\t\t\t} \n-\t\t\tcatch (DMLRuntimeException e) {\n+\t\t\t\t_omitColList = TfMetaUtils.parseJsonIDList(spec, colnames, TfMethod.OMIT.toString());\n+\t\t\t}\n+\t\t\tcatch(DMLRuntimeException e) {\n \t\t\t\tthrow new RuntimeException(e);\n \t\t\t}\n \t\t}\n \n \t\t@Override\n-\t\tpublic Tuple2<Long,Long> call(Tuple2<Long, FrameBlock> in) \n-\t\t\tthrows Exception \n-\t\t{\n+\t\tpublic Tuple2<Long, Long> call(Tuple2<Long, FrameBlock> in) throws Exception {\n \t\t\tlong key = in._1();\n \t\t\tlong rmRows = 0;\n-\t\t\t\n+\n \t\t\tFrameBlock blk = in._2();\n-\t\t\t\n-\t\t\tfor( int i=0; i<blk.getNumRows(); i++ ) {\n+\n+\t\t\tfor(int i = 0; i < blk.getNumRows(); i++) {\n \t\t\t\tboolean valid = true;\n-\t\t\t\tfor( int j=0; j<_omitColList.length; j++ ) {\n+\t\t\t\tfor(int j = 0; j < _omitColList.length; j++) {\n \t\t\t\t\tint colID = _omitColList[j];\n-\t\t\t\t\tObject val = blk.get(i, colID-1);\n-\t\t\t\t\tvalid &= !(val==null || (blk.getSchema()[colID-1]==\n-\t\t\t\t\t\t\tValueType.STRING &&  val.toString().isEmpty())); \n+\t\t\t\t\tObject val = blk.get(i, colID - 1);\n+\t\t\t\t\tvalid &= !(val == null ||\n+\t\t\t\t\t\t(blk.getSchema()[colID - 1] == ValueType.STRING && val.toString().isEmpty()));\n \t\t\t\t}\n \t\t\t\trmRows += valid ? 0 : 1;\n \t\t\t}\n-\t\t\t\n+\n \t\t\treturn new Tuple2<>(key, rmRows);\n \t\t}\n \t}\n \n-\tpublic static class RDDTransformDecodeFunction implements PairFunction<Tuple2<MatrixIndexes,MatrixBlock>,Long,FrameBlock> \n-\t{\n+\tpublic static class RDDTransformDecodeFunction\n+\t\timplements PairFunction<Tuple2<MatrixIndexes, MatrixBlock>, Long, FrameBlock> {\n \t\tprivate static final long serialVersionUID = -4797324742568170756L;\n-\t\t\n+\n \t\tprivate Decoder _decoder = null;\n \t\tprivate int _blen = -1;\n-\t\t\n+\n \t\tpublic RDDTransformDecodeFunction(Decoder decoder, int blen) {\n \t\t\t_decoder = decoder;\n \t\t\t_blen = blen;\n \t\t}\n \n \t\t@Override\n-\t\tpublic Tuple2<Long,FrameBlock> call(Tuple2<MatrixIndexes, MatrixBlock> in) \n-\t\t\tthrows Exception \n-\t\t{\n+\t\tpublic Tuple2<Long, FrameBlock> call(Tuple2<MatrixIndexes, MatrixBlock> in) throws Exception {\n \t\t\tlong rix = UtilFunctions.computeCellIndex(in._1().getRowIndex(), _blen, 0);\n \t\t\tFrameBlock fbout = _decoder.decode(in._2(), new FrameBlock(_decoder.getSchema()));\n \t\t\tfbout.setColumnNames(Arrays.copyOfRange(_decoder.getColnames(), 0, fbout.getNumColumns()));\n \t\t\treturn new Tuple2<>(rix, fbout);\n \t\t}\n \t}\n-\t\n-\tpublic static class RDDTransformDecodeExpandFunction implements PairFunction<Tuple2<MatrixIndexes,MatrixBlock>,MatrixIndexes,MatrixBlock> \n-\t{\n+\n+\tpublic static class RDDTransformDecodeExpandFunction\n+\t\timplements PairFunction<Tuple2<MatrixIndexes, MatrixBlock>, MatrixIndexes, MatrixBlock> {\n \t\tprivate static final long serialVersionUID = -8187400248076127598L;\n-\t\t\n+\n \t\tprivate int _clen = -1;\n \t\tprivate int _blen = -1;\n-\t\t\n+\n \t\tpublic RDDTransformDecodeExpandFunction(int clen, int blen) {\n \t\t\t_clen = clen;\n \t\t\t_blen = blen;\n \t\t}\n \n \t\t@Override\n-\t\tpublic Tuple2<MatrixIndexes,MatrixBlock> call(Tuple2<MatrixIndexes, MatrixBlock> in) \n-\t\t\tthrows Exception \n-\t\t{\n+\t\tpublic Tuple2<MatrixIndexes, MatrixBlock> call(Tuple2<MatrixIndexes, MatrixBlock> in) throws Exception {\n \t\t\tMatrixIndexes inIx = in._1();\n \t\t\tMatrixBlock inBlk = in._2();\n-\t\t\t\n-\t\t\t//construct expanded block via leftindexing\n-\t\t\tint cl = (int)UtilFunctions.computeCellIndex(inIx.getColumnIndex(), _blen, 0)-1;\n-\t\t\tint cu = (int)UtilFunctions.computeCellIndex(inIx.getColumnIndex(), _blen, inBlk.getNumColumns()-1)-1;\n+\n+\t\t\t// construct expanded block via leftindexing\n+\t\t\tint cl = (int) UtilFunctions.computeCellIndex(inIx.getColumnIndex(), _blen, 0) - 1;\n+\t\t\tint cu = (int) UtilFunctions.computeCellIndex(inIx.getColumnIndex(), _blen, inBlk.getNumColumns() - 1) - 1;\n \t\t\tMatrixBlock out = new MatrixBlock(inBlk.getNumRows(), _clen, false);\n-\t\t\tout = out.leftIndexingOperations(inBlk, 0, inBlk.getNumRows()-1, cl, cu, null, UpdateType.INPLACE_PINNED);\n-\t\t\t\n+\t\t\tout = out.leftIndexingOperations(inBlk, 0, inBlk.getNumRows() - 1, cl, cu, null, UpdateType.INPLACE_PINNED);\n+\n \t\t\treturn new Tuple2<>(new MatrixIndexes(inIx.getRowIndex(), 1), out);\n \t\t}\n \t}\n \n-\tpublic void setOutputCharacteristicsForGroupedAgg(DataCharacteristics mc1, DataCharacteristics mcOut, JavaPairRDD<MatrixIndexes, MatrixCell> out) {\n+\tpublic void setOutputCharacteristicsForGroupedAgg(DataCharacteristics mc1, DataCharacteristics mcOut,\n+\t\tJavaPairRDD<MatrixIndexes, MatrixCell> out) {\n \t\tif(!mcOut.dimsKnown()) {\n \t\t\tif(!mc1.dimsKnown()) {\n \t\t\t\tthrow new DMLRuntimeException(\"The output dimensions are not specified for grouped aggregate\");\n \t\t\t}\n-\t\t\t\n-\t\t\tif ( params.get(Statement.GAGG_NUM_GROUPS) != null) {\n+\n+\t\t\tif(params.get(Statement.GAGG_NUM_GROUPS) != null) {\n \t\t\t\tint ngroups = (int) Double.parseDouble(params.get(Statement.GAGG_NUM_GROUPS));\n-\t\t\t\tmcOut.set(ngroups, mc1.getCols(), -1, -1); //grouped aggregate with cell output\n+\t\t\t\tmcOut.set(ngroups, mc1.getCols(), -1, -1); // grouped aggregate with cell output\n \t\t\t}\n \t\t\telse {\n \t\t\t\tout = SparkUtils.cacheBinaryCellRDD(out);\n \t\t\t\tmcOut.set(SparkUtils.computeDataCharacteristics(out));\n-\t\t\t\tmcOut.setBlocksize(-1); //grouped aggregate with cell output\n+\t\t\t\tmcOut.setBlocksize(-1); // grouped aggregate with cell output\n \t\t\t}\n \t\t}\n \t}"
  },
  {
    "sha": "acce6ca44f04ce7bc870234864fd40472df03f0b",
    "filename": "src/main/java/org/apache/sysds/runtime/matrix/data/FrameBlock.java",
    "status": "modified",
    "additions": 18,
    "deletions": 4,
    "changes": 22,
    "blob_url": "https://github.com/apache/systemds/blob/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/matrix/data/FrameBlock.java",
    "raw_url": "https://github.com/apache/systemds/raw/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/matrix/data/FrameBlock.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/matrix/data/FrameBlock.java?ref=cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd",
    "patch": "@@ -52,17 +52,20 @@\n import org.apache.sysds.runtime.controlprogram.caching.CacheBlock;\n import org.apache.sysds.runtime.controlprogram.parfor.util.IDSequence;\n import org.apache.sysds.runtime.functionobjects.ValueComparisonFunction;\n-import org.apache.sysds.runtime.instructions.cp.*;\n+import org.apache.sysds.runtime.instructions.cp.BooleanObject;\n+import org.apache.sysds.runtime.instructions.cp.DoubleObject;\n+import org.apache.sysds.runtime.instructions.cp.IntObject;\n+import org.apache.sysds.runtime.instructions.cp.ScalarObject;\n import org.apache.sysds.runtime.io.IOUtilFunctions;\n import org.apache.sysds.runtime.matrix.operators.BinaryOperator;\n import org.apache.sysds.runtime.meta.DataCharacteristics;\n import org.apache.sysds.runtime.meta.MatrixCharacteristics;\n-import org.apache.sysds.runtime.transform.encode.EncoderRecode;\n+import org.apache.sysds.runtime.transform.encode.ColumnEncoderRecode;\n import org.apache.sysds.runtime.util.CommonThreadPool;\n import org.apache.sysds.runtime.util.DMVUtils;\n+import org.apache.sysds.runtime.util.EMAUtils;\n import org.apache.sysds.runtime.util.IndexRange;\n import org.apache.sysds.runtime.util.UtilFunctions;\n-import org.apache.sysds.runtime.util.EMAUtils;\n \n @SuppressWarnings({\"rawtypes\",\"unchecked\"}) //allow generic native arrays\n public class FrameBlock implements CacheBlock, Externalizable  {\n@@ -611,6 +614,17 @@ public void setColumn(int c, Array column) {\n \t\treturn new StringRowIterator(0, _numRows, cols);\n \t}\n \n+\t/**\n+\t * Get a row iterator over the frame where all selected fields are encoded as strings independent of their value\n+\t * types.\n+\t *\n+\t * @param colID column selection, 1-based\n+\t * @return string array iterator\n+\t */\n+\tpublic Iterator<String[]> getStringRowIterator(int colID) {\n+\t\treturn new StringRowIterator(0, _numRows, new int[] {colID});\n+\t}\n+\n \t/**\n \t * Get a row iterator over the frame where all fields are encoded\n \t * as strings independent of their value types.\n@@ -1237,7 +1251,7 @@ public void copy(int rl, int ru, int cl, int cu, FrameBlock src)\n \t\tfor( int i=0; i<getNumRows(); i++ ) {\n \t\t\tObject val = ldata.get(i);\n \t\t\tif( val != null ) {\n-\t\t\t\tString[] tmp = EncoderRecode.splitRecodeMapEntry(val.toString());\n+\t\t\t\tString[] tmp = ColumnEncoderRecode.splitRecodeMapEntry(val.toString());\n \t\t\t\tmap.put(tmp[0], Long.parseLong(tmp[1]));\n \t\t\t}\n \t\t}"
  },
  {
    "sha": "2fbab366bc1b2641ce75260c41892c948b9fb0bd",
    "filename": "src/main/java/org/apache/sysds/runtime/transform/decode/DecoderRecode.java",
    "status": "modified",
    "additions": 2,
    "deletions": 2,
    "changes": 4,
    "blob_url": "https://github.com/apache/systemds/blob/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/decode/DecoderRecode.java",
    "raw_url": "https://github.com/apache/systemds/raw/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/decode/DecoderRecode.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/transform/decode/DecoderRecode.java?ref=cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd",
    "patch": "@@ -33,7 +33,7 @@\n import org.apache.sysds.runtime.matrix.data.MatrixBlock;\n import org.apache.sysds.runtime.matrix.data.Pair;\n import org.apache.sysds.runtime.transform.TfUtils;\n-import org.apache.sysds.runtime.transform.encode.EncoderRecode;\n+import org.apache.sysds.runtime.transform.encode.ColumnEncoderRecode;\n import org.apache.sysds.runtime.util.UtilFunctions;\n \n /**\n@@ -117,7 +117,7 @@ public void initMetaData(FrameBlock meta) {\n \t\t\tfor( int i=0; i<meta.getNumRows(); i++ ) {\n \t\t\t\tif( meta.get(i, _colList[j]-1)==null )\n \t\t\t\t\tbreak; //reached end of recode map\n-\t\t\t\tString[] tmp = EncoderRecode.splitRecodeMapEntry(meta.get(i, _colList[j]-1).toString());\n+\t\t\t\tString[] tmp = ColumnEncoderRecode.splitRecodeMapEntry(meta.get(i, _colList[j]-1).toString());\n \t\t\t\tObject obj = UtilFunctions.stringToObject(_schema[_colList[j]-1], tmp[0]);\n \t\t\t\tmap.put(Long.parseLong(tmp[1]), obj);\n \t\t\t}"
  },
  {
    "sha": "943e0a720310d52de4385c07e37d7e2622adc74c",
    "filename": "src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoder.java",
    "status": "added",
    "additions": 162,
    "deletions": 0,
    "changes": 162,
    "blob_url": "https://github.com/apache/systemds/blob/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoder.java",
    "raw_url": "https://github.com/apache/systemds/raw/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoder.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoder.java?ref=cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd",
    "patch": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.sysds.runtime.transform.encode;\n+\n+import static org.apache.sysds.runtime.transform.encode.EncoderFactory.getEncoderType;\n+\n+import java.io.Externalizable;\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.sysds.runtime.DMLRuntimeException;\n+import org.apache.sysds.runtime.matrix.data.FrameBlock;\n+import org.apache.sysds.runtime.matrix.data.MatrixBlock;\n+\n+/**\n+ * Base class for all transform encoders providing both a row and block interface for decoding frames to matrices.\n+ *\n+ */\n+public abstract class ColumnEncoder implements Externalizable, Encoder, Comparable<ColumnEncoder> {\n+\tprotected static final Log LOG = LogFactory.getLog(ColumnEncoder.class.getName());\n+\tprivate static final long serialVersionUID = 2299156350718979064L;\n+\tprotected int _colID;\n+\n+\tprotected ColumnEncoder(int colID) {\n+\t\t_colID = colID;\n+\t}\n+\n+\tpublic abstract MatrixBlock apply(MatrixBlock in, MatrixBlock out, int outputCol);\n+\n+\t/**\n+\t * Indicates if this encoder is applicable, i.e, if there is a column to encode.\n+\t *\n+\t * @return true if a colID is set\n+\t */\n+\tpublic boolean isApplicable() {\n+\t\treturn _colID != -1;\n+\t}\n+\n+\t/**\n+\t * Indicates if this encoder is applicable for the given column ID, i.e., if it is subject to this transformation.\n+\t *\n+\t * @param colID column ID\n+\t * @return true if encoder is applicable for given column\n+\t */\n+\tpublic boolean isApplicable(int colID) {\n+\t\treturn colID == _colID;\n+\t}\n+\n+\t/**\n+\t * Allocates internal data structures for partial build.\n+\t */\n+\tpublic void prepareBuildPartial() {\n+\t\t// do nothing\n+\t}\n+\n+\t/**\n+\t * Partial build of internal data structures (e.g., in distributed spark operations).\n+\t *\n+\t * @param in input frame block\n+\t */\n+\tpublic void buildPartial(FrameBlock in) {\n+\t\t// do nothing\n+\t}\n+\n+\t/**\n+\t * Merges another encoder, of a compatible type, in after a certain position. Resizes as necessary.\n+\t * <code>ColumnEncoders</code> are compatible with themselves and <code>EncoderComposite</code> is compatible with\n+\t * every other <code>ColumnEncoders</code>. <code>MultiColumnEncoders</code> are compatible with every encoder\n+\t *\n+\t * @param other the encoder that should be merged in\n+\t */\n+\tpublic void mergeAt(ColumnEncoder other) {\n+\t\tthrow new DMLRuntimeException(\n+\t\t\tthis.getClass().getSimpleName() + \" does not support merging with \" + other.getClass().getSimpleName());\n+\t}\n+\n+\t/**\n+\t * Update index-ranges to after encoding. Note that only Dummycoding changes the ranges.\n+\t *\n+\t * @param beginDims begin dimensions of range\n+\t * @param endDims   end dimensions of range\n+\t */\n+\tpublic void updateIndexRanges(long[] beginDims, long[] endDims, int colOffset) {\n+\t\t// do nothing - default\n+\t}\n+\n+\t/**\n+\t * Obtain the column mapping of encoded frames based on the passed meta data frame.\n+\t *\n+\t * @param meta meta data frame block\n+\t * @return matrix with column mapping (one row per attribute)\n+\t */\n+\tpublic MatrixBlock getColMapping(FrameBlock meta) {\n+\t\t// default: do nothing\n+\t\treturn null;\n+\t}\n+\n+\t/**\n+\t * Redirects the default java serialization via externalizable to our default hadoop writable serialization for\n+\t * efficient broadcast/rdd serialization.\n+\t *\n+\t * @param os object output\n+\t * @throws IOException if IOException occurs\n+\t */\n+\t@Override\n+\tpublic void writeExternal(ObjectOutput os) throws IOException {\n+\t\tos.writeInt(_colID);\n+\t}\n+\n+\t/**\n+\t * Redirects the default java serialization via externalizable to our default hadoop writable serialization for\n+\t * efficient broadcast/rdd deserialization.\n+\t *\n+\t * @param in object input\n+\t * @throws IOException if IOException occur\n+\t */\n+\t@Override\n+\tpublic void readExternal(ObjectInput in) throws IOException {\n+\t\t_colID = in.readInt();\n+\t}\n+\n+\tpublic int getColID() {\n+\t\treturn _colID;\n+\t}\n+\n+\tpublic void setColID(int colID) {\n+\t\t_colID = colID;\n+\t}\n+\n+\tpublic void shiftCol(int columnOffset) {\n+\t\t_colID += columnOffset;\n+\t}\n+\n+\t@Override\n+\tpublic int compareTo(ColumnEncoder o) {\n+\t\treturn Integer.compare(getEncoderType(this), getEncoderType(o));\n+\t}\n+\n+\tpublic enum EncoderType {\n+\t\tRecode, FeatureHash, PassThrough, Bin, Dummycode, Omit, MVImpute, Composite\n+\t}\n+}"
  },
  {
    "sha": "45dbfdecb8952ef44fe7f498e7b304debabe209c",
    "filename": "src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoderBin.java",
    "status": "added",
    "additions": 239,
    "deletions": 0,
    "changes": 239,
    "blob_url": "https://github.com/apache/systemds/blob/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoderBin.java",
    "raw_url": "https://github.com/apache/systemds/raw/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoderBin.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoderBin.java?ref=cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd",
    "patch": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.sysds.runtime.transform.encode;\n+\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.util.Arrays;\n+\n+import org.apache.commons.lang3.tuple.MutableTriple;\n+import org.apache.sysds.lops.Lop;\n+import org.apache.sysds.runtime.matrix.data.FrameBlock;\n+import org.apache.sysds.runtime.matrix.data.MatrixBlock;\n+import org.apache.sysds.runtime.util.UtilFunctions;\n+\n+public class ColumnEncoderBin extends ColumnEncoder {\n+\tpublic static final String MIN_PREFIX = \"min\";\n+\tpublic static final String MAX_PREFIX = \"max\";\n+\tpublic static final String NBINS_PREFIX = \"nbins\";\n+\tprivate static final long serialVersionUID = 1917445005206076078L;\n+\tprotected int _numBin = -1;\n+\n+\t// frame transform-apply attributes\n+\t// a) column bin boundaries\n+\t// TODO binMins is redundant and could be removed - necessary for correct fed results\n+\tprivate double[] _binMins = null;\n+\tprivate double[] _binMaxs = null;\n+\t// b) column min/max (for partial build)\n+\tprivate double _colMins = -1f;\n+\tprivate double _colMaxs = -1f;\n+\n+\tpublic ColumnEncoderBin() {\n+\t\tsuper(-1);\n+\t}\n+\n+\tpublic ColumnEncoderBin(int colID, int numBin) {\n+\t\tsuper(colID);\n+\t\t_numBin = numBin;\n+\t}\n+\n+\tpublic ColumnEncoderBin(int colID, int numBin, double[] binMins, double[] binMaxs) {\n+\t\tsuper(colID);\n+\t\t_numBin = numBin;\n+\t\t_binMins = binMins;\n+\t\t_binMaxs = binMaxs;\n+\t}\n+\n+\tpublic double getColMins() {\n+\t\treturn _colMins;\n+\t}\n+\n+\tpublic double getColMaxs() {\n+\t\treturn _colMaxs;\n+\t}\n+\n+\tpublic double[] getBinMins() {\n+\t\treturn _binMins;\n+\t}\n+\n+\tpublic double[] getBinMaxs() {\n+\t\treturn _binMaxs;\n+\t}\n+\n+\t@Override\n+\tpublic void build(FrameBlock in) {\n+\t\tif(!isApplicable())\n+\t\t\treturn;\n+\n+\t\t// derive bin boundaries from min/max per column\n+\t\tdouble min = Double.POSITIVE_INFINITY;\n+\t\tdouble max = Double.NEGATIVE_INFINITY;\n+\t\tfor(int i = 0; i < in.getNumRows(); i++) {\n+\t\t\tdouble inVal = UtilFunctions.objectToDouble(in.getSchema()[_colID - 1], in.get(i, _colID - 1));\n+\t\t\tmin = Math.min(min, inVal);\n+\t\t\tmax = Math.max(max, inVal);\n+\t\t}\n+\t\tcomputeBins(min, max);\n+\t}\n+\n+\tpublic void computeBins(double min, double max) {\n+\t\t// ensure allocated internal transformation metadata\n+\t\tif(_binMins == null || _binMaxs == null) {\n+\t\t\t_binMins = new double[_numBin];\n+\t\t\t_binMaxs = new double[_numBin];\n+\t\t}\n+\t\tfor(int i = 0; i < _numBin; i++) {\n+\t\t\t_binMins[i] = min + i * (max - min) / _numBin;\n+\t\t\t_binMaxs[i] = min + (i + 1) * (max - min) / _numBin;\n+\t\t}\n+\t}\n+\n+\tpublic void prepareBuildPartial() {\n+\t\t// ensure allocated min/max arrays\n+\t\t_colMins = -1f;\n+\t\t_colMaxs = -1f;\n+\t}\n+\n+\tpublic void buildPartial(FrameBlock in) {\n+\t\tif(!isApplicable())\n+\t\t\treturn;\n+\t\t// derive bin boundaries from min/max per column\n+\t\tdouble min = Double.POSITIVE_INFINITY;\n+\t\tdouble max = Double.NEGATIVE_INFINITY;\n+\t\tfor(int i = 0; i < in.getNumRows(); i++) {\n+\t\t\tdouble inVal = UtilFunctions.objectToDouble(in.getSchema()[_colID - 1], in.get(i, _colID - 1));\n+\t\t\tmin = Math.min(min, inVal);\n+\t\t\tmax = Math.max(max, inVal);\n+\t\t}\n+\t\t_colMins = min;\n+\t\t_colMaxs = max;\n+\t}\n+\n+\t@Override\n+\tpublic MatrixBlock apply(FrameBlock in, MatrixBlock out, int outputCol) {\n+\t\tfor(int i = 0; i < in.getNumRows(); i++) {\n+\t\t\tdouble inVal = UtilFunctions.objectToDouble(in.getSchema()[_colID - 1], in.get(i, _colID - 1));\n+\t\t\tint ix = Arrays.binarySearch(_binMaxs, inVal);\n+\t\t\tint binID = ((ix < 0) ? Math.abs(ix + 1) : ix) + 1;\n+\t\t\tout.quickSetValue(i, outputCol, binID);\n+\t\t}\n+\t\treturn out;\n+\t}\n+\n+\t@Override\n+\tpublic MatrixBlock apply(MatrixBlock in, MatrixBlock out, int outputCol) {\n+\t\tfor(int i = 0; i < in.getNumRows(); i++) {\n+\t\t\tdouble inVal = in.quickGetValue(i, _colID - 1);\n+\t\t\tint ix = Arrays.binarySearch(_binMaxs, inVal);\n+\t\t\tint binID = ((ix < 0) ? Math.abs(ix + 1) : ix) + 1;\n+\t\t\tout.quickSetValue(i, outputCol, binID);\n+\t\t}\n+\t\treturn out;\n+\t}\n+\n+\t@Override\n+\tpublic void mergeAt(ColumnEncoder other) {\n+\t\tif(other instanceof ColumnEncoderBin) {\n+\t\t\tColumnEncoderBin otherBin = (ColumnEncoderBin) other;\n+\t\t\tassert other._colID == _colID;\n+\t\t\t// save the min, max as well as the number of bins for the column indexes\n+\t\t\tMutableTriple<Integer, Double, Double> entry = new MutableTriple<>(_numBin, _binMins[0],\n+\t\t\t\t_binMaxs[_binMaxs.length - 1]);\n+\t\t\t// num bins will match\n+\t\t\tentry.middle = Math.min(entry.middle, otherBin._binMins[0]);\n+\t\t\tentry.right = Math.max(entry.right, otherBin._binMaxs[otherBin._binMaxs.length - 1]);\n+\n+\t\t\t// use the saved values to fill the arrays again\n+\t\t\t_numBin = entry.left;\n+\t\t\t_binMins = new double[_numBin];\n+\t\t\t_binMaxs = new double[_numBin];\n+\n+\t\t\tdouble min = entry.middle;\n+\t\t\tdouble max = entry.right;\n+\t\t\tfor(int j = 0; j < _numBin; j++) {\n+\t\t\t\t_binMins[j] = min + j * (max - min) / _numBin;\n+\t\t\t\t_binMaxs[j] = min + (j + 1) * (max - min) / _numBin;\n+\t\t\t}\n+\t\t\treturn;\n+\t\t}\n+\t\tsuper.mergeAt(other);\n+\t}\n+\n+\t@Override\n+\tpublic FrameBlock getMetaData(FrameBlock meta) {\n+\t\t// allocate frame if necessary\n+\t\tmeta.ensureAllocatedColumns(_binMaxs.length);\n+\n+\t\t// serialize the internal state into frame meta data\n+\t\tmeta.getColumnMetadata(_colID - 1).setNumDistinct(_numBin);\n+\t\tfor(int i = 0; i < _binMaxs.length; i++) {\n+\t\t\tString sb = _binMins[i] +\n+\t\t\t\t\tLop.DATATYPE_PREFIX +\n+\t\t\t\t\t_binMaxs[i];\n+\t\t\tmeta.set(i, _colID - 1, sb);\n+\t\t}\n+\t\treturn meta;\n+\t}\n+\n+\t@Override\n+\tpublic void initMetaData(FrameBlock meta) {\n+\t\tif(meta == null || _binMaxs != null)\n+\t\t\treturn;\n+\t\t// deserialize the frame meta data into internal state\n+\t\tint nbins = (int) meta.getColumnMetadata()[_colID - 1].getNumDistinct();\n+\t\t_binMins = new double[nbins];\n+\t\t_binMaxs = new double[nbins];\n+\t\tfor(int i = 0; i < nbins; i++) {\n+\t\t\tString[] tmp = meta.get(i, _colID - 1).toString().split(Lop.DATATYPE_PREFIX);\n+\t\t\t_binMins[i] = Double.parseDouble(tmp[0]);\n+\t\t\t_binMaxs[i] = Double.parseDouble(tmp[1]);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void writeExternal(ObjectOutput out) throws IOException {\n+\t\tsuper.writeExternal(out);\n+\n+\t\tout.writeInt(_numBin);\n+\t\tout.writeBoolean(_binMaxs != null);\n+\t\tif(_binMaxs != null) {\n+\t\t\tfor(int j = 0; j < _binMaxs.length; j++) {\n+\t\t\t\tout.writeDouble(_binMaxs[j]);\n+\t\t\t\tout.writeDouble(_binMins[j]);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readExternal(ObjectInput in) throws IOException {\n+\t\tsuper.readExternal(in);\n+\t\t_numBin = in.readInt();\n+\t\tboolean minmax = in.readBoolean();\n+\t\t_binMaxs = minmax ? new double[_numBin] : null;\n+\t\t_binMins = minmax ? new double[_numBin] : null;\n+\t\tif(!minmax)\n+\t\t\treturn;\n+\t\tfor(int j = 0; j < _binMaxs.length; j++) {\n+\t\t\t_binMaxs[j] = in.readDouble();\n+\t\t\t_binMins[j] = in.readDouble();\n+\t\t}\n+\t}\n+}"
  },
  {
    "sha": "11aa15d4c7f54da9db5d2781723e911adeb21320",
    "filename": "src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoderComposite.java",
    "status": "added",
    "additions": 268,
    "deletions": 0,
    "changes": 268,
    "blob_url": "https://github.com/apache/systemds/blob/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoderComposite.java",
    "raw_url": "https://github.com/apache/systemds/raw/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoderComposite.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoderComposite.java?ref=cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd",
    "patch": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.sysds.runtime.transform.encode;\n+\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+\n+import org.apache.sysds.runtime.DMLRuntimeException;\n+import org.apache.sysds.runtime.matrix.data.FrameBlock;\n+import org.apache.sysds.runtime.matrix.data.MatrixBlock;\n+\n+/**\n+ * Simple composite encoder that applies a list of encoders in specified order. By implementing the default encoder API\n+ * it can be used as a drop-in replacement for any other encoder.\n+ * \n+ */\n+// TODO assert each type of encoder can only be present once\n+public class ColumnEncoderComposite extends ColumnEncoder {\n+\tprivate static final long serialVersionUID = -8473768154646831882L;\n+\n+\tprivate List<ColumnEncoder> _columnEncoders = null;\n+\tprivate FrameBlock _meta = null;\n+\n+\tpublic ColumnEncoderComposite() {\n+\t\tsuper(-1);\n+\t}\n+\n+\tpublic ColumnEncoderComposite(List<ColumnEncoder> columnEncoders, FrameBlock meta) {\n+\t\tsuper(-1);\n+\t\tif(!(columnEncoders.size() > 0 &&\n+\t\t\tcolumnEncoders.stream().allMatch((encoder -> encoder._colID == columnEncoders.get(0)._colID))))\n+\t\t\tthrow new DMLRuntimeException(\"Tried to create Composite Encoder with no encoders or mismatching columIDs\");\n+\t\t_colID = columnEncoders.get(0)._colID;\n+\t\t_meta = meta;\n+\t\t_columnEncoders = columnEncoders;\n+\t}\n+\n+\tpublic ColumnEncoderComposite(List<ColumnEncoder> columnEncoders) {\n+\t\tthis(columnEncoders, null);\n+\t}\n+\n+\tpublic ColumnEncoderComposite(ColumnEncoder columnEncoder) {\n+\t\tsuper(columnEncoder._colID);\n+\t\t_columnEncoders = new ArrayList<>();\n+\t\t_columnEncoders.add(columnEncoder);\n+\t}\n+\n+\tpublic List<ColumnEncoder> getEncoders() {\n+\t\treturn _columnEncoders;\n+\t}\n+\n+\tpublic <T extends ColumnEncoder> T getEncoder(Class<T> type) {\n+\t\tfor(ColumnEncoder columnEncoder : _columnEncoders) {\n+\t\t\tif(columnEncoder.getClass().equals(type))\n+\t\t\t\treturn (T) columnEncoder;\n+\t\t}\n+\t\treturn null;\n+\t}\n+\n+\tpublic boolean isEncoder(int colID, Class<?> type) {\n+\t\tfor(ColumnEncoder columnEncoder : _columnEncoders) {\n+\t\t\tif(columnEncoder.getClass().equals(type) && columnEncoder._colID == colID)\n+\t\t\t\treturn true;\n+\t\t}\n+\t\treturn false;\n+\t}\n+\n+\t@Override\n+\tpublic void build(FrameBlock in) {\n+\t\tfor(ColumnEncoder columnEncoder : _columnEncoders)\n+\t\t\tcolumnEncoder.build(in);\n+\t}\n+\n+\t@Override\n+\tpublic void prepareBuildPartial() {\n+\t\tfor(ColumnEncoder columnEncoder : _columnEncoders)\n+\t\t\tcolumnEncoder.prepareBuildPartial();\n+\t}\n+\n+\t@Override\n+\tpublic void buildPartial(FrameBlock in) {\n+\t\tfor(ColumnEncoder columnEncoder : _columnEncoders)\n+\t\t\tcolumnEncoder.buildPartial(in);\n+\t}\n+\n+\t@Override\n+\tpublic MatrixBlock apply(FrameBlock in, MatrixBlock out, int outputCol) {\n+\t\ttry {\n+\t\t\tfor(int i = 0; i < _columnEncoders.size(); i++) {\n+\t\t\t\tif(i == 0) {\n+\t\t\t\t\t// 1. encoder writes data into MatrixBlock Column all others use this column for further encoding\n+\t\t\t\t\t_columnEncoders.get(i).apply(in, out, outputCol);\n+\t\t\t\t}\n+\t\t\t\telse {\n+\t\t\t\t\t_columnEncoders.get(i).apply(out, out, outputCol);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tcatch(Exception ex) {\n+\t\t\tLOG.error(\"Failed to transform-apply frame with \\n\" + this);\n+\t\t\tthrow ex;\n+\t\t}\n+\t\treturn out;\n+\t}\n+\n+\t@Override\n+\tpublic MatrixBlock apply(MatrixBlock in, MatrixBlock out, int outputCol) {\n+\t\ttry {\n+\t\t\tfor(int i = 0; i < _columnEncoders.size(); i++) {\n+\t\t\t\tif(i == 0) {\n+\t\t\t\t\t// 1. encoder writes data into MatrixBlock Column all others use this column for further encoding\n+\t\t\t\t\t_columnEncoders.get(i).apply(in, out, outputCol);\n+\t\t\t\t}\n+\t\t\t\telse {\n+\t\t\t\t\t_columnEncoders.get(i).apply(out, out, outputCol);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tcatch(Exception ex) {\n+\t\t\tLOG.error(\"Failed to transform-apply frame with \\n\" + this);\n+\t\t\tthrow ex;\n+\t\t}\n+\t\treturn in;\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif(this == o)\n+\t\t\treturn true;\n+\t\tif(o == null || getClass() != o.getClass())\n+\t\t\treturn false;\n+\t\tColumnEncoderComposite that = (ColumnEncoderComposite) o;\n+\t\treturn _columnEncoders.equals(that._columnEncoders) && Objects.equals(_meta, that._meta);\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\treturn Objects.hash(_columnEncoders, _meta);\n+\t}\n+\n+\t@Override\n+\tpublic void mergeAt(ColumnEncoder other) {\n+\t\tif(other instanceof ColumnEncoderComposite) {\n+\t\t\tColumnEncoderComposite otherComposite = (ColumnEncoderComposite) other;\n+\t\t\tassert otherComposite._colID == _colID;\n+\t\t\t// TODO maybe assert that the _encoders never have the same type of encoder twice or more\n+\t\t\tfor(ColumnEncoder otherEnc : otherComposite.getEncoders()) {\n+\t\t\t\taddEncoder(otherEnc);\n+\t\t\t}\n+\t\t}\n+\t\telse {\n+\t\t\taddEncoder(other);\n+\t\t}\n+\t\t// update dummycode encoder domain sizes based on distinctness information from other encoders\n+\t\tColumnEncoderDummycode dc = getEncoder(ColumnEncoderDummycode.class);\n+\t\tif(dc != null)\n+\t\t\tdc.updateDomainSizes(_columnEncoders);\n+\t}\n+\n+\tpublic void addEncoder(ColumnEncoder other) {\n+\t\tColumnEncoder encoder = getEncoder(other.getClass());\n+\t\tassert _colID == other._colID;\n+\t\tif(encoder != null)\n+\t\t\tencoder.mergeAt(other);\n+\t\telse {\n+\t\t\t_columnEncoders.add(other);\n+\t\t\t_columnEncoders.sort(null);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void updateIndexRanges(long[] beginDims, long[] endDims, int colOffset) {\n+\t\tfor(ColumnEncoder enc : _columnEncoders) {\n+\t\t\tenc.updateIndexRanges(beginDims, endDims, colOffset);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic FrameBlock getMetaData(FrameBlock out) {\n+\t\tif(_meta != null)\n+\t\t\treturn _meta;\n+\t\tfor(ColumnEncoder columnEncoder : _columnEncoders)\n+\t\t\tcolumnEncoder.getMetaData(out);\n+\t\treturn out;\n+\t}\n+\n+\t@Override\n+\tpublic void initMetaData(FrameBlock out) {\n+\t\tfor(ColumnEncoder columnEncoder : _columnEncoders)\n+\t\t\tcolumnEncoder.initMetaData(out);\n+\t}\n+\n+\t@Override\n+\tpublic String toString() {\n+\t\tStringBuilder sb = new StringBuilder();\n+\t\tsb.append(\"CompositeEncoder(\").append(_columnEncoders.size()).append(\"):\\n\");\n+\t\tfor(ColumnEncoder columnEncoder : _columnEncoders) {\n+\t\t\tsb.append(\"-- \");\n+\t\t\tsb.append(columnEncoder.getClass().getSimpleName());\n+\t\t\tsb.append(\": \");\n+\t\t\tsb.append(columnEncoder._colID);\n+\t\t\tsb.append(\"\\n\");\n+\t\t}\n+\t\treturn sb.toString();\n+\t}\n+\n+\t@Override\n+\tpublic void writeExternal(ObjectOutput out) throws IOException {\n+\t\tout.writeInt(_columnEncoders.size());\n+\t\tfor(ColumnEncoder columnEncoder : _columnEncoders) {\n+\t\t\tout.writeInt(columnEncoder._colID);\n+\t\t\tout.writeByte(EncoderFactory.getEncoderType(columnEncoder));\n+\t\t\tcolumnEncoder.writeExternal(out);\n+\t\t}\n+\t\tout.writeBoolean(_meta != null);\n+\t\tif(_meta != null)\n+\t\t\t_meta.write(out);\n+\t}\n+\n+\t@Override\n+\tpublic void readExternal(ObjectInput in) throws IOException {\n+\t\tint encodersSize = in.readInt();\n+\t\t_columnEncoders = new ArrayList<>();\n+\t\tfor(int i = 0; i < encodersSize; i++) {\n+\t\t\tint colID = in.readInt();\n+\t\t\tColumnEncoder columnEncoder = EncoderFactory.createInstance(in.readByte());\n+\t\t\tcolumnEncoder.readExternal(in);\n+\t\t\tcolumnEncoder.setColID(colID);\n+\t\t\t_columnEncoders.add(columnEncoder);\n+\t\t}\n+\t\tif(in.readBoolean()) {\n+\t\t\tFrameBlock meta = new FrameBlock();\n+\t\t\tmeta.readFields(in);\n+\t\t\t_meta = meta;\n+\t\t}\n+\t}\n+\n+\tpublic <T extends ColumnEncoder> boolean hasEncoder(Class<T> type) {\n+\t\treturn _columnEncoders.stream().anyMatch(encoder -> encoder.getClass().equals(type));\n+\t}\n+\n+\t@Override\n+\tpublic void shiftCol(int columnOffset) {\n+\t\tsuper.shiftCol(columnOffset);\n+\t\t_columnEncoders.forEach(e -> e.shiftCol(columnOffset));\n+\t}\n+}"
  },
  {
    "sha": "83fdf18f098058812de8537f33b9283b609f06af",
    "filename": "src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoderDummycode.java",
    "status": "added",
    "additions": 158,
    "deletions": 0,
    "changes": 158,
    "blob_url": "https://github.com/apache/systemds/blob/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoderDummycode.java",
    "raw_url": "https://github.com/apache/systemds/raw/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoderDummycode.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoderDummycode.java?ref=cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd",
    "patch": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.sysds.runtime.transform.encode;\n+\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.util.List;\n+import java.util.Objects;\n+\n+import org.apache.sysds.runtime.DMLRuntimeException;\n+import org.apache.sysds.runtime.matrix.data.FrameBlock;\n+import org.apache.sysds.runtime.matrix.data.MatrixBlock;\n+\n+public class ColumnEncoderDummycode extends ColumnEncoder {\n+\tprivate static final long serialVersionUID = 5832130477659116489L;\n+\n+\tpublic int _domainSize = -1; // length = #of dummycoded columns\n+\n+\tpublic ColumnEncoderDummycode() {\n+\t\tsuper(-1);\n+\t}\n+\n+\tpublic ColumnEncoderDummycode(int colID) {\n+\t\tsuper(colID);\n+\t}\n+\n+\tpublic ColumnEncoderDummycode(int colID, int domainSize) {\n+\t\tsuper(colID);\n+\t\t_domainSize = domainSize;\n+\t}\n+\n+\t@Override\n+\tpublic void build(FrameBlock in) {\n+\t\t// do nothing\n+\t}\n+\n+\t@Override\n+\tpublic MatrixBlock apply(FrameBlock in, MatrixBlock out, int outputCol) {\n+\t\tthrow new DMLRuntimeException(\"Called DummyCoder with FrameBlock\");\n+\t}\n+\n+\tpublic MatrixBlock apply(MatrixBlock in, MatrixBlock out, int outputCol) {\n+\t\t// Out Matrix should already be correct size!\n+\t\t// append dummy coded or unchanged values to output\n+\t\tfinal int clen = in.getNumColumns();\n+\t\tfor(int i = 0; i < in.getNumRows(); i++) {\n+\t\t\t// Using outputCol here as index since we have a MatrixBlock as input where dummycoding could have been\n+\t\t\t// applied in a previous encoder\n+\t\t\tdouble val = in.quickGetValue(i, outputCol);\n+\t\t\tint nCol = outputCol + (int) val - 1;\n+\t\t\tout.quickSetValue(i, nCol, 1);\n+\t\t\tif(nCol != outputCol)\n+\t\t\t\tout.quickSetValue(i, outputCol, 0);\n+\t\t}\n+\t\treturn out;\n+\t}\n+\n+\t@Override\n+\tpublic void mergeAt(ColumnEncoder other) {\n+\t\tif(other instanceof ColumnEncoderDummycode) {\n+\t\t\tassert other._colID == _colID;\n+\t\t\t// temporary, will be updated later\n+\t\t\t_domainSize = 0;\n+\t\t\treturn;\n+\t\t}\n+\t\tsuper.mergeAt(other);\n+\t}\n+\n+\t@Override\n+\tpublic void updateIndexRanges(long[] beginDims, long[] endDims, int colOffset) {\n+\n+\t\t// new columns inserted in this (federated) block\n+\t\tbeginDims[1] += colOffset;\n+\t\tendDims[1] += _domainSize - 1 + colOffset;\n+\t}\n+\n+\tpublic void updateDomainSizes(List<ColumnEncoder> columnEncoders) {\n+\t\tif(_colID == -1)\n+\t\t\treturn;\n+\t\tfor(ColumnEncoder columnEncoder : columnEncoders) {\n+\t\t\tint distinct = -1;\n+\t\t\tif(columnEncoder instanceof ColumnEncoderRecode) {\n+\t\t\t\tColumnEncoderRecode columnEncoderRecode = (ColumnEncoderRecode) columnEncoder;\n+\t\t\t\tdistinct = columnEncoderRecode.getNumDistinctValues();\n+\t\t\t}\n+\t\t\telse if(columnEncoder instanceof ColumnEncoderBin) {\n+\t\t\t\tdistinct = ((ColumnEncoderBin) columnEncoder)._numBin;\n+\t\t\t}\n+\n+\t\t\tif(distinct != -1) {\n+\t\t\t\t_domainSize = distinct;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic FrameBlock getMetaData(FrameBlock meta) {\n+\t\treturn meta;\n+\t}\n+\n+\t@Override\n+\tpublic void initMetaData(FrameBlock meta) {\n+\t\t// initialize domain sizes and output num columns\n+\t\t_domainSize = -1;\n+\t\t_domainSize = (int) meta.getColumnMetadata()[_colID - 1].getNumDistinct();\n+\t}\n+\n+\t@Override\n+\tpublic void writeExternal(ObjectOutput out) throws IOException {\n+\t\tsuper.writeExternal(out);\n+\t\tout.writeInt(_domainSize);\n+\t}\n+\n+\t@Override\n+\tpublic void readExternal(ObjectInput in) throws IOException {\n+\t\tsuper.readExternal(in);\n+\t\t_domainSize = in.readInt();\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif(this == o)\n+\t\t\treturn true;\n+\t\tif(o == null || getClass() != o.getClass())\n+\t\t\treturn false;\n+\t\tColumnEncoderDummycode that = (ColumnEncoderDummycode) o;\n+\t\treturn _colID == that._colID && (_domainSize == that._domainSize);\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\tint result = Objects.hash(_colID);\n+\t\tresult = 31 * result + Objects.hashCode(_domainSize);\n+\t\treturn result;\n+\t}\n+\n+\tpublic int getDomainSize() {\n+\t\treturn _domainSize;\n+\t}\n+}"
  },
  {
    "sha": "cd4b272b6b25afde404a0b0c434ff5cb6c0c47f7",
    "filename": "src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoderFeatureHash.java",
    "status": "added",
    "additions": 128,
    "deletions": 0,
    "changes": 128,
    "blob_url": "https://github.com/apache/systemds/blob/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoderFeatureHash.java",
    "raw_url": "https://github.com/apache/systemds/raw/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoderFeatureHash.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoderFeatureHash.java?ref=cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd",
    "patch": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.sysds.runtime.transform.encode;\n+\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+\n+import org.apache.sysds.runtime.DMLRuntimeException;\n+import org.apache.sysds.runtime.matrix.data.FrameBlock;\n+import org.apache.sysds.runtime.matrix.data.MatrixBlock;\n+import org.apache.sysds.runtime.util.UtilFunctions;\n+\n+/**\n+ * Class used for feature hashing transformation of frames.\n+ */\n+public class ColumnEncoderFeatureHash extends ColumnEncoder {\n+\tprivate static final long serialVersionUID = 7435806042138687342L;\n+\tprivate long _K;\n+\n+\t/*\n+\t * public EncoderFeatureHash(JSONObject parsedSpec, String[] colnames, int clen, int minCol, int maxCol) throws\n+\t * JSONException { super(null, clen); _colList = TfMetaUtils.parseJsonIDList(parsedSpec, colnames,\n+\t * TfMethod.HASH.toString(), minCol, maxCol); _K = getK(parsedSpec); }\n+\t * \n+\t */\n+\tpublic ColumnEncoderFeatureHash(int colID, long K) {\n+\t\tsuper(colID);\n+\t\t_K = K;\n+\t}\n+\n+\tpublic ColumnEncoderFeatureHash() {\n+\t\tsuper(-1);\n+\t\t_K = 0;\n+\t}\n+\n+\tprivate long getCode(String key) {\n+\t\treturn key.hashCode() % _K;\n+\t}\n+\n+\t@Override\n+\tpublic void build(FrameBlock in) {\n+\t\t// do nothing (no meta data other than K)\n+\t}\n+\n+\t@Override\n+\tpublic MatrixBlock apply(FrameBlock in, MatrixBlock out, int outputCol) {\n+\t\t// apply feature hashing column wise\n+\t\tfor(int i = 0; i < in.getNumRows(); i++) {\n+\t\t\tObject okey = in.get(i, _colID - 1);\n+\t\t\tString key = (okey != null) ? okey.toString() : null;\n+\t\t\tif(key == null)\n+\t\t\t\tthrow new DMLRuntimeException(\"Missing Value encountered in input Frame for FeatureHash\");\n+\t\t\tlong code = getCode(key);\n+\t\t\tout.quickSetValue(i, outputCol, (code >= 0) ? code : Double.NaN);\n+\t\t}\n+\t\treturn out;\n+\t}\n+\n+\t@Override\n+\tpublic MatrixBlock apply(MatrixBlock in, MatrixBlock out, int outputCol) {\n+\t\t// apply feature hashing column wise\n+\t\tfor(int i = 0; i < in.getNumRows(); i++) {\n+\t\t\tObject okey = in.quickGetValue(i, _colID - 1);\n+\t\t\tString key = okey.toString();\n+\t\t\tlong code = getCode(key);\n+\t\t\tout.quickSetValue(i, outputCol, (code >= 0) ? code : Double.NaN);\n+\t\t}\n+\t\treturn out;\n+\t}\n+\n+\t@Override\n+\tpublic void mergeAt(ColumnEncoder other) {\n+\t\tif(other instanceof ColumnEncoderFeatureHash) {\n+\t\t\tassert other._colID == _colID;\n+\t\t\tif(((ColumnEncoderFeatureHash) other)._K != 0 && _K == 0)\n+\t\t\t\t_K = ((ColumnEncoderFeatureHash) other)._K;\n+\t\t\treturn;\n+\t\t}\n+\t\tsuper.mergeAt(other);\n+\t}\n+\n+\t@Override\n+\tpublic FrameBlock getMetaData(FrameBlock meta) {\n+\t\tif(!isApplicable())\n+\t\t\treturn meta;\n+\n+\t\tmeta.ensureAllocatedColumns(1);\n+\t\tmeta.set(0, _colID - 1, String.valueOf(_K));\n+\t\treturn meta;\n+\t}\n+\n+\t@Override\n+\tpublic void initMetaData(FrameBlock meta) {\n+\t\tif(meta == null || meta.getNumRows() <= 0)\n+\t\t\treturn;\n+\t\t_K = UtilFunctions.parseToLong(meta.get(0, _colID - 1).toString());\n+\t}\n+\n+\t@Override\n+\tpublic void writeExternal(ObjectOutput out) throws IOException {\n+\t\tsuper.writeExternal(out);\n+\t\tout.writeLong(_K);\n+\t}\n+\n+\t@Override\n+\tpublic void readExternal(ObjectInput in) throws IOException {\n+\t\tsuper.readExternal(in);\n+\t\t_K = in.readLong();\n+\t}\n+}"
  },
  {
    "sha": "2f5ac7785a927f868901c2f73dfc74a1e14b1d8b",
    "filename": "src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoderPassThrough.java",
    "status": "added",
    "additions": 91,
    "deletions": 0,
    "changes": 91,
    "blob_url": "https://github.com/apache/systemds/blob/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoderPassThrough.java",
    "raw_url": "https://github.com/apache/systemds/raw/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoderPassThrough.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoderPassThrough.java?ref=cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd",
    "patch": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.sysds.runtime.transform.encode;\n+\n+import org.apache.sysds.common.Types.ValueType;\n+import org.apache.sysds.runtime.matrix.data.FrameBlock;\n+import org.apache.sysds.runtime.matrix.data.MatrixBlock;\n+import org.apache.sysds.runtime.util.UtilFunctions;\n+\n+/**\n+ * Simple composite encoder that applies a list of encoders in specified order. By implementing the default encoder API\n+ * it can be used as a drop-in replacement for any other encoder.\n+ * \n+ */\n+public class ColumnEncoderPassThrough extends ColumnEncoder {\n+\tprivate static final long serialVersionUID = -8473768154646831882L;\n+\n+\tprotected ColumnEncoderPassThrough(int ptCols) {\n+\t\tsuper(ptCols); // 1-based\n+\t}\n+\n+\tpublic ColumnEncoderPassThrough() {\n+\t\tthis(-1);\n+\t}\n+\n+\t@Override\n+\tpublic void build(FrameBlock in) {\n+\t\t// do nothing\n+\t}\n+\n+\t@Override\n+\tpublic MatrixBlock apply(FrameBlock in, MatrixBlock out, int outputCol) {\n+\t\tint col = _colID - 1; // 1-based\n+\t\tValueType vt = in.getSchema()[col];\n+\t\tfor(int i = 0; i < in.getNumRows(); i++) {\n+\t\t\tObject val = in.get(i, col);\n+\t\t\tout.quickSetValue(i,\n+\t\t\t\toutputCol,\n+\t\t\t\t(val == null || (vt == ValueType.STRING && val.toString().isEmpty())) ? Double.NaN : UtilFunctions\n+\t\t\t\t\t.objectToDouble(vt, val));\n+\t\t}\n+\t\treturn out;\n+\t}\n+\n+\t@Override\n+\tpublic MatrixBlock apply(MatrixBlock in, MatrixBlock out, int outputCol) {\n+\t\t// only transfer from in to out\n+\t\tint col = _colID - 1; // 1-based\n+\t\tfor(int i = 0; i < in.getNumRows(); i++) {\n+\t\t\tdouble val = in.quickGetValue(i, col);\n+\t\t\tout.quickSetValue(i, outputCol, val);\n+\t\t}\n+\t\treturn out;\n+\t}\n+\n+\t@Override\n+\tpublic void mergeAt(ColumnEncoder other) {\n+\t\tif(other instanceof ColumnEncoderPassThrough) {\n+\t\t\treturn;\n+\t\t}\n+\t\tsuper.mergeAt(other);\n+\t}\n+\n+\t@Override\n+\tpublic FrameBlock getMetaData(FrameBlock meta) {\n+\t\t// do nothing\n+\t\treturn meta;\n+\t}\n+\n+\t@Override\n+\tpublic void initMetaData(FrameBlock meta) {\n+\t\t// do nothing\n+\t}\n+}"
  },
  {
    "sha": "25c4550d84ad631d43a01cb3d6c4b33359453e56",
    "filename": "src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoderRecode.java",
    "status": "added",
    "additions": 274,
    "deletions": 0,
    "changes": 274,
    "blob_url": "https://github.com/apache/systemds/blob/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoderRecode.java",
    "raw_url": "https://github.com/apache/systemds/raw/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoderRecode.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/transform/encode/ColumnEncoderRecode.java?ref=cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd",
    "patch": "@@ -0,0 +1,274 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.sysds.runtime.transform.encode;\n+\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Objects;\n+\n+import org.apache.sysds.lops.Lop;\n+import org.apache.sysds.runtime.DMLRuntimeException;\n+import org.apache.sysds.runtime.matrix.data.FrameBlock;\n+import org.apache.sysds.runtime.matrix.data.MatrixBlock;\n+\n+public class ColumnEncoderRecode extends ColumnEncoder {\n+\tprivate static final long serialVersionUID = 8213163881283341874L;\n+\n+\t// test property to ensure consistent encoding for local and federated\n+\tpublic static boolean SORT_RECODE_MAP = false;\n+\n+\t// recode maps and custom map for partial recode maps\n+\tprivate HashMap<String, Long> _rcdMap = new HashMap<>();\n+\tprivate HashSet<Object> _rcdMapPart = null;\n+\n+\tpublic ColumnEncoderRecode(int colID) {\n+\t\tsuper(colID);\n+\t}\n+\n+\tpublic ColumnEncoderRecode() {\n+\t\tthis(-1);\n+\t}\n+\n+\tprivate ColumnEncoderRecode(int colID, HashMap<String, Long> rcdMap) {\n+\t\tsuper(colID);\n+\t\t_rcdMap = rcdMap;\n+\t}\n+\n+\t/**\n+\t * Returns the Recode map entry which consists of concatenation of code, delimiter and token.\n+\t *\n+\t * @param token is part of Recode map\n+\t * @param code  is code for token\n+\t * @return the concatenation of token and code with delimiter in between\n+\t */\n+\tpublic static String constructRecodeMapEntry(String token, Long code) {\n+\t\tStringBuilder sb = new StringBuilder(token.length() + 16);\n+\t\treturn constructRecodeMapEntry(token, code, sb);\n+\t}\n+\n+\tprivate static String constructRecodeMapEntry(String token, Long code, StringBuilder sb) {\n+\t\tsb.setLength(0); // reset reused string builder\n+\t\treturn sb.append(token).append(Lop.DATATYPE_PREFIX).append(code.longValue()).toString();\n+\t}\n+\n+\t/**\n+\t * Splits a Recode map entry into its token and code.\n+\t *\n+\t * @param value concatenation of token and code with delimiter in between\n+\t * @return string array of token and code\n+\t */\n+\tpublic static String[] splitRecodeMapEntry(String value) {\n+\t\t// Instead of using splitCSV which is forcing string with RFC-4180 format,\n+\t\t// using Lop.DATATYPE_PREFIX separator to split token and code\n+\t\tint pos = value.lastIndexOf(Lop.DATATYPE_PREFIX);\n+\t\treturn new String[] {value.substring(0, pos), value.substring(pos + 1)};\n+\t}\n+\n+\tpublic HashMap<String, Long> getCPRecodeMaps() {\n+\t\treturn _rcdMap;\n+\t}\n+\n+\tpublic HashSet<Object> getCPRecodeMapsPartial() {\n+\t\treturn _rcdMapPart;\n+\t}\n+\n+\tpublic void sortCPRecodeMaps() {\n+\t\tString[] keys = _rcdMap.keySet().toArray(new String[0]);\n+\t\tArrays.sort(keys);\n+\t\t_rcdMap.clear();\n+\t\tfor(String key : keys)\n+\t\t\tputCode(_rcdMap, key);\n+\t}\n+\n+\tprivate long lookupRCDMap(String key) {\n+\t\tLong tmp = _rcdMap.get(key);\n+\t\treturn (tmp != null) ? tmp : -1;\n+\t}\n+\n+\t@Override\n+\tpublic void build(FrameBlock in) {\n+\t\tif(!isApplicable())\n+\t\t\treturn;\n+\n+\t\tIterator<String[]> iter = in.getStringRowIterator(_colID);\n+\t\twhile(iter.hasNext()) {\n+\t\t\tString[] row = iter.next();\n+\t\t\t// probe and build column map\n+\t\t\tString key = row[0]; // 0 since there is only one column in the row\n+\t\t\tif(key != null && !key.isEmpty() && !_rcdMap.containsKey(key))\n+\t\t\t\tputCode(_rcdMap, key);\n+\t\t}\n+\n+\t\tif(SORT_RECODE_MAP) {\n+\t\t\tsortCPRecodeMaps();\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Put the code into the map with the provided key. The code depends on the type of encoder.\n+\t *\n+\t * @param map column map\n+\t * @param key key for the new entry\n+\t */\n+\tprotected void putCode(HashMap<String, Long> map, String key) {\n+\t\tmap.put(key, (long) (map.size() + 1));\n+\t}\n+\n+\t@Override\n+\tpublic void prepareBuildPartial() {\n+\t\t// ensure allocated partial recode map\n+\t\tif(_rcdMapPart == null)\n+\t\t\t_rcdMapPart = new HashSet<>();\n+\t}\n+\n+\t@Override\n+\tpublic void buildPartial(FrameBlock in) {\n+\t\tif(!isApplicable())\n+\t\t\treturn;\n+\n+\t\t// construct partial recode map (tokens w/o codes)\n+\t\t// probe and build column map\n+\t\tfor(int i = 0; i < in.getNumRows(); i++)\n+\t\t\t_rcdMapPart.add(in.get(i, _colID - 1));\n+\t\t// cleanup unnecessary entries once\n+\t\t_rcdMapPart.remove(null);\n+\t\t_rcdMapPart.remove(\"\");\n+\t}\n+\n+\t@Override\n+\tpublic MatrixBlock apply(FrameBlock in, MatrixBlock out, int outputCol) {\n+\t\tfor(int i = 0; i < in.getNumRows(); i++) {\n+\t\t\tObject okey = in.get(i, _colID - 1);\n+\t\t\tString key = (okey != null) ? okey.toString() : null;\n+\t\t\tlong code = lookupRCDMap(key);\n+\t\t\tout.quickSetValue(i, outputCol, (code >= 0) ? code : Double.NaN);\n+\t\t}\n+\t\treturn out;\n+\t}\n+\n+\t@Override\n+\tpublic MatrixBlock apply(MatrixBlock in, MatrixBlock out, int outputCol) {\n+\t\tthrow new DMLRuntimeException(\n+\t\t\t\"Recode called with MatrixBlock. Should not happen since Recode is the first \" + \"encoder in the Stack\");\n+\t}\n+\n+\t@Override\n+\tpublic void mergeAt(ColumnEncoder other) {\n+\t\tif(!(other instanceof ColumnEncoderRecode)) {\n+\t\t\tsuper.mergeAt(other);\n+\t\t\treturn;\n+\t\t}\n+\t\tassert other._colID == _colID;\n+\t\t// merge together overlapping columns\n+\t\tColumnEncoderRecode otherRec = (ColumnEncoderRecode) other;\n+\t\tHashMap<String, Long> otherMap = otherRec._rcdMap;\n+\t\tif(otherMap != null) {\n+\t\t\t// for each column, add all non present recode values\n+\t\t\tfor(Map.Entry<String, Long> entry : otherMap.entrySet()) {\n+\t\t\t\tif(lookupRCDMap(entry.getKey()) == -1) {\n+\t\t\t\t\t// key does not yet exist\n+\t\t\t\t\tputCode(_rcdMap, entry.getKey());\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tpublic int getNumDistinctValues() {\n+\t\treturn _rcdMap.size();\n+\t}\n+\n+\t@Override\n+\tpublic FrameBlock getMetaData(FrameBlock meta) {\n+\t\tif(!isApplicable())\n+\t\t\treturn meta;\n+\n+\t\t// inverse operation to initRecodeMaps\n+\n+\t\t// allocate output rows\n+\t\tmeta.ensureAllocatedColumns(getNumDistinctValues());\n+\n+\t\t// create compact meta data representation\n+\t\tStringBuilder sb = new StringBuilder(); // for reuse\n+\t\tint rowID = 0;\n+\t\tfor(Entry<String, Long> e : _rcdMap.entrySet()) {\n+\t\t\tmeta.set(rowID++,\n+\t\t\t\t_colID - 1, // 1-based\n+\t\t\t\tconstructRecodeMapEntry(e.getKey(), e.getValue(), sb));\n+\t\t}\n+\t\tmeta.getColumnMetadata(_colID - 1).setNumDistinct(getNumDistinctValues());\n+\n+\t\treturn meta;\n+\t}\n+\n+\t/**\n+\t * Construct the recodemaps from the given input frame for all columns registered for recode.\n+\t *\n+\t * @param meta frame block\n+\t */\n+\t@Override\n+\tpublic void initMetaData(FrameBlock meta) {\n+\t\tif(meta == null || meta.getNumRows() <= 0)\n+\t\t\treturn;\n+\t\t_rcdMap = meta.getRecodeMap(_colID - 1); // 1-based\n+\t}\n+\n+\t@Override\n+\tpublic void writeExternal(ObjectOutput out) throws IOException {\n+\t\tsuper.writeExternal(out);\n+\t\tout.writeInt(_rcdMap.size());\n+\t\tfor(Entry<String, Long> e : _rcdMap.entrySet()) {\n+\t\t\tout.writeUTF(e.getKey());\n+\t\t\tout.writeLong(e.getValue());\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic void readExternal(ObjectInput in) throws IOException {\n+\t\tsuper.readExternal(in);\n+\t\tint size = in.readInt();\n+\t\tfor(int j = 0; j < size; j++) {\n+\t\t\tString key = in.readUTF();\n+\t\t\tLong value = in.readLong();\n+\t\t\t_rcdMap.put(key, value);\n+\t\t}\n+\t}\n+\n+\t@Override\n+\tpublic boolean equals(Object o) {\n+\t\tif(this == o)\n+\t\t\treturn true;\n+\t\tif(o == null || getClass() != o.getClass())\n+\t\t\treturn false;\n+\t\tColumnEncoderRecode that = (ColumnEncoderRecode) o;\n+\t\treturn Objects.equals(_rcdMap, that._rcdMap);\n+\t}\n+\n+\t@Override\n+\tpublic int hashCode() {\n+\t\treturn Objects.hash(_rcdMap);\n+\t}\n+}"
  },
  {
    "sha": "756f6efc9504dd0f1983329784285f423109485b",
    "filename": "src/main/java/org/apache/sysds/runtime/transform/encode/Encoder.java",
    "status": "modified",
    "additions": 23,
    "deletions": 230,
    "changes": 253,
    "blob_url": "https://github.com/apache/systemds/blob/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/encode/Encoder.java",
    "raw_url": "https://github.com/apache/systemds/raw/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/encode/Encoder.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/transform/encode/Encoder.java?ref=cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd",
    "patch": "@@ -1,274 +1,67 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n package org.apache.sysds.runtime.transform.encode;\n \n import java.io.Externalizable;\n-import java.io.IOException;\n-import java.io.ObjectInput;\n-import java.io.ObjectOutput;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.HashSet;\n-import java.util.List;\n-import java.util.Set;\n \n-import org.apache.commons.logging.Log;\n-import org.apache.commons.logging.LogFactory;\n-import org.apache.sysds.runtime.DMLRuntimeException;\n import org.apache.sysds.runtime.matrix.data.FrameBlock;\n import org.apache.sysds.runtime.matrix.data.MatrixBlock;\n-import org.apache.sysds.runtime.util.IndexRange;\n-import org.apache.sysds.runtime.util.UtilFunctions;\n-import org.apache.wink.json4j.JSONArray;\n \n-/**\n- * Base class for all transform encoders providing both a row and block\n- * interface for decoding frames to matrices.\n- *\n+/*\n+Interface for all Encoder like objects\n  */\n-public abstract class Encoder implements Externalizable\n-{\n-\tprivate static final long serialVersionUID = 2299156350718979064L;\n-\tprotected static final Log LOG = LogFactory.getLog(Encoder.class.getName());\n-\n-\tprotected int _clen = -1;\n-\tprotected int[] _colList = null;\n-\n-\tprotected Encoder( int[] colList, int clen ) {\n-\t\t_colList = colList;\n-\t\t_clen = clen;\n-\t}\n-\n-\tpublic int[] getColList() {\n-\t\treturn _colList;\n-\t}\n-\n-\tpublic void setColList(int[] colList) {\n-\t\t_colList = colList;\n-\t}\n-\n-\tpublic int getNumCols() {\n-\t\treturn _clen;\n-\t}\n-\n-\tpublic int initColList(JSONArray attrs) {\n-\t\t_colList = new int[attrs.size()];\n-\t\tfor(int i=0; i < _colList.length; i++)\n-\t\t\t_colList[i] = UtilFunctions.toInt(attrs.get(i));\n-\t\treturn _colList.length;\n-\t}\n-\n-\tpublic int initColList(int[] colList) {\n-\t\t_colList = colList;\n-\t\treturn _colList.length;\n-\t}\n-\n-\t/**\n-\t * Indicates if this encoder is applicable, i.e, if there is at\n-\t * least one column to encode.\n-\t *\n-\t * @return true if at least one column to encode\n-\t */\n-\tpublic boolean isApplicable()  {\n-\t\treturn (_colList != null && _colList.length > 0);\n-\t}\n \n-\t/**\n-\t * Indicates if this encoder is applicable for the given column ID,\n-\t * i.e., if it is subject to this transformation.\n-\t *\n-\t * @param colID column ID\n-\t * @return true if encoder is applicable for given column\n-\t */\n-\tpublic int isApplicable(int colID) {\n-\t\tif(_colList == null)\n-\t\t\treturn -1;\n-\t\tint idx = Arrays.binarySearch(_colList, colID);\n-\t\treturn ( idx >= 0 ? idx : -1);\n-\t}\n+public interface Encoder extends Externalizable {\n \n \t/**\n-\t * Block encode: build and apply (transform encode).\n+\t * Build the transform meta data for the given block input. This call modifies and keeps meta data as encoder state.\n \t *\n \t * @param in input frame block\n-\t * @param out output matrix block\n-\t * @return output matrix block\n \t */\n-\tpublic abstract MatrixBlock encode(FrameBlock in, MatrixBlock out);\n+\tvoid build(FrameBlock in);\n \n \t/**\n-\t * Build the transform meta data for the given block input. This call modifies\n-\t * and keeps meta data as encoder state.\n+\t * Apply the generated metadata to the FrameBlock and saved the result in out.\n \t *\n-\t * @param in input frame block\n-\t */\n-\tpublic abstract void build(FrameBlock in);\n-\n-\t/**\n-\t * Allocates internal data structures for partial build.\n-\t */\n-\tpublic void prepareBuildPartial() {\n-\t\t//do nothing\n-\t}\n-\t\n-\t/**\n-\t * Partial build of internal data structures (e.g., in distributed spark operations).\n-\t * \n-\t * @param in input frame block\n-\t */\n-\tpublic void buildPartial(FrameBlock in) {\n-\t\t//do nothing\n-\t}\n-\t\n-\t/**\n-\t * Encode input data blockwise according to existing transform meta\n-\t * data (transform apply).\n-\t *\n-\t * @param in input frame block\n-\t * @param out output matrix block\n+\t * @param in        input frame block\n+\t * @param out       output matrix block\n+\t * @param outputCol is a offset in the output matrix. column in FrameBlock + outputCol = column in out\n \t * @return output matrix block\n \t */\n-\tpublic abstract MatrixBlock apply(FrameBlock in, MatrixBlock out);\n-\n-\tprotected int[] subRangeColList(IndexRange ixRange) {\n-\t\tList<Integer> cols = new ArrayList<>();\n-\t\tfor(int col : _colList) {\n-\t\t\tif(ixRange.inColRange(col)) {\n-\t\t\t\t// add the correct column, removed columns before start\n-\t\t\t\t// colStart - 1 because colStart is 1-based\n-\t\t\t\tint corrColumn = (int) (col - (ixRange.colStart - 1));\n-\t\t\t\tcols.add(corrColumn);\n-\t\t\t}\n-\t\t}\n-\t\treturn cols.stream().mapToInt(i -> i).toArray();\n-\t}\n-\n-\t/**\n-\t * Returns a new Encoder that only handles a sub range of columns.\n-\t *\n-\t * @param ixRange the range (1-based, begin inclusive, end exclusive)\n-\t * @return an encoder of the same type, just for the sub-range\n-\t */\n-\tpublic Encoder subRangeEncoder(IndexRange ixRange) {\n-\t\tthrow new DMLRuntimeException(\n-\t\t\tthis.getClass().getSimpleName() + \" does not support the creation of a sub-range encoder\");\n-\t}\n-\n-\t/**\n-\t * Merges the column information, like how many columns the frame needs and which columns this encoder operates on.\n-\t *\n-\t * @param other the other encoder of the same type\n-\t * @param col   column at which the second encoder will be merged in (1-based)\n-\t */\n-\tprotected void mergeColumnInfo(Encoder other, int col) {\n-\t\t// update number of columns\n-\t\t_clen = Math.max(_clen, col - 1 + other._clen);\n-\n-\t\t// update the new columns that this encoder operates on\n-\t\tSet<Integer> colListAgg = new HashSet<>(); // for dedup\n-\t\tfor(int i : _colList)\n-\t\t\tcolListAgg.add(i);\n-\t\tfor(int i : other._colList)\n-\t\t\tcolListAgg.add(col - 1 + i);\n-\t\t_colList = colListAgg.stream().mapToInt(i -> i).toArray();\n-\t}\n-\n-\t/**\n-\t * Merges another encoder, of a compatible type, in after a certain position. Resizes as necessary.\n-\t * <code>Encoders</code> are compatible with themselves and <code>EncoderComposite</code> is compatible with every\n-\t * other <code>Encoder</code>.\n-\t *\n-\t * @param other the encoder that should be merged in\n-\t * @param row   the row where it should be placed (1-based)\n-\t * @param col   the col where it should be placed (1-based)\n-\t */\n-\tpublic void mergeAt(Encoder other, int row, int col) {\n-\t\tthrow new DMLRuntimeException(\n-\t\t\tthis.getClass().getSimpleName() + \" does not support merging with \" + other.getClass().getSimpleName());\n-\t}\n-\n-\t/**\n-\t * Update index-ranges to after encoding. Note that only Dummycoding changes the ranges.\n-\t *\n-\t * @param beginDims begin dimensions of range\n-\t * @param endDims end dimensions of range\n-\t */\n-\tpublic void updateIndexRanges(long[] beginDims, long[] endDims) {\n-\t\t// do nothing - default\n-\t}\n+\tMatrixBlock apply(FrameBlock in, MatrixBlock out, int outputCol);\n \n \t/**\n \t * Construct a frame block out of the transform meta data.\n \t *\n \t * @param out output frame block\n \t * @return output frame block?\n \t */\n-\tpublic abstract FrameBlock getMetaData(FrameBlock out);\n+\tFrameBlock getMetaData(FrameBlock out);\n \n \t/**\n \t * Sets up the required meta data for a subsequent call to apply.\n \t *\n \t * @param meta frame block\n \t */\n-\tpublic abstract void initMetaData(FrameBlock meta);\n+\tvoid initMetaData(FrameBlock meta);\n \n \t/**\n-\t * Obtain the column mapping of encoded frames based on the passed\n-\t * meta data frame.\n-\t *\n-\t * @param meta meta data frame block\n-\t * @param out output matrix\n-\t * @return matrix with column mapping (one row per attribute)\n+\t * Allocates internal data structures for partial build.\n \t */\n-\tpublic MatrixBlock getColMapping(FrameBlock meta, MatrixBlock out) {\n-\t\t//default: do nothing\n-\t\treturn out;\n-\t}\n+    void prepareBuildPartial();\n \n \t/**\n-\t * Redirects the default java serialization via externalizable to our default\n-\t * hadoop writable serialization for efficient broadcast/rdd serialization.\n+\t * Partial build of internal data structures (e.g., in distributed spark operations).\n \t *\n-\t * @param os object output\n-\t * @throws IOException if IOException occurs\n+\t * @param in input frame block\n \t */\n-\t@Override\n-\tpublic void writeExternal(ObjectOutput os) throws IOException {\n-\t\tos.writeInt(_clen);\n-\t\tos.writeInt(_colList.length);\n-\t\tfor(int col : _colList)\n-\t\t\tos.writeInt(col);\n-\t}\n+    void buildPartial(FrameBlock in);\n \n \t/**\n-\t * Redirects the default java serialization via externalizable to our default\n-\t * hadoop writable serialization for efficient broadcast/rdd deserialization.\n+\t * Update index-ranges to after encoding. Note that only Dummycoding changes the ranges.\n \t *\n-\t * @param in object input\n-\t * @throws IOException if IOException occur\n+\t * @param beginDims begin dimensions of range\n+\t * @param endDims   end dimensions of range\n+\t * @param offset    is applied to begin and endDims\n \t */\n-\t@Override\n-\tpublic void readExternal(ObjectInput in) throws IOException {\n-\t\t_clen = in.readInt();\n-\t\t_colList = new int[in.readInt()];\n-\t\tfor(int i = 0; i < _colList.length; i++)\n-\t\t\t_colList[i] = in.readInt();\n-\t}\n+    void updateIndexRanges(long[] beginDims, long[] endDims, int offset);\n+\n }"
  },
  {
    "sha": "f01e8732f07fd7b55e7252942a741a875e7ea5d6",
    "filename": "src/main/java/org/apache/sysds/runtime/transform/encode/EncoderBin.java",
    "status": "removed",
    "additions": 0,
    "deletions": 358,
    "changes": 358,
    "blob_url": "https://github.com/apache/systemds/blob/1b77f5e81a6d0981fa76c17f66937e94e1322785/src/main/java/org/apache/sysds/runtime/transform/encode/EncoderBin.java",
    "raw_url": "https://github.com/apache/systemds/raw/1b77f5e81a6d0981fa76c17f66937e94e1322785/src/main/java/org/apache/sysds/runtime/transform/encode/EncoderBin.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/transform/encode/EncoderBin.java?ref=1b77f5e81a6d0981fa76c17f66937e94e1322785",
    "patch": "@@ -1,358 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.apache.sysds.runtime.transform.encode;\n-\n-import java.io.IOException;\n-import java.io.ObjectInput;\n-import java.io.ObjectOutput;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.Map;\n-\n-import org.apache.commons.lang.ArrayUtils;\n-import org.apache.commons.lang3.tuple.MutableTriple;\n-import org.apache.sysds.lops.Lop;\n-import org.apache.sysds.runtime.matrix.data.FrameBlock;\n-import org.apache.sysds.runtime.matrix.data.MatrixBlock;\n-import org.apache.sysds.runtime.transform.TfUtils.TfMethod;\n-import org.apache.sysds.runtime.transform.meta.TfMetaUtils;\n-import org.apache.sysds.runtime.util.IndexRange;\n-import org.apache.sysds.runtime.util.UtilFunctions;\n-import org.apache.wink.json4j.JSONArray;\n-import org.apache.wink.json4j.JSONException;\n-import org.apache.wink.json4j.JSONObject;\n-\n-public class EncoderBin extends Encoder\n-{\n-\tprivate static final long serialVersionUID = 1917445005206076078L;\n-\n-\tpublic static final String MIN_PREFIX = \"min\";\n-\tpublic static final String MAX_PREFIX = \"max\";\n-\tpublic static final String NBINS_PREFIX = \"nbins\";\n-\n-\tprotected int[] _numBins = null;\n-\n-\t//frame transform-apply attributes\n-\t// a) column bin boundaries\n-\t//TODO binMins is redundant and could be removed - necessary for correct fed results\n-\tprivate double[][] _binMins = null;\n-\tprivate double[][] _binMaxs = null;\n-\t// b) column min/max (for partial build)\n-\tprivate double[] _colMins = null;\n-\tprivate double[] _colMaxs = null;\n-\t\n-\tpublic EncoderBin(JSONObject parsedSpec, String[] colnames, int clen, int minCol, int maxCol)\n-\t\tthrows JSONException, IOException\n-\t{\n-\t\tsuper( null, clen );\n-\t\tif ( !parsedSpec.containsKey(TfMethod.BIN.toString()) )\n-\t\t\treturn;\n-\n-\t\t//parse column names or column ids\n-\t\tList<Integer> collist = TfMetaUtils.parseBinningColIDs(parsedSpec, colnames, minCol, maxCol);\n-\t\tinitColList(ArrayUtils.toPrimitive(collist.toArray(new Integer[0])));\n-\n-\t\t//parse number of bins per column\n-\t\tboolean ids = parsedSpec.containsKey(\"ids\") && parsedSpec.getBoolean(\"ids\");\n-\t\tJSONArray group = (JSONArray) parsedSpec.get(TfMethod.BIN.toString());\n-\t\t_numBins = new int[collist.size()];\n-\t\tfor (Object o : group) {\n-\t\t\tJSONObject colspec = (JSONObject) o;\n-\t\t\tint ixOffset = minCol == -1 ? 0 : minCol - 1;\n-\t\t\tint pos = collist.indexOf(ids ? colspec.getInt(\"id\") - ixOffset :\n-\t\t\t\tArrayUtils.indexOf(colnames, colspec.get(\"name\")) + 1);\n-\t\t\tif(pos >= 0)\n-\t\t\t\t_numBins[pos] = colspec.containsKey(\"numbins\") ? colspec.getInt(\"numbins\") : 1;\n-\t\t}\n-\t}\n-\n-\tpublic EncoderBin() {\n-\t\tsuper(new int[0], 0);\n-\t\t_numBins = new int[0];\n-\t}\n-\n-\tprivate EncoderBin(int[] colList, int clen, int[] numBins, double[][] binMins, double[][] binMaxs) {\n-\t\tsuper(colList, clen);\n-\t\t_numBins = numBins;\n-\t\t_binMins = binMins;\n-\t\t_binMaxs = binMaxs;\n-\t}\n-\t\n-\tpublic double[] getColMins() {\n-\t\treturn _colMins;\n-\t}\n-\t\n-\tpublic double[] getColMaxs() {\n-\t\treturn _colMaxs;\n-\t}\n-\t\n-\tpublic double[] getBinMins(int j) {\n-\t\treturn _binMins[j];\n-\t}\n-\t\n-\tpublic double[] getBinMaxs(int j) {\n-\t\treturn _binMaxs[j];\n-\t}\n-\t\n-\t@Override\n-\tpublic MatrixBlock encode(FrameBlock in, MatrixBlock out) {\n-\t\tbuild(in);\n-\t\treturn apply(in, out);\n-\t}\n-\n-\t@Override\n-\tpublic void build(FrameBlock in) {\n-\t\tif ( !isApplicable() )\n-\t\t\treturn;\n-\n-\t\t// derive bin boundaries from min/max per column\n-\t\tfor(int j=0; j <_colList.length; j++) {\n-\t\t\tdouble min = Double.POSITIVE_INFINITY;\n-\t\t\tdouble max = Double.NEGATIVE_INFINITY;\n-\t\t\tint colID = _colList[j];\n-\t\t\tfor( int i=0; i<in.getNumRows(); i++ ) {\n-\t\t\t\tdouble inVal = UtilFunctions.objectToDouble(\n-\t\t\t\t\tin.getSchema()[colID-1], in.get(i, colID-1));\n-\t\t\t\tmin = Math.min(min, inVal);\n-\t\t\t\tmax = Math.max(max, inVal);\n-\t\t\t}\n-\t\t\tcomputeBins(j, min, max);\n-\t\t}\n-\t}\n-\t\n-\tpublic void computeBins(int j, double min, double max) {\n-\t\t// ensure allocated internal transformation metadata\n-\t\tif( _binMins == null || _binMaxs == null ) {\n-\t\t\t_binMins = new double[_colList.length][];\n-\t\t\t_binMaxs = new double[_colList.length][];\n-\t\t}\n-\t\t_binMins[j] = new double[_numBins[j]];\n-\t\t_binMaxs[j] = new double[_numBins[j]];\n-\t\tfor(int i=0; i<_numBins[j]; i++) {\n-\t\t\t_binMins[j][i] = min + i*(max-min)/_numBins[j];\n-\t\t\t_binMaxs[j][i] = min + (i+1)*(max-min)/_numBins[j];\n-\t\t}\n-\t}\n-\t\n-\tpublic void prepareBuildPartial() {\n-\t\t//ensure allocated min/max arrays\n-\t\tif( _colMins == null ) {\n-\t\t\t_colMins = new double[_colList.length];\n-\t\t\t_colMaxs = new double[_colList.length];\n-\t\t}\n-\t}\n-\t\n-\tpublic void buildPartial(FrameBlock in) {\n-\t\tif ( !isApplicable() )\n-\t\t\treturn;\n-\t\t\n-\t\t// derive bin boundaries from min/max per column\n-\t\tfor(int j=0; j <_colList.length; j++) {\n-\t\t\tdouble min = Double.POSITIVE_INFINITY;\n-\t\t\tdouble max = Double.NEGATIVE_INFINITY;\n-\t\t\tint colID = _colList[j];\n-\t\t\tfor( int i=0; i<in.getNumRows(); i++ ) {\n-\t\t\t\tdouble inVal = UtilFunctions.objectToDouble(\n-\t\t\t\t\tin.getSchema()[colID-1], in.get(i, colID-1));\n-\t\t\t\tmin = Math.min(min, inVal);\n-\t\t\t\tmax = Math.max(max, inVal);\n-\t\t\t}\n-\t\t\t_colMins[j] = min;\n-\t\t\t_colMaxs[j] = max;\n-\t\t}\n-\t}\n-\n-\t@Override\n-\tpublic MatrixBlock apply(FrameBlock in, MatrixBlock out) {\n-\t\tfor(int j=0; j<_colList.length; j++) {\n-\t\t\tint colID = _colList[j];\n-\t\t\tfor( int i=0; i<in.getNumRows(); i++ ) {\n-\t\t\t\tdouble inVal = UtilFunctions.objectToDouble(\n-\t\t\t\t\tin.getSchema()[colID-1], in.get(i, colID-1));\n-\t\t\t\tint ix = Arrays.binarySearch(_binMaxs[j], inVal);\n-\t\t\t\tint binID = ((ix < 0) ? Math.abs(ix+1) : ix) + 1;\n-\t\t\t\tout.quickSetValue(i, colID-1, binID);\n-\t\t\t}\n-\t\t}\n-\t\treturn out;\n-\t}\n-\n-\t@Override\n-\tpublic Encoder subRangeEncoder(IndexRange ixRange) {\n-\t\tList<Integer> colsList = new ArrayList<>();\n-\t\tList<Integer> numBinsList = new ArrayList<>();\n-\t\tList<double[]> binMinsList = new ArrayList<>();\n-\t\tList<double[]> binMaxsList = new ArrayList<>();\n-\t\tfor(int i = 0; i < _colList.length; i++) {\n-\t\t\tint col = _colList[i];\n-\t\t\tif(col >= ixRange.colStart && col < ixRange.colEnd) {\n-\t\t\t\t// add the correct column, removed columns before start\n-\t\t\t\t// colStart - 1 because colStart is 1-based\n-\t\t\t\tint corrColumn = (int) (col - (ixRange.colStart - 1));\n-\t\t\t\tcolsList.add(corrColumn);\n-\t\t\t\tnumBinsList.add(_numBins[i]);\n-\t\t\t\tbinMinsList.add(_binMins[i]);\n-\t\t\t\tbinMaxsList.add(_binMaxs[i]);\n-\t\t\t}\n-\t\t}\n-\t\tif(colsList.isEmpty())\n-\t\t\t// empty encoder -> sub range encoder does not exist\n-\t\t\treturn null;\n-\n-\t\tint[] colList = colsList.stream().mapToInt(i -> i).toArray();\n-\t\treturn new EncoderBin(colList, (int) (ixRange.colEnd - ixRange.colStart),\n-\t\t\tnumBinsList.stream().mapToInt((i) -> i).toArray(), binMinsList.toArray(new double[0][0]),\n-\t\t\tbinMaxsList.toArray(new double[0][0]));\n-\t}\n-\n-\t@Override\n-\tpublic void mergeAt(Encoder other, int row, int col) {\n-\t\tif(other instanceof EncoderBin) {\n-\t\t\tEncoderBin otherBin = (EncoderBin) other;\n-\n-\t\t\t// save the min, max as well as the number of bins for the column indexes\n-\t\t\tMap<Integer, MutableTriple<Integer, Double, Double>> ixBinsMap = new HashMap<>();\n-\t\t\tfor(int i = 0; i < _colList.length; i++) {\n-\t\t\t\tixBinsMap.put(_colList[i],\n-\t\t\t\t\tnew MutableTriple<>(_numBins[i], _binMins[i][0], _binMaxs[i][_binMaxs[i].length - 1]));\n-\t\t\t}\n-\t\t\tfor(int i = 0; i < otherBin._colList.length; i++) {\n-\t\t\t\tint column = otherBin._colList[i] + (col - 1);\n-\t\t\t\tMutableTriple<Integer, Double, Double> entry = ixBinsMap.get(column);\n-\t\t\t\tif(entry == null) {\n-\t\t\t\t\tixBinsMap.put(column,\n-\t\t\t\t\t\tnew MutableTriple<>(otherBin._numBins[i], otherBin._binMins[i][0],\n-\t\t\t\t\t\t\totherBin._binMaxs[i][otherBin._binMaxs[i].length - 1]));\n-\t\t\t\t}\n-\t\t\t\telse {\n-\t\t\t\t\t// num bins will match\n-\t\t\t\t\tentry.middle = Math.min(entry.middle, otherBin._binMins[i][0]);\n-\t\t\t\t\tentry.right = Math.max(entry.right, otherBin._binMaxs[i][otherBin._binMaxs[i].length - 1]);\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\tmergeColumnInfo(other, col);\n-\n-\t\t\t// use the saved values to fill the arrays again\n-\t\t\t_numBins = new int[_colList.length];\n-\t\t\t_binMins = new double[_colList.length][];\n-\t\t\t_binMaxs = new double[_colList.length][];\n-\n-\t\t\tfor(int i = 0; i < _colList.length; i++) {\n-\t\t\t\tint column = _colList[i];\n-\t\t\t\tMutableTriple<Integer, Double, Double> entry = ixBinsMap.get(column);\n-\t\t\t\t_numBins[i] = entry.left;\n-\n-\t\t\t\tdouble min = entry.middle;\n-\t\t\t\tdouble max = entry.right;\n-\t\t\t\t_binMins[i] = new double[_numBins[i]];\n-\t\t\t\t_binMaxs[i] = new double[_numBins[i]];\n-\t\t\t\tfor(int j = 0; j < _numBins[i]; j++) {\n-\t\t\t\t\t_binMins[i][j] = min + j * (max - min) / _numBins[i];\n-\t\t\t\t\t_binMaxs[i][j] = min + (j + 1) * (max - min) / _numBins[i];\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\treturn;\n-\t\t}\n-\t\tsuper.mergeAt(other, row, col);\n-\t}\n-\n-\t@Override\n-\tpublic FrameBlock getMetaData(FrameBlock meta) {\n-\t\t//allocate frame if necessary\n-\t\tint maxLength = 0;\n-\t\tfor( int j=0; j<_colList.length; j++ )\n-\t\t\tmaxLength = Math.max(maxLength, _binMaxs[j].length);\n-\t\tmeta.ensureAllocatedColumns(maxLength);\n-\n-\t\t//serialize the internal state into frame meta data\n-\t\tfor( int j=0; j<_colList.length; j++ ) {\n-\t\t\tint colID = _colList[j]; //1-based\n-\t\t\tmeta.getColumnMetadata(colID-1).setNumDistinct(_numBins[j]);\n-\t\t\tfor( int i=0; i<_binMaxs[j].length; i++ ) {\n-\t\t\t\tStringBuilder sb = new StringBuilder(16);\n-\t\t\t\tsb.append(_binMins[j][i]);\n-\t\t\t\tsb.append(Lop.DATATYPE_PREFIX);\n-\t\t\t\tsb.append(_binMaxs[j][i]);\n-\t\t\t\tmeta.set(i, colID-1, sb.toString());\n-\t\t\t}\n-\t\t}\n-\t\treturn meta;\n-\t}\n-\n-\t@Override\n-\tpublic void initMetaData(FrameBlock meta) {\n-\t\tif( meta == null || _binMaxs != null )\n-\t\t\treturn;\n-\t\t//deserialize the frame meta data into internal state\n-\t\t_binMins = new double[_colList.length][];\n-\t\t_binMaxs = new double[_colList.length][];\n-\t\tfor( int j=0; j<_colList.length; j++ ) {\n-\t\t\tint colID = _colList[j]; //1-based\n-\t\t\tint nbins = (int)meta.getColumnMetadata()[colID-1].getNumDistinct();\n-\t\t\t_binMins[j] = new double[nbins];\n-\t\t\t_binMaxs[j] = new double[nbins];\n-\t\t\tfor( int i=0; i<nbins; i++ ) {\n-\t\t\t\tString[] tmp = meta.get(i, colID-1).toString().split(Lop.DATATYPE_PREFIX);\n-\t\t\t\t_binMins[j][i] = Double.parseDouble(tmp[0]);\n-\t\t\t\t_binMaxs[j][i] = Double.parseDouble(tmp[1]);\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t@Override\n-\tpublic void writeExternal(ObjectOutput out) throws IOException {\n-\t\tsuper.writeExternal(out);\n-\t\t\n-\t\tout.writeInt(_numBins.length);\n-\t\tout.writeBoolean(_binMaxs!=null);\n-\t\tfor(int i = 0; i < _numBins.length; i++) {\n-\t\t\tout.writeInt(_numBins[i]);\n-\t\t\tif( _binMaxs != null )\n-\t\t\t\tfor(int j = 0; j < _binMaxs[i].length; j++) {\n-\t\t\t\t\tout.writeDouble(_binMaxs[i][j]);\n-\t\t\t\t\tout.writeDouble(_binMins[i][j]);\n-\t\t\t\t}\n-\t\t}\n-\t}\n-\n-\t@Override\n-\tpublic void readExternal(ObjectInput in) throws IOException {\n-\t\tsuper.readExternal(in);\n-\t\tint d1 = in.readInt();\n-\t\tboolean minmax = in.readBoolean();\n-\t\t_numBins = new int[d1];\n-\t\t_binMaxs = minmax ? new double[d1][] : null;\n-\t\t_binMins = minmax ? new double[d1][] : null;\n-\n-\t\tfor(int i = 0; i < d1; i++) {\n-\t\t\t_numBins[i] = in.readInt();\n-\t\t\tif( !minmax ) continue;\n-\t\t\t_binMaxs[i] = new double[_numBins[i]];\n-\t\t\t_binMins[i] = new double[_numBins[i]];\n-\t\t\tfor(int j = 0; j < _binMaxs[i].length; j++) {\n-\t\t\t\t_binMaxs[i][j] = in.readDouble();\n-\t\t\t\t_binMins[i][j] = in.readDouble();\n-\t\t\t}\n-\t\t}\n-\t}\n-}"
  },
  {
    "sha": "1115ba2891b8d7d710c5d2a1892236e04c9d2264",
    "filename": "src/main/java/org/apache/sysds/runtime/transform/encode/EncoderComposite.java",
    "status": "removed",
    "additions": 0,
    "deletions": 303,
    "changes": 303,
    "blob_url": "https://github.com/apache/systemds/blob/1b77f5e81a6d0981fa76c17f66937e94e1322785/src/main/java/org/apache/sysds/runtime/transform/encode/EncoderComposite.java",
    "raw_url": "https://github.com/apache/systemds/raw/1b77f5e81a6d0981fa76c17f66937e94e1322785/src/main/java/org/apache/sysds/runtime/transform/encode/EncoderComposite.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/transform/encode/EncoderComposite.java?ref=1b77f5e81a6d0981fa76c17f66937e94e1322785",
    "patch": "@@ -1,303 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- * \n- *   http://www.apache.org/licenses/LICENSE-2.0\n- * \n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.apache.sysds.runtime.transform.encode;\n-\n-import java.io.IOException;\n-import java.io.ObjectInput;\n-import java.io.ObjectOutput;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.List;\n-import java.util.Objects;\n-\n-import org.apache.commons.lang3.ArrayUtils;\n-import org.apache.sysds.common.Types.ValueType;\n-import org.apache.sysds.runtime.DMLRuntimeException;\n-import org.apache.sysds.runtime.matrix.data.FrameBlock;\n-import org.apache.sysds.runtime.matrix.data.MatrixBlock;\n-import org.apache.sysds.runtime.util.IndexRange;\n-\n-/**\n- * Simple composite encoder that applies a list of encoders \n- * in specified order. By implementing the default encoder API\n- * it can be used as a drop-in replacement for any other encoder. \n- * \n- */\n-public class EncoderComposite extends Encoder\n-{\n-\tprivate static final long serialVersionUID = -8473768154646831882L;\n-\t\n-\tprivate List<Encoder> _encoders = null;\n-\tprivate FrameBlock _meta = null;\n-\n-\tpublic EncoderComposite() {\n-\t\tsuper(null, -1);\n-\t}\n-\n-\tpublic EncoderComposite(List<Encoder> encoders) {\n-\t\tsuper(null, -1);\n-\t\t_encoders = encoders;\n-\t}\n-\n-\t@Override\n-\tpublic int getNumCols() {\n-\t\tint clen = 0;\n-\t\tfor( Encoder encoder : _encoders )\n-\t\t\tclen = Math.max(clen, encoder.getNumCols());\n-\t\treturn clen;\n-\t}\n-\n-\tpublic List<Encoder> getEncoders() {\n-\t\treturn _encoders;\n-\t}\n-\t\n-\tpublic Encoder getEncoder(Class<?> type) {\n-\t\tfor( Encoder encoder : _encoders ) {\n-\t\t\tif( encoder.getClass().equals(type) )\n-\t\t\t\treturn encoder;\n-\t\t}\n-\t\treturn null;\n-\t}\n-\t\n-\tpublic boolean isEncoder(int colID, Class<?> type) {\n-\t\tfor( Encoder encoder : _encoders ) {\n-\t\t\tif( encoder.getClass().equals(type) )\n-\t\t\t\treturn ArrayUtils.contains(encoder.getColList(), colID);\n-\t\t}\n-\t\treturn false;\n-\t}\n-\t\n-\t@Override\n-\tpublic MatrixBlock encode(FrameBlock in, MatrixBlock out) {\n-\t\ttry {\n-\t\t\t//build meta data first (for all encoders)\n-\t\t\tfor( Encoder encoder : _encoders )\n-\t\t\t\tencoder.build(in);\n-\t\t\t\n-\t\t\t//propagate meta data \n-\t\t\t_meta = new FrameBlock(in.getNumColumns(), ValueType.STRING);\n-\t\t\tfor( Encoder encoder : _encoders )\n-\t\t\t\t_meta = encoder.getMetaData(_meta);\n-\t\t\tfor( Encoder encoder : _encoders )\n-\t\t\t\tencoder.initMetaData(_meta);\n-\t\t\t\n-\t\t\t//apply meta data\n-\t\t\tfor( Encoder encoder : _encoders )\n-\t\t\t\tout = encoder.apply(in, out);\n-\t\t}\n-\t\tcatch(Exception ex) {\n-\t\t\tLOG.error(\"Failed transform-encode frame with \\n\" + this);\n-\t\t\tthrow ex;\n-\t\t}\n-\t\t\n-\t\treturn out;\n-\t}\n-\n-\t@Override\n-\tpublic void build(FrameBlock in) {\n-\t\tfor( Encoder encoder : _encoders )\n-\t\t\tencoder.build(in);\n-\t}\n-\t\n-\t@Override\n-\tpublic void prepareBuildPartial() {\n-\t\tfor( Encoder encoder : _encoders )\n-\t\t\tencoder.prepareBuildPartial();\n-\t}\n-\t\n-\t@Override\n-\tpublic void buildPartial(FrameBlock in) {\n-\t\tfor( Encoder encoder : _encoders )\n-\t\t\tencoder.buildPartial(in);\n-\t}\n-\t\n-\t@Override\n-\tpublic MatrixBlock apply(FrameBlock in, MatrixBlock out) {\n-\t\ttry {\n-\t\t\tfor( Encoder encoder : _encoders )\n-\t\t\t\tout = encoder.apply(in, out);\n-\t\t}\n-\t\tcatch(Exception ex) {\n-\t\t\tLOG.error(\"Failed to transform-apply frame with \\n\" + this);\n-\t\t\tthrow ex;\n-\t\t}\n-\t\treturn out;\n-\t}\n-\n-\t@Override\n-\tpublic boolean equals(Object o) {\n-\t\tif(this == o)\n-\t\t\treturn true;\n-\t\tif(o == null || getClass() != o.getClass())\n-\t\t\treturn false;\n-\t\tEncoderComposite that = (EncoderComposite) o;\n-\t\treturn _encoders.equals(that._encoders)\n-\t\t\t&& Objects.equals(_meta, that._meta);\n-\t}\n-\n-\t@Override\n-\tpublic int hashCode() {\n-\t\treturn Objects.hash(_encoders, _meta);\n-\t}\n-\n-\t@Override\n-\tpublic Encoder subRangeEncoder(IndexRange ixRange) {\n-\t\tList<Encoder> subRangeEncoders = new ArrayList<>();\n-\t\tfor (Encoder encoder : _encoders) {\n-\t\t\tEncoder subEncoder = encoder.subRangeEncoder(ixRange);\n-\t\t\tif (subEncoder != null) {\n-\t\t\t\tsubRangeEncoders.add(subEncoder);\n-\t\t\t}\n-\t\t}\n-\t\treturn new EncoderComposite(subRangeEncoders);\n-\t}\n-\n-\t@Override\n-\tpublic void mergeAt(Encoder other, int row, int col) {\n-\t\tif (other instanceof EncoderComposite) {\n-\t\t\tEncoderComposite otherComposite = (EncoderComposite) other;\n-\t\t\t// TODO maybe assert that the _encoders never have the same type of encoder twice or more\n-\t\t\tfor (Encoder otherEnc : otherComposite.getEncoders()) {\n-\t\t\t\tboolean mergedIn = false;\n-\t\t\t\tfor (Encoder encoder : _encoders) {\n-\t\t\t\t\tif (encoder.getClass() == otherEnc.getClass()) {\n-\t\t\t\t\t\tencoder.mergeAt(otherEnc, row, col);\n-\t\t\t\t\t\tmergedIn = true;\n-\t\t\t\t\t\tbreak;\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tif(!mergedIn) {\n-\t\t\t\t\tthrow new DMLRuntimeException(\"Tried to merge in encoder of class that is not present in \"\n-\t\t\t\t\t\t+ \"EncoderComposite: \" + otherEnc.getClass().getSimpleName());\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\t// update dummycode encoder domain sizes based on distinctness information from other encoders\n-\t\t\tfor (Encoder encoder : _encoders) {\n-\t\t\t\tif (encoder instanceof EncoderDummycode) {\n-\t\t\t\t\t((EncoderDummycode) encoder).updateDomainSizes(_encoders);\n-\t\t\t\t\treturn;\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\treturn;\n-\t\t}\n-\t\tfor (Encoder encoder : _encoders) {\n-\t\t\tif (encoder.getClass() == other.getClass()) {\n-\t\t\t\tencoder.mergeAt(other, row, col);\n-\t\t\t\t// update dummycode encoder domain sizes based on distinctness information from other encoders\n-\t\t\t\tfor (Encoder encDummy : _encoders) {\n-\t\t\t\t\tif (encDummy instanceof EncoderDummycode) {\n-\t\t\t\t\t\t((EncoderDummycode) encDummy).updateDomainSizes(_encoders);\n-\t\t\t\t\t\treturn;\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\treturn;\n-\t\t\t}\n-\t\t}\n-\t\tsuper.mergeAt(other, row, col);\n-\t}\n-\t\n-\t@Override\n-\tpublic void updateIndexRanges(long[] beginDims, long[] endDims) {\n-\t\tfor(Encoder enc : _encoders) {\n-\t\t\tenc.updateIndexRanges(beginDims, endDims);\n-\t\t}\n-\t}\n-\t\n-\t@Override\n-\tpublic FrameBlock getMetaData(FrameBlock out) {\n-\t\tif( _meta != null )\n-\t\t\treturn _meta;\n-\t\tfor( Encoder encoder : _encoders )\n-\t\t\tencoder.getMetaData(out);\n-\t\treturn out;\n-\t}\n-\t\n-\t@Override\n-\tpublic void initMetaData(FrameBlock out) {\n-\t\tfor( Encoder encoder : _encoders )\n-\t\t\tencoder.initMetaData(out);\n-\t}\n-\t\n-\t@Override\n-\tpublic MatrixBlock getColMapping(FrameBlock meta, MatrixBlock out) {\n-\t\t//determine if dummycode encoder exists\n-\t\tEncoderDummycode dummy = null;\n-\t\tfor( Encoder encoder : _encoders )\n-\t\t\tif( encoder instanceof EncoderDummycode )\n-\t\t\t\tdummy = (EncoderDummycode) encoder;\n-\t\t//computed shifted start positions\n-\t\tif( dummy != null ) {\n-\t\t\t//delete to dummycode encoder\n-\t\t\tout = dummy.getColMapping(meta, out);\n-\t\t}\n-\t\t//use simple 1-1 mapping\n-\t\telse {\n-\t\t\tfor(int i=0; i<out.getNumRows(); i++) {\n-\t\t\t\tout.quickSetValue(i, 0, i+1);\n-\t\t\t\tout.quickSetValue(i, 1, i+1);\n-\t\t\t\tout.quickSetValue(i, 2, i+1);\n-\t\t\t}\n-\t\t}\n-\t\t\n-\t\treturn out;\n-\t}\n-\t\n-\t@Override\n-\tpublic String toString() {\n-\t\tStringBuilder sb = new StringBuilder();\n-\t\tsb.append(\"CompositeEncoder(\"+_encoders.size()+\"):\\n\");\n-\t\tfor( Encoder encoder : _encoders ) {\n-\t\t\tsb.append(\"-- \");\n-\t\t\tsb.append(encoder.getClass().getSimpleName());\n-\t\t\tsb.append(\": \");\n-\t\t\tsb.append(Arrays.toString(encoder.getColList()));\n-\t\t\tsb.append(\"\\n\");\n-\t\t}\n-\t\treturn sb.toString();\n-\t}\n-\n-\t@Override\n-\tpublic void writeExternal(ObjectOutput out) throws IOException {\n-\t\tout.writeInt(_encoders.size());\n-\t\tfor(Encoder encoder : _encoders) {\n-\t\t\tout.writeByte(EncoderFactory.getEncoderType(encoder));\n-\t\t\tencoder.writeExternal(out);\n-\t\t}\n-\t\tout.writeBoolean(_meta != null);\n-\t\tif(_meta != null)\n-\t\t\t_meta.write(out);\n-\t}\n-\n-\t@Override\n-\tpublic void readExternal(ObjectInput in) throws IOException {\n-\t\tint encodersSize = in.readInt();\n-\t\t_encoders = new ArrayList<>();\n-\t\tfor(int i = 0; i < encodersSize; i++) {\n-\t\t\tEncoder encoder = EncoderFactory.createInstance(in.readByte());\n-\t\t\tencoder.readExternal(in);\n-\t\t\t_encoders.add(encoder);\n-\t\t}\n-\t\tif (in.readBoolean()) {\n-\t\t\tFrameBlock meta = new FrameBlock();\n-\t\t\tmeta.readFields(in);\n-\t\t\t_meta = meta;\n-\t\t}\n-\t}\n-}"
  },
  {
    "sha": "7fddecbc6617da1d95655b4cd77e178ad530a1f8",
    "filename": "src/main/java/org/apache/sysds/runtime/transform/encode/EncoderDummycode.java",
    "status": "removed",
    "additions": 0,
    "deletions": 277,
    "changes": 277,
    "blob_url": "https://github.com/apache/systemds/blob/1b77f5e81a6d0981fa76c17f66937e94e1322785/src/main/java/org/apache/sysds/runtime/transform/encode/EncoderDummycode.java",
    "raw_url": "https://github.com/apache/systemds/raw/1b77f5e81a6d0981fa76c17f66937e94e1322785/src/main/java/org/apache/sysds/runtime/transform/encode/EncoderDummycode.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/transform/encode/EncoderDummycode.java?ref=1b77f5e81a6d0981fa76c17f66937e94e1322785",
    "patch": "@@ -1,277 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- * \n- *   http://www.apache.org/licenses/LICENSE-2.0\n- * \n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.apache.sysds.runtime.transform.encode;\n-\n-import java.io.IOException;\n-import java.io.ObjectInput;\n-import java.io.ObjectOutput;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Objects;\n-\n-import org.apache.sysds.runtime.matrix.data.FrameBlock;\n-import org.apache.sysds.runtime.matrix.data.MatrixBlock;\n-import org.apache.sysds.runtime.transform.TfUtils.TfMethod;\n-import org.apache.sysds.runtime.transform.meta.TfMetaUtils;\n-import org.apache.sysds.runtime.util.IndexRange;\n-import org.apache.wink.json4j.JSONException;\n-import org.apache.wink.json4j.JSONObject;\n-\n-public class EncoderDummycode extends Encoder \n-{\n-\tprivate static final long serialVersionUID = 5832130477659116489L;\n-\n-\tpublic int[] _domainSizes = null;  // length = #of dummycoded columns\n-\tprivate long _dummycodedLength = 0; // #of columns after dummycoded\n-\n-\tpublic EncoderDummycode(JSONObject parsedSpec, String[] colnames, int clen, int minCol, int maxCol)\n-\t\tthrows JSONException {\n-\t\tsuper(null, clen);\n-\n-\t\tif(parsedSpec.containsKey(TfMethod.DUMMYCODE.toString())) {\n-\t\t\tint[] collist = TfMetaUtils\n-\t\t\t\t.parseJsonIDList(parsedSpec, colnames, TfMethod.DUMMYCODE.toString(), minCol, maxCol);\n-\t\t\tinitColList(collist);\n-\t\t}\n-\t}\n-\n-\tpublic EncoderDummycode() {\n-\t\tsuper(new int[0], 0);\n-\t}\n-\t\n-\tpublic EncoderDummycode(int[] colList, int clen, int[] domainSizes, long dummycodedLength) {\n-\t\tsuper(colList, clen);\n-\t\t_domainSizes = domainSizes;\n-\t\t_dummycodedLength = dummycodedLength;\n-\t}\n-\t\n-\t@Override\n-\tpublic int getNumCols() {\n-\t\treturn (int)_dummycodedLength;\n-\t}\n-\t\n-\t@Override\n-\tpublic MatrixBlock encode(FrameBlock in, MatrixBlock out) {\n-\t\treturn apply(in, out);\n-\t}\n-\n-\t@Override\n-\tpublic void build(FrameBlock in) {\n-\t\t//do nothing\n-\t}\n-\t\n-\t@Override\n-\tpublic MatrixBlock apply(FrameBlock in, MatrixBlock out) {\n-\t\t//allocate output in dense or sparse representation\n-\t\tfinal boolean sparse = MatrixBlock.evalSparseFormatInMemory(\n-\t\t\tout.getNumRows(), getNumCols(), out.getNonZeros());\n-\t\tMatrixBlock ret = new MatrixBlock(out.getNumRows(), getNumCols(), sparse);\n-\t\t\n-\t\t//append dummy coded or unchanged values to output\n-\t\tfinal int clen = out.getNumColumns();\n-\t\tfor( int i=0; i<out.getNumRows(); i++ ) {\n-\t\t\tfor(int colID=1, idx=0, ncolID=1; colID <= clen; colID++) {\n-\t\t\t\tdouble val = out.quickGetValue(i, colID-1);\n-\t\t\t\tif( idx < _colList.length && colID==_colList[idx] ) {\n-\t\t\t\t\tret.appendValue(i, ncolID-1+(int)val-1, 1);\n-\t\t\t\t\tncolID += _domainSizes[idx];\n-\t\t\t\t\tidx ++;\n-\t\t\t\t}\n-\t\t\t\telse {\n-\t\t\t\t\tdouble ptval = out.quickGetValue(i, colID-1);\n-\t\t\t\t\tret.appendValue(i, ncolID-1, ptval);\n-\t\t\t\t\tncolID ++;\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t\treturn ret;\n-\t}\n-\n-\t@Override\n-\tpublic Encoder subRangeEncoder(IndexRange ixRange) {\n-\t\tList<Integer> cols = new ArrayList<>();\n-\t\tList<Integer> domainSizes = new ArrayList<>();\n-\t\tint newDummycodedLength = (int) ixRange.colSpan();\n-\t\tfor(int i = 0; i < _colList.length; i++) {\n-\t\t\tint col = _colList[i];\n-\t\t\tif(ixRange.inColRange(col)) {\n-\t\t\t\t// add the correct column, removed columns before start\n-\t\t\t\t// colStart - 1 because colStart is 1-based\n-\t\t\t\tint corrColumn = (int) (col - (ixRange.colStart - 1));\n-\t\t\t\tcols.add(corrColumn);\n-\t\t\t\tdomainSizes.add(_domainSizes[i]);\n-\t\t\t\tnewDummycodedLength += _domainSizes[i] - 1;\n-\t\t\t}\n-\t\t}\n-\t\tif(cols.isEmpty())\n-\t\t\t// empty encoder -> sub range encoder does not exist\n-\t\t\treturn null;\n-\n-\t\treturn new EncoderDummycode(cols.stream().mapToInt(i -> i).toArray(), (int) ixRange.colSpan(),\n-\t\t\tdomainSizes.stream().mapToInt(i -> i).toArray(), newDummycodedLength);\n-\t}\n-\n-\t@Override\n-\tpublic void mergeAt(Encoder other, int row, int col) {\n-\t\tif(other instanceof EncoderDummycode) {\n-\t\t\tmergeColumnInfo(other, col);\n-\n-\t\t\t_domainSizes = new int[_colList.length];\n-\t\t\t_dummycodedLength = _clen;\n-\t\t\t// temporary, will be updated later\n-\t\t\tArrays.fill(_domainSizes, 0, _colList.length, 1);\n-\t\t\treturn;\n-\t\t}\n-\t\tsuper.mergeAt(other, row, col);\n-\t}\n-\t\n-\t@Override\n-\tpublic void updateIndexRanges(long[] beginDims, long[] endDims) {\n-\t\tlong[] initialBegin = Arrays.copyOf(beginDims, beginDims.length);\n-\t\tlong[] initialEnd = Arrays.copyOf(endDims, endDims.length);\n-\t\tfor(int i = 0; i < _colList.length; i++) {\n-\t\t\t// 1-based vs 0-based\n-\t\t\tif(_colList[i] < initialBegin[1] + 1) {\n-\t\t\t\t// new columns inserted left of the columns of this partial (federated) block\n-\t\t\t\tbeginDims[1] += _domainSizes[i] - 1;\n-\t\t\t\tendDims[1] += _domainSizes[i] - 1;\n-\t\t\t}\n-\t\t\telse if(_colList[i] < initialEnd[1] + 1) {\n-\t\t\t\t// new columns inserted in this (federated) block\n-\t\t\t\tendDims[1] += _domainSizes[i] - 1;\n-\t\t\t}\n-\t\t}\n-\t}\n-\t\n-\tpublic void updateDomainSizes(List<Encoder> encoders) {\n-\t\tif(_colList == null)\n-\t\t\treturn;\n-\n-\t\t// maps the column ids of the columns encoded by this Dummycode Encoder to their respective indexes\n-\t\t// in the _colList\n-\t\tMap<Integer, Integer> colIDToIxMap = new HashMap<>();\n-\t\tfor (int i = 0; i < _colList.length; i++)\n-\t\t\tcolIDToIxMap.put(_colList[i], i);\n-\t\t\n-\t\t_dummycodedLength = _clen;\n-\t\tfor (Encoder encoder : encoders) {\n-\t\t\tint[] distinct = null;\n-\t\t\tif (encoder instanceof EncoderRecode) {\n-\t\t\t\tEncoderRecode encoderRecode = (EncoderRecode) encoder;\n-\t\t\t\tdistinct = encoderRecode.numDistinctValues();\n-\t\t\t}\n-\t\t\telse if (encoder instanceof EncoderBin) {\n-\t\t\t\tdistinct = ((EncoderBin) encoder)._numBins;\n-\t\t\t}\n-\t\t\t\n-\t\t\tif (distinct != null) {\n-\t\t\t\t// search for match of encoded columns\n-\t\t\t\tfor (int i = 0; i < encoder._colList.length; i++) {\n-\t\t\t\t\tInteger ix = colIDToIxMap.get(encoder._colList[i]);\n-\t\t\t\t\t\n-\t\t\t\t\tif (ix != null) {\n-\t\t\t\t\t\t// set size\n-\t\t\t\t\t\t_domainSizes[ix] = distinct[i];\n-\t\t\t\t\t\t_dummycodedLength += _domainSizes[ix] - 1;\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t@Override\n-\tpublic FrameBlock getMetaData(FrameBlock out) {\n-\t\treturn out;\n-\t}\n-\t\n-\t@Override\n-\tpublic void initMetaData(FrameBlock meta) {\n-\t\t//initialize domain sizes and output num columns\n-\t\t_domainSizes = new int[_colList.length];\n-\t\t_dummycodedLength = _clen;\n-\t\tfor( int j=0; j<_colList.length; j++ ) {\n-\t\t\tint colID = _colList[j]; //1-based\n-\t\t\t_domainSizes[j] = (int)meta.getColumnMetadata()[colID-1].getNumDistinct();\n-\t\t\t_dummycodedLength += _domainSizes[j]-1;\n-\t\t}\n-\t}\n-\t\n-\t@Override\n-\tpublic MatrixBlock getColMapping(FrameBlock meta, MatrixBlock out) {\n-\t\tfinal int clen = out.getNumRows();\n-\t\tfor(int colID=1, idx=0, ncolID=1; colID <= clen; colID++) {\n-\t\t\tint start = ncolID;\n-\t\t\tif( idx < _colList.length && colID==_colList[idx] ) {\n-\t\t\t\tncolID += meta.getColumnMetadata(colID-1).getNumDistinct();\n-\t\t\t\tidx ++;\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\tncolID ++;\n-\t\t\t}\n-\t\t\tout.quickSetValue(colID-1, 0, colID);\n-\t\t\tout.quickSetValue(colID-1, 1, start);\n-\t\t\tout.quickSetValue(colID-1, 2, ncolID-1);\n-\t\t}\n-\t\t\n-\t\treturn out;\n-\t}\n-\n-\t@Override\n-\tpublic void writeExternal(ObjectOutput out) throws IOException {\n-\t\tsuper.writeExternal(out);\n-\t\tout.writeLong(_dummycodedLength);\n-\t\tint size1 = _domainSizes == null ? 0 : _domainSizes.length;\n-\t\tout.writeInt(size1);\n-\t\tfor(int i = 0; i < size1; i++)\n-\t\t\tout.writeInt(_domainSizes[i]);\n-\t}\n-\n-\t@Override\n-\tpublic void readExternal(ObjectInput in) throws IOException {\n-\t\tsuper.readExternal(in);\n-\t\t_dummycodedLength = in.readLong();\n-\t\tif(_domainSizes == null || _domainSizes.length == 0) {\n-\t\t\t_domainSizes = new int[in.readInt()];\n-\t\t\tfor(int i = 0; i < _domainSizes.length; i++)\n-\t\t\t\t_domainSizes[i] = in.readInt();\n-\t\t}\n-\t}\n-\n-\t@Override\n-\tpublic boolean equals(Object o) {\n-\t\tif(this == o)\n-\t\t\treturn true;\n-\t\tif(o == null || getClass() != o.getClass())\n-\t\t\treturn false;\n-\t\tEncoderDummycode that = (EncoderDummycode) o;\n-\t\treturn _dummycodedLength == that._dummycodedLength\n-\t\t\t&& Arrays.equals(_domainSizes, that._domainSizes);\n-\t}\n-\n-\t@Override\n-\tpublic int hashCode() {\n-\t\tint result = Objects.hash(_dummycodedLength);\n-\t\tresult = 31 * result + Arrays.hashCode(_domainSizes);\n-\t\treturn result;\n-\t}\n-}"
  },
  {
    "sha": "4b48d2a09fd8881345ccad00fe600c9c97e0a074",
    "filename": "src/main/java/org/apache/sysds/runtime/transform/encode/EncoderFactory.java",
    "status": "modified",
    "additions": 122,
    "deletions": 103,
    "changes": 225,
    "blob_url": "https://github.com/apache/systemds/blob/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/encode/EncoderFactory.java",
    "raw_url": "https://github.com/apache/systemds/raw/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/encode/EncoderFactory.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/transform/encode/EncoderFactory.java?ref=cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd",
    "patch": "@@ -19,124 +19,137 @@\n \n package org.apache.sysds.runtime.transform.encode;\n \n+import static org.apache.sysds.runtime.util.CollectionUtils.except;\n+import static org.apache.sysds.runtime.util.CollectionUtils.unionDistinct;\n+\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.HashMap;\n import java.util.List;\n+import java.util.Map.Entry;\n \n import org.apache.commons.lang.ArrayUtils;\n-import org.apache.wink.json4j.JSONObject;\n import org.apache.sysds.common.Types.ValueType;\n import org.apache.sysds.runtime.DMLRuntimeException;\n import org.apache.sysds.runtime.matrix.data.FrameBlock;\n import org.apache.sysds.runtime.transform.TfUtils.TfMethod;\n+import org.apache.sysds.runtime.transform.encode.ColumnEncoder.EncoderType;\n import org.apache.sysds.runtime.transform.meta.TfMetaUtils;\n import org.apache.sysds.runtime.util.UtilFunctions;\n-import static org.apache.sysds.runtime.util.CollectionUtils.except;\n-import static org.apache.sysds.runtime.util.CollectionUtils.unionDistinct;\n+import org.apache.wink.json4j.JSONArray;\n+import org.apache.wink.json4j.JSONObject;\n \n+public class EncoderFactory {\n \n-public class EncoderFactory \n-{\n-\tpublic enum EncoderType {\n-\t\tBin,\n-\t\tDummycode,\n-\t\tFeatureHash,\n-\t\tMVImpute,\n-\t\tOmit,\n-\t\tPassThrough,\n-\t\tRecode\n-\t};\n-\n-\t\n-\tpublic static Encoder createEncoder(String spec, String[] colnames, int clen, FrameBlock meta) {\n+\tpublic static MultiColumnEncoder createEncoder(String spec, String[] colnames, int clen, FrameBlock meta) {\n \t\treturn createEncoder(spec, colnames, UtilFunctions.nCopies(clen, ValueType.STRING), meta);\n \t}\n-\t\n-\tpublic static Encoder createEncoder(String spec, String[] colnames, int clen, FrameBlock meta, int minCol,\n-\t\tint maxCol) {\n+\n+\tpublic static MultiColumnEncoder createEncoder(String spec, String[] colnames, int clen, FrameBlock meta,\n+\t\tint minCol, int maxCol) {\n \t\treturn createEncoder(spec, colnames, UtilFunctions.nCopies(clen, ValueType.STRING), meta, minCol, maxCol);\n \t}\n \n-\tpublic static Encoder createEncoder(String spec, String[] colnames, ValueType[] schema, int clen, FrameBlock meta) {\n-\t\tValueType[] lschema = (schema==null) ? UtilFunctions.nCopies(clen, ValueType.STRING) : schema;\n+\tpublic static MultiColumnEncoder createEncoder(String spec, String[] colnames, ValueType[] schema, int clen,\n+\t\tFrameBlock meta) {\n+\t\tValueType[] lschema = (schema == null) ? UtilFunctions.nCopies(clen, ValueType.STRING) : schema;\n \t\treturn createEncoder(spec, colnames, lschema, meta);\n \t}\n-\t\n-\tpublic static Encoder createEncoder(String spec, String[] colnames, ValueType[] schema, FrameBlock meta) {\n+\n+\tpublic static MultiColumnEncoder createEncoder(String spec, String[] colnames, ValueType[] schema,\n+\t\tFrameBlock meta) {\n \t\treturn createEncoder(spec, colnames, schema, meta, -1, -1);\n \t}\n-\t\n-\tpublic static Encoder createEncoder(String spec, String[] colnames, ValueType[] schema, FrameBlock meta, int minCol,\n-\t\tint maxCol) {\n-\t\tEncoder encoder = null;\n+\n+\tpublic static MultiColumnEncoder createEncoder(String spec, String[] colnames, ValueType[] schema, FrameBlock meta,\n+\t\tint minCol, int maxCol) {\n+\t\tMultiColumnEncoder encoder;\n \t\tint clen = schema.length;\n-\t\t\n+\n \t\ttry {\n-\t\t\t//parse transform specification\n+\t\t\t// parse transform specification\n \t\t\tJSONObject jSpec = new JSONObject(spec);\n-\t\t\tList<Encoder> lencoders = new ArrayList<>();\n-\t\t\t\n-\t\t\t//prepare basic id lists (recode, feature hash, dummycode, pass-through)\n-\t\t\tList<Integer> rcIDs = Arrays.asList(ArrayUtils.toObject(\n-\t\t\t\tTfMetaUtils.parseJsonIDList(jSpec, colnames, TfMethod.RECODE.toString(), minCol, maxCol)));\n-\t\t\tList<Integer>haIDs = Arrays.asList(ArrayUtils.toObject(\n-\t\t\t\tTfMetaUtils.parseJsonIDList(jSpec, colnames, TfMethod.HASH.toString(), minCol, maxCol)));\n-\t\t\tList<Integer> dcIDs = Arrays.asList(ArrayUtils.toObject(\n-\t\t\t\tTfMetaUtils.parseJsonIDList(jSpec, colnames, TfMethod.DUMMYCODE.toString(), minCol, maxCol)));\n+\t\t\tList<ColumnEncoderComposite> lencoders = new ArrayList<>();\n+\t\t\tHashMap<Integer, List<ColumnEncoder>> colEncoders = new HashMap<>();\n+\t\t\tboolean ids = jSpec.containsKey(\"ids\") && jSpec.getBoolean(\"ids\");\n+\n+\t\t\t// prepare basic id lists (recode, feature hash, dummycode, pass-through)\n+\t\t\tList<Integer> rcIDs = Arrays.asList(ArrayUtils\n+\t\t\t\t.toObject(TfMetaUtils.parseJsonIDList(jSpec, colnames, TfMethod.RECODE.toString(), minCol, maxCol)));\n+\t\t\tList<Integer> haIDs = Arrays.asList(ArrayUtils\n+\t\t\t\t.toObject(TfMetaUtils.parseJsonIDList(jSpec, colnames, TfMethod.HASH.toString(), minCol, maxCol)));\n+\t\t\tList<Integer> dcIDs = Arrays.asList(ArrayUtils\n+\t\t\t\t.toObject(TfMetaUtils.parseJsonIDList(jSpec, colnames, TfMethod.DUMMYCODE.toString(), minCol, maxCol)));\n \t\t\tList<Integer> binIDs = TfMetaUtils.parseBinningColIDs(jSpec, colnames, minCol, maxCol);\n-\t\t\t//note: any dummycode column requires recode as preparation, unless it follows binning\n+\t\t\t// note: any dummycode column requires recode as preparation, unless it follows binning\n \t\t\trcIDs = except(unionDistinct(rcIDs, except(dcIDs, binIDs)), haIDs);\n-\t\t\tList<Integer> ptIDs = except(except(UtilFunctions.getSeqList(1, clen, 1),\n-\t\t\t\tunionDistinct(rcIDs,haIDs)), binIDs);\n-\t\t\tList<Integer> oIDs = Arrays.asList(ArrayUtils.toObject(\n-\t\t\t\tTfMetaUtils.parseJsonIDList(jSpec, colnames, TfMethod.OMIT.toString(), minCol, maxCol)));\n+\t\t\tList<Integer> ptIDs = except(except(UtilFunctions.getSeqList(1, clen, 1), unionDistinct(rcIDs, haIDs)),\n+\t\t\t\tbinIDs);\n+\t\t\tList<Integer> oIDs = Arrays.asList(ArrayUtils\n+\t\t\t\t.toObject(TfMetaUtils.parseJsonIDList(jSpec, colnames, TfMethod.OMIT.toString(), minCol, maxCol)));\n \t\t\tList<Integer> mvIDs = Arrays.asList(ArrayUtils.toObject(\n \t\t\t\tTfMetaUtils.parseJsonObjectIDList(jSpec, colnames, TfMethod.IMPUTE.toString(), minCol, maxCol)));\n-\t\t\t\n-\t\t\t//create individual encoders\n-\t\t\tif( !rcIDs.isEmpty() ) {\n-\t\t\t\tEncoderRecode ra = new EncoderRecode(jSpec, colnames, clen, minCol, maxCol);\n-\t\t\t\tra.setColList(ArrayUtils.toPrimitive(rcIDs.toArray(new Integer[0])));\n-\t\t\t\tlencoders.add(ra);\n+\n+\t\t\t// create individual encoders\n+\t\t\tif(!rcIDs.isEmpty()) {\n+\t\t\t\tfor(Integer id : rcIDs) {\n+\t\t\t\t\tColumnEncoderRecode ra = new ColumnEncoderRecode(id);\n+\t\t\t\t\taddEncoderToMap(ra, colEncoders);\n+\t\t\t\t}\n \t\t\t}\n-\t\t\tif( !haIDs.isEmpty() ) {\n-\t\t\t\tEncoderFeatureHash ha = new EncoderFeatureHash(jSpec, colnames, clen, minCol, maxCol);\n-\t\t\t\tha.setColList(ArrayUtils.toPrimitive(haIDs.toArray(new Integer[0])));\n-\t\t\t\tlencoders.add(ha);\n+\t\t\tif(!haIDs.isEmpty()) {\n+\t\t\t\tfor(Integer id : haIDs) {\n+\t\t\t\t\tColumnEncoderFeatureHash ha = new ColumnEncoderFeatureHash(id, TfMetaUtils.getK(jSpec));\n+\t\t\t\t\taddEncoderToMap(ha, colEncoders);\n+\t\t\t\t}\n \t\t\t}\n-\t\t\tif( !ptIDs.isEmpty() )\n-\t\t\t\tlencoders.add(new EncoderPassThrough(\n-\t\t\t\t\t\tArrayUtils.toPrimitive(ptIDs.toArray(new Integer[0])), clen));\n-\t\t\tif( !binIDs.isEmpty() )\n-\t\t\t\tlencoders.add(new EncoderBin(jSpec, colnames, schema.length, minCol, maxCol));\n-\t\t\tif( !dcIDs.isEmpty() )\n-\t\t\t\tlencoders.add(new EncoderDummycode(jSpec, colnames, schema.length, minCol, maxCol));\n-\t\t\tif( !oIDs.isEmpty() )\n-\t\t\t\tlencoders.add(new EncoderOmit(jSpec, colnames, schema.length, minCol, maxCol));\n-\t\t\tif( !mvIDs.isEmpty() ) {\n+\t\t\tif(!ptIDs.isEmpty())\n+\t\t\t\tfor(Integer id : ptIDs) {\n+\t\t\t\t\tColumnEncoderPassThrough pt = new ColumnEncoderPassThrough(id);\n+\t\t\t\t\taddEncoderToMap(pt, colEncoders);\n+\t\t\t\t}\n+\t\t\tif(!binIDs.isEmpty())\n+\t\t\t\tfor(Object o : (JSONArray) jSpec.get(TfMethod.BIN.toString())) {\n+\t\t\t\t\tJSONObject colspec = (JSONObject) o;\n+\t\t\t\t\tint numBins = colspec.containsKey(\"numbins\") ? colspec.getInt(\"numbins\") : 1;\n+\t\t\t\t\tint id = TfMetaUtils.parseJsonObjectID(colspec, colnames, minCol, maxCol, ids);\n+\t\t\t\t\tif(id <= 0)\n+\t\t\t\t\t\tcontinue;\n+\t\t\t\t\tColumnEncoderBin bin = new ColumnEncoderBin(id, numBins);\n+\t\t\t\t\taddEncoderToMap(bin, colEncoders);\n+\t\t\t\t}\n+\t\t\tif(!dcIDs.isEmpty())\n+\t\t\t\tfor(Integer id : dcIDs) {\n+\t\t\t\t\tColumnEncoderDummycode dc = new ColumnEncoderDummycode(id);\n+\t\t\t\t\taddEncoderToMap(dc, colEncoders);\n+\t\t\t\t}\n+\t\t\t// create composite decoder of all created encoders\n+\t\t\tfor(Entry<Integer, List<ColumnEncoder>> listEntry : colEncoders.entrySet()) {\n+\t\t\t\tlencoders.add(new ColumnEncoderComposite(listEntry.getValue()));\n+\t\t\t}\n+\t\t\tencoder = new MultiColumnEncoder(lencoders);\n+\t\t\tif(!oIDs.isEmpty()) {\n+\t\t\t\tencoder.addReplaceLegacyEncoder(new EncoderOmit(jSpec, colnames, schema.length, minCol, maxCol));\n+\t\t\t}\n+\t\t\tif(!mvIDs.isEmpty()) {\n \t\t\t\tEncoderMVImpute ma = new EncoderMVImpute(jSpec, colnames, schema.length, minCol, maxCol);\n \t\t\t\tma.initRecodeIDList(rcIDs);\n-\t\t\t\tlencoders.add(ma);\n+\t\t\t\tencoder.addReplaceLegacyEncoder(ma);\n \t\t\t}\n-\t\t\t\n-\t\t\t//create composite decoder of all created encoders\n-\t\t\tencoder = new EncoderComposite(lencoders);\n-\t\t\t\n-\t\t\t//initialize meta data w/ robustness for superset of cols\n-\t\t\tif( meta != null ) {\n+\n+\t\t\t// initialize meta data w/ robustness for superset of cols\n+\t\t\tif(meta != null) {\n \t\t\t\tString[] colnames2 = meta.getColumnNames();\n-\t\t\t\tif( !TfMetaUtils.isIDSpec(jSpec) && colnames!=null && colnames2!=null\n-\t\t\t\t\t&& !ArrayUtils.isEquals(colnames, colnames2) )\n-\t\t\t\t{\n+\t\t\t\tif(!TfMetaUtils.isIDSpec(jSpec) && colnames != null && colnames2 != null &&\n+\t\t\t\t\t!ArrayUtils.isEquals(colnames, colnames2)) {\n \t\t\t\t\tHashMap<String, Integer> colPos = getColumnPositions(colnames2);\n-\t\t\t\t\t//create temporary meta frame block w/ shallow column copy\n+\t\t\t\t\t// create temporary meta frame block w/ shallow column copy\n \t\t\t\t\tFrameBlock meta2 = new FrameBlock(meta.getSchema(), colnames2);\n \t\t\t\t\tmeta2.setNumRows(meta.getNumRows());\n-\t\t\t\t\tfor( int i=0; i<colnames.length; i++ ) {\n-\t\t\t\t\t\tif( !colPos.containsKey(colnames[i]) ) {\n-\t\t\t\t\t\t\tthrow new DMLRuntimeException(\"Column name not found in meta data: \"\n-\t\t\t\t\t\t\t\t+ colnames[i]+\" (meta: \"+Arrays.toString(colnames2)+\")\");\n+\t\t\t\t\tfor(int i = 0; i < colnames.length; i++) {\n+\t\t\t\t\t\tif(!colPos.containsKey(colnames[i])) {\n+\t\t\t\t\t\t\tthrow new DMLRuntimeException(\"Column name not found in meta data: \" + colnames[i]\n+\t\t\t\t\t\t\t\t+ \" (meta: \" + Arrays.toString(colnames2) + \")\");\n \t\t\t\t\t\t}\n \t\t\t\t\t\tint pos = colPos.get(colnames[i]);\n \t\t\t\t\t\tmeta2.setColumn(i, meta.getColumn(pos));\n@@ -152,45 +165,51 @@ public static Encoder createEncoder(String spec, String[] colnames, ValueType[]\n \t\t}\n \t\treturn encoder;\n \t}\n-\t\n-\tpublic static int getEncoderType(Encoder encoder) {\n-\t\tif( encoder instanceof EncoderBin )\n+\n+\tprivate static void addEncoderToMap(ColumnEncoder encoder, HashMap<Integer, List<ColumnEncoder>> map) {\n+\t\tif(!map.containsKey(encoder._colID)) {\n+\t\t\tmap.put(encoder._colID, new ArrayList<>());\n+\t\t}\n+\t\tmap.get(encoder._colID).add(encoder);\n+\t}\n+\n+\tpublic static int getEncoderType(ColumnEncoder columnEncoder) {\n+\t\tif(columnEncoder instanceof ColumnEncoderBin)\n \t\t\treturn EncoderType.Bin.ordinal();\n-\t\telse if( encoder instanceof EncoderDummycode )\n+\t\telse if(columnEncoder instanceof ColumnEncoderDummycode)\n \t\t\treturn EncoderType.Dummycode.ordinal();\n-\t\telse if( encoder instanceof EncoderFeatureHash )\n+\t\telse if(columnEncoder instanceof ColumnEncoderFeatureHash)\n \t\t\treturn EncoderType.FeatureHash.ordinal();\n-\t\telse if( encoder instanceof EncoderMVImpute )\n-\t\t\treturn EncoderType.MVImpute.ordinal();\n-\t\telse if( encoder instanceof EncoderOmit )\n-\t\t\treturn EncoderType.Omit.ordinal();\n-\t\telse if( encoder instanceof EncoderPassThrough )\n+\t\telse if(columnEncoder instanceof ColumnEncoderPassThrough)\n \t\t\treturn EncoderType.PassThrough.ordinal();\n-\t\telse if( encoder instanceof EncoderRecode )\n+\t\telse if(columnEncoder instanceof ColumnEncoderRecode)\n \t\t\treturn EncoderType.Recode.ordinal();\n-\t\tthrow new DMLRuntimeException(\"Unsupported encoder type: \"\n-\t\t\t+ encoder.getClass().getCanonicalName());\n+\t\tthrow new DMLRuntimeException(\"Unsupported encoder type: \" + columnEncoder.getClass().getCanonicalName());\n \t}\n-\t\n-\tpublic static Encoder createInstance(int type) {\n+\n+\tpublic static ColumnEncoder createInstance(int type) {\n \t\tEncoderType etype = EncoderType.values()[type];\n \t\tswitch(etype) {\n-\t\t\tcase Bin:         return new EncoderBin();\n-\t\t\tcase Dummycode:   return new EncoderDummycode();\n-\t\t\tcase FeatureHash: return new EncoderFeatureHash();\n-\t\t\tcase MVImpute:    return new EncoderMVImpute();\n-\t\t\tcase Omit:        return new EncoderOmit();\n-\t\t\tcase PassThrough: return new EncoderPassThrough();\n-\t\t\tcase Recode:      return new EncoderRecode();\n+\t\t\tcase Bin:\n+\t\t\t\treturn new ColumnEncoderBin();\n+\t\t\tcase Dummycode:\n+\t\t\t\treturn new ColumnEncoderDummycode();\n+\t\t\tcase FeatureHash:\n+\t\t\t\treturn new ColumnEncoderFeatureHash();\n+\t\t\tcase PassThrough:\n+\t\t\t\treturn new ColumnEncoderPassThrough();\n+\t\t\tcase Recode:\n+\t\t\t\treturn new ColumnEncoderRecode();\n \t\t\tdefault:\n \t\t\t\tthrow new DMLRuntimeException(\"Unsupported encoder type: \" + etype);\n \t\t}\n \t}\n-\t\n+\n \tprivate static HashMap<String, Integer> getColumnPositions(String[] colnames) {\n \t\tHashMap<String, Integer> ret = new HashMap<>();\n-\t\tfor(int i=0; i<colnames.length; i++)\n+\t\tfor(int i = 0; i < colnames.length; i++)\n \t\t\tret.put(colnames[i], i);\n \t\treturn ret;\n \t}\n+\n }"
  },
  {
    "sha": "cf6d67af02f778652e929e24e387952183f1d3df",
    "filename": "src/main/java/org/apache/sysds/runtime/transform/encode/EncoderFeatureHash.java",
    "status": "removed",
    "additions": 0,
    "deletions": 163,
    "changes": 163,
    "blob_url": "https://github.com/apache/systemds/blob/1b77f5e81a6d0981fa76c17f66937e94e1322785/src/main/java/org/apache/sysds/runtime/transform/encode/EncoderFeatureHash.java",
    "raw_url": "https://github.com/apache/systemds/raw/1b77f5e81a6d0981fa76c17f66937e94e1322785/src/main/java/org/apache/sysds/runtime/transform/encode/EncoderFeatureHash.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/transform/encode/EncoderFeatureHash.java?ref=1b77f5e81a6d0981fa76c17f66937e94e1322785",
    "patch": "@@ -1,163 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.apache.sysds.runtime.transform.encode;\n-\n-import java.io.IOException;\n-import java.io.ObjectInput;\n-import java.io.ObjectOutput;\n-\n-import org.apache.sysds.runtime.matrix.data.FrameBlock;\n-import org.apache.sysds.runtime.matrix.data.MatrixBlock;\n-import org.apache.sysds.runtime.transform.TfUtils.TfMethod;\n-import org.apache.sysds.runtime.transform.meta.TfMetaUtils;\n-import org.apache.sysds.runtime.util.IndexRange;\n-import org.apache.sysds.runtime.util.UtilFunctions;\n-import org.apache.wink.json4j.JSONException;\n-import org.apache.wink.json4j.JSONObject;\n-\n-/**\n- * Class used for feature hashing transformation of frames. \n- */\n-public class EncoderFeatureHash extends Encoder\n-{\n-\tprivate static final long serialVersionUID = 7435806042138687342L;\n-\tprivate long _K;\n-\n-\tpublic EncoderFeatureHash(JSONObject parsedSpec, String[] colnames, int clen, int minCol, int maxCol)\n-\t\tthrows JSONException {\n-\t\tsuper(null, clen);\n-\t\t_colList = TfMetaUtils.parseJsonIDList(parsedSpec, colnames, TfMethod.HASH.toString(), minCol, maxCol);\n-\t\t_K = getK(parsedSpec);\n-\t}\n-\t\n-\tpublic EncoderFeatureHash(int[] colList, int clen, long K) {\n-\t\tsuper(colList, clen);\n-\t\t_K = K;\n-\t}\n-\t\n-\tpublic EncoderFeatureHash() {\n-\t\tsuper(new int[0], 0);\n-\t\t_K = 0;\n-\t}\n-\t\n-\t/**\n-\t * Get K value used for calculation during feature hashing from parsed specifications.\n-\t * @param parsedSpec parsed specifications\n-\t * @return K value\n-\t * @throws JSONException\n-\t */\n-\tprivate static long getK(JSONObject parsedSpec) throws JSONException {\n-\t\t//TODO generalize to different k per feature\n-\t\treturn parsedSpec.getLong(\"K\");\n-\t}\n-\t\n-\tprivate long getCode(String key) {\n-\t\treturn key.hashCode() % _K;\n-\t}\n-\t\n-\t@Override\n-\tpublic MatrixBlock encode(FrameBlock in, MatrixBlock out) {\n-\t\tif( !isApplicable() )\n-\t\t\treturn out;\n-\t\t\n-\t\t//apply only\n-\t\tapply(in, out);\n-\t\t\n-\t\treturn out;\n-\t}\n-\n-\t@Override\n-\tpublic void build(FrameBlock in) {\n-\t\t//do nothing (no meta data other than K)\n-\t}\n-\n-\t@Override\n-\tpublic MatrixBlock apply(FrameBlock in, MatrixBlock out) {\n-\t\t//apply feature hashing column wise\n-\t\tfor( int j=0; j<_colList.length; j++ ) {\n-\t\t\tint colID = _colList[j];\n-\t\t\tfor( int i=0; i<in.getNumRows(); i++ ) {\n-\t\t\t\tObject okey = in.get(i, colID-1);\n-\t\t\t\tString key = (okey!=null) ? okey.toString() : null;\n-\t\t\t\tlong code = getCode(key);\n-\t\t\t\tout.quickSetValue(i, colID-1,\n-\t\t\t\t\t(code >= 0) ? code : Double.NaN);\n-\t\t\t}\n-\t\t}\n-\t\treturn out;\n-\t}\n-\t\n-\t@Override\n-\tpublic Encoder subRangeEncoder(IndexRange ixRange) {\n-\t\tint[] colList = subRangeColList(ixRange);\n-\t\tif(colList.length == 0)\n-\t\t\t// empty encoder -> sub range encoder does not exist\n-\t\t\treturn null;\n-\t\treturn new EncoderFeatureHash(colList, (int) ixRange.colSpan(), _K);\n-\t}\n-\t\n-\t@Override\n-\tpublic void mergeAt(Encoder other, int row, int col) {\n-\t\tif(other instanceof EncoderFeatureHash) {\n-\t\t\tmergeColumnInfo(other, col);\n-\t\t\tif (((EncoderFeatureHash) other)._K != 0 && _K == 0)\n-\t\t\t\t_K = ((EncoderFeatureHash) other)._K;\n-\t\t\treturn;\n-\t\t}\n-\t\tsuper.mergeAt(other, row, col);\n-\t}\n-\t\n-\t@Override\n-\tpublic FrameBlock getMetaData(FrameBlock meta) {\n-\t\tif( !isApplicable() )\n-\t\t\treturn meta;\n-\t\t\n-\t\tmeta.ensureAllocatedColumns(1);\n-\t\tfor( int j=0; j<_colList.length; j++ ) {\n-\t\t\tint colID = _colList[j]; //1-based\n-\t\t\tmeta.set(0, colID-1, String.valueOf(_K));\n-\t\t}\n-\t\t\n-\t\treturn meta;\n-\t}\n-\n-\t@Override\n-\tpublic void initMetaData( FrameBlock meta ) {\n-\t\tif( meta == null || meta.getNumRows()<=0 )\n-\t\t\treturn;\n-\t\t\n-\t\tfor( int j=0; j<_colList.length; j++ ) {\n-\t\t\tint colID = _colList[j]; //1-based\n-\t\t\t_K = UtilFunctions.parseToLong(meta.get(0, colID-1).toString());\n-\t\t}\n-\t}\n-\n-\t@Override\n-\tpublic void writeExternal(ObjectOutput out) throws IOException {\n-\t\tsuper.writeExternal(out);\n-\t\tout.writeLong(_K);\n-\t}\n-\n-\t@Override\n-\tpublic void readExternal(ObjectInput in) throws IOException {\n-\t\tsuper.readExternal(in);\n-\t\t_K = in.readLong();\n-\t}\n-}"
  },
  {
    "sha": "cda6b2af0eecf0eb1b9d9c79ed0c1a98bd833d5d",
    "filename": "src/main/java/org/apache/sysds/runtime/transform/encode/EncoderMVImpute.java",
    "status": "modified",
    "additions": 110,
    "deletions": 108,
    "changes": 218,
    "blob_url": "https://github.com/apache/systemds/blob/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/encode/EncoderMVImpute.java",
    "raw_url": "https://github.com/apache/systemds/raw/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/encode/EncoderMVImpute.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/transform/encode/EncoderMVImpute.java?ref=cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd",
    "patch": "@@ -6,9 +6,9 @@\n  * to you under the Apache License, Version 2.0 (the\n  * \"License\"); you may not use this file except in compliance\n  * with the License.  You may obtain a copy of the License at\n- * \n+ *\n  *   http://www.apache.org/licenses/LICENSE-2.0\n- * \n+ *\n  * Unless required by applicable law or agreed to in writing,\n  * software distributed under the License is distributed on an\n  * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n@@ -48,57 +48,72 @@\n import org.apache.wink.json4j.JSONException;\n import org.apache.wink.json4j.JSONObject;\n \n-public class EncoderMVImpute extends Encoder {\n+public class EncoderMVImpute extends LegacyEncoder {\n \tprivate static final long serialVersionUID = 9057868620144662194L;\n-\n-\tpublic enum MVMethod { INVALID, GLOBAL_MEAN, GLOBAL_MODE, CONSTANT }\n-\t\n+\t// objects required to compute mean and variance of all non-missing entries\n+\tprivate final Mean _meanFn = Mean.getMeanFnObject(); // function object that understands mean computation\n \tprivate MVMethod[] _mvMethodList = null;\n-\t\n-\t// objects required to compute mean and variance of all non-missing entries \n-\tprivate final Mean _meanFn = Mean.getMeanFnObject();  // function object that understands mean computation\n-\tprivate KahanObject[] _meanList = null;         // column-level means, computed so far\n-\tprivate long[] _countList = null;               // #of non-missing values\n-\t\n-\tprivate String[] _replacementList = null; // replacements: for global_mean, mean; and for global_mode, recode id of mode category\n+\tprivate KahanObject[] _meanList = null; // column-level means, computed so far\n+\tprivate long[] _countList = null; // #of non-missing values\n+\tprivate String[] _replacementList = null; // replacements: for global_mean, mean; and for global_mode, recode id of\n+\t\t\t\t\t\t\t\t\t\t\t\t// mode category\n \tprivate List<Integer> _rcList = null;\n-\tprivate HashMap<Integer,HashMap<String,Long>> _hist = null;\n-\n-\tpublic String[] getReplacements() { return _replacementList; }\n-\tpublic KahanObject[] getMeans()   { return _meanList; }\n-\t\n+\tprivate HashMap<Integer, HashMap<String, Long>> _hist = null;\n \tpublic EncoderMVImpute(JSONObject parsedSpec, String[] colnames, int clen, int minCol, int maxCol)\n-\t\t\tthrows JSONException {\n+\t\tthrows JSONException {\n \t\tsuper(null, clen);\n-\t\t\n-\t\t//handle column list\n+\n+\t\t// handle column list\n \t\tint[] collist = TfMetaUtils\n \t\t\t.parseJsonObjectIDList(parsedSpec, colnames, TfMethod.IMPUTE.toString(), minCol, maxCol);\n \t\tinitColList(collist);\n-\t\n-\t\t//handle method list\n+\n+\t\t// handle method list\n \t\tparseMethodsAndReplacements(parsedSpec, colnames, minCol);\n-\t\t\n-\t\t//create reuse histograms\n+\n+\t\t// create reuse histograms\n \t\t_hist = new HashMap<>();\n \t}\n-\t\n+\n \tpublic EncoderMVImpute() {\n \t\tsuper(new int[0], 0);\n \t}\n-\t\n-\t\n+\n \tpublic EncoderMVImpute(int[] colList, MVMethod[] mvMethodList, String[] replacementList, KahanObject[] meanList,\n-\t\t\tlong[] countList, List<Integer> rcList, int clen) {\n+\t\tlong[] countList, List<Integer> rcList, int clen) {\n \t\tsuper(colList, clen);\n \t\t_mvMethodList = mvMethodList;\n \t\t_replacementList = replacementList;\n \t\t_meanList = meanList;\n \t\t_countList = countList;\n \t\t_rcList = rcList;\n \t}\n-\t\n-\tprivate void parseMethodsAndReplacements(JSONObject parsedSpec, String[] colnames, int offset) throws JSONException {\n+\n+\tprivate static void fillListsFromMap(Map<Integer, ColInfo> map, int[] colList, MVMethod[] mvMethodList,\n+\t\tString[] replacementList, KahanObject[] meanList, long[] countList,\n+\t\tHashMap<Integer, HashMap<String, Long>> hist) {\n+\t\tint i = 0;\n+\t\tfor(Entry<Integer, ColInfo> entry : map.entrySet()) {\n+\t\t\tcolList[i] = entry.getKey();\n+\t\t\tmvMethodList[i] = entry.getValue()._method;\n+\t\t\treplacementList[i] = entry.getValue()._replacement;\n+\t\t\tmeanList[i] = entry.getValue()._mean;\n+\t\t\tcountList[i++] = entry.getValue()._count;\n+\n+\t\t\thist.put(entry.getKey(), entry.getValue()._hist);\n+\t\t}\n+\t}\n+\n+\tpublic String[] getReplacements() {\n+\t\treturn _replacementList;\n+\t}\n+\n+\tpublic KahanObject[] getMeans() {\n+\t\treturn _meanList;\n+\t}\n+\n+\tprivate void parseMethodsAndReplacements(JSONObject parsedSpec, String[] colnames, int offset)\n+\t\tthrows JSONException {\n \t\tJSONArray mvspec = (JSONArray) parsedSpec.get(TfMethod.IMPUTE.toString());\n \t\tboolean ids = parsedSpec.containsKey(\"ids\") && parsedSpec.getBoolean(\"ids\");\n \t\t// make space for all elements\n@@ -108,7 +123,7 @@ private void parseMethodsAndReplacements(JSONObject parsedSpec, String[] colname\n \t\t_countList = new long[mvspec.size()];\n \t\t// sort for binary search\n \t\tArrays.sort(_colList);\n-\t\t\n+\n \t\tint listIx = 0;\n \t\tfor(Object o : mvspec) {\n \t\t\tJSONObject mvobj = (JSONObject) o;\n@@ -131,66 +146,66 @@ private void parseMethodsAndReplacements(JSONObject parsedSpec, String[] colname\n \t\t_meanList = Arrays.copyOf(_meanList, listIx);\n \t\t_countList = Arrays.copyOf(_countList, listIx);\n \t}\n-\t\n+\n \tpublic MVMethod getMethod(int colID) {\n \t\tint idx = isApplicable(colID);\n \t\tif(idx == -1)\n \t\t\treturn MVMethod.INVALID;\n \t\telse\n \t\t\treturn _mvMethodList[idx];\n \t}\n-\t\n+\n \tpublic long getNonMVCount(int colID) {\n \t\tint idx = isApplicable(colID);\n \t\treturn (idx == -1) ? 0 : _countList[idx];\n \t}\n-\t\n+\n \tpublic String getReplacement(int colID) {\n \t\tint idx = isApplicable(colID);\n \t\treturn (idx == -1) ? null : _replacementList[idx];\n \t}\n-\t\n+\n \t@Override\n \tpublic MatrixBlock encode(FrameBlock in, MatrixBlock out) {\n \t\tbuild(in);\n \t\treturn apply(in, out);\n \t}\n-\t\n+\n \t@Override\n \tpublic void build(FrameBlock in) {\n \t\ttry {\n-\t\t\tfor( int j=0; j<_colList.length; j++ ) {\n+\t\t\tfor(int j = 0; j < _colList.length; j++) {\n \t\t\t\tint colID = _colList[j];\n-\t\t\t\tif( _mvMethodList[j] == MVMethod.GLOBAL_MEAN ) {\n-\t\t\t\t\t//compute global column mean (scale)\n+\t\t\t\tif(_mvMethodList[j] == MVMethod.GLOBAL_MEAN) {\n+\t\t\t\t\t// compute global column mean (scale)\n \t\t\t\t\tlong off = _countList[j];\n-\t\t\t\t\tfor( int i=0; i<in.getNumRows(); i++ ){\n-\t\t\t\t\t\tObject key = in.get(i, colID-1);\n-\t\t\t\t\t\tif(key == null){\n+\t\t\t\t\tfor(int i = 0; i < in.getNumRows(); i++) {\n+\t\t\t\t\t\tObject key = in.get(i, colID - 1);\n+\t\t\t\t\t\tif(key == null) {\n \t\t\t\t\t\t\toff--;\n \t\t\t\t\t\t\tcontinue;\n \t\t\t\t\t\t}\n-\t\t\t\t\t\t_meanFn.execute2(_meanList[j], UtilFunctions.objectToDouble(\n-\t\t\t\t\t\t\t\tin.getSchema()[colID-1], key), off+i+1);\n+\t\t\t\t\t\t_meanFn.execute2(_meanList[j],\n+\t\t\t\t\t\t\tUtilFunctions.objectToDouble(in.getSchema()[colID - 1], key),\n+\t\t\t\t\t\t\toff + i + 1);\n \t\t\t\t\t}\n \t\t\t\t\t_replacementList[j] = String.valueOf(_meanList[j]._sum);\n \t\t\t\t\t_countList[j] += in.getNumRows();\n \t\t\t\t}\n-\t\t\t\telse if( _mvMethodList[j] == MVMethod.GLOBAL_MODE ) {\n-\t\t\t\t\t//compute global column mode (categorical), i.e., most frequent category\n-\t\t\t\t\tHashMap<String,Long> hist = _hist.containsKey(colID) ? \n-\t\t\t\t\t\t\t_hist.get(colID) : new HashMap<>();\n-\t\t\t\t\tfor( int i=0; i<in.getNumRows(); i++ ) {\n-\t\t\t\t\t\tString key = String.valueOf(in.get(i, colID-1));\n-\t\t\t\t\t\tif(!key.equals(\"null\") && !key.isEmpty() ) {\n+\t\t\t\telse if(_mvMethodList[j] == MVMethod.GLOBAL_MODE) {\n+\t\t\t\t\t// compute global column mode (categorical), i.e., most frequent category\n+\t\t\t\t\tHashMap<String, Long> hist = _hist.containsKey(colID) ? _hist.get(colID) : new HashMap<>();\n+\t\t\t\t\tfor(int i = 0; i < in.getNumRows(); i++) {\n+\t\t\t\t\t\tString key = String.valueOf(in.get(i, colID - 1));\n+\t\t\t\t\t\tif(!key.equals(\"null\") && !key.isEmpty()) {\n \t\t\t\t\t\t\tLong val = hist.get(key);\n-\t\t\t\t\t\t\thist.put(key, (val!=null) ? val+1 : 1);\n+\t\t\t\t\t\t\thist.put(key, (val != null) ? val + 1 : 1);\n \t\t\t\t\t\t}\n \t\t\t\t\t}\n \t\t\t\t\t_hist.put(colID, hist);\n-\t\t\t\t\tlong max = Long.MIN_VALUE; \n-\t\t\t\t\tfor( Entry<String, Long> e : hist.entrySet() ) \n-\t\t\t\t\t\tif( e.getValue() > max  ) {\n+\t\t\t\t\tlong max = Long.MIN_VALUE;\n+\t\t\t\t\tfor(Entry<String, Long> e : hist.entrySet())\n+\t\t\t\t\t\tif(e.getValue() > max) {\n \t\t\t\t\t\t\t_replacementList[j] = e.getKey();\n \t\t\t\t\t\t\tmax = e.getValue();\n \t\t\t\t\t\t}\n@@ -201,26 +216,26 @@ else if( _mvMethodList[j] == MVMethod.GLOBAL_MODE ) {\n \t\t\tthrow new RuntimeException(ex);\n \t\t}\n \t}\n-\t\n+\n \t@Override\n \tpublic MatrixBlock apply(FrameBlock in, MatrixBlock out) {\n-\t\tfor(int i=0; i<in.getNumRows(); i++) {\n-\t\t\tfor(int j=0; j<_colList.length; j++) {\n+\t\tfor(int i = 0; i < in.getNumRows(); i++) {\n+\t\t\tfor(int j = 0; j < _colList.length; j++) {\n \t\t\t\tint colID = _colList[j];\n-\t\t\t\tif( Double.isNaN(out.quickGetValue(i, colID-1)) )\n-\t\t\t\t\tout.quickSetValue(i, colID-1, Double.parseDouble(_replacementList[j]));\n+\t\t\t\tif(Double.isNaN(out.quickGetValue(i, colID - 1)))\n+\t\t\t\t\tout.quickSetValue(i, colID - 1, Double.parseDouble(_replacementList[j]));\n \t\t\t}\n \t\t}\n \t\treturn out;\n \t}\n \n \t@Override\n-\tpublic Encoder subRangeEncoder(IndexRange ixRange) {\n+\tpublic LegacyEncoder subRangeEncoder(IndexRange ixRange) {\n \t\tMap<Integer, ColInfo> map = new HashMap<>();\n \t\tfor(int i = 0; i < _colList.length; i++) {\n \t\t\tint col = _colList[i];\n \t\t\tif(ixRange.inColRange(col))\n-\t\t\t\tmap.put((int) (_colList[i] - (ixRange.colStart - 1)),\n+\t\t\t\tmap.put(_colList[i],\n \t\t\t\t\tnew ColInfo(_mvMethodList[i], _replacementList[i], _meanList[i], _countList[i], _hist.get(i)));\n \t\t}\n \t\tif(map.size() == 0)\n@@ -244,23 +259,8 @@ public Encoder subRangeEncoder(IndexRange ixRange) {\n \t\t\t(int) ixRange.colSpan());\n \t}\n \n-\tprivate static void fillListsFromMap(Map<Integer, ColInfo> map, int[] colList, MVMethod[] mvMethodList,\n-\t\t\tString[] replacementList, KahanObject[] meanList, long[] countList,\n-\t\t\tHashMap<Integer, HashMap<String, Long>> hist) {\n-\t\tint i = 0;\n-\t\tfor(Entry<Integer, ColInfo> entry : map.entrySet()) {\n-\t\t\tcolList[i] = entry.getKey();\n-\t\t\tmvMethodList[i] = entry.getValue()._method;\n-\t\t\treplacementList[i] = entry.getValue()._replacement;\n-\t\t\tmeanList[i] = entry.getValue()._mean;\n-\t\t\tcountList[i++] = entry.getValue()._count;\n-\n-\t\t\thist.put(entry.getKey(), entry.getValue()._hist);\n-\t\t}\n-\t}\n-\n \t@Override\n-\tpublic void mergeAt(Encoder other, int row, int col) {\n+\tpublic void mergeAt(LegacyEncoder other, int row, int col) {\n \t\tif(other instanceof EncoderMVImpute) {\n \t\t\tEncoderMVImpute otherImpute = (EncoderMVImpute) other;\n \t\t\tMap<Integer, ColInfo> map = new HashMap<>();\n@@ -269,9 +269,9 @@ public void mergeAt(Encoder other, int row, int col) {\n \t\t\t\t\tnew ColInfo(_mvMethodList[i], _replacementList[i], _meanList[i], _countList[i], _hist.get(i + 1)));\n \t\t\t}\n \t\t\tfor(int i = 0; i < other._colList.length; i++) {\n-\t\t\t\tint column = other._colList[i] + (col - 1);\n+\t\t\t\tint column = other._colList[i];\n \t\t\t\tColInfo otherColInfo = new ColInfo(otherImpute._mvMethodList[i], otherImpute._replacementList[i],\n-\t\t\t\t\t\totherImpute._meanList[i], otherImpute._countList[i], otherImpute._hist.get(i + 1));\n+\t\t\t\t\totherImpute._meanList[i], otherImpute._countList[i], otherImpute._hist.get(i + 1));\n \t\t\t\tColInfo colInfo = map.get(column);\n \t\t\t\tif(colInfo == null)\n \t\t\t\t\tmap.put(column, otherColInfo);\n@@ -287,8 +287,6 @@ public void mergeAt(Encoder other, int row, int col) {\n \t\t\t_hist = new HashMap<>();\n \n \t\t\tfillListsFromMap(map, _colList, _mvMethodList, _replacementList, _meanList, _countList, _hist);\n-\t\t\t// update number of columns\n-\t\t\t_clen = Math.max(_clen, col - 1 + other._clen);\n \n \t\t\tif(_rcList == null)\n \t\t\t\t_rcList = new ArrayList<>();\n@@ -299,27 +297,27 @@ public void mergeAt(Encoder other, int row, int col) {\n \t\t}\n \t\tsuper.mergeAt(other, row, col);\n \t}\n-\t\n+\n \t@Override\n \tpublic FrameBlock getMetaData(FrameBlock out) {\n-\t\tfor( int j=0; j<_colList.length; j++ ) {\n-\t\t\tout.getColumnMetadata(_colList[j]-1)\n-\t\t\t\t.setMvValue(_replacementList[j]);\n+\t\tfor(int j = 0; j < _colList.length; j++) {\n+\t\t\tout.getColumnMetadata(_colList[j] - 1).setMvValue(_replacementList[j]);\n \t\t}\n \t\treturn out;\n \t}\n \n \t@Override\n \tpublic void initMetaData(FrameBlock meta) {\n-\t\t//init replacement lists, replace recoded values to\n-\t\t//apply mv imputation potentially after recoding\n-\t\tfor( int j=0; j<_colList.length; j++ ) {\n-\t\t\tint colID = _colList[j];\t\n-\t\t\tString mvVal = UtilFunctions.unquote(meta.getColumnMetadata(colID-1).getMvValue()); \n-\t\t\tif( _rcList.contains(colID) ) {\n-\t\t\t\tLong mvVal2 = meta.getRecodeMap(colID-1).get(mvVal);\n-\t\t\t\tif( mvVal2 == null)\n-\t\t\t\t\tthrow new RuntimeException(\"Missing recode value for impute value '\"+mvVal+\"' (colID=\"+colID+\").\");\n+\t\t// init replacement lists, replace recoded values to\n+\t\t// apply mv imputation potentially after recoding\n+\t\tfor(int j = 0; j < _colList.length; j++) {\n+\t\t\tint colID = _colList[j];\n+\t\t\tString mvVal = UtilFunctions.unquote(meta.getColumnMetadata(colID - 1).getMvValue());\n+\t\t\tif(_rcList.contains(colID)) {\n+\t\t\t\tLong mvVal2 = meta.getRecodeMap(colID - 1).get(mvVal);\n+\t\t\t\tif(mvVal2 == null)\n+\t\t\t\t\tthrow new RuntimeException(\n+\t\t\t\t\t\t\"Missing recode value for impute value '\" + mvVal + \"' (colID=\" + colID + \").\");\n \t\t\t\t_replacementList[j] = mvVal2.toString();\n \t\t\t}\n \t\t\telse {\n@@ -331,14 +329,14 @@ public void initMetaData(FrameBlock meta) {\n \tpublic void initRecodeIDList(List<Integer> rcList) {\n \t\t_rcList = rcList;\n \t}\n-\t\n+\n \t/**\n \t * Exposes the internal histogram after build.\n-\t * \n+\t *\n \t * @param colID column ID\n \t * @return histogram (map of string keys and long values)\n \t */\n-\tpublic HashMap<String,Long> getHistogram( int colID ) {\n+\tpublic HashMap<String, Long> getHistogram(int colID) {\n \t\treturn _hist.get(colID);\n \t}\n \n@@ -360,13 +358,13 @@ public void writeExternal(ObjectOutput out) throws IOException {\n \t\t\t}\n \n \t\tout.writeInt(_rcList.size());\n-\t\tfor(int rc: _rcList)\n+\t\tfor(int rc : _rcList)\n \t\t\tout.writeInt(rc);\n \n \t\tint histSize = _hist == null ? 0 : _hist.size();\n \t\tout.writeInt(histSize);\n-\t\tif (histSize > 0)\n-\t\t\tfor(Entry<Integer,HashMap<String,Long>> e1 : _hist.entrySet()) {\n+\t\tif(histSize > 0)\n+\t\t\tfor(Entry<Integer, HashMap<String, Long>> e1 : _hist.entrySet()) {\n \t\t\t\tout.writeInt(e1.getKey());\n \t\t\t\tout.writeInt(e1.getValue().size());\n \t\t\t\tfor(Entry<String, Long> e2 : e1.getValue().entrySet()) {\n@@ -391,7 +389,7 @@ public void readExternal(ObjectInput in) throws IOException {\n \t\t\t_meanList[i] = new KahanObject(0, 0);\n \t\t}\n \n-\t\tint size4 =  in.readInt();\n+\t\tint size4 = in.readInt();\n \t\tfor(int i = 0; i < size4; i++) {\n \t\t\tint index = in.readInt();\n \t\t\t_replacementList[index] = in.readUTF();\n@@ -409,15 +407,19 @@ public void readExternal(ObjectInput in) throws IOException {\n \t\t\tint size2 = in.readInt();\n \n \t\t\tHashMap<String, Long> maps = new HashMap<>();\n-\t\t\tfor(int j = 0; j < size2; j++){\n+\t\t\tfor(int j = 0; j < size2; j++) {\n \t\t\t\tString key2 = in.readUTF();\n \t\t\t\tLong value = in.readLong();\n \t\t\t\tmaps.put(key2, value);\n \t\t\t}\n \t\t\t_hist.put(key1, maps);\n \t\t}\n \t}\n-\t\n+\n+\tpublic enum MVMethod {\n+\t\tINVALID, GLOBAL_MEAN, GLOBAL_MODE, CONSTANT\n+\t}\n+\n \tprivate static class ColInfo {\n \t\tMVMethod _method;\n \t\tString _replacement;\n@@ -436,7 +438,7 @@ public void readExternal(ObjectInput in) throws IOException {\n \t\tpublic void merge(ColInfo otherColInfo) {\n \t\t\tif(_method != otherColInfo._method)\n \t\t\t\tthrow new DMLRuntimeException(\"Tried to merge two different impute methods: \" + _method.name() + \" vs. \"\n-\t\t\t\t\t\t+ otherColInfo._method.name());\n+\t\t\t\t\t+ otherColInfo._method.name());\n \t\t\tswitch(_method) {\n \t\t\t\tcase CONSTANT:\n \t\t\t\t\tassert _replacement.equals(otherColInfo._replacement);\n@@ -450,7 +452,7 @@ public void merge(ColInfo otherColInfo) {\n \t\t\t\t\t_count += otherColInfo._count;\n \t\t\t\t\tbreak;\n \t\t\t\tcase GLOBAL_MODE:\n-\t\t\t\t\tif (_hist == null)\n+\t\t\t\t\tif(_hist == null)\n \t\t\t\t\t\t_hist = new HashMap<>(otherColInfo._hist);\n \t\t\t\t\telse\n \t\t\t\t\t\t// add counts\n@@ -461,4 +463,4 @@ public void merge(ColInfo otherColInfo) {\n \t\t\t}\n \t\t}\n \t}\n-}\n+}\n\\ No newline at end of file"
  },
  {
    "sha": "db61fc158f856d25b781e55be86e46355d8912ab",
    "filename": "src/main/java/org/apache/sysds/runtime/transform/encode/EncoderOmit.java",
    "status": "modified",
    "additions": 73,
    "deletions": 53,
    "changes": 126,
    "blob_url": "https://github.com/apache/systemds/blob/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/encode/EncoderOmit.java",
    "raw_url": "https://github.com/apache/systemds/raw/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/encode/EncoderOmit.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/transform/encode/EncoderOmit.java?ref=cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd",
    "patch": "@@ -6,9 +6,9 @@\n  * to you under the Apache License, Version 2.0 (the\n  * \"License\"); you may not use this file except in compliance\n  * with the License.  You may obtain a copy of the License at\n- * \n+ *\n  *   http://www.apache.org/licenses/LICENSE-2.0\n- * \n+ *\n  * Unless required by applicable law or agreed to in writing,\n  * software distributed under the License is distributed on an\n  * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n@@ -22,9 +22,13 @@\n import java.io.IOException;\n import java.io.ObjectInput;\n import java.io.ObjectOutput;\n+import java.util.ArrayList;\n import java.util.Arrays;\n+import java.util.List;\n import java.util.Objects;\n \n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n import org.apache.sysds.common.Types.ValueType;\n import org.apache.sysds.runtime.matrix.data.FrameBlock;\n import org.apache.sysds.runtime.matrix.data.MatrixBlock;\n@@ -36,39 +40,59 @@\n import org.apache.wink.json4j.JSONException;\n import org.apache.wink.json4j.JSONObject;\n \n-public class EncoderOmit extends Encoder \n-{\t\n-\tprivate static final long serialVersionUID = 1978852120416654195L;\n+public class EncoderOmit extends LegacyEncoder {\n+\t/*\n+\t * THIS CLASS IS ONLY FOR LEGACY SUPPORT!!! and will be fazed out slowly.\n+\t */\n \n+\tprotected static final Log LOG = LogFactory.getLog(Encoder.class.getName());\n+\tprivate static final long serialVersionUID = 1978852120416654195L;\n \tprivate boolean _federated = false;\n \tprivate boolean[] _rmRows = new boolean[0];\n \n \tpublic EncoderOmit(JSONObject parsedSpec, String[] colnames, int clen, int minCol, int maxCol)\n-\t\tthrows JSONException \n-\t{\n-\t\tsuper(null, clen);\n-\t\tif (!parsedSpec.containsKey(TfMethod.OMIT.toString()))\n+\t\tthrows JSONException {\n+\t\tthis(null, clen);\n+\t\tif(!parsedSpec.containsKey(TfMethod.OMIT.toString()))\n \t\t\treturn;\n \t\tint[] collist = TfMetaUtils.parseJsonIDList(parsedSpec, colnames, TfMethod.OMIT.toString(), minCol, maxCol);\n \t\tinitColList(collist);\n \t\t_federated = minCol != -1 || maxCol != -1;\n \t}\n-\t\n+\n \tpublic EncoderOmit() {\n \t\tsuper(new int[0], 0);\n \t}\n-\t\n+\n+\tpublic EncoderOmit(int[] colList, int clen) {\n+\t\tsuper(colList, clen);\n+\t}\n+\n \tpublic EncoderOmit(boolean federated) {\n \t\tthis();\n \t\t_federated = federated;\n \t}\n-\t\n+\n \tprivate EncoderOmit(int[] colList, int clen, boolean[] rmRows) {\n-\t\tsuper(colList, clen);\n+\t\tthis(colList, clen);\n \t\t_rmRows = rmRows;\n \t\t_federated = true;\n \t}\n \n+\tpublic int initColList(int[] colList) {\n+\t\t_colList = colList;\n+\t\treturn _colList.length;\n+\t}\n+\n+\t/**\n+\t * Indicates if this encoder is applicable, i.e, if there is at least one column to encode.\n+\t *\n+\t * @return true if at least one column to encode\n+\t */\n+\tpublic boolean isApplicable() {\n+\t\treturn(_colList != null && _colList.length > 0);\n+\t}\n+\n \tpublic int getNumRemovedRows(boolean[] rmRows) {\n \t\tint cnt = 0;\n \t\tfor(boolean v : rmRows)\n@@ -79,31 +103,27 @@ public int getNumRemovedRows(boolean[] rmRows) {\n \tpublic int getNumRemovedRows() {\n \t\treturn getNumRemovedRows(_rmRows);\n \t}\n-\t\n+\n \tpublic boolean omit(String[] words, TfUtils agents) {\n-\t\tif( !isApplicable() )\n+\t\tif(!isApplicable())\n \t\t\treturn false;\n-\t\t\n-\t\tfor(int i=0; i<_colList.length; i++) {\n-\t\t\tint colID = _colList[i];\n-\t\t\tif(TfUtils.isNA(agents.getNAStrings(),UtilFunctions.unquote(words[colID-1].trim())))\n+\n+\t\tfor (int colID : _colList) {\n+\t\t\tif (TfUtils.isNA(agents.getNAStrings(), UtilFunctions.unquote(words[colID - 1].trim())))\n \t\t\t\treturn true;\n \t\t}\n \t\treturn false;\n \t}\n \n-\t@Override\n \tpublic MatrixBlock encode(FrameBlock in, MatrixBlock out) {\n \t\treturn apply(in, out);\n \t}\n-\t\n-\t@Override\n+\n \tpublic void build(FrameBlock in) {\n \t\tif(_federated)\n \t\t\t_rmRows = computeRmRows(in);\n \t}\n \n-\t@Override\n \tpublic MatrixBlock apply(FrameBlock in, MatrixBlock out) {\n \t\t// local rmRows for broadcasting encoder in spark\n \t\tboolean[] rmRows;\n@@ -135,12 +155,12 @@ public MatrixBlock apply(FrameBlock in, MatrixBlock out) {\n \tprivate boolean[] computeRmRows(FrameBlock in) {\n \t\tboolean[] rmRows = new boolean[in.getNumRows()];\n \t\tValueType[] schema = in.getSchema();\n-\t\t//TODO perf evaluate if column-wise scan more efficient\n-\t\t//  (sequential but less impact of early abort)\n+\t\t// TODO perf evaluate if column-wise scan more efficient\n+\t\t// (sequential but less impact of early abort)\n \t\tfor(int i = 0; i < in.getNumRows(); i++) {\n \t\t\tfor(int colID : _colList) {\n \t\t\t\tObject val = in.get(i, colID - 1);\n-\t\t\t\tif (val == null || (schema[colID - 1] == ValueType.STRING && val.toString().isEmpty())) {\n+\t\t\t\tif(val == null || (schema[colID - 1] == ValueType.STRING && val.toString().isEmpty())) {\n \t\t\t\t\trmRows[i] = true;\n \t\t\t\t\tbreak; // early abort\n \t\t\t\t}\n@@ -149,56 +169,58 @@ public MatrixBlock apply(FrameBlock in, MatrixBlock out) {\n \t\treturn rmRows;\n \t}\n \n-\t@Override\n-\tpublic Encoder subRangeEncoder(IndexRange ixRange) {\n+\tpublic EncoderOmit subRangeEncoder(IndexRange ixRange) {\n \t\tint[] colList = subRangeColList(ixRange);\n \t\tif(colList.length == 0)\n \t\t\t// empty encoder -> sub range encoder does not exist\n \t\t\treturn null;\n \t\tboolean[] rmRows = _rmRows;\n-\t\tif (_rmRows.length > 0)\n+\t\tif(_rmRows.length > 0)\n \t\t\trmRows = Arrays.copyOfRange(rmRows, (int) ixRange.rowStart - 1, (int) ixRange.rowEnd - 1);\n \n \t\treturn new EncoderOmit(colList, (int) (ixRange.colSpan()), rmRows);\n \t}\n \n-\t@Override\n-\tpublic void mergeAt(Encoder other, int row, int col) {\n-\t\tif(other instanceof EncoderOmit) {\n-\t\t\tmergeColumnInfo(other, col);\n-\t\t\tEncoderOmit otherOmit = (EncoderOmit) other;\n-\t\t\t_rmRows = Arrays.copyOf(_rmRows, Math.max(_rmRows.length, (row - 1) + otherOmit._rmRows.length));\n-\t\t\tfor (int i = 0; i < otherOmit._rmRows.length; i++)\n-\t\t\t\t_rmRows[(row - 1) + 1] |= otherOmit._rmRows[i];\n-\t\t\treturn;\n+\tprotected int[] subRangeColList(IndexRange ixRange) {\n+\t\tList<Integer> cols = new ArrayList<>();\n+\t\tfor(int col : _colList) {\n+\t\t\tif(ixRange.inColRange(col)) {\n+\t\t\t\t// add the correct column, removed columns before start\n+\t\t\t\t// colStart - 1 because colStart is 1-based\n+\t\t\t\tcols.add(col);\n+\t\t\t}\n \t\t}\n-\t\tsuper.mergeAt(other, row, col);\n+\t\treturn cols.stream().mapToInt(i -> i).toArray();\n \t}\n-\t\n-\t@Override\n+\n+\tpublic void mergeAt(EncoderOmit other, int row, int col) {\n+\t\tmergeColumnInfo(other, col);\n+\t\t_rmRows = Arrays.copyOf(_rmRows, Math.max(_rmRows.length, (row - 1) + other._rmRows.length));\n+\t\tfor(int i = 0; i < other._rmRows.length; i++)\n+\t\t\t_rmRows[(row - 1) + 1] |= other._rmRows[i];\n+\t}\n+\n \tpublic void updateIndexRanges(long[] beginDims, long[] endDims) {\n \t\t// first update begin dims\n \t\tint numRowsToRemove = 0;\n-\t\tfor (int i = 0; i < beginDims[0] - 1 && i < _rmRows.length; i++)\n-\t\t\tif (_rmRows[i])\n+\t\tfor(int i = 0; i < beginDims[0] - 1 && i < _rmRows.length; i++)\n+\t\t\tif(_rmRows[i])\n \t\t\t\tnumRowsToRemove++;\n \t\tbeginDims[0] -= numRowsToRemove;\n \t\t// update end dims\n-\t\tfor (int i = 0; i < endDims[0] - 1 && i < _rmRows.length; i++)\n-\t\t\tif (_rmRows[i])\n+\t\tfor(int i = 0; i < endDims[0] - 1 && i < _rmRows.length; i++)\n+\t\t\tif(_rmRows[i])\n \t\t\t\tnumRowsToRemove++;\n \t\tendDims[0] -= numRowsToRemove;\n \t}\n-\t\n-\t@Override\n+\n \tpublic FrameBlock getMetaData(FrameBlock out) {\n-\t\t//do nothing\n+\t\t// do nothing\n \t\treturn out;\n \t}\n-\t\n-\t@Override\n+\n \tpublic void initMetaData(FrameBlock meta) {\n-\t\t//do nothing\n+\t\t// do nothing\n \t}\n \n \t@Override\n@@ -221,7 +243,6 @@ public void readExternal(ObjectInput in) throws IOException {\n \t\t}\n \t}\n \n-\t@Override\n \tpublic boolean equals(Object o) {\n \t\tif(this == o)\n \t\t\treturn true;\n@@ -231,7 +252,6 @@ public boolean equals(Object o) {\n \t\treturn _federated == that._federated && Arrays.equals(_rmRows, that._rmRows);\n \t}\n \n-\t@Override\n \tpublic int hashCode() {\n \t\tint result = Objects.hash(_federated);\n \t\tresult = 31 * result + Arrays.hashCode(_rmRows);"
  },
  {
    "sha": "0a603afbfb4a285a8c3a3069b1e4fe98eac8371f",
    "filename": "src/main/java/org/apache/sysds/runtime/transform/encode/EncoderPassThrough.java",
    "status": "removed",
    "additions": 0,
    "deletions": 109,
    "changes": 109,
    "blob_url": "https://github.com/apache/systemds/blob/1b77f5e81a6d0981fa76c17f66937e94e1322785/src/main/java/org/apache/sysds/runtime/transform/encode/EncoderPassThrough.java",
    "raw_url": "https://github.com/apache/systemds/raw/1b77f5e81a6d0981fa76c17f66937e94e1322785/src/main/java/org/apache/sysds/runtime/transform/encode/EncoderPassThrough.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/transform/encode/EncoderPassThrough.java?ref=1b77f5e81a6d0981fa76c17f66937e94e1322785",
    "patch": "@@ -1,109 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- * \n- *   http://www.apache.org/licenses/LICENSE-2.0\n- * \n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.apache.sysds.runtime.transform.encode;\n-\n-import java.util.ArrayList;\n-import java.util.List;\n-\n-import org.apache.sysds.common.Types.ValueType;\n-import org.apache.sysds.runtime.matrix.data.FrameBlock;\n-import org.apache.sysds.runtime.matrix.data.MatrixBlock;\n-import org.apache.sysds.runtime.util.IndexRange;\n-import org.apache.sysds.runtime.util.UtilFunctions;\n-\n-/**\n- * Simple composite encoder that applies a list of encoders \n- * in specified order. By implementing the default encoder API\n- * it can be used as a drop-in replacement for any other encoder. \n- * \n- */\n-public class EncoderPassThrough extends Encoder\n-{\n-\tprivate static final long serialVersionUID = -8473768154646831882L;\n-\t\n-\tprotected EncoderPassThrough(int[] ptCols, int clen) {\n-\t\tsuper(ptCols, clen); //1-based \n-\t}\n-\t\n-\tpublic EncoderPassThrough() {\n-\t\tthis(new int[0], 0);\n-\t}\n-\n-\t@Override\n-\tpublic MatrixBlock encode(FrameBlock in, MatrixBlock out) {\n-\t\treturn apply(in, out);\n-\t}\n-\n-\t@Override\n-\tpublic void build(FrameBlock in) {\n-\t\t//do nothing\n-\t}\n-\t\n-\t@Override \n-\tpublic MatrixBlock apply(FrameBlock in, MatrixBlock out) {\n-\t\tfor( int j=0; j<_colList.length; j++ ) {\n-\t\t\tint col = _colList[j]-1;\n-\t\t\tValueType vt = in.getSchema()[col];\n-\t\t\tfor( int i=0; i<in.getNumRows(); i++ ) {\n-\t\t\t\tObject val = in.get(i, col);\n-\t\t\t\tout.quickSetValue(i, col, (val==null||(vt==ValueType.STRING \n-\t\t\t\t\t&& val.toString().isEmpty())) ? Double.NaN : \n-\t\t\t\t\tUtilFunctions.objectToDouble(vt, val));\n-\t\t\t}\n-\t\t}\n-\t\t\n-\t\treturn out;\n-\t}\n-\t\n-\t@Override\n-\tpublic Encoder subRangeEncoder(IndexRange ixRange) {\n-\t\tList<Integer> colList = new ArrayList<>();\n-\t\tfor(int col : _colList) {\n-\t\t\tif(col >= ixRange.colStart && col < ixRange.colEnd)\n-\t\t\t\t// add the correct column, removed columns before start\n-\t\t\t\tcolList.add((int) (col - (ixRange.colStart - 1)));\n-\t\t}\n-\t\tif(colList.isEmpty())\n-\t\t\t// empty encoder -> return null\n-\t\t\treturn null;\n-\t\treturn new EncoderPassThrough(colList.stream().mapToInt(i -> i).toArray(),\n-\t\t\t(int) (ixRange.colEnd - ixRange.colStart));\n-\t}\n-\t\n-\t@Override\n-\tpublic void mergeAt(Encoder other, int row, int col) {\n-\t\tif(other instanceof EncoderPassThrough) {\n-\t\t\tmergeColumnInfo(other, col);\n-\t\t\treturn;\n-\t\t}\n-\t\tsuper.mergeAt(other, row, col);\n-\t}\n-\n-\t@Override\n-\tpublic FrameBlock getMetaData(FrameBlock meta) {\n-\t\t//do nothing\n-\t\treturn meta;\n-\t}\n-\t\n-\t@Override\n-\tpublic void initMetaData(FrameBlock meta) {\n-\t\t//do nothing\n-\t}\n-}"
  },
  {
    "sha": "23b166f0a44f6f492f98f19740692c554ff5c44c",
    "filename": "src/main/java/org/apache/sysds/runtime/transform/encode/EncoderRecode.java",
    "status": "removed",
    "additions": 0,
    "deletions": 380,
    "changes": 380,
    "blob_url": "https://github.com/apache/systemds/blob/1b77f5e81a6d0981fa76c17f66937e94e1322785/src/main/java/org/apache/sysds/runtime/transform/encode/EncoderRecode.java",
    "raw_url": "https://github.com/apache/systemds/raw/1b77f5e81a6d0981fa76c17f66937e94e1322785/src/main/java/org/apache/sysds/runtime/transform/encode/EncoderRecode.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/transform/encode/EncoderRecode.java?ref=1b77f5e81a6d0981fa76c17f66937e94e1322785",
    "patch": "@@ -1,380 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.apache.sysds.runtime.transform.encode;\n-\n-import java.io.IOException;\n-import java.io.ObjectInput;\n-import java.io.ObjectOutput;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.Iterator;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Map.Entry;\n-import java.util.Objects;\n-\n-import org.apache.sysds.lops.Lop;\n-import org.apache.sysds.runtime.matrix.data.FrameBlock;\n-import org.apache.sysds.runtime.matrix.data.MatrixBlock;\n-import org.apache.sysds.runtime.transform.TfUtils.TfMethod;\n-import org.apache.sysds.runtime.transform.meta.TfMetaUtils;\n-import org.apache.sysds.runtime.util.IndexRange;\n-import org.apache.wink.json4j.JSONException;\n-import org.apache.wink.json4j.JSONObject;\n-\n-public class EncoderRecode extends Encoder\n-{\n-\tprivate static final long serialVersionUID = 8213163881283341874L;\n-\n-\t//test property to ensure consistent encoding for local and federated\n-\tpublic static boolean SORT_RECODE_MAP = false;\n-\n-\t//recode maps and custom map for partial recode maps\n-\tprivate HashMap<Integer, HashMap<String, Long>> _rcdMaps  = new HashMap<>();\n-\tprivate HashMap<Integer, HashSet<Object>> _rcdMapsPart = null;\n-\n-\tpublic EncoderRecode(JSONObject parsedSpec, String[] colnames, int clen, int minCol, int maxCol)\n-\t\tthrows JSONException\n-\t{\n-\t\tsuper(null, clen);\n-\t\t_colList = TfMetaUtils.parseJsonIDList(parsedSpec, colnames, TfMethod.RECODE.toString(), minCol, maxCol);\n-\t}\n-\n-\tprivate EncoderRecode(int[] colList, int clen) {\n-\t\tsuper(colList, clen);\n-\t}\n-\n-\tpublic EncoderRecode() {\n-\t\tthis(new int[0], 0);\n-\t}\n-\n-\tprivate EncoderRecode(int[] colList, int clen, HashMap<Integer, HashMap<String, Long>> rcdMaps) {\n-\t\tsuper(colList, clen);\n-\t\t_rcdMaps = rcdMaps;\n-\t}\n-\n-\tpublic HashMap<Integer, HashMap<String,Long>> getCPRecodeMaps() {\n-\t\treturn _rcdMaps;\n-\t}\n-\n-\tpublic HashMap<Integer, HashSet<Object>> getCPRecodeMapsPartial() {\n-\t\treturn _rcdMapsPart;\n-\t}\n-\n-\tpublic void sortCPRecodeMaps() {\n-\t\tfor( HashMap<String,Long> map : _rcdMaps.values() ) {\n-\t\t\tString[] keys= map.keySet().toArray(new String[0]);\n-\t\t\tArrays.sort(keys);\n-\t\t\tmap.clear();\n-\t\t\tfor(String key : keys)\n-\t\t\t\tputCode(map, key);\n-\t\t}\n-\t}\n-\n-\tprivate long lookupRCDMap(int colID, String key) {\n-\t\tif( !_rcdMaps.containsKey(colID) )\n-\t\t\treturn -1; //empty recode map\n-\t\tLong tmp = _rcdMaps.get(colID).get(key);\n-\t\treturn (tmp!=null) ? tmp : -1;\n-\t}\n-\n-\t@Override\n-\tpublic MatrixBlock encode(FrameBlock in, MatrixBlock out) {\n-\t\tif( !isApplicable() )\n-\t\t\treturn out;\n-\n-\t\t//build and apply recode maps\n-\t\tbuild(in);\n-\t\tapply(in, out);\n-\n-\t\treturn out;\n-\t}\n-\n-\t@Override\n-\tpublic void build(FrameBlock in) {\n-\t\tif( !isApplicable() )\n-\t\t\treturn;\n-\n-\t\tIterator<String[]> iter = in.getStringRowIterator(_colList);\n-\t\twhile( iter.hasNext() ) {\n-\t\t\tString[] row = iter.next();\n-\t\t\tfor( int j=0; j<_colList.length; j++ ) {\n-\t\t\t\tint colID = _colList[j]; //1-based\n-\t\t\t\t//allocate column map if necessary\n-\t\t\t\tif( !_rcdMaps.containsKey(colID) )\n-\t\t\t\t\t_rcdMaps.put(colID, new HashMap<String,Long>());\n-\t\t\t\t//probe and build column map\n-\t\t\t\tHashMap<String,Long> map = _rcdMaps.get(colID);\n-\t\t\t\tString key = row[j];\n-\t\t\t\tif( key!=null && !key.isEmpty() && !map.containsKey(key) )\n-\t\t\t\t\tputCode(map, key);\n-\t\t\t}\n-\t\t}\n-\n-\t\tif( SORT_RECODE_MAP ) {\n-\t\t\tsortCPRecodeMaps();\n-\t\t}\n-\t}\n-\n-\t/**\n-\t * Put the code into the map with the provided key. The code depends on the type of encoder.\n-\t * @param map column map\n-\t * @param key key for the new entry\n-\t */\n-\tprotected void putCode(HashMap<String,Long> map, String key) {\n-\t\tmap.put(key, Long.valueOf(map.size()+1));\n-\t}\n-\n-\t@Override\n-\tpublic void prepareBuildPartial() {\n-\t\t//ensure allocated partial recode map\n-\t\tif( _rcdMapsPart == null )\n-\t\t\t_rcdMapsPart = new HashMap<>();\n-\t}\n-\n-\t@Override\n-\tpublic void buildPartial(FrameBlock in) {\n-\t\tif( !isApplicable() )\n-\t\t\treturn;\n-\n-\t\t//construct partial recode map (tokens w/o codes)\n-\t\t//iterate over columns for sequential access\n-\t\tfor( int j=0; j<_colList.length; j++ ) {\n-\t\t\tint colID = _colList[j]; //1-based\n-\t\t\t//allocate column map if necessary\n-\t\t\tif( !_rcdMapsPart.containsKey(colID) )\n-\t\t\t\t_rcdMapsPart.put(colID, new HashSet<>());\n-\t\t\tHashSet<Object> map = _rcdMapsPart.get(colID);\n-\t\t\t//probe and build column map\n-\t\t\tfor( int i=0; i<in.getNumRows(); i++ )\n-\t\t\t\tmap.add(in.get(i, colID-1));\n-\t\t\t//cleanup unnecessary entries once\n-\t\t\tmap.remove(null);\n-\t\t\tmap.remove(\"\");\n-\t\t}\n-//\t\t_rcdMapsPart = null;\n-\t}\n-\n-\t@Override\n-\tpublic MatrixBlock apply(FrameBlock in, MatrixBlock out) {\n-\t\t//apply recode maps column wise\n-\t\tfor( int j=0; j<_colList.length; j++ ) {\n-\t\t\tint colID = _colList[j];\n-\t\t\tfor( int i=0; i<in.getNumRows(); i++ ) {\n-\t\t\t\tObject okey = in.get(i, colID-1);\n-\t\t\t\tString key = (okey!=null) ? okey.toString() : null;\n-\t\t\t\tlong code = lookupRCDMap(colID, key);\n-\t\t\t\tout.quickSetValue(i, colID-1,\n-\t\t\t\t\t(code >= 0) ? code : Double.NaN);\n-\t\t\t}\n-\t\t}\n-\n-\t\treturn out;\n-\t}\n-\n-\t@Override\n-\tpublic Encoder subRangeEncoder(IndexRange ixRange) {\n-\t\tList<Integer> cols = new ArrayList<>();\n-\t\tHashMap<Integer, HashMap<String, Long>> rcdMaps = new HashMap<>();\n-\t\tfor(int col : _colList) {\n-\t\t\tif(ixRange.inColRange(col)) {\n-\t\t\t\t// add the correct column, removed columns before start\n-\t\t\t\t// colStart - 1 because colStart is 1-based\n-\t\t\t\tint corrColumn = (int) (col - (ixRange.colStart - 1));\n-\t\t\t\tcols.add(corrColumn);\n-\t\t\t\t// copy rcdMap for column\n-\t\t\t\trcdMaps.put(corrColumn, new HashMap<>(_rcdMaps.get(col)));\n-\t\t\t}\n-\t\t}\n-\t\tif(cols.isEmpty())\n-\t\t\t// empty encoder -> sub range encoder does not exist\n-\t\t\treturn null;\n-\n-\t\tint[] colList = cols.stream().mapToInt(i -> i).toArray();\n-\t\treturn new EncoderRecode(colList, (int) ixRange.colSpan(), rcdMaps);\n-\t}\n-\n-\t@Override\n-\tpublic void mergeAt(Encoder other, int row, int col) {\n-\t\tif(other instanceof EncoderRecode) {\n-\t\t\tmergeColumnInfo(other, col);\n-\n-\t\t\t// merge together overlapping columns or add new columns\n-\t\t\tEncoderRecode otherRec = (EncoderRecode) other;\n-\t\t\tfor (int otherColID : other._colList) {\n-\t\t\t\tint colID = otherColID + col - 1;\n-\t\t\t\t//allocate column map if necessary\n-\t\t\t\tif( !_rcdMaps.containsKey(colID) )\n-\t\t\t\t\t_rcdMaps.put(colID, new HashMap<>());\n-\n-\t\t\t\tHashMap<String, Long> otherMap = otherRec._rcdMaps.get(otherColID);\n-\t\t\t\tif(otherMap != null) {\n-\t\t\t\t\t// for each column, add all non present recode values\n-\t\t\t\t\tfor(Map.Entry<String, Long> entry : otherMap.entrySet()) {\n-\t\t\t\t\t\tif (lookupRCDMap(colID, entry.getKey()) == -1) {\n-\t\t\t\t\t\t\t// key does not yet exist\n-\t\t\t\t\t\t\tputCode(_rcdMaps.get(colID), entry.getKey());\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\treturn;\n-\t\t}\n-\t\tsuper.mergeAt(other, row, col);\n-\t}\n-\n-\tpublic int[] numDistinctValues() {\n-\t\tint[] numDistinct = new int[_colList.length];\n-\n-\t\tfor( int j=0; j<_colList.length; j++ ) {\n-\t\t\tint colID = _colList[j]; //1-based\n-\t\t\tnumDistinct[j] = _rcdMaps.get(colID).size();\n-\t\t}\n-\t\treturn numDistinct;\n-\t}\n-\n-\t@Override\n-\tpublic FrameBlock getMetaData(FrameBlock meta) {\n-\t\tif( !isApplicable() )\n-\t\t\treturn meta;\n-\n-\t\t//inverse operation to initRecodeMaps\n-\n-\t\t//allocate output rows\n-\t\tint maxDistinct = 0;\n-\t\tfor( int j=0; j<_colList.length; j++ )\n-\t\t\tif( _rcdMaps.containsKey(_colList[j]) )\n-\t\t\t\tmaxDistinct = Math.max(maxDistinct, _rcdMaps.get(_colList[j]).size());\n-\t\tmeta.ensureAllocatedColumns(maxDistinct);\n-\n-\t\t//create compact meta data representation\n-\t\tStringBuilder sb = new StringBuilder(); //for reuse\n-\t\tfor( int j=0; j<_colList.length; j++ ) {\n-\t\t\tint colID = _colList[j]; //1-based\n-\t\t\tint rowID = 0;\n-\t\t\tif( _rcdMaps.containsKey(_colList[j]) )\n-\t\t\t\tfor( Entry<String, Long> e : _rcdMaps.get(colID).entrySet() ) {\n-\t\t\t\t\tmeta.set(rowID++, colID-1,\n-\t\t\t\t\t\tconstructRecodeMapEntry(e.getKey(), e.getValue(), sb));\n-\t\t\t\t}\n-\t\t\tmeta.getColumnMetadata(colID-1).setNumDistinct(\n-\t\t\t\t\t_rcdMaps.get(colID).size());\n-\t\t}\n-\n-\t\treturn meta;\n-\t}\n-\n-\n-\t/**\n-\t * Construct the recodemaps from the given input frame for all\n-\t * columns registered for recode.\n-\t *\n-\t * @param meta frame block\n-\t */\n-\t@Override\n-\tpublic void initMetaData( FrameBlock meta ) {\n-\t\tif( meta == null || meta.getNumRows()<=0 )\n-\t\t\treturn;\n-\n-\t\tfor( int j=0; j<_colList.length; j++ ) {\n-\t\t\tint colID = _colList[j]; //1-based\n-\t\t\t_rcdMaps.put(colID, meta.getRecodeMap(colID-1));\n-\t\t}\n-\t}\n-\n-\t@Override\n-\tpublic void writeExternal(ObjectOutput out) throws IOException {\n-\t\tsuper.writeExternal(out);\n-\t\tout.writeInt(_rcdMaps.size());\n-\t\tfor(Entry<Integer, HashMap<String,Long>> e1 : _rcdMaps.entrySet()) {\n-\t\t\tout.writeInt(e1.getKey());\n-\t\t\tout.writeInt(e1.getValue().size());\n-\t\t\tfor(Entry<String, Long> e2 : e1.getValue().entrySet()) {\n-\t\t\t\tout.writeUTF(e2.getKey());\n-\t\t\t\tout.writeLong(e2.getValue());\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t@Override\n-\tpublic void readExternal(ObjectInput in) throws IOException {\n-\t\tsuper.readExternal(in);\n-\t\tint size1 = in.readInt();\n-\t\tfor(int i = 0; i < size1; i++) {\n-\t\t\tInteger key1 = in.readInt();\n-\t\t\tint size2 = in.readInt();\n-\t\t\tHashMap<String, Long> maps = new HashMap<>();\n-\t\t\tfor(int j = 0; j < size2; j++){\n-\t\t\t\tString key2 = in.readUTF();\n-\t\t\t\tLong value = in.readLong();\n-\t\t\t\tmaps.put(key2, value);\n-\t\t\t}\n-\t\t\t_rcdMaps.put(key1, maps);\n-\t\t}\n-\t}\n-\n-\t/**\n-\t * Returns the Recode map entry which consists of concatenation of code, delimiter and token.\n-\t *\n-\t * @param token\tis part of Recode map\n-\t * @param code  is code for token\n-\t * @return the concatenation of token and code with delimiter in between\n-\t */\n-\tpublic static String constructRecodeMapEntry(String token, Long code) {\n-\t\tStringBuilder sb = new StringBuilder(token.length()+16);\n-\t\treturn constructRecodeMapEntry(token, code, sb);\n-\t}\n-\n-\tprivate static String constructRecodeMapEntry(String token, Long code, StringBuilder sb) {\n-\t\tsb.setLength(0); //reset reused string builder\n-\t\treturn sb.append(token).append(Lop.DATATYPE_PREFIX)\n-\t\t\t.append(code.longValue()).toString();\n-\t}\n-\n-\t/**\n-\t * Splits a Recode map entry into its token and code.\n-\t *\n-\t * @param value concatenation of token and code with delimiter in between\n-\t * @return string array of token and code\n-\t */\n-\tpublic static String[] splitRecodeMapEntry(String value) {\n-\t\t// Instead of using splitCSV which is forcing string with RFC-4180 format,\n-\t\t// using Lop.DATATYPE_PREFIX separator to split token and code\n-\t\tint pos = value.toString().lastIndexOf(Lop.DATATYPE_PREFIX);\n-\t\treturn new String[] {value.substring(0, pos), value.substring(pos+1)};\n-\t}\n-\n-\t@Override\n-\tpublic boolean equals(Object o) {\n-\t\tif(this == o)\n-\t\t\treturn true;\n-\t\tif(o == null || getClass() != o.getClass())\n-\t\t\treturn false;\n-\t\tEncoderRecode that = (EncoderRecode) o;\n-\t\treturn Objects.equals(_rcdMaps, that._rcdMaps);\n-\t}\n-\n-\t@Override\n-\tpublic int hashCode() {\n-\t\treturn Objects.hash(_rcdMaps);\n-\t}\n-}"
  },
  {
    "sha": "83bf7890699cc2402ac3218f685fa5d726b07a2d",
    "filename": "src/main/java/org/apache/sysds/runtime/transform/encode/LegacyEncoder.java",
    "status": "added",
    "additions": 264,
    "deletions": 0,
    "changes": 264,
    "blob_url": "https://github.com/apache/systemds/blob/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/encode/LegacyEncoder.java",
    "raw_url": "https://github.com/apache/systemds/raw/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/encode/LegacyEncoder.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/transform/encode/LegacyEncoder.java?ref=cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd",
    "patch": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.sysds.runtime.transform.encode;\n+\n+import java.io.Externalizable;\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.sysds.runtime.DMLRuntimeException;\n+import org.apache.sysds.runtime.matrix.data.FrameBlock;\n+import org.apache.sysds.runtime.matrix.data.MatrixBlock;\n+import org.apache.sysds.runtime.util.IndexRange;\n+import org.apache.sysds.runtime.util.UtilFunctions;\n+import org.apache.wink.json4j.JSONArray;\n+\n+/**\n+ * Base class for all transform encoders providing both a row and block interface for decoding frames to matrices.\n+ *\n+ */\n+public abstract class LegacyEncoder implements Externalizable {\n+\tprotected static final Log LOG = LogFactory.getLog(Encoder.class.getName());\n+\tprivate static final long serialVersionUID = 2299156350718979064L;\n+\tprotected int[] _colList;\n+\n+\tprotected LegacyEncoder(int[] colList, int clen) {\n+\t\t_colList = colList;\n+\t}\n+\n+\tpublic int[] getColList() {\n+\t\treturn _colList;\n+\t}\n+\n+\tpublic void setColList(int[] colList) {\n+\t\t_colList = colList;\n+\t}\n+\n+\tpublic int initColList(JSONArray attrs) {\n+\t\t_colList = new int[attrs.size()];\n+\t\tfor(int i = 0; i < _colList.length; i++)\n+\t\t\t_colList[i] = UtilFunctions.toInt(attrs.get(i));\n+\t\treturn _colList.length;\n+\t}\n+\n+\tpublic int initColList(int[] colList) {\n+\t\t_colList = colList;\n+\t\treturn _colList.length;\n+\t}\n+\n+\t/**\n+\t * Indicates if this encoder is applicable, i.e, if there is at least one column to encode.\n+\t *\n+\t * @return true if at least one column to encode\n+\t */\n+\tpublic boolean isApplicable() {\n+\t\treturn(_colList != null && _colList.length > 0);\n+\t}\n+\n+\t/**\n+\t * Indicates if this encoder is applicable for the given column ID, i.e., if it is subject to this transformation.\n+\t *\n+\t * @param colID column ID\n+\t * @return true if encoder is applicable for given column\n+\t */\n+\tpublic int isApplicable(int colID) {\n+\t\tif(_colList == null)\n+\t\t\treturn -1;\n+\t\tint idx = Arrays.binarySearch(_colList, colID);\n+\t\treturn(idx >= 0 ? idx : -1);\n+\t}\n+\n+\t/**\n+\t * Block encode: build and apply (transform encode).\n+\t *\n+\t * @param in  input frame block\n+\t * @param out output matrix block\n+\t * @return output matrix block\n+\t */\n+\tpublic abstract MatrixBlock encode(FrameBlock in, MatrixBlock out);\n+\n+\t/**\n+\t * Build the transform meta data for the given block input. This call modifies and keeps meta data as encoder state.\n+\t *\n+\t * @param in input frame block\n+\t */\n+\tpublic abstract void build(FrameBlock in);\n+\n+\t/**\n+\t * Allocates internal data structures for partial build.\n+\t */\n+\tpublic void prepareBuildPartial() {\n+\t\t// do nothing\n+\t}\n+\n+\t/**\n+\t * Partial build of internal data structures (e.g., in distributed spark operations).\n+\t *\n+\t * @param in input frame block\n+\t */\n+\tpublic void buildPartial(FrameBlock in) {\n+\t\t// do nothing\n+\t}\n+\n+\t/**\n+\t * Encode input data blockwise according to existing transform meta data (transform apply).\n+\t *\n+\t * @param in  input frame block\n+\t * @param out output matrix block\n+\t * @return output matrix block\n+\t */\n+\tpublic abstract MatrixBlock apply(FrameBlock in, MatrixBlock out);\n+\n+\tprotected int[] subRangeColList(IndexRange ixRange) {\n+\t\tList<Integer> cols = new ArrayList<>();\n+\t\tfor(int col : _colList) {\n+\t\t\tif(ixRange.inColRange(col)) {\n+\t\t\t\t// add the correct column, removed columns before start\n+\t\t\t\t// colStart - 1 because colStart is 1-based\n+\t\t\t\tint corrColumn = (int) (col - (ixRange.colStart - 1));\n+\t\t\t\tcols.add(corrColumn);\n+\t\t\t}\n+\t\t}\n+\t\treturn cols.stream().mapToInt(i -> i).toArray();\n+\t}\n+\n+\t/**\n+\t * Returns a new Encoder that only handles a sub range of columns.\n+\t *\n+\t * @param ixRange the range (1-based, begin inclusive, end exclusive)\n+\t * @return an encoder of the same type, just for the sub-range\n+\t */\n+\tpublic LegacyEncoder subRangeEncoder(IndexRange ixRange) {\n+\t\tthrow new DMLRuntimeException(\n+\t\t\tthis.getClass().getSimpleName() + \" does not support the creation of a sub-range encoder\");\n+\t}\n+\n+\t/**\n+\t * Merges the column information, like how many columns the frame needs and which columns this encoder operates on.\n+\t *\n+\t * @param other the other encoder of the same type\n+\t * @param col   column at which the second encoder will be merged in (1-based)\n+\t */\n+\tprotected void mergeColumnInfo(LegacyEncoder other, int col) {\n+\t\t// update number of columns\n+\n+\t\t// update the new columns that this encoder operates on\n+\t\tSet<Integer> colListAgg = new HashSet<>(); // for dedup\n+\t\tfor(int i : _colList)\n+\t\t\tcolListAgg.add(i);\n+\t\tfor(int i : other._colList)\n+\t\t\tcolListAgg.add(i);\n+\t\t_colList = colListAgg.stream().mapToInt(i -> i).toArray();\n+\t}\n+\n+\t/**\n+\t * Merges another encoder, of a compatible type, in after a certain position. Resizes as necessary.\n+\t * <code>Encoders</code> are compatible with themselves and <code>EncoderComposite</code> is compatible with every\n+\t * other <code>Encoder</code>.\n+\t *\n+\t * @param other the encoder that should be merged in\n+\t * @param row   the row where it should be placed (1-based)\n+\t * @param col   the col where it should be placed (1-based)\n+\t */\n+\tpublic void mergeAt(LegacyEncoder other, int row, int col) {\n+\t\tthrow new DMLRuntimeException(\n+\t\t\tthis.getClass().getSimpleName() + \" does not support merging with \" + other.getClass().getSimpleName());\n+\t}\n+\n+\t/**\n+\t * Update index-ranges to after encoding. Note that only Dummycoding changes the ranges.\n+\t *\n+\t * @param beginDims begin dimensions of range\n+\t * @param endDims   end dimensions of range\n+\t */\n+\tpublic void updateIndexRanges(long[] beginDims, long[] endDims) {\n+\t\t// do nothing - default\n+\t}\n+\n+\t/**\n+\t * Construct a frame block out of the transform meta data.\n+\t *\n+\t * @param out output frame block\n+\t * @return output frame block?\n+\t */\n+\tpublic abstract FrameBlock getMetaData(FrameBlock out);\n+\n+\t/**\n+\t * Sets up the required meta data for a subsequent call to apply.\n+\t *\n+\t * @param meta frame block\n+\t */\n+\tpublic abstract void initMetaData(FrameBlock meta);\n+\n+\t/**\n+\t * Obtain the column mapping of encoded frames based on the passed meta data frame.\n+\t *\n+\t * @param meta meta data frame block\n+\t * @param out  output matrix\n+\t * @return matrix with column mapping (one row per attribute)\n+\t */\n+\tpublic MatrixBlock getColMapping(FrameBlock meta, MatrixBlock out) {\n+\t\t// default: do nothing\n+\t\treturn out;\n+\t}\n+\n+\t/**\n+\t * Redirects the default java serialization via externalizable to our default hadoop writable serialization for\n+\t * efficient broadcast/rdd serialization.\n+\t *\n+\t * @param os object output\n+\t * @throws IOException if IOException occurs\n+\t */\n+\t@Override\n+\tpublic void writeExternal(ObjectOutput os) throws IOException {\n+\t\tos.writeInt(_colList.length);\n+\t\tfor(int col : _colList)\n+\t\t\tos.writeInt(col);\n+\t}\n+\n+\t/**\n+\t * Redirects the default java serialization via externalizable to our default hadoop writable serialization for\n+\t * efficient broadcast/rdd deserialization.\n+\t *\n+\t * @param in object input\n+\t * @throws IOException if IOException occur\n+\t */\n+\t@Override\n+\tpublic void readExternal(ObjectInput in) throws IOException {\n+\t\t_colList = new int[in.readInt()];\n+\t\tfor(int i = 0; i < _colList.length; i++)\n+\t\t\t_colList[i] = in.readInt();\n+\t}\n+\n+\tpublic void shiftCols(int offset) {\n+\t\tfor(int i = 0; i < _colList.length; i++) {\n+\t\t\t_colList[i] += offset;\n+\t\t}\n+\t}\n+}\n\\ No newline at end of file"
  },
  {
    "sha": "189603d23d7f2cb5338c07ba87784cb0da15861c",
    "filename": "src/main/java/org/apache/sysds/runtime/transform/encode/MultiColumnEncoder.java",
    "status": "added",
    "additions": 446,
    "deletions": 0,
    "changes": 446,
    "blob_url": "https://github.com/apache/systemds/blob/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/encode/MultiColumnEncoder.java",
    "raw_url": "https://github.com/apache/systemds/raw/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/encode/MultiColumnEncoder.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/transform/encode/MultiColumnEncoder.java?ref=cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd",
    "patch": "@@ -0,0 +1,446 @@\n+package org.apache.sysds.runtime.transform.encode;\n+\n+import java.io.IOException;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.sysds.common.Types;\n+import org.apache.sysds.runtime.DMLRuntimeException;\n+import org.apache.sysds.runtime.matrix.data.FrameBlock;\n+import org.apache.sysds.runtime.matrix.data.MatrixBlock;\n+import org.apache.sysds.runtime.util.IndexRange;\n+\n+public class MultiColumnEncoder implements Encoder {\n+\n+\tprotected static final Log LOG = LogFactory.getLog(MultiColumnEncoder.class.getName());\n+\tprivate List<ColumnEncoderComposite> _columnEncoders;\n+\t// These encoders are deprecated and will be fazed out soon.\n+\tprivate EncoderMVImpute _legacyMVImpute = null;\n+\tprivate EncoderOmit _legacyOmit = null;\n+\tprivate int _colOffset = 0; // offset for federated Workers who are using subrange encoders\n+\tprivate FrameBlock _meta = null;\n+\n+\tpublic MultiColumnEncoder(List<ColumnEncoderComposite> columnEncoders) {\n+\t\t_columnEncoders = columnEncoders;\n+\t}\n+\n+\tpublic MultiColumnEncoder() {\n+\t\t_columnEncoders = new ArrayList<>();\n+\t}\n+\n+\tpublic MatrixBlock encode(FrameBlock in) {\n+\t\tMatrixBlock out;\n+\t\ttry {\n+\t\t\tbuild(in);\n+\t\t\t_meta = getMetaData(new FrameBlock(in.getNumColumns(), Types.ValueType.STRING));\n+\t\t\tinitMetaData(_meta);\n+\t\t\t// apply meta data\n+\t\t\tout = apply(in);\n+\t\t}\n+\t\tcatch(Exception ex) {\n+\t\t\tLOG.error(\"Failed transform-encode frame with \\n\" + this);\n+\t\t\tthrow ex;\n+\t\t}\n+\t\treturn out;\n+\t}\n+\n+\tpublic void build(FrameBlock in) {\n+\t\tfor(ColumnEncoder columnEncoder : _columnEncoders)\n+\t\t\tcolumnEncoder.build(in);\n+\t\tlegacyBuild(in);\n+\t}\n+\n+\tpublic void legacyBuild(FrameBlock in) {\n+\t\tif(_legacyOmit != null)\n+\t\t\t_legacyOmit.build(in);\n+\t\tif(_legacyMVImpute != null)\n+\t\t\t_legacyMVImpute.build(in);\n+\t}\n+\n+\tpublic MatrixBlock apply(FrameBlock in) {\n+\t\tint numCols = in.getNumColumns() + getNumExtraCols();\n+\t\tMatrixBlock out = new MatrixBlock(in.getNumRows(), numCols, false);\n+\t\treturn apply(in, out, 0);\n+\t}\n+\n+\tpublic MatrixBlock apply(FrameBlock in, MatrixBlock out, int outputCol) {\n+\t\t// There should be a encoder for every column\n+\t\tint numEncoders = getFromAll(ColumnEncoderComposite.class, ColumnEncoder::getColID).size();\n+\t\tif(in.getNumColumns() != numEncoders)\n+\t\t\tthrow new DMLRuntimeException(\"Not every column in has a CompositeEncoder. Please make sure every column \"\n+\t\t\t\t+ \"has a encoder or slice the input accordingly\");\n+\n+\t\ttry {\n+\t\t\tint offset = outputCol;\n+\t\t\tfor(ColumnEncoderComposite columnEncoder : _columnEncoders) {\n+\t\t\t\tcolumnEncoder.apply(in, out, columnEncoder._colID - 1 + offset);\n+\t\t\t\tif(columnEncoder.hasEncoder(ColumnEncoderDummycode.class))\n+\t\t\t\t\toffset += columnEncoder.getEncoder(ColumnEncoderDummycode.class)._domainSize - 1;\n+\t\t\t}\n+\t\t\tif(_legacyOmit != null)\n+\t\t\t\tout = _legacyOmit.apply(in, out);\n+\t\t\tif(_legacyMVImpute != null)\n+\t\t\t\tout = _legacyMVImpute.apply(in, out);\n+\t\t}\n+\t\tcatch(Exception ex) {\n+\t\t\tLOG.error(\"Failed to transform-apply frame with \\n\" + this);\n+\t\t\tthrow ex;\n+\t\t}\n+\t\treturn out;\n+\t}\n+\n+\t@Override\n+\tpublic FrameBlock getMetaData(FrameBlock meta) {\n+\t\tif(_meta != null)\n+\t\t\treturn _meta;\n+\t\tfor(ColumnEncoder columnEncoder : _columnEncoders)\n+\t\t\tcolumnEncoder.getMetaData(meta);\n+\t\tif(_legacyOmit != null)\n+\t\t\t_legacyOmit.getMetaData(meta);\n+\t\tif(_legacyMVImpute != null)\n+\t\t\t_legacyMVImpute.getMetaData(meta);\n+\t\treturn meta;\n+\t}\n+\n+\t@Override\n+\tpublic void initMetaData(FrameBlock meta) {\n+\t\tfor(ColumnEncoder columnEncoder : _columnEncoders)\n+\t\t\tcolumnEncoder.initMetaData(meta);\n+\t\tif(_legacyOmit != null)\n+\t\t\t_legacyOmit.initMetaData(meta);\n+\t\tif(_legacyMVImpute != null)\n+\t\t\t_legacyMVImpute.initMetaData(meta);\n+\t}\n+\n+\t@Override\n+\tpublic void prepareBuildPartial() {\n+\t\tfor(Encoder encoder : _columnEncoders)\n+\t\t\tencoder.prepareBuildPartial();\n+\t}\n+\n+\t@Override\n+\tpublic void buildPartial(FrameBlock in) {\n+\t\tfor(Encoder encoder : _columnEncoders)\n+\t\t\tencoder.buildPartial(in);\n+\t}\n+\n+\t/**\n+\t * Obtain the column mapping of encoded frames based on the passed meta data frame.\n+\t *\n+\t * @param meta meta data frame block\n+\t * @return matrix with column mapping (one row per attribute)\n+\t */\n+\tpublic MatrixBlock getColMapping(FrameBlock meta) {\n+\t\tMatrixBlock out = new MatrixBlock(meta.getNumColumns(), 3, false);\n+\t\tList<ColumnEncoderDummycode> dc = getColumnEncoders(ColumnEncoderDummycode.class);\n+\n+\t\tfor(int i = 0, ni = 0; i < out.getNumRows(); i++) {\n+\t\t\tfinal int colID = i + 1; // 1-based\n+\t\t\tint nColID = ni + 1;\n+\t\t\tList<ColumnEncoderDummycode> encoder = dc.stream().filter(e -> e.getColID() == colID)\n+\t\t\t\t.collect(Collectors.toList());\n+\t\t\tassert encoder.size() <= 1;\n+\t\t\tif(encoder.size() == 1) {\n+\t\t\t\tni += meta.getColumnMetadata(i).getNumDistinct();\n+\t\t\t}\n+\t\t\telse {\n+\t\t\t\tni++;\n+\t\t\t}\n+\t\t\tout.quickSetValue(i, 0, colID);\n+\t\t\tout.quickSetValue(i, 1, nColID);\n+\t\t\tout.quickSetValue(i, 2, ni);\n+\t\t}\n+\t\treturn out;\n+\t}\n+\n+\t@Override\n+\tpublic void updateIndexRanges(long[] beginDims, long[] endDims, int offset) {\n+\t\t_columnEncoders.forEach(encoder -> encoder.updateIndexRanges(beginDims, endDims, offset));\n+\t\tif(_legacyOmit != null)\n+\t\t\t_legacyOmit.updateIndexRanges(beginDims, endDims);\n+\t\tif(_legacyMVImpute != null)\n+\t\t\t_legacyMVImpute.updateIndexRanges(beginDims, endDims);\n+\t}\n+\n+\t@Override\n+\tpublic void writeExternal(ObjectOutput out) throws IOException {\n+\t\tout.writeBoolean(_legacyMVImpute != null);\n+\t\tif(_legacyMVImpute != null)\n+\t\t\t_legacyMVImpute.writeExternal(out);\n+\t\tout.writeBoolean(_legacyOmit != null);\n+\t\tif(_legacyOmit != null)\n+\t\t\t_legacyOmit.writeExternal(out);\n+\n+\t\tout.writeInt(_colOffset);\n+\t\tout.writeInt(_columnEncoders.size());\n+\t\tfor(ColumnEncoder columnEncoder : _columnEncoders) {\n+\t\t\tout.writeInt(columnEncoder._colID);\n+\t\t\tcolumnEncoder.writeExternal(out);\n+\t\t}\n+\t\tout.writeBoolean(_meta != null);\n+\t\tif(_meta != null)\n+\t\t\t_meta.write(out);\n+\t}\n+\n+\t@Override\n+\tpublic void readExternal(ObjectInput in) throws IOException, ClassNotFoundException {\n+\t\tif(in.readBoolean()) {\n+\t\t\t_legacyMVImpute = new EncoderMVImpute();\n+\t\t\t_legacyMVImpute.readExternal(in);\n+\t\t}\n+\t\tif(in.readBoolean()) {\n+\t\t\t_legacyOmit = new EncoderOmit();\n+\t\t\t_legacyOmit.readExternal(in);\n+\t\t}\n+\n+\t\t_colOffset = in.readInt();\n+\t\tint encodersSize = in.readInt();\n+\t\t_columnEncoders = new ArrayList<>();\n+\t\tfor(int i = 0; i < encodersSize; i++) {\n+\t\t\tint colID = in.readInt();\n+\t\t\tColumnEncoderComposite columnEncoder = new ColumnEncoderComposite();\n+\t\t\tcolumnEncoder.readExternal(in);\n+\t\t\tcolumnEncoder.setColID(colID);\n+\t\t\t_columnEncoders.add(columnEncoder);\n+\t\t}\n+\t\tif(in.readBoolean()) {\n+\t\t\tFrameBlock meta = new FrameBlock();\n+\t\t\tmeta.readFields(in);\n+\t\t\t_meta = meta;\n+\t\t}\n+\t}\n+\n+\tpublic <T extends ColumnEncoder> List<T> getColumnEncoders(Class<T> type) {\n+\t\t// TODO cache results for faster access\n+\t\tList<T> ret = new ArrayList<>();\n+\t\tfor(ColumnEncoder encoder : _columnEncoders) {\n+\t\t\tif(encoder.getClass().equals(ColumnEncoderComposite.class) && type != ColumnEncoderComposite.class) {\n+\t\t\t\tencoder = ((ColumnEncoderComposite) encoder).getEncoder(type);\n+\t\t\t}\n+\t\t\tif(encoder != null && encoder.getClass().equals(type)) {\n+\t\t\t\tret.add((T) encoder);\n+\t\t\t}\n+\t\t}\n+\t\treturn ret;\n+\t}\n+\n+\tpublic <T extends ColumnEncoder> T getColumnEncoder(int colID, Class<T> type) {\n+\t\tfor(T encoder : getColumnEncoders(type)) {\n+\t\t\tif(encoder._colID == colID) {\n+\t\t\t\treturn encoder;\n+\t\t\t}\n+\t\t}\n+\t\treturn null;\n+\t}\n+\n+\tpublic <T extends ColumnEncoder, E> List<E> getFromAll(Class<T> type, Function<? super T, ? extends E> mapper) {\n+\t\treturn getColumnEncoders(type).stream().map(mapper).collect(Collectors.toList());\n+\t}\n+\n+\tpublic <T extends ColumnEncoder> int[] getFromAllIntArray(Class<T> type,\n+\t\tFunction<? super T, ? extends Integer> mapper) {\n+\t\treturn getFromAll(type, mapper).stream().mapToInt(i -> i).toArray();\n+\t}\n+\n+\tpublic <T extends ColumnEncoder> double[] getFromAllDoubleArray(Class<T> type,\n+\t\tFunction<? super T, ? extends Double> mapper) {\n+\t\treturn getFromAll(type, mapper).stream().mapToDouble(i -> i).toArray();\n+\t}\n+\n+\tpublic List<ColumnEncoderComposite> getColumnEncoders() {\n+\t\treturn _columnEncoders;\n+\t}\n+\n+\tpublic List<ColumnEncoderComposite> getCompositeEncodersForID(int colID) {\n+\t\treturn _columnEncoders.stream().filter(encoder -> encoder._colID == colID).collect(Collectors.toList());\n+\t}\n+\n+\tpublic <T extends ColumnEncoder> List<Class<T>> getEncoderTypes(int colID) {\n+\t\tHashSet<Class<T>> set = new HashSet<>();\n+\t\tfor(ColumnEncoderComposite encoderComp : _columnEncoders) {\n+\t\t\tif(encoderComp._colID != colID && colID != -1)\n+\t\t\t\tcontinue;\n+\t\t\tfor(ColumnEncoder encoder : encoderComp.getEncoders()) {\n+\t\t\t\tset.add((Class<T>) encoder.getClass());\n+\t\t\t}\n+\t\t}\n+\t\treturn new ArrayList<>(set);\n+\t}\n+\n+\tpublic <T extends ColumnEncoder> List<Class<T>> getEncoderTypes() {\n+\t\treturn getEncoderTypes(-1);\n+\t}\n+\n+\tpublic int getNumExtraCols() {\n+\t\tList<ColumnEncoderDummycode> dc = getColumnEncoders(ColumnEncoderDummycode.class);\n+\t\tif(dc.isEmpty()) {\n+\t\t\treturn 0;\n+\t\t}\n+\t\treturn dc.stream().map(ColumnEncoderDummycode::getDomainSize).mapToInt(i -> i).sum() - dc.size();\n+\t}\n+\n+\tpublic int getNumExtraCols(IndexRange ixRange) {\n+\t\tList<ColumnEncoderDummycode> dc = getColumnEncoders(ColumnEncoderDummycode.class).stream()\n+\t\t\t.filter(dce -> ixRange.inColRange(dce._colID)).collect(Collectors.toList());\n+\t\tif(dc.isEmpty()) {\n+\t\t\treturn 0;\n+\t\t}\n+\t\treturn dc.stream().map(ColumnEncoderDummycode::getDomainSize).mapToInt(i -> i).sum() - dc.size();\n+\t}\n+\n+\tpublic <T extends ColumnEncoder> boolean containsEncoderForID(int colID, Class<T> type) {\n+\t\treturn getColumnEncoders(type).stream().anyMatch(encoder -> encoder.getColID() == colID);\n+\t}\n+\n+\tpublic <T extends ColumnEncoder, E> void applyToAll(Class<T> type, Consumer<? super T> function) {\n+\t\tgetColumnEncoders(type).forEach(function);\n+\t}\n+\n+\tpublic <T extends ColumnEncoder, E> void applyToAll(Consumer<? super ColumnEncoderComposite> function) {\n+\t\tgetColumnEncoders().forEach(function);\n+\t}\n+\n+\tpublic MultiColumnEncoder subRangeEncoder(IndexRange ixRange) {\n+\t\tList<ColumnEncoderComposite> encoders = new ArrayList<>();\n+\t\tfor(long i = ixRange.colStart; i < ixRange.colEnd; i++) {\n+\t\t\tencoders.addAll(getCompositeEncodersForID((int) i));\n+\t\t}\n+\t\tMultiColumnEncoder subRangeEncoder = new MultiColumnEncoder(encoders);\n+\t\tsubRangeEncoder._colOffset = (int) -ixRange.colStart + 1;\n+\t\tif(_legacyOmit != null)\n+\t\t\tsubRangeEncoder.addReplaceLegacyEncoder(_legacyOmit.subRangeEncoder(ixRange));\n+\t\tif(_legacyMVImpute != null)\n+\t\t\tsubRangeEncoder.addReplaceLegacyEncoder(_legacyMVImpute.subRangeEncoder(ixRange));\n+\t\treturn subRangeEncoder;\n+\t}\n+\n+\tpublic <T extends ColumnEncoder> MultiColumnEncoder subRangeEncoder(IndexRange ixRange, Class<T> type) {\n+\t\tList<T> encoders = new ArrayList<>();\n+\t\tfor(long i = ixRange.colStart; i < ixRange.colEnd; i++) {\n+\t\t\tencoders.add(getColumnEncoder((int) i, type));\n+\t\t}\n+\t\tif(type.equals(ColumnEncoderComposite.class))\n+\t\t\treturn new MultiColumnEncoder((List<ColumnEncoderComposite>) encoders);\n+\t\telse\n+\t\t\treturn new MultiColumnEncoder(\n+\t\t\t\tencoders.stream().map(ColumnEncoderComposite::new).collect(Collectors.toList()));\n+\t}\n+\n+\tpublic void mergeReplace(MultiColumnEncoder multiEncoder) {\n+\t\tfor(ColumnEncoderComposite otherEncoder : multiEncoder._columnEncoders) {\n+\t\t\tColumnEncoderComposite encoder = getColumnEncoder(otherEncoder._colID, otherEncoder.getClass());\n+\t\t\tif(encoder != null) {\n+\t\t\t\t_columnEncoders.remove(encoder);\n+\t\t\t}\n+\t\t\t_columnEncoders.add(otherEncoder);\n+\t\t}\n+\t}\n+\n+\tpublic void mergeAt(Encoder other, int columnOffset, int row) {\n+\t\tif(other instanceof MultiColumnEncoder) {\n+\t\t\tfor(ColumnEncoder encoder : ((MultiColumnEncoder) other)._columnEncoders) {\n+\t\t\t\taddEncoder(encoder, columnOffset);\n+\t\t\t}\n+\t\t\t// +1 since legacy function uses 1-based\n+\t\t\tlegacyMergeAt((MultiColumnEncoder) other, row, columnOffset + 1);\n+\t\t}\n+\t\telse {\n+\t\t\taddEncoder((ColumnEncoder) other, columnOffset);\n+\t\t}\n+\t}\n+\n+\tprivate void legacyMergeAt(MultiColumnEncoder other, int row, int col) {\n+\t\tif(other._legacyOmit != null)\n+\t\t\tother._legacyOmit.shiftCols(col - 1);\n+\t\tif(other._legacyOmit != null) {\n+\t\t\tif(_legacyOmit == null)\n+\t\t\t\t_legacyOmit = new EncoderOmit();\n+\t\t\t_legacyOmit.mergeAt(other._legacyOmit, row, col);\n+\t\t}\n+\n+\t\tif(other._legacyMVImpute != null)\n+\t\t\tother._legacyMVImpute.shiftCols(col - 1);\n+\t\tif(_legacyMVImpute != null && other._legacyMVImpute != null)\n+\t\t\t_legacyMVImpute.mergeAt(other._legacyMVImpute, row, col);\n+\t\telse if(_legacyMVImpute == null)\n+\t\t\t_legacyMVImpute = other._legacyMVImpute;\n+\n+\t}\n+\n+\tprivate void addEncoder(ColumnEncoder encoder, int columnOffset) {\n+\t\t// Check if same encoder exists\n+\t\tint colId = encoder._colID + columnOffset;\n+\t\tColumnEncoder presentEncoder = getColumnEncoder(colId, encoder.getClass());\n+\t\tif(presentEncoder != null) {\n+\t\t\tencoder.shiftCol(columnOffset);\n+\t\t\tpresentEncoder.mergeAt(encoder);\n+\t\t}\n+\t\telse {\n+\t\t\t// Check if CompositeEncoder for this colID exists\n+\t\t\tColumnEncoderComposite presentComposite = getColumnEncoder(colId, ColumnEncoderComposite.class);\n+\t\t\tif(presentComposite != null) {\n+\t\t\t\t// if here encoder can never be a CompositeEncoder\n+\t\t\t\tencoder.shiftCol(columnOffset);\n+\t\t\t\tpresentComposite.mergeAt(encoder);\n+\t\t\t}\n+\t\t\telse {\n+\t\t\t\tencoder.shiftCol(columnOffset);\n+\t\t\t\tif(encoder instanceof ColumnEncoderComposite) {\n+\t\t\t\t\t_columnEncoders.add((ColumnEncoderComposite) encoder);\n+\t\t\t\t}\n+\t\t\t\telse {\n+\t\t\t\t\t_columnEncoders.add(new ColumnEncoderComposite(encoder));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tpublic <T extends LegacyEncoder> void addReplaceLegacyEncoder(T encoder) {\n+\t\tif(encoder.getClass() == EncoderMVImpute.class) {\n+\t\t\t_legacyMVImpute = (EncoderMVImpute) encoder;\n+\t\t}\n+\t\telse if(encoder.getClass().equals(EncoderOmit.class)) {\n+\t\t\t_legacyOmit = (EncoderOmit) encoder;\n+\t\t}\n+\t\telse {\n+\t\t\tthrow new DMLRuntimeException(\"Tried to add non legacy Encoder\");\n+\t\t}\n+\t}\n+\n+\tpublic <T extends LegacyEncoder> boolean hasLegacyEncoder(Class<T> type) {\n+\t\tif(type.equals(EncoderMVImpute.class))\n+\t\t\treturn _legacyMVImpute != null;\n+\t\tif(type.equals(EncoderOmit.class))\n+\t\t\treturn _legacyOmit != null;\n+\t\tassert false;\n+\t\treturn false;\n+\t}\n+\n+\tpublic <T extends LegacyEncoder> T getLegacyEncoder(Class<T> type) {\n+\t\tif(type.equals(EncoderMVImpute.class))\n+\t\t\treturn (T) _legacyMVImpute;\n+\t\tif(type.equals(EncoderOmit.class))\n+\t\t\treturn (T) _legacyOmit;\n+\t\tassert false;\n+\t\treturn null;\n+\t}\n+\n+\t/*\n+\t * This function applies the _columOffset to all encoders. Used in federated env.\n+\t */\n+\tpublic void applyColumnOffset() {\n+\t\tapplyToAll(e -> e.shiftCol(_colOffset));\n+\t\tif(_legacyOmit != null)\n+\t\t\t_legacyOmit.shiftCols(_colOffset);\n+\t\tif(_legacyMVImpute != null)\n+\t\t\t_legacyMVImpute.shiftCols(_colOffset);\n+\t}\n+}"
  },
  {
    "sha": "7899323ebf816d1938d531a6c654c903b6047457",
    "filename": "src/main/java/org/apache/sysds/runtime/transform/meta/TfMetaUtils.java",
    "status": "modified",
    "additions": 33,
    "deletions": 16,
    "changes": 49,
    "blob_url": "https://github.com/apache/systemds/blob/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/meta/TfMetaUtils.java",
    "raw_url": "https://github.com/apache/systemds/raw/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/main/java/org/apache/sysds/runtime/transform/meta/TfMetaUtils.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/main/java/org/apache/sysds/runtime/transform/meta/TfMetaUtils.java?ref=cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd",
    "patch": "@@ -147,6 +147,25 @@ else if(minCol == -1 && maxCol == -1)\n \t\treturn arr;\n \t}\n \n+\tpublic static int parseJsonObjectID(JSONObject colspec, String[] colnames, int minCol, int maxCol, boolean ids) throws JSONException {\n+\t\tint ix;\n+\t\tif(ids) {\n+\t\t\tix = colspec.getInt(\"id\");\n+\t\t\tif(maxCol != -1 && ix >= maxCol)\n+\t\t\t\tix = -1;\n+\t\t\tif(minCol != -1 && ix >= 0)\n+\t\t\t\tix -= minCol - 1;\n+\t\t}\n+\t\telse {\n+\t\t\tix = ArrayUtils.indexOf(colnames, colspec.get(\"name\")) + 1;\n+\t\t}\n+\t\tif(ix > 0)\n+\t\t\treturn ix;\n+\t\telse if(minCol == -1 && maxCol == -1)\n+\t\t\tthrow new RuntimeException(\"Specified column '\" + colspec.get(ids ? \"id\" : \"name\") + \"' does not exist.\");\n+\t\treturn -1;\n+\t}\n+\n \tpublic static int[] parseJsonObjectIDList(JSONObject spec, String[] colnames, String group, int minCol, int maxCol)\n \t\tthrows JSONException {\n \t\tList<Integer> colList = new ArrayList<>();\n@@ -157,22 +176,9 @@ else if(minCol == -1 && maxCol == -1)\n \t\t\tJSONArray colspecs = (JSONArray) spec.get(group);\n \t\t\tfor(Object o : colspecs) {\n \t\t\t\tJSONObject colspec = (JSONObject) o;\n-\t\t\t\tint ix;\n-\t\t\t\tif(ids) {\n-\t\t\t\t\tix = colspec.getInt(\"id\");\n-\t\t\t\t\tif(maxCol != -1 && ix >= maxCol)\n-\t\t\t\t\t\tix = -1;\n-\t\t\t\t\tif(minCol != -1 && ix >= 0)\n-\t\t\t\t\t\tix -= minCol - 1;\n-\t\t\t\t}\n-\t\t\t\telse {\n-\t\t\t\t\tix = ArrayUtils.indexOf(colnames, colspec.get(\"name\")) + 1;\n-\t\t\t\t}\n-\t\t\t\tif(ix > 0)\n-\t\t\t\t\tcolList.add(ix);\n-\t\t\t\telse if(minCol == -1 && maxCol == -1)\n-\t\t\t\t\tthrow new RuntimeException(\n-\t\t\t\t\t\t\"Specified column '\" + colspec.get(ids ? \"id\" : \"name\") + \"' does not exist.\");\n+\t\t\t\tint id = parseJsonObjectID(colspec, colnames, minCol, maxCol, ids);\n+\t\t\t\tif(id > 0)\n+\t\t\t\t\tcolList.add(id);\n \t\t\t}\n \n \t\t\t// ensure ascending order of column IDs\n@@ -182,6 +188,17 @@ else if(minCol == -1 && maxCol == -1)\n \t\treturn arr;\n \t}\n \n+\t/**\n+\t * Get K value used for calculation during feature hashing from parsed specifications.\n+\t * @param parsedSpec parsed specifications\n+\t * @return K value\n+\t * @throws JSONException\n+\t */\n+\tpublic static long getK(JSONObject parsedSpec) throws JSONException {\n+\t\treturn parsedSpec.getLong(\"K\");\n+\t}\n+\n+\n \t/**\n \t * Reads transform meta data from an HDFS file path and converts it into an in-memory\n \t * FrameBlock object."
  },
  {
    "sha": "96c95f5d1eac0b13fa0ccb20f756c7ed095229ee",
    "filename": "src/test/java/org/apache/sysds/test/functions/federated/algorithms/FederatedLmPipeline.java",
    "status": "modified",
    "additions": 4,
    "deletions": 4,
    "changes": 8,
    "blob_url": "https://github.com/apache/systemds/blob/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/test/java/org/apache/sysds/test/functions/federated/algorithms/FederatedLmPipeline.java",
    "raw_url": "https://github.com/apache/systemds/raw/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/test/java/org/apache/sysds/test/functions/federated/algorithms/FederatedLmPipeline.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/test/java/org/apache/sysds/test/functions/federated/algorithms/FederatedLmPipeline.java?ref=cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd",
    "patch": "@@ -24,7 +24,7 @@\n import org.apache.sysds.runtime.instructions.InstructionUtils;\n import org.apache.sysds.runtime.matrix.data.LibMatrixMult;\n import org.apache.sysds.runtime.matrix.data.MatrixBlock;\n-import org.apache.sysds.runtime.transform.encode.EncoderRecode;\n+import org.apache.sysds.runtime.transform.encode.ColumnEncoderRecode;\n import org.apache.sysds.test.AutomatedTestBase;\n import org.apache.sysds.test.TestConfiguration;\n import org.apache.sysds.test.TestUtils;\n@@ -72,8 +72,8 @@ public void federatedLmPipelineSampled4Workers() {\n \n \tpublic void federatedLmPipeline(ExecMode execMode, boolean contSplits, String TEST_NAME) {\n \t\tExecMode oldExec = setExecMode(execMode);\n-\t\tboolean oldSort = EncoderRecode.SORT_RECODE_MAP;\n-\t\tEncoderRecode.SORT_RECODE_MAP = true;\n+\t\tboolean oldSort = ColumnEncoderRecode.SORT_RECODE_MAP;\n+\t\tColumnEncoderRecode.SORT_RECODE_MAP = true;\n \n \t\tgetAndLoadTestConfiguration(TEST_NAME);\n \t\tString HOME = SCRIPT_DIR + TEST_DIR;\n@@ -138,7 +138,7 @@ public void federatedLmPipeline(ExecMode execMode, boolean contSplits, String TE\n \t\t}\n \t\tfinally {\n \t\t\tresetExecMode(oldExec);\n-\t\t\tEncoderRecode.SORT_RECODE_MAP = oldSort;\n+\t\t\tColumnEncoderRecode.SORT_RECODE_MAP = oldSort;\n \t\t}\n \t}\n }"
  },
  {
    "sha": "90861a6f8666aa31fb56f94aa0bd31b2285a627a",
    "filename": "src/test/java/org/apache/sysds/test/functions/federated/transform/TransformFederatedEncodeApplyTest.java",
    "status": "modified",
    "additions": 1,
    "deletions": 3,
    "changes": 4,
    "blob_url": "https://github.com/apache/systemds/blob/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/test/java/org/apache/sysds/test/functions/federated/transform/TransformFederatedEncodeApplyTest.java",
    "raw_url": "https://github.com/apache/systemds/raw/cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd/src/test/java/org/apache/sysds/test/functions/federated/transform/TransformFederatedEncodeApplyTest.java",
    "contents_url": "https://api.github.com/repos/apache/systemds/contents/src/test/java/org/apache/sysds/test/functions/federated/transform/TransformFederatedEncodeApplyTest.java?ref=cc59bdddb2bfdc0b28cf9618db35cfd9eb3b81dd",
    "patch": "@@ -148,9 +148,7 @@ public void testHomesBinningDummyColnamesCSV() {\n \t}\n \n \t@Test\n-\tpublic void testHomesOmitColnamesCSV() {\n-\t\trunTransformTest(TransformType.OMIT, true, false);\n-\t}\n+\tpublic void testHomesOmitColnamesCSV() { runTransformTest(TransformType.OMIT, true, false); }\n \n \t@Test\n \tpublic void testHomesImputeColnamesCSV() {"
  }
]
