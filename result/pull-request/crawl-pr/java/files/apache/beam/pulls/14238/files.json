[
  {
    "sha": "f5a32612efdf6cf0aa14ac41f7c0acc971e2e699",
    "filename": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BatchLoads.java",
    "status": "modified",
    "additions": 37,
    "deletions": 30,
    "changes": 67,
    "blob_url": "https://github.com/apache/beam/blob/9843c78456243f8bb877bea528597879b4a40676/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BatchLoads.java",
    "raw_url": "https://github.com/apache/beam/raw/9843c78456243f8bb877bea528597879b4a40676/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BatchLoads.java",
    "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BatchLoads.java?ref=9843c78456243f8bb877bea528597879b4a40676",
    "patch": "@@ -30,10 +30,8 @@\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.coders.IterableCoder;\n import org.apache.beam.sdk.coders.KvCoder;\n-import org.apache.beam.sdk.coders.ListCoder;\n import org.apache.beam.sdk.coders.NullableCoder;\n import org.apache.beam.sdk.coders.ShardedKeyCoder;\n-import org.apache.beam.sdk.coders.StringUtf8Coder;\n import org.apache.beam.sdk.coders.VoidCoder;\n import org.apache.beam.sdk.extensions.gcp.util.gcsfs.GcsPath;\n import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.CreateDisposition;\n@@ -147,8 +145,8 @@\n   private ValueProvider<String> loadJobProjectId;\n   private final Coder<ElementT> elementCoder;\n   private final RowWriterFactory<ElementT, DestinationT> rowWriterFactory;\n-  private String kmsKey;\n-  private boolean clusteringEnabled;\n+  private final String kmsKey;\n+  private final boolean clusteringEnabled;\n \n   // The maximum number of times to retry failed load or copy jobs.\n   private int maxRetryJobs = DEFAULT_MAX_RETRY_JOBS;\n@@ -274,6 +272,8 @@ public void validate(PipelineOptions options) {\n   private WriteResult expandTriggered(PCollection<KV<DestinationT, ElementT>> input) {\n     Pipeline p = input.getPipeline();\n     final PCollectionView<String> loadJobIdPrefixView = createJobIdPrefixView(p, JobType.LOAD);\n+    final PCollectionView<String> tempLoadJobIdPrefixView =\n+        createJobIdPrefixView(p, JobType.TEMP_TABLE_LOAD);\n     final PCollectionView<String> copyJobIdPrefixView = createJobIdPrefixView(p, JobType.COPY);\n     final PCollectionView<String> tempFilePrefixView =\n         createTempFilePrefixView(p, loadJobIdPrefixView);\n@@ -321,20 +321,20 @@ private WriteResult expandTriggered(PCollection<KV<DestinationT, ElementT>> inpu\n                             .plusDelayOf(triggeringFrequency)))\n                 .discardingFiredPanes());\n \n-    TupleTag<KV<ShardedKey<DestinationT>, List<String>>> multiPartitionsTag =\n+    TupleTag<KV<ShardedKey<DestinationT>, WritePartition.Result>> multiPartitionsTag =\n         new TupleTag<>(\"multiPartitionsTag\");\n-    TupleTag<KV<ShardedKey<DestinationT>, List<String>>> singlePartitionTag =\n+    TupleTag<KV<ShardedKey<DestinationT>, WritePartition.Result>> singlePartitionTag =\n         new TupleTag<>(\"singlePartitionTag\");\n \n     // If we have non-default triggered output, we can't use the side-input technique used in\n     // expandUntriggered. Instead make the result list a main input. Apply a GroupByKey first for\n     // determinism.\n     PCollectionTuple partitions =\n         results\n-            .apply(\"AttachSingletonKey\", WithKeys.of((Void) null))\n+            .apply(\"AttachDestinationKey\", WithKeys.of(result -> result.destination))\n             .setCoder(\n-                KvCoder.of(VoidCoder.of(), WriteBundlesToFiles.ResultCoder.of(destinationCoder)))\n-            .apply(\"GroupOntoSingleton\", GroupByKey.create())\n+                KvCoder.of(destinationCoder, WriteBundlesToFiles.ResultCoder.of(destinationCoder)))\n+            .apply(\"GroupFilesByDestination\", GroupByKey.create())\n             .apply(\"ExtractResultValues\", Values.create())\n             .apply(\n                 \"WritePartitionTriggered\",\n@@ -350,13 +350,13 @@ private WriteResult expandTriggered(PCollection<KV<DestinationT, ElementT>> inpu\n                             rowWriterFactory))\n                     .withSideInputs(tempFilePrefixView)\n                     .withOutputTags(multiPartitionsTag, TupleTagList.of(singlePartitionTag)));\n-    PCollection<KV<TableDestination, String>> tempTables =\n-        writeTempTables(partitions.get(multiPartitionsTag), loadJobIdPrefixView);\n+    PCollection<KV<TableDestination, WriteTables.Result>> tempTables =\n+        writeTempTables(partitions.get(multiPartitionsTag), tempLoadJobIdPrefixView);\n \n     tempTables\n         // Now that the load job has happened, we want the rename to happen immediately.\n         .apply(\n-            Window.<KV<TableDestination, String>>into(new GlobalWindows())\n+            Window.<KV<TableDestination, WriteTables.Result>>into(new GlobalWindows())\n                 .triggering(Repeatedly.forever(AfterPane.elementCountAtLeast(1))))\n         .apply(WithKeys.of((Void) null))\n         .setCoder(KvCoder.of(VoidCoder.of(), tempTables.getCoder()))\n@@ -381,6 +381,9 @@ private WriteResult expandTriggered(PCollection<KV<DestinationT, ElementT>> inpu\n   public WriteResult expandUntriggered(PCollection<KV<DestinationT, ElementT>> input) {\n     Pipeline p = input.getPipeline();\n     final PCollectionView<String> loadJobIdPrefixView = createJobIdPrefixView(p, JobType.LOAD);\n+    final PCollectionView<String> tempLoadJobIdPrefixView =\n+        createJobIdPrefixView(p, JobType.TEMP_TABLE_LOAD);\n+    final PCollectionView<String> copyJobIdPrefixView = createJobIdPrefixView(p, JobType.COPY);\n     final PCollectionView<String> tempFilePrefixView =\n         createTempFilePrefixView(p, loadJobIdPrefixView);\n     PCollection<KV<DestinationT, ElementT>> inputInGlobalWindow =\n@@ -394,10 +397,10 @@ public WriteResult expandUntriggered(PCollection<KV<DestinationT, ElementT>> inp\n             ? writeDynamicallyShardedFilesUntriggered(inputInGlobalWindow, tempFilePrefixView)\n             : writeStaticallyShardedFiles(inputInGlobalWindow, tempFilePrefixView);\n \n-    TupleTag<KV<ShardedKey<DestinationT>, List<String>>> multiPartitionsTag =\n-        new TupleTag<KV<ShardedKey<DestinationT>, List<String>>>(\"multiPartitionsTag\") {};\n-    TupleTag<KV<ShardedKey<DestinationT>, List<String>>> singlePartitionTag =\n-        new TupleTag<KV<ShardedKey<DestinationT>, List<String>>>(\"singlePartitionTag\") {};\n+    TupleTag<KV<ShardedKey<DestinationT>, WritePartition.Result>> multiPartitionsTag =\n+        new TupleTag<KV<ShardedKey<DestinationT>, WritePartition.Result>>(\"multiPartitionsTag\") {};\n+    TupleTag<KV<ShardedKey<DestinationT>, WritePartition.Result>> singlePartitionTag =\n+        new TupleTag<KV<ShardedKey<DestinationT>, WritePartition.Result>>(\"singlePartitionTag\") {};\n \n     // This transform will look at the set of files written for each table, and if any table has\n     // too many files or bytes, will partition that table's files into multiple partitions for\n@@ -420,8 +423,8 @@ public WriteResult expandUntriggered(PCollection<KV<DestinationT, ElementT>> inp\n                             rowWriterFactory))\n                     .withSideInputs(tempFilePrefixView)\n                     .withOutputTags(multiPartitionsTag, TupleTagList.of(singlePartitionTag)));\n-    PCollection<KV<TableDestination, String>> tempTables =\n-        writeTempTables(partitions.get(multiPartitionsTag), loadJobIdPrefixView);\n+    PCollection<KV<TableDestination, WriteTables.Result>> tempTables =\n+        writeTempTables(partitions.get(multiPartitionsTag), tempLoadJobIdPrefixView);\n \n     tempTables\n         .apply(\"ReifyRenameInput\", new ReifyAsIterable<>())\n@@ -430,7 +433,7 @@ public WriteResult expandUntriggered(PCollection<KV<DestinationT, ElementT>> inp\n             ParDo.of(\n                     new WriteRename(\n                         bigQueryServices,\n-                        loadJobIdPrefixView,\n+                        copyJobIdPrefixView,\n                         writeDisposition,\n                         createDisposition,\n                         maxRetryJobs,\n@@ -631,23 +634,22 @@ public void processElement(\n         .apply(\n             \"WriteGroupedRecords\",\n             ParDo.of(\n-                    new WriteGroupedRecordsToFiles<DestinationT, ElementT>(\n-                        tempFilePrefix, maxFileSize, rowWriterFactory))\n+                    new WriteGroupedRecordsToFiles<>(tempFilePrefix, maxFileSize, rowWriterFactory))\n                 .withSideInputs(tempFilePrefix))\n         .setCoder(WriteBundlesToFiles.ResultCoder.of(destinationCoder));\n   }\n \n   // Take in a list of files and write them to temporary tables.\n-  private PCollection<KV<TableDestination, String>> writeTempTables(\n-      PCollection<KV<ShardedKey<DestinationT>, List<String>>> input,\n+  private PCollection<KV<TableDestination, WriteTables.Result>> writeTempTables(\n+      PCollection<KV<ShardedKey<DestinationT>, WritePartition.Result>> input,\n       PCollectionView<String> jobIdTokenView) {\n     List<PCollectionView<?>> sideInputs = Lists.newArrayList(jobIdTokenView);\n     sideInputs.addAll(dynamicDestinations.getSideInputs());\n \n-    Coder<KV<ShardedKey<DestinationT>, List<String>>> partitionsCoder =\n+    Coder<KV<ShardedKey<DestinationT>, WritePartition.Result>> partitionsCoder =\n         KvCoder.of(\n             ShardedKeyCoder.of(NullableCoder.of(destinationCoder)),\n-            ListCoder.of(StringUtf8Coder.of()));\n+            WritePartition.ResultCoder.INSTANCE);\n \n     // If the final destination table exists already (and we're appending to it), then the temp\n     // tables must exactly match schema, partitioning, etc. Wrap the DynamicDestinations object\n@@ -689,20 +691,24 @@ public void processElement(\n                 rowWriterFactory.getSourceFormat(),\n                 useAvroLogicalTypes,\n                 schemaUpdateOptions))\n-        .setCoder(KvCoder.of(tableDestinationCoder, StringUtf8Coder.of()));\n+        .setCoder(KvCoder.of(tableDestinationCoder, WriteTables.ResultCoder.INSTANCE));\n   }\n \n   // In the case where the files fit into a single load job, there's no need to write temporary\n   // tables and rename. We can load these files directly into the target BigQuery table.\n   void writeSinglePartition(\n-      PCollection<KV<ShardedKey<DestinationT>, List<String>>> input,\n+      PCollection<KV<ShardedKey<DestinationT>, WritePartition.Result>> input,\n       PCollectionView<String> loadJobIdPrefixView) {\n     List<PCollectionView<?>> sideInputs = Lists.newArrayList(loadJobIdPrefixView);\n     sideInputs.addAll(dynamicDestinations.getSideInputs());\n-    Coder<KV<ShardedKey<DestinationT>, List<String>>> partitionsCoder =\n+\n+    Coder<TableDestination> tableDestinationCoder =\n+        clusteringEnabled ? TableDestinationCoderV3.of() : TableDestinationCoderV2.of();\n+\n+    Coder<KV<ShardedKey<DestinationT>, WritePartition.Result>> partitionsCoder =\n         KvCoder.of(\n             ShardedKeyCoder.of(NullableCoder.of(destinationCoder)),\n-            ListCoder.of(StringUtf8Coder.of()));\n+            WritePartition.ResultCoder.INSTANCE);\n     // Write single partition to final table\n     input\n         .setCoder(partitionsCoder)\n@@ -725,7 +731,8 @@ void writeSinglePartition(\n                 kmsKey,\n                 rowWriterFactory.getSourceFormat(),\n                 useAvroLogicalTypes,\n-                schemaUpdateOptions));\n+                schemaUpdateOptions))\n+        .setCoder(KvCoder.of(tableDestinationCoder, WriteTables.ResultCoder.INSTANCE));\n   }\n \n   private WriteResult writeResult(Pipeline p) {"
  },
  {
    "sha": "7eae6fef33b5570d6188d6d33191f82018c51f63",
    "filename": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryResourceNaming.java",
    "status": "modified",
    "additions": 1,
    "deletions": 0,
    "changes": 1,
    "blob_url": "https://github.com/apache/beam/blob/9843c78456243f8bb877bea528597879b4a40676/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryResourceNaming.java",
    "raw_url": "https://github.com/apache/beam/raw/9843c78456243f8bb877bea528597879b4a40676/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryResourceNaming.java",
    "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryResourceNaming.java?ref=9843c78456243f8bb877bea528597879b4a40676",
    "patch": "@@ -69,6 +69,7 @@ static String createJobIdWithDestination(\n \n   public enum JobType {\n     LOAD,\n+    TEMP_TABLE_LOAD,\n     COPY,\n     EXPORT,\n     QUERY,"
  },
  {
    "sha": "e1e0566c17048b35b0c657717310423625b6e3a2",
    "filename": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/WritePartition.java",
    "status": "modified",
    "additions": 49,
    "deletions": 9,
    "changes": 58,
    "blob_url": "https://github.com/apache/beam/blob/9843c78456243f8bb877bea528597879b4a40676/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/WritePartition.java",
    "raw_url": "https://github.com/apache/beam/raw/9843c78456243f8bb877bea528597879b4a40676/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/WritePartition.java",
    "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/WritePartition.java?ref=9843c78456243f8bb877bea528597879b4a40676",
    "patch": "@@ -17,8 +17,17 @@\n  */\n package org.apache.beam.sdk.io.gcp.bigquery;\n \n+import com.google.auto.value.AutoValue;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n import java.util.List;\n import java.util.Map;\n+import org.apache.beam.sdk.coders.AtomicCoder;\n+import org.apache.beam.sdk.coders.BooleanCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.ListCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n import org.apache.beam.sdk.io.gcp.bigquery.WriteBundlesToFiles.Result;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.values.KV;\n@@ -39,16 +48,42 @@\n class WritePartition<DestinationT>\n     extends DoFn<\n         Iterable<WriteBundlesToFiles.Result<DestinationT>>,\n-        KV<ShardedKey<DestinationT>, List<String>>> {\n+        KV<ShardedKey<DestinationT>, WritePartition.Result>> {\n+  @AutoValue\n+  abstract static class Result {\n+    public abstract List<String> getFilenames();\n+\n+    abstract Boolean isFirstPane();\n+  }\n+\n+  static class ResultCoder extends AtomicCoder<Result> {\n+    private static final Coder<List<String>> FILENAMES_CODER = ListCoder.of(StringUtf8Coder.of());\n+    private static final Coder<Boolean> FIRST_PANE_CODER = BooleanCoder.of();\n+    static final ResultCoder INSTANCE = new ResultCoder();\n+\n+    @Override\n+    public void encode(Result value, OutputStream outStream) throws IOException {\n+      FILENAMES_CODER.encode(value.getFilenames(), outStream);\n+      FIRST_PANE_CODER.encode(value.isFirstPane(), outStream);\n+    }\n+\n+    @Override\n+    public Result decode(InputStream inStream) throws IOException {\n+      return new AutoValue_WritePartition_Result(\n+          FILENAMES_CODER.decode(inStream), FIRST_PANE_CODER.decode(inStream));\n+    }\n+  }\n+\n   private final boolean singletonTable;\n   private final DynamicDestinations<?, DestinationT> dynamicDestinations;\n   private final PCollectionView<String> tempFilePrefix;\n   private final int maxNumFiles;\n   private final long maxSizeBytes;\n   private final RowWriterFactory<?, DestinationT> rowWriterFactory;\n \n-  private @Nullable TupleTag<KV<ShardedKey<DestinationT>, List<String>>> multiPartitionsTag;\n-  private TupleTag<KV<ShardedKey<DestinationT>, List<String>>> singlePartitionTag;\n+  private @Nullable TupleTag<KV<ShardedKey<DestinationT>, WritePartition.Result>>\n+      multiPartitionsTag;\n+  private TupleTag<KV<ShardedKey<DestinationT>, WritePartition.Result>> singlePartitionTag;\n \n   private static class PartitionData {\n     private int numFiles = 0;\n@@ -131,8 +166,8 @@ void addPartition(PartitionData partition) {\n       PCollectionView<String> tempFilePrefix,\n       int maxNumFiles,\n       long maxSizeBytes,\n-      TupleTag<KV<ShardedKey<DestinationT>, List<String>>> multiPartitionsTag,\n-      TupleTag<KV<ShardedKey<DestinationT>, List<String>>> singlePartitionTag,\n+      TupleTag<KV<ShardedKey<DestinationT>, WritePartition.Result>> multiPartitionsTag,\n+      TupleTag<KV<ShardedKey<DestinationT>, WritePartition.Result>> singlePartitionTag,\n       RowWriterFactory<?, DestinationT> rowWriterFactory) {\n     this.singletonTable = singletonTable;\n     this.dynamicDestinations = dynamicDestinations;\n@@ -147,7 +182,6 @@ void addPartition(PartitionData partition) {\n   @ProcessElement\n   public void processElement(ProcessContext c) throws Exception {\n     List<WriteBundlesToFiles.Result<DestinationT>> results = Lists.newArrayList(c.element());\n-\n     // If there are no elements to write _and_ the user specified a constant output table, then\n     // generate an empty table of that name.\n     if (results.isEmpty() && singletonTable) {\n@@ -161,7 +195,8 @@ public void processElement(ProcessContext c) throws Exception {\n       BigQueryRowWriter.Result writerResult = writer.getResult();\n \n       results.add(\n-          new Result<>(writerResult.resourceId.toString(), writerResult.byteSize, destination));\n+          new WriteBundlesToFiles.Result<>(\n+              writerResult.resourceId.toString(), writerResult.byteSize, destination));\n     }\n \n     Map<DestinationT, DestinationData> currentResults = Maps.newHashMap();\n@@ -190,11 +225,16 @@ public void processElement(ProcessContext c) throws Exception {\n       // In the fast-path case where we only output one table, the transform loads it directly\n       // to the final table. In this case, we output on a special TupleTag so the enclosing\n       // transform knows to skip the rename step.\n-      TupleTag<KV<ShardedKey<DestinationT>, List<String>>> outputTag =\n+      TupleTag<KV<ShardedKey<DestinationT>, WritePartition.Result>> outputTag =\n           (destinationData.getPartitions().size() == 1) ? singlePartitionTag : multiPartitionsTag;\n       for (int i = 0; i < destinationData.getPartitions().size(); ++i) {\n         PartitionData partitionData = destinationData.getPartitions().get(i);\n-        c.output(outputTag, KV.of(ShardedKey.of(destination, i + 1), partitionData.getFilenames()));\n+        c.output(\n+            outputTag,\n+            KV.of(\n+                ShardedKey.of(destination, i + 1),\n+                new AutoValue_WritePartition_Result(\n+                    partitionData.getFilenames(), c.pane().isFirst())));\n       }\n     }\n   }"
  },
  {
    "sha": "241fdf3fde2a4492884c71f617039086219b6bf0",
    "filename": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/WriteRename.java",
    "status": "modified",
    "additions": 24,
    "deletions": 10,
    "changes": 34,
    "blob_url": "https://github.com/apache/beam/blob/9843c78456243f8bb877bea528597879b4a40676/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/WriteRename.java",
    "raw_url": "https://github.com/apache/beam/raw/9843c78456243f8bb877bea528597879b4a40676/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/WriteRename.java",
    "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/WriteRename.java?ref=9843c78456243f8bb877bea528597879b4a40676",
    "patch": "@@ -37,6 +37,7 @@\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ArrayListMultimap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Iterables;\n import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Lists;\n import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Multimap;\n import org.slf4j.Logger;\n@@ -49,7 +50,7 @@\n @SuppressWarnings({\n   \"nullness\" // TODO(https://issues.apache.org/jira/browse/BEAM-10402)\n })\n-class WriteRename extends DoFn<Iterable<KV<TableDestination, String>>, Void> {\n+class WriteRename extends DoFn<Iterable<KV<TableDestination, WriteTables.Result>>, Void> {\n   private static final Logger LOG = LoggerFactory.getLogger(WriteRename.class);\n \n   private final BigQueryServices bqServices;\n@@ -101,12 +102,15 @@ public void startBundle(StartBundleContext c) {\n   }\n \n   @ProcessElement\n-  public void processElement(ProcessContext c) throws Exception {\n-    Multimap<TableDestination, String> tempTables = ArrayListMultimap.create();\n-    for (KV<TableDestination, String> entry : c.element()) {\n+  public void processElement(\n+      @Element Iterable<KV<TableDestination, WriteTables.Result>> element, ProcessContext c)\n+      throws Exception {\n+    Multimap<TableDestination, WriteTables.Result> tempTables = ArrayListMultimap.create();\n+    for (KV<TableDestination, WriteTables.Result> entry : element) {\n       tempTables.put(entry.getKey(), entry.getValue());\n     }\n-    for (Map.Entry<TableDestination, Collection<String>> entry : tempTables.asMap().entrySet()) {\n+    for (Map.Entry<TableDestination, Collection<WriteTables.Result>> entry :\n+        tempTables.asMap().entrySet()) {\n       // Process each destination table.\n       // Do not copy if no temp tables are provided.\n       if (!entry.getValue().isEmpty()) {\n@@ -143,17 +147,27 @@ public void finishBundle(FinishBundleContext c) throws Exception {\n   }\n \n   private PendingJobData startWriteRename(\n-      TableDestination finalTableDestination, Iterable<String> tempTableNames, ProcessContext c)\n+      TableDestination finalTableDestination,\n+      Iterable<WriteTables.Result> tempTableNames,\n+      ProcessContext c)\n       throws Exception {\n+    // The pane may have advanced either here due to triggering or due to an upstream trigger. We\n+    // check the upstream\n+    // trigger to handle the case where an earlier pane triggered the single-partition path. If this\n+    // happened, then the\n+    // table will already exist so we want to append to the table.\n+    boolean isFirstPane =\n+        Iterables.getFirst(tempTableNames, null).isFirstPane() && c.pane().isFirst();\n     WriteDisposition writeDisposition =\n-        (c.pane().getIndex() == 0) ? firstPaneWriteDisposition : WriteDisposition.WRITE_APPEND;\n+        isFirstPane ? firstPaneWriteDisposition : WriteDisposition.WRITE_APPEND;\n     CreateDisposition createDisposition =\n-        (c.pane().getIndex() == 0) ? firstPaneCreateDisposition : CreateDisposition.CREATE_NEVER;\n+        isFirstPane ? firstPaneCreateDisposition : CreateDisposition.CREATE_NEVER;\n     List<TableReference> tempTables =\n         StreamSupport.stream(tempTableNames.spliterator(), false)\n-            .map(table -> BigQueryHelpers.fromJsonString(table, TableReference.class))\n+            .map(\n+                result ->\n+                    BigQueryHelpers.fromJsonString(result.getTableName(), TableReference.class))\n             .collect(Collectors.toList());\n-    ;\n \n     // Make sure each destination table gets a unique job id.\n     String jobIdPrefix ="
  },
  {
    "sha": "20302b737b135542dfeb44bd29dabb7497cbe00d",
    "filename": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/WriteTables.java",
    "status": "modified",
    "additions": 71,
    "deletions": 16,
    "changes": 87,
    "blob_url": "https://github.com/apache/beam/blob/9843c78456243f8bb877bea528597879b4a40676/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/WriteTables.java",
    "raw_url": "https://github.com/apache/beam/raw/9843c78456243f8bb877bea528597879b4a40676/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/WriteTables.java",
    "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/WriteTables.java?ref=9843c78456243f8bb877bea528597879b4a40676",
    "patch": "@@ -26,11 +26,17 @@\n import com.google.api.services.bigquery.model.TableReference;\n import com.google.api.services.bigquery.model.TableSchema;\n import com.google.api.services.bigquery.model.TimePartitioning;\n+import com.google.auto.value.AutoValue;\n import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n import java.util.List;\n import java.util.Map;\n import java.util.Set;\n import java.util.stream.Collectors;\n+import org.apache.beam.sdk.coders.AtomicCoder;\n+import org.apache.beam.sdk.coders.BooleanCoder;\n+import org.apache.beam.sdk.coders.CoderException;\n import org.apache.beam.sdk.coders.KvCoder;\n import org.apache.beam.sdk.coders.StringUtf8Coder;\n import org.apache.beam.sdk.coders.VoidCoder;\n@@ -43,6 +49,7 @@\n import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.WriteDisposition;\n import org.apache.beam.sdk.io.gcp.bigquery.BigQueryServices.DatasetService;\n import org.apache.beam.sdk.io.gcp.bigquery.BigQueryServices.JobService;\n+import org.apache.beam.sdk.io.gcp.bigquery.WritePartition.Result;\n import org.apache.beam.sdk.options.ValueProvider;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.GroupByKey;\n@@ -67,7 +74,10 @@\n import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Lists;\n import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Maps;\n+import org.checkerframework.checker.initialization.qual.Initialized;\n+import org.checkerframework.checker.nullness.qual.NonNull;\n import org.checkerframework.checker.nullness.qual.Nullable;\n+import org.checkerframework.checker.nullness.qual.UnknownKeyFor;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -88,8 +98,35 @@\n })\n class WriteTables<DestinationT>\n     extends PTransform<\n-        PCollection<KV<ShardedKey<DestinationT>, List<String>>>,\n-        PCollection<KV<TableDestination, String>>> {\n+        PCollection<KV<ShardedKey<DestinationT>, WritePartition.Result>>,\n+        PCollection<KV<TableDestination, WriteTables.Result>>> {\n+  @AutoValue\n+  abstract static class Result {\n+    abstract String getTableName();\n+\n+    abstract Boolean isFirstPane();\n+  }\n+\n+  static class ResultCoder extends AtomicCoder<WriteTables.Result> {\n+    static ResultCoder INSTANCE = new ResultCoder();\n+\n+    @Override\n+    public void encode(Result value, @UnknownKeyFor @NonNull @Initialized OutputStream outStream)\n+        throws @UnknownKeyFor @NonNull @Initialized CoderException, @UnknownKeyFor @NonNull\n+            @Initialized IOException {\n+      StringUtf8Coder.of().encode(value.getTableName(), outStream);\n+      BooleanCoder.of().encode(value.isFirstPane(), outStream);\n+    }\n+\n+    @Override\n+    public Result decode(@UnknownKeyFor @NonNull @Initialized InputStream inStream)\n+        throws @UnknownKeyFor @NonNull @Initialized CoderException, @UnknownKeyFor @NonNull\n+            @Initialized IOException {\n+      return new AutoValue_WriteTables_Result(\n+          StringUtf8Coder.of().decode(inStream), BooleanCoder.of().decode(inStream));\n+    }\n+  }\n+\n   private static final Logger LOG = LoggerFactory.getLogger(WriteTables.class);\n \n   private final boolean tempTable;\n@@ -100,7 +137,7 @@\n   private final Set<SchemaUpdateOption> schemaUpdateOptions;\n   private final DynamicDestinations<?, DestinationT> dynamicDestinations;\n   private final List<PCollectionView<?>> sideInputs;\n-  private final TupleTag<KV<TableDestination, String>> mainOutputTag;\n+  private final TupleTag<KV<TableDestination, WriteTables.Result>> mainOutputTag;\n   private final TupleTag<String> temporaryFilesTag;\n   private final ValueProvider<String> loadJobProjectId;\n   private final int maxRetryJobs;\n@@ -110,7 +147,9 @@\n   private final boolean useAvroLogicalTypes;\n \n   private class WriteTablesDoFn\n-      extends DoFn<KV<ShardedKey<DestinationT>, List<String>>, KV<TableDestination, String>> {\n+      extends DoFn<\n+          KV<ShardedKey<DestinationT>, WritePartition.Result>, KV<TableDestination, Result>> {\n+\n     private Map<DestinationT, String> jsonSchemas = Maps.newHashMap();\n \n     // Represents a pending BigQuery load job.\n@@ -120,18 +159,21 @@\n       final List<String> partitionFiles;\n       final TableDestination tableDestination;\n       final TableReference tableReference;\n+      final boolean isFirstPane;\n \n       public PendingJobData(\n           BoundedWindow window,\n           BigQueryHelpers.PendingJob retryJob,\n           List<String> partitionFiles,\n           TableDestination tableDestination,\n-          TableReference tableReference) {\n+          TableReference tableReference,\n+          boolean isFirstPane) {\n         this.window = window;\n         this.retryJob = retryJob;\n         this.partitionFiles = partitionFiles;\n         this.tableDestination = tableDestination;\n         this.tableReference = tableReference;\n+        this.isFirstPane = isFirstPane;\n       }\n     }\n     // All pending load jobs.\n@@ -146,7 +188,11 @@ public void startBundle(StartBundleContext c) {\n     }\n \n     @ProcessElement\n-    public void processElement(ProcessContext c, BoundedWindow window) throws Exception {\n+    public void processElement(\n+        @Element KV<ShardedKey<DestinationT>, WritePartition.Result> element,\n+        ProcessContext c,\n+        BoundedWindow window)\n+        throws Exception {\n       dynamicDestinations.setSideInputAccessorFromProcessContext(c);\n       DestinationT destination = c.element().getKey().getKey();\n       TableSchema tableSchema;\n@@ -192,8 +238,8 @@ public void processElement(ProcessContext c, BoundedWindow window) throws Except\n         tableDestination = tableDestination.withTableReference(tableReference);\n       }\n \n-      Integer partition = c.element().getKey().getShardNumber();\n-      List<String> partitionFiles = Lists.newArrayList(c.element().getValue());\n+      Integer partition = element.getKey().getShardNumber();\n+      List<String> partitionFiles = Lists.newArrayList(element.getValue().getFilenames());\n       String jobIdPrefix =\n           BigQueryResourceNaming.createJobIdWithDestination(\n               c.sideInput(loadJobIdPrefixView), tableDestination, partition, c.pane().getIndex());\n@@ -205,7 +251,7 @@ public void processElement(ProcessContext c, BoundedWindow window) throws Except\n \n       WriteDisposition writeDisposition = firstPaneWriteDisposition;\n       CreateDisposition createDisposition = firstPaneCreateDisposition;\n-      if (c.pane().getIndex() > 0 && !tempTable) {\n+      if (!element.getValue().isFirstPane() && !tempTable) {\n         // If writing directly to the destination, then the table is created on the first write\n         // and we should change the disposition for subsequent writes.\n         writeDisposition = WriteDisposition.WRITE_APPEND;\n@@ -231,7 +277,13 @@ public void processElement(ProcessContext c, BoundedWindow window) throws Except\n               createDisposition,\n               schemaUpdateOptions);\n       pendingJobs.add(\n-          new PendingJobData(window, retryJob, partitionFiles, tableDestination, tableReference));\n+          new PendingJobData(\n+              window,\n+              retryJob,\n+              partitionFiles,\n+              tableDestination,\n+              tableReference,\n+              element.getValue().isFirstPane()));\n     }\n \n     @Override\n@@ -247,7 +299,7 @@ public void finishBundle(FinishBundleContext c) throws Exception {\n           bqServices.getDatasetService(c.getPipelineOptions().as(BigQueryOptions.class));\n \n       PendingJobManager jobManager = new PendingJobManager();\n-      for (PendingJobData pendingJob : pendingJobs) {\n+      for (final PendingJobData pendingJob : pendingJobs) {\n         jobManager =\n             jobManager.addPendingJob(\n                 pendingJob.retryJob,\n@@ -262,11 +314,14 @@ public void finishBundle(FinishBundleContext c) throws Exception {\n                                   BigQueryHelpers.stripPartitionDecorator(ref.getTableId())),\n                           pendingJob.tableDestination.getTableDescription());\n                     }\n+\n+                    Result result =\n+                        new AutoValue_WriteTables_Result(\n+                            BigQueryHelpers.toJsonString(pendingJob.tableReference),\n+                            pendingJob.isFirstPane);\n                     c.output(\n                         mainOutputTag,\n-                        KV.of(\n-                            pendingJob.tableDestination,\n-                            BigQueryHelpers.toJsonString(pendingJob.tableReference)),\n+                        KV.of(pendingJob.tableDestination, result),\n                         pendingJob.window.maxTimestamp(),\n                         pendingJob.window);\n                     for (String file : pendingJob.partitionFiles) {\n@@ -328,8 +383,8 @@ public WriteTables(\n   }\n \n   @Override\n-  public PCollection<KV<TableDestination, String>> expand(\n-      PCollection<KV<ShardedKey<DestinationT>, List<String>>> input) {\n+  public PCollection<KV<TableDestination, Result>> expand(\n+      PCollection<KV<ShardedKey<DestinationT>, WritePartition.Result>> input) {\n     PCollectionTuple writeTablesOutputs =\n         input.apply(\n             ParDo.of(new WriteTablesDoFn())"
  },
  {
    "sha": "1b499e81f0551390f24abb2368c2e463ff010625",
    "filename": "sdks/java/io/google-cloud-platform/src/test/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIOWriteTest.java",
    "status": "modified",
    "additions": 38,
    "deletions": 18,
    "changes": 56,
    "blob_url": "https://github.com/apache/beam/blob/9843c78456243f8bb877bea528597879b4a40676/sdks/java/io/google-cloud-platform/src/test/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIOWriteTest.java",
    "raw_url": "https://github.com/apache/beam/raw/9843c78456243f8bb877bea528597879b4a40676/sdks/java/io/google-cloud-platform/src/test/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIOWriteTest.java",
    "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/io/google-cloud-platform/src/test/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIOWriteTest.java?ref=9843c78456243f8bb877bea528597879b4a40676",
    "patch": "@@ -65,19 +65,24 @@\n import java.util.regex.Matcher;\n import java.util.regex.Pattern;\n import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n import org.apache.avro.generic.GenericData;\n import org.apache.avro.generic.GenericDatumWriter;\n import org.apache.avro.generic.GenericRecord;\n import org.apache.avro.io.DatumWriter;\n import org.apache.avro.io.Encoder;\n import org.apache.beam.sdk.coders.AtomicCoder;\n import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.KvCoder;\n import org.apache.beam.sdk.coders.SerializableCoder;\n+import org.apache.beam.sdk.coders.ShardedKeyCoder;\n import org.apache.beam.sdk.coders.StringUtf8Coder;\n import org.apache.beam.sdk.io.GenerateSequence;\n import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write;\n import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.Method;\n import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.SchemaUpdateOption;\n+import org.apache.beam.sdk.io.gcp.bigquery.WritePartition.ResultCoder;\n+import org.apache.beam.sdk.io.gcp.bigquery.WriteTables.Result;\n import org.apache.beam.sdk.io.gcp.testing.FakeBigQueryServices;\n import org.apache.beam.sdk.io.gcp.testing.FakeDatasetService;\n import org.apache.beam.sdk.io.gcp.testing.FakeJobService;\n@@ -1655,10 +1660,12 @@ private void testWritePartition(\n       }\n     }\n \n-    TupleTag<KV<ShardedKey<TableDestination>, List<String>>> multiPartitionsTag =\n-        new TupleTag<KV<ShardedKey<TableDestination>, List<String>>>(\"multiPartitionsTag\") {};\n-    TupleTag<KV<ShardedKey<TableDestination>, List<String>>> singlePartitionTag =\n-        new TupleTag<KV<ShardedKey<TableDestination>, List<String>>>(\"singlePartitionTag\") {};\n+    TupleTag<KV<ShardedKey<TableDestination>, WritePartition.Result>> multiPartitionsTag =\n+        new TupleTag<KV<ShardedKey<TableDestination>, WritePartition.Result>>(\n+            \"multiPartitionsTag\") {};\n+    TupleTag<KV<ShardedKey<TableDestination>, WritePartition.Result>> singlePartitionTag =\n+        new TupleTag<KV<ShardedKey<TableDestination>, WritePartition.Result>>(\n+            \"singlePartitionTag\") {};\n \n     String tempFilePrefix = testFolder.newFolder(\"BigQueryIOTest\").getAbsolutePath();\n     PCollectionView<String> tempFilePrefixView =\n@@ -1678,12 +1685,12 @@ private void testWritePartition(\n \n     DoFnTester<\n             Iterable<WriteBundlesToFiles.Result<TableDestination>>,\n-            KV<ShardedKey<TableDestination>, List<String>>>\n+            KV<ShardedKey<TableDestination>, WritePartition.Result>>\n         tester = DoFnTester.of(writePartition);\n     tester.setSideInput(tempFilePrefixView, GlobalWindow.INSTANCE, tempFilePrefix);\n     tester.processElement(files);\n \n-    List<KV<ShardedKey<TableDestination>, List<String>>> partitions;\n+    List<KV<ShardedKey<TableDestination>, WritePartition.Result>> partitions;\n     if (expectedNumPartitionsPerTable > 1) {\n       partitions = tester.takeOutputElements(multiPartitionsTag);\n     } else {\n@@ -1692,12 +1699,12 @@ private void testWritePartition(\n \n     List<ShardedKey<TableDestination>> partitionsResult = Lists.newArrayList();\n     Map<String, List<String>> filesPerTableResult = Maps.newHashMap();\n-    for (KV<ShardedKey<TableDestination>, List<String>> partition : partitions) {\n+    for (KV<ShardedKey<TableDestination>, WritePartition.Result> partition : partitions) {\n       String table = partition.getKey().getKey().getTableSpec();\n       partitionsResult.add(partition.getKey());\n       List<String> tableFilesResult =\n           filesPerTableResult.computeIfAbsent(table, k -> Lists.newArrayList());\n-      tableFilesResult.addAll(partition.getValue());\n+      tableFilesResult.addAll(partition.getValue().getFilenames());\n     }\n \n     assertThat(\n@@ -1744,7 +1751,7 @@ public void testWriteTables() throws Exception {\n     String jobIdToken = \"jobId\";\n     final Multimap<TableDestination, String> expectedTempTables = ArrayListMultimap.create();\n \n-    List<KV<ShardedKey<String>, List<String>>> partitions = Lists.newArrayList();\n+    List<KV<ShardedKey<String>, WritePartition.Result>> partitions = Lists.newArrayList();\n     for (int i = 0; i < numTables; ++i) {\n       String tableName = String.format(\"project-id:dataset-id.table%05d\", i);\n       TableDestination tableDestination = new TableDestination(tableName, tableName);\n@@ -1766,7 +1773,10 @@ public void testWriteTables() throws Exception {\n           }\n           filesPerPartition.add(writer.getResult().resourceId.toString());\n         }\n-        partitions.add(KV.of(ShardedKey.of(tableDestination.getTableSpec(), j), filesPerPartition));\n+        partitions.add(\n+            KV.of(\n+                ShardedKey.of(tableDestination.getTableSpec(), j),\n+                new AutoValue_WritePartition_Result(filesPerPartition, true)));\n \n         String json =\n             String.format(\n@@ -1776,8 +1786,11 @@ public void testWriteTables() throws Exception {\n       }\n     }\n \n-    PCollection<KV<ShardedKey<String>, List<String>>> writeTablesInput =\n-        p.apply(Create.of(partitions));\n+    PCollection<KV<ShardedKey<String>, WritePartition.Result>> writeTablesInput =\n+        p.apply(\n+            Create.of(partitions)\n+                .withCoder(\n+                    KvCoder.of(ShardedKeyCoder.of(StringUtf8Coder.of()), ResultCoder.INSTANCE)));\n     PCollectionView<String> jobIdTokenView =\n         p.apply(\"CreateJobId\", Create.of(\"jobId\")).apply(View.asSingleton());\n     List<PCollectionView<?>> sideInputs = ImmutableList.of(jobIdTokenView);\n@@ -1800,18 +1813,23 @@ public void testWriteTables() throws Exception {\n             false,\n             Collections.emptySet());\n \n-    PCollection<KV<TableDestination, String>> writeTablesOutput =\n+    PCollection<KV<TableDestination, WriteTables.Result>> writeTablesOutput =\n         writeTablesInput.apply(writeTables);\n \n     PAssert.thatMultimap(writeTablesOutput)\n         .satisfies(\n             input -> {\n               assertEquals(input.keySet(), expectedTempTables.keySet());\n-              for (Map.Entry<TableDestination, Iterable<String>> entry : input.entrySet()) {\n+              for (Map.Entry<TableDestination, Iterable<WriteTables.Result>> entry :\n+                  input.entrySet()) {\n+                Iterable<String> tableNames =\n+                    StreamSupport.stream(entry.getValue().spliterator(), false)\n+                        .map(Result::getTableName)\n+                        .collect(Collectors.toList());\n                 @SuppressWarnings(\"unchecked\")\n                 String[] expectedValues =\n                     Iterables.toArray(expectedTempTables.get(entry.getKey()), String.class);\n-                assertThat(entry.getValue(), containsInAnyOrder(expectedValues));\n+                assertThat(tableNames, containsInAnyOrder(expectedValues));\n               }\n               return null;\n             });\n@@ -1848,7 +1866,7 @@ public void testWriteRename() throws Exception {\n     Multimap<TableDestination, TableRow> expectedRowsPerTable = ArrayListMultimap.create();\n     String jobIdToken = \"jobIdToken\";\n     Multimap<TableDestination, String> tempTables = ArrayListMultimap.create();\n-    List<KV<TableDestination, String>> tempTablesElement = Lists.newArrayList();\n+    List<KV<TableDestination, WriteTables.Result>> tempTablesElement = Lists.newArrayList();\n     for (int i = 0; i < numFinalTables; ++i) {\n       String tableName = \"project-id:dataset-id.table_\" + i;\n       TableDestination tableDestination = new TableDestination(tableName, \"table_\" + i + \"_desc\");\n@@ -1868,7 +1886,8 @@ public void testWriteRename() throws Exception {\n         expectedRowsPerTable.putAll(tableDestination, rows);\n         String tableJson = toJsonString(tempTable);\n         tempTables.put(tableDestination, tableJson);\n-        tempTablesElement.add(KV.of(tableDestination, tableJson));\n+        tempTablesElement.add(\n+            KV.of(tableDestination, new AutoValue_WriteTables_Result(tableJson, true)));\n       }\n     }\n \n@@ -1884,7 +1903,8 @@ public void testWriteRename() throws Exception {\n             3,\n             \"kms_key\");\n \n-    DoFnTester<Iterable<KV<TableDestination, String>>, Void> tester = DoFnTester.of(writeRename);\n+    DoFnTester<Iterable<KV<TableDestination, WriteTables.Result>>, Void> tester =\n+        DoFnTester.of(writeRename);\n     tester.setSideInput(jobIdTokenView, GlobalWindow.INSTANCE, jobIdToken);\n     tester.processElement(tempTablesElement);\n     tester.finishBundle();"
  }
]
