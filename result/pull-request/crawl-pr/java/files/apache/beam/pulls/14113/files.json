[
  {
    "sha": "15b2637b4d57c30f21b5dbdc3022173eeb9f2c8c",
    "filename": "sdks/python/apache_beam/io/gcp/bigquery_file_loads.py",
    "status": "modified",
    "additions": 169,
    "deletions": 27,
    "changes": 196,
    "blob_url": "https://github.com/apache/beam/blob/911945c7f70bb6387ccd72fb1f4f9684613b5681/sdks/python/apache_beam/io/gcp/bigquery_file_loads.py",
    "raw_url": "https://github.com/apache/beam/raw/911945c7f70bb6387ccd72fb1f4f9684613b5681/sdks/python/apache_beam/io/gcp/bigquery_file_loads.py",
    "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/python/apache_beam/io/gcp/bigquery_file_loads.py?ref=911945c7f70bb6387ccd72fb1f4f9684613b5681",
    "patch": "@@ -31,8 +31,8 @@\n from __future__ import absolute_import\n \n import hashlib\n+import io\n import logging\n-import pickle\n import random\n import time\n import uuid\n@@ -51,6 +51,13 @@\n from apache_beam.transforms.util import GroupIntoBatches\n from apache_beam.transforms.window import GlobalWindows\n \n+# Protect against environments where bigquery library is not available.\n+# pylint: disable=wrong-import-order, wrong-import-position\n+try:\n+  from apitools.base.py.exceptions import HttpError\n+except ImportError:\n+  pass\n+\n _LOGGER = logging.getLogger(__name__)\n \n ONE_TERABYTE = (1 << 40)\n@@ -256,7 +263,7 @@ def process(self, element, file_prefix, *schema_side_inputs):\n       self._destination_to_file_writer.pop(destination)\n       yield pvalue.TaggedOutput(\n           WriteRecordsToFile.WRITTEN_FILE_TAG,\n-          (element[0], (file_path, file_size)))\n+          (destination, (file_path, file_size)))\n \n   def finish_bundle(self):\n     for destination, file_path_writer in \\\n@@ -287,7 +294,7 @@ def __init__(\n     self.file_format = file_format or bigquery_tools.FileFormat.JSON\n \n   def process(self, element, file_prefix, *schema_side_inputs):\n-    destination = element[0]\n+    destination = bigquery_tools.get_hashable_destination(element[0])\n     rows = element[1]\n \n     file_path, writer = None, None\n@@ -313,6 +320,111 @@ def process(self, element, file_prefix, *schema_side_inputs):\n       yield (destination, (file_path, file_size))\n \n \n+class UpdateDestinationSchema(beam.DoFn):\n+  \"\"\"Update destination schema based on data that is about to be copied into it.\n+\n+  Unlike load and query jobs, BigQuery copy jobs do not support schema field\n+  addition or relaxation on the destination table. This DoFn fills that gap by\n+  updating the destination table schemas to be compatible with the data coming\n+  from the source table so that schema field modification options are respected\n+  regardless of whether data is loaded directly to the destination table or\n+  loaded into temporary tables before being copied into the destination.\n+\n+  This tranform takes as input a (destination, job_reference) pair where the\n+  job_reference refers to a completed load job into a temporary table.\n+\n+  This transform emits (destination, job_reference) pairs where the\n+  job_reference refers to a submitted load job for performing the schema\n+  modification. Note that the input and output job references are not the same.\n+\n+  Experimental; no backwards compatibility guarantees.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      write_disposition=None,\n+      test_client=None,\n+      additional_bq_parameters=None,\n+      step_name=None):\n+    self._test_client = test_client\n+    self._write_disposition = write_disposition\n+    self._additional_bq_parameters = additional_bq_parameters or {}\n+    self._step_name = step_name\n+\n+  def setup(self):\n+    self._bq_wrapper = bigquery_tools.BigQueryWrapper(client=self._test_client)\n+    self._bq_io_metadata = create_bigquery_io_metadata(self._step_name)\n+\n+  def process(self, element, schema_mod_job_name_prefix):\n+    destination = element[0]\n+    temp_table_load_job_reference = element[1]\n+\n+    if callable(self._additional_bq_parameters):\n+      additional_parameters = self._additional_bq_parameters(destination)\n+    elif isinstance(self._additional_bq_parameters, vp.ValueProvider):\n+      additional_parameters = self._additional_bq_parameters.get()\n+    else:\n+      additional_parameters = self._additional_bq_parameters\n+\n+    # When writing to normal tables WRITE_TRUNCATE will overwrite the schema but\n+    # when writing to a partition, care needs to be taken to update the schema\n+    # even on WRITE_TRUNCATE.\n+    if (self._write_disposition not in ('WRITE_TRUNCATE', 'WRITE_APPEND') or\n+        not additional_parameters or\n+        not additional_parameters.get(\"schemaUpdateOptions\")):\n+      # No need to modify schema of destination table\n+      return\n+\n+    table_reference = bigquery_tools.parse_table_reference(destination)\n+    if table_reference.projectId is None:\n+      table_reference.projectId = vp.RuntimeValueProvider.get_value(\n+          'project', str, '')\n+\n+    try:\n+      # Check if destination table exists\n+      _ = self._bq_wrapper.get_table(\n+          project_id=table_reference.projectId,\n+          dataset_id=table_reference.datasetId,\n+          table_id=table_reference.tableId)\n+    except HttpError as exn:\n+      if exn.status_code == 404:\n+        # Destination table does not exist, so no need to modify its schema\n+        # ahead of the copy jobs.\n+        return\n+      else:\n+        raise\n+\n+    temp_table_load_job = self._bq_wrapper.get_job(\n+        project=temp_table_load_job_reference.projectId,\n+        job_id=temp_table_load_job_reference.jobId,\n+        location=temp_table_load_job_reference.location)\n+    temp_table_schema = temp_table_load_job.configuration.load.schema\n+\n+    destination_hash = _bq_uuid(\n+        '%s:%s.%s' % (\n+            table_reference.projectId,\n+            table_reference.datasetId,\n+            table_reference.tableId))\n+    uid = _bq_uuid()\n+    job_name = '%s_%s_%s' % (schema_mod_job_name_prefix, destination_hash, uid)\n+\n+    _LOGGER.debug(\n+        'Triggering schema modification job %s on %s',\n+        job_name,\n+        table_reference)\n+    # Trigger potential schema modification by loading zero rows into the\n+    # destination table with the temporary table schema.\n+    schema_update_job_reference = self._bq_wrapper.perform_load_job(\n+        destination=table_reference,\n+        source_stream=io.BytesIO(),  # file with zero rows\n+        job_id=job_name,\n+        schema=temp_table_schema,\n+        write_disposition='WRITE_APPEND',\n+        create_disposition='CREATE_NEVER',\n+        additional_load_parameters=additional_parameters,\n+        job_labels=self._bq_io_metadata.add_additional_bq_job_labels())\n+    yield (destination, schema_update_job_reference)\n+\n+\n class TriggerCopyJobs(beam.DoFn):\n   \"\"\"Launches jobs to copy from temporary tables into the main target table.\n \n@@ -352,7 +464,7 @@ def start_bundle(self):\n     if not self.bq_io_metadata:\n       self.bq_io_metadata = create_bigquery_io_metadata(self._step_name)\n \n-  def process(self, element, job_name_prefix=None):\n+  def process(self, element, job_name_prefix=None, unused_schema_mod_jobs=None):\n     destination = element[0]\n     job_reference = element[1]\n \n@@ -509,7 +621,9 @@ def process(self, element, load_job_name_prefix, *schema_side_inputs):\n       create_disposition = 'CREATE_IF_NEEDED'\n       # For temporary tables, we create a new table with the name with JobId.\n       table_reference.tableId = job_name\n-      yield pvalue.TaggedOutput(TriggerLoadJobs.TEMP_TABLES, table_reference)\n+      yield pvalue.TaggedOutput(\n+          TriggerLoadJobs.TEMP_TABLES,\n+          bigquery_tools.get_hashable_destination(table_reference))\n \n     _LOGGER.info(\n         'Triggering job %s to load data to BigQuery table %s.'\n@@ -521,9 +635,9 @@ def process(self, element, load_job_name_prefix, *schema_side_inputs):\n     if not self.bq_io_metadata:\n       self.bq_io_metadata = create_bigquery_io_metadata(self._step_name)\n     job_reference = self.bq_wrapper.perform_load_job(\n-        table_reference,\n-        files,\n-        job_name,\n+        destination=table_reference,\n+        source_uris=files,\n+        job_id=job_name,\n         schema=schema,\n         write_disposition=self.write_disposition,\n         create_disposition=create_disposition,\n@@ -839,6 +953,7 @@ def _load_data(\n       partitions_using_temp_tables,\n       partitions_direct_to_destination,\n       load_job_name_pcv,\n+      schema_mod_job_name_pcv,\n       copy_job_name_pcv,\n       p,\n       step_name):\n@@ -857,6 +972,8 @@ def _load_data(\n          of the load jobs would fail but not other. If any of them fails, then\n          copy jobs are not triggered.\n     \"\"\"\n+    singleton_pc = p | \"ImpulseLoadData\" >> beam.Create([None])\n+\n     # Load data using temp tables\n     trigger_loads_outputs = (\n         partitions_using_temp_tables\n@@ -877,40 +994,56 @@ def _load_data(\n     temp_tables_load_job_ids_pc = trigger_loads_outputs['main']\n     temp_tables_pc = trigger_loads_outputs[TriggerLoadJobs.TEMP_TABLES]\n \n-    destination_copy_job_ids_pc = (\n-        p\n-        | \"ImpulseMonitorLoadJobs\" >> beam.Create([None])\n+    finished_temp_tables_load_jobs_pc = (\n+        singleton_pc\n         | \"WaitForTempTableLoadJobs\" >> beam.ParDo(\n             WaitForBQJobs(self.test_client),\n-            beam.pvalue.AsList(temp_tables_load_job_ids_pc))\n+            pvalue.AsList(temp_tables_load_job_ids_pc)))\n+\n+    schema_mod_job_ids_pc = (\n+        finished_temp_tables_load_jobs_pc\n+        | beam.ParDo(\n+            UpdateDestinationSchema(\n+                write_disposition=self.write_disposition,\n+                test_client=self.test_client,\n+                additional_bq_parameters=self.additional_bq_parameters,\n+                step_name=step_name),\n+            schema_mod_job_name_pcv))\n+\n+    finished_schema_mod_jobs_pc = (\n+        singleton_pc\n+        | \"WaitForSchemaModJobs\" >> beam.ParDo(\n+            WaitForBQJobs(self.test_client),\n+            pvalue.AsList(schema_mod_job_ids_pc)))\n+\n+    destination_copy_job_ids_pc = (\n+        finished_temp_tables_load_jobs_pc\n         | beam.ParDo(\n             TriggerCopyJobs(\n                 create_disposition=self.create_disposition,\n                 write_disposition=self.write_disposition,\n                 test_client=self.test_client,\n                 step_name=step_name),\n-            copy_job_name_pcv))\n+            copy_job_name_pcv,\n+            pvalue.AsIter(finished_schema_mod_jobs_pc)))\n \n     finished_copy_jobs_pc = (\n-        p\n-        | \"ImpulseMonitorCopyJobs\" >> beam.Create([None])\n+        singleton_pc\n         | \"WaitForCopyJobs\" >> beam.ParDo(\n             WaitForBQJobs(self.test_client),\n-            beam.pvalue.AsList(destination_copy_job_ids_pc)))\n+            pvalue.AsList(destination_copy_job_ids_pc)))\n \n     _ = (\n-        finished_copy_jobs_pc\n+        singleton_pc\n         | \"RemoveTempTables/PassTables\" >> beam.FlatMap(\n-            lambda x,\n+            lambda _,\n+            unused_copy_jobs,\n             deleting_tables: deleting_tables,\n+            pvalue.AsIter(finished_copy_jobs_pc),\n             pvalue.AsIter(temp_tables_pc))\n-        # TableReference has no deterministic coder, but as this de-duplication\n-        # is best-effort, pickling should be good enough.\n-        | \"RemoveTempTables/AddUselessValue\" >>\n-        beam.Map(lambda x: (pickle.dumps(x), None))\n+        | \"RemoveTempTables/AddUselessValue\" >> beam.Map(lambda x: (x, None))\n         | \"RemoveTempTables/DeduplicateTables\" >> beam.GroupByKey()\n-        | \"RemoveTempTables/GetTableNames\" >>\n-        beam.MapTuple(lambda k, nones: pickle.loads(k))\n+        | \"RemoveTempTables/GetTableNames\" >> beam.Keys()\n         | \"RemoveTempTables/Delete\" >> beam.ParDo(\n             DeleteTablesFn(self.test_client)))\n \n@@ -931,11 +1064,10 @@ def _load_data(\n             *self.schema_side_inputs))\n \n     _ = (\n-        p\n-        | \"ImpulseMonitorDestLoadJobs\" >> beam.Create([None])\n+        singleton_pc\n         | \"WaitForDestinationLoadJobs\" >> beam.ParDo(\n             WaitForBQJobs(self.test_client),\n-            beam.pvalue.AsList(destination_load_job_ids_pc)))\n+            pvalue.AsList(destination_load_job_ids_pc)))\n \n     destination_load_job_ids_pc = (\n         (temp_tables_load_job_ids_pc, destination_load_job_ids_pc)\n@@ -964,6 +1096,14 @@ def expand(self, pcoll):\n             lambda _: _generate_job_name(\n                 job_name, bigquery_tools.BigQueryJobTypes.LOAD, 'LOAD_STEP')))\n \n+    schema_mod_job_name_pcv = pvalue.AsSingleton(\n+        singleton_pc\n+        | \"SchemaModJobNamePrefix\" >> beam.Map(\n+            lambda _: _generate_job_name(\n+                job_name,\n+                bigquery_tools.BigQueryJobTypes.LOAD,\n+                'SCHEMA_MOD_STEP')))\n+\n     copy_job_name_pcv = pvalue.AsSingleton(\n         singleton_pc\n         | \"CopyJobNamePrefix\" >> beam.Map(\n@@ -1020,6 +1160,7 @@ def expand(self, pcoll):\n           self._load_data(all_partitions,\n                           empty_pc,\n                           load_job_name_pcv,\n+                          schema_mod_job_name_pcv,\n                           copy_job_name_pcv,\n                           p,\n                           step_name))\n@@ -1028,6 +1169,7 @@ def expand(self, pcoll):\n           self._load_data(multiple_partitions_per_destination_pc,\n                           single_partition_per_destination_pc,\n                           load_job_name_pcv,\n+                          schema_mod_job_name_pcv,\n                           copy_job_name_pcv,\n                           p,\n                           step_name))"
  },
  {
    "sha": "82ed8679ef903cc900c50ae700bd3b350b84e455",
    "filename": "sdks/python/apache_beam/io/gcp/bigquery_tools.py",
    "status": "modified",
    "additions": 46,
    "deletions": 8,
    "changes": 54,
    "blob_url": "https://github.com/apache/beam/blob/911945c7f70bb6387ccd72fb1f4f9684613b5681/sdks/python/apache_beam/io/gcp/bigquery_tools.py",
    "raw_url": "https://github.com/apache/beam/raw/911945c7f70bb6387ccd72fb1f4f9684613b5681/sdks/python/apache_beam/io/gcp/bigquery_tools.py",
    "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/python/apache_beam/io/gcp/bigquery_tools.py?ref=911945c7f70bb6387ccd72fb1f4f9684613b5681",
    "patch": "@@ -69,6 +69,7 @@\n # Protect against environments where bigquery library is not available.\n # pylint: disable=wrong-import-order, wrong-import-position\n try:\n+  from apitools.base.py.transfer import Upload\n   from apitools.base.py.exceptions import HttpError, HttpForbiddenError\n except ImportError:\n   pass\n@@ -83,9 +84,9 @@\n \n _LOGGER = logging.getLogger(__name__)\n \n-MAX_RETRIES = 3\n-\n JSON_COMPLIANCE_ERROR = 'NAN, INF and -INF values are not JSON compliant.'\n+MAX_RETRIES = 3\n+UNKNOWN_MIME_TYPE = 'application/octet-stream'\n \n \n class FileFormat(object):\n@@ -407,13 +408,29 @@ def _insert_load_job(\n       project_id,\n       job_id,\n       table_reference,\n-      source_uris,\n+      source_uris=None,\n+      source_stream=None,\n       schema=None,\n       write_disposition=None,\n       create_disposition=None,\n       additional_load_parameters=None,\n       source_format=None,\n       job_labels=None):\n+\n+    if not source_uris and not source_stream:\n+      raise ValueError(\n+          'Either a non-empty list of fully-qualified source URIs must be '\n+          'provided via the source_uris parameter or an open file object must '\n+          'be provided via the source_stream parameter. Got neither.')\n+\n+    if source_uris and source_stream:\n+      raise ValueError(\n+          'Only one of source_uris and source_stream may be specified. '\n+          'Got both.')\n+\n+    if source_uris is None:\n+      source_uris = []\n+\n     additional_load_parameters = additional_load_parameters or {}\n     job_schema = None if schema == 'SCHEMA_AUTODETECT' else schema\n     reference = bigquery.JobReference(jobId=job_id, projectId=project_id)\n@@ -435,18 +452,26 @@ def _insert_load_job(\n             ),\n             jobReference=reference,\n         ))\n-    return self._start_job(request).jobReference\n+    return self._start_job(request, stream=source_stream).jobReference\n \n   def _start_job(\n       self,\n-      request  # type: bigquery.BigqueryJobsInsertRequest\n+      request,  # type: bigquery.BigqueryJobsInsertRequest\n+      stream=None,\n   ):\n     \"\"\"Inserts a BigQuery job.\n \n     If the job exists already, it returns it.\n+\n+    Args:\n+      request (bigquery.BigqueryJobsInsertRequest): An insert job request.\n+      stream (IO[bytes]): A bytes IO object open for reading.\n     \"\"\"\n     try:\n-      response = self.client.jobs.Insert(request)\n+      upload = None\n+      if stream:\n+        upload = Upload.FromStream(stream, mime_type=UNKNOWN_MIME_TYPE)\n+      response = self.client.jobs.Insert(request, upload=upload)\n       _LOGGER.info(\n           \"Stated BigQuery job: %s\\n \"\n           \"bq show -j --format=prettyjson --project_id=%s %s\",\n@@ -809,8 +834,9 @@ def get_job(self, project, job_id, location=None):\n   def perform_load_job(\n       self,\n       destination,\n-      files,\n       job_id,\n+      source_uris=None,\n+      source_stream=None,\n       schema=None,\n       write_disposition=None,\n       create_disposition=None,\n@@ -822,11 +848,23 @@ def perform_load_job(\n     Returns:\n       bigquery.JobReference with the information about the job that was started.\n     \"\"\"\n+    if not source_uris and not source_stream:\n+      raise ValueError(\n+          'Either a non-empty list of fully-qualified source URIs must be '\n+          'provided via the source_uris parameter or an open file object must '\n+          'be provided via the source_stream parameter. Got neither.')\n+\n+    if source_uris and source_stream:\n+      raise ValueError(\n+          'Only one of source_uris and source_stream may be specified. '\n+          'Got both.')\n+\n     return self._insert_load_job(\n         destination.projectId,\n         job_id,\n         destination,\n-        files,\n+        source_uris=source_uris,\n+        source_stream=source_stream,\n         schema=schema,\n         create_disposition=create_disposition,\n         write_disposition=write_disposition,"
  },
  {
    "sha": "558c8738dd44cfa4c95a379ca63ae3f7a9d5292a",
    "filename": "sdks/python/apache_beam/io/gcp/bigquery_tools_test.py",
    "status": "modified",
    "additions": 29,
    "deletions": 0,
    "changes": 29,
    "blob_url": "https://github.com/apache/beam/blob/911945c7f70bb6387ccd72fb1f4f9684613b5681/sdks/python/apache_beam/io/gcp/bigquery_tools_test.py",
    "raw_url": "https://github.com/apache/beam/raw/911945c7f70bb6387ccd72fb1f4f9684613b5681/sdks/python/apache_beam/io/gcp/bigquery_tools_test.py",
    "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/python/apache_beam/io/gcp/bigquery_tools_test.py?ref=911945c7f70bb6387ccd72fb1f4f9684613b5681",
    "patch": "@@ -386,6 +386,35 @@ def test_get_query_location(self):\n         project_id=\"second_project_id\", query=query, use_legacy_sql=False)\n     self.assertEqual(\"US\", location)\n \n+  def test_perform_load_job_source_mutual_exclusivity(self):\n+    client = mock.Mock()\n+    wrapper = beam.io.gcp.bigquery_tools.BigQueryWrapper(client)\n+\n+    # Both source_uri and source_stream specified.\n+    with self.assertRaises(ValueError):\n+      wrapper.perform_load_job(\n+          destination=parse_table_reference('project:dataset.table'),\n+          job_id='job_id',\n+          source_uris=['gs://example.com/*'],\n+          source_stream=io.BytesIO())\n+\n+    # Neither source_uri nor source_stream specified.\n+    with self.assertRaises(ValueError):\n+      wrapper.perform_load_job(destination='P:D.T', job_id='J')\n+\n+  def test_perform_load_job_with_source_stream(self):\n+    client = mock.Mock()\n+    wrapper = beam.io.gcp.bigquery_tools.BigQueryWrapper(client)\n+\n+    wrapper.perform_load_job(\n+        destination=parse_table_reference('project:dataset.table'),\n+        job_id='job_id',\n+        source_stream=io.BytesIO(b'some,data'))\n+\n+    client.jobs.Insert.assert_called_once()\n+    upload = client.jobs.Insert.call_args[1][\"upload\"]\n+    self.assertEqual(b'some,data', upload.stream.read())\n+\n \n @unittest.skipIf(HttpError is None, 'GCP dependencies are not installed')\n class TestBigQueryReader(unittest.TestCase):"
  },
  {
    "sha": "6a7c91bc812480c81307c9fcf4c2c48d234ff51a",
    "filename": "sdks/python/apache_beam/io/gcp/bigquery_write_it_test.py",
    "status": "modified",
    "additions": 45,
    "deletions": 0,
    "changes": 45,
    "blob_url": "https://github.com/apache/beam/blob/911945c7f70bb6387ccd72fb1f4f9684613b5681/sdks/python/apache_beam/io/gcp/bigquery_write_it_test.py",
    "raw_url": "https://github.com/apache/beam/raw/911945c7f70bb6387ccd72fb1f4f9684613b5681/sdks/python/apache_beam/io/gcp/bigquery_write_it_test.py",
    "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/python/apache_beam/io/gcp/bigquery_write_it_test.py?ref=911945c7f70bb6387ccd72fb1f4f9684613b5681",
    "patch": "@@ -31,6 +31,7 @@\n from decimal import Decimal\n \n import hamcrest as hc\n+import mock\n import pytz\n from future.utils import iteritems\n from nose.plugins.attrib import attr\n@@ -354,6 +355,50 @@ def test_big_query_write_without_schema(self):\n               write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n               temp_file_format=FileFormat.JSON))\n \n+  @attr('IT')\n+  @mock.patch(\n+      \"apache_beam.io.gcp.bigquery_file_loads._MAXIMUM_SOURCE_URIS\", new=1)\n+  def test_big_query_write_temp_table_append_schema_update(self):\n+    \"\"\"\n+    Test that schema update options are respected when appending to an existing\n+    table via temporary tables.\n+\n+    _MAXIMUM_SOURCE_URIS and max_file_size are both set to 1 to force multiple\n+    load jobs and usage of temporary tables.\n+    \"\"\"\n+    table_name = 'python_append_schema_update'\n+    self.create_table(table_name)\n+    table_id = '{}.{}'.format(self.dataset_id, table_name)\n+\n+    input_data = [{\"int64\": 1, \"bool\": True}, {\"int64\": 2, \"bool\": False}]\n+\n+    table_schema = {\n+        \"fields\": [{\n+            \"name\": \"int64\", \"type\": \"INT64\"\n+        }, {\n+            \"name\": \"bool\", \"type\": \"BOOL\"\n+        }]\n+    }\n+\n+    args = self.test_pipeline.get_full_options_as_args(\n+        on_success_matcher=BigqueryFullResultMatcher(\n+            project=self.project,\n+            query=\"SELECT bytes, date, time, int64, bool FROM %s\" % table_id,\n+            data=[(None, None, None, 1, True), (None, None, None, 2, False)]))\n+\n+    with beam.Pipeline(argv=args) as p:\n+      # pylint: disable=expression-not-assigned\n+      (\n+          p | 'create' >> beam.Create(input_data)\n+          | 'write' >> beam.io.WriteToBigQuery(\n+              table_id,\n+              schema=table_schema,\n+              write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n+              max_file_size=1,  # bytes\n+              method=beam.io.WriteToBigQuery.Method.FILE_LOADS,\n+              additional_bq_parameters={\n+                  'schemaUpdateOptions': ['ALLOW_FIELD_ADDITION']}))\n+\n \n if __name__ == '__main__':\n   logging.getLogger().setLevel(logging.INFO)"
  }
]
