[
  {
    "sha": "d9b530c8eb059ecccf5f38dc4efc3df2a73e413d",
    "filename": "hive3/src/main/java/org/apache/iceberg/mr/hive/serde/objectinspector/IcebergTimestampObjectInspectorHive3.java",
    "status": "modified",
    "additions": 1,
    "deletions": 3,
    "changes": 4,
    "blob_url": "https://github.com/apache/iceberg/blob/ef6cbafd81434b9db11377fa6c65eded0afc583b/hive3/src/main/java/org/apache/iceberg/mr/hive/serde/objectinspector/IcebergTimestampObjectInspectorHive3.java",
    "raw_url": "https://github.com/apache/iceberg/raw/ef6cbafd81434b9db11377fa6c65eded0afc583b/hive3/src/main/java/org/apache/iceberg/mr/hive/serde/objectinspector/IcebergTimestampObjectInspectorHive3.java",
    "contents_url": "https://api.github.com/repos/apache/iceberg/contents/hive3/src/main/java/org/apache/iceberg/mr/hive/serde/objectinspector/IcebergTimestampObjectInspectorHive3.java?ref=ef6cbafd81434b9db11377fa6c65eded0afc583b",
    "patch": "@@ -55,9 +55,7 @@ public Timestamp getPrimitiveJavaObject(Object o) {\n       return null;\n     }\n     LocalDateTime time = (LocalDateTime) o;\n-    Timestamp timestamp = Timestamp.ofEpochMilli(time.toInstant(ZoneOffset.UTC).toEpochMilli());\n-    timestamp.setNanos(time.getNano());\n-    return timestamp;\n+    return Timestamp.ofEpochMilli(time.toInstant(ZoneOffset.UTC).toEpochMilli(), time.getNano());\n   }\n \n   @Override"
  },
  {
    "sha": "ab1347e2bcc09dde3ba35c6a930d53b1688fb7ce",
    "filename": "mr/src/main/java/org/apache/iceberg/mr/hive/serde/objectinspector/IcebergTimestampWithZoneObjectInspector.java",
    "status": "modified",
    "additions": 3,
    "deletions": 2,
    "changes": 5,
    "blob_url": "https://github.com/apache/iceberg/blob/ef6cbafd81434b9db11377fa6c65eded0afc583b/mr/src/main/java/org/apache/iceberg/mr/hive/serde/objectinspector/IcebergTimestampWithZoneObjectInspector.java",
    "raw_url": "https://github.com/apache/iceberg/raw/ef6cbafd81434b9db11377fa6c65eded0afc583b/mr/src/main/java/org/apache/iceberg/mr/hive/serde/objectinspector/IcebergTimestampWithZoneObjectInspector.java",
    "contents_url": "https://api.github.com/repos/apache/iceberg/contents/mr/src/main/java/org/apache/iceberg/mr/hive/serde/objectinspector/IcebergTimestampWithZoneObjectInspector.java?ref=ef6cbafd81434b9db11377fa6c65eded0afc583b",
    "patch": "@@ -42,12 +42,13 @@ private IcebergTimestampWithZoneObjectInspector() {\n \n   @Override\n   public OffsetDateTime convert(Object o) {\n-    return o == null ? null : OffsetDateTime.ofInstant(((Timestamp) o).toInstant(), ZoneOffset.UTC);\n+    return o == null ? null : OffsetDateTime.of(((Timestamp) o).toLocalDateTime(), ZoneOffset.UTC);\n   }\n \n   @Override\n   public Timestamp getPrimitiveJavaObject(Object o) {\n-    return o == null ? null : Timestamp.from(((OffsetDateTime) o).toInstant());\n+    return o == null ? null :\n+        Timestamp.valueOf(((OffsetDateTime) o).withOffsetSameInstant(ZoneOffset.UTC).toLocalDateTime());\n   }\n \n   @Override"
  },
  {
    "sha": "dac411437a940b1b920ef625a37596ed0c77d978",
    "filename": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergTestUtils.java",
    "status": "modified",
    "additions": 70,
    "deletions": 3,
    "changes": 73,
    "blob_url": "https://github.com/apache/iceberg/blob/ef6cbafd81434b9db11377fa6c65eded0afc583b/mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergTestUtils.java",
    "raw_url": "https://github.com/apache/iceberg/raw/ef6cbafd81434b9db11377fa6c65eded0afc583b/mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergTestUtils.java",
    "contents_url": "https://api.github.com/repos/apache/iceberg/contents/mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergTestUtils.java?ref=ef6cbafd81434b9db11377fa6c65eded0afc583b",
    "patch": "@@ -32,6 +32,10 @@\n import java.time.LocalTime;\n import java.time.OffsetDateTime;\n import java.time.ZoneOffset;\n+import java.time.ZonedDateTime;\n+import java.time.format.DateTimeFormatter;\n+import java.time.format.DateTimeFormatterBuilder;\n+import java.time.temporal.ChronoField;\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.Comparator;\n@@ -59,7 +63,9 @@\n import org.apache.iceberg.data.GenericRecord;\n import org.apache.iceberg.data.IcebergGenerics;\n import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hive.MetastoreUtil;\n import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.types.Type;\n import org.apache.iceberg.types.Types;\n import org.apache.iceberg.util.ByteBuffers;\n import org.junit.Assert;\n@@ -107,6 +113,13 @@\n               PrimitiveObjectInspectorFactory.writableStringObjectInspector\n           ));\n \n+  public static final DateTimeFormatter TIMESTAMP_WITH_TZ_FORMATTER = new DateTimeFormatterBuilder()\n+      .appendPattern(\"yyyy-MM-dd HH:mm:ss\")\n+      .appendFraction(ChronoField.MICRO_OF_SECOND, 0, 9, true)\n+      .appendLiteral(' ')\n+      .appendZoneOrOffsetId()\n+      .toFormatter();\n+\n   private HiveIcebergTestUtils() {\n     // Empty constructor for the utility class\n   }\n@@ -210,14 +223,14 @@ public static void assertEquals(Record expected, Record actual) {\n   }\n \n   /**\n-   * Validates whether the table contains the expected records. The results should be sorted by a unique key so we do\n-   * not end up with flaky tests.\n+   * Validates whether the table contains the expected records reading the data through the Iceberg API.\n+   * The results should be sorted by a unique key so we do not end up with flaky tests.\n    * @param table The table we should read the records from\n    * @param expected The expected list of Records\n    * @param sortBy The column position by which we will sort\n    * @throws IOException Exceptions when reading the table data\n    */\n-  public static void validateData(Table table, List<Record> expected, int sortBy) throws IOException {\n+  public static void validateDataWithIceberg(Table table, List<Record> expected, int sortBy) throws IOException {\n     // Refresh the table, so we get the new data as well\n     table.refresh();\n     List<Record> records = new ArrayList<>(expected.size());\n@@ -265,4 +278,58 @@ public static void validateFiles(Table table, Configuration conf, JobID jobId, i\n     Assert.assertEquals(dataFileNum, dataFiles.size());\n     Assert.assertFalse(new File(HiveIcebergOutputCommitter.generateJobLocation(conf, jobId)).exists());\n   }\n+\n+  /**\n+   * Simplified implementation for checking that the returned results from a SELECT statement are the same than the\n+   * inserted values. We expect that the values are inserted using {@link #getStringValueForInsert(Object, Type)}.\n+   * <p>\n+   * For the full implementation we might want to add sorting so the check could work for every table/query.\n+   * @param shell The shell used for executing the query\n+   * @param tableName The name of the table to query\n+   * @param expected The records we inserted\n+   */\n+  public static void validateDataWithSql(TestHiveShell shell, String tableName, List<Record> expected) {\n+    List<Object[]> actual = shell.executeStatement(\"SELECT * from \" + tableName);\n+\n+    for (int rowId = 0; rowId < expected.size(); ++rowId) {\n+      Record record = expected.get(rowId);\n+      Object[] row = actual.get(rowId);\n+      Assert.assertEquals(record.size(), row.length);\n+      for (int fieldId = 0; fieldId < record.size(); ++fieldId) {\n+        Types.NestedField field = record.struct().fields().get(fieldId);\n+        String inserted = getStringValueForInsert(record.getField(field.name()), field.type())\n+            // If there are enclosing quotes then remove them\n+            .replaceAll(\"'(.*)'\", \"$1\");\n+        String returned = row[fieldId].toString();\n+        if (field.type().equals(Types.TimestampType.withZone()) && MetastoreUtil.hive3PresentOnClasspath()) {\n+          Timestamp timestamp = Timestamp.from(ZonedDateTime.parse(returned, TIMESTAMP_WITH_TZ_FORMATTER).toInstant());\n+          returned = timestamp.toString();\n+        }\n+        Assert.assertEquals(inserted, returned);\n+      }\n+    }\n+  }\n+\n+  public static String getStringValueForInsert(Object value, Type type) {\n+    String template = \"\\'%s\\'\";\n+    if (type.equals(Types.TimestampType.withoutZone())) {\n+      return String.format(template, Timestamp.valueOf((LocalDateTime) value).toString());\n+    } else if (type.equals(Types.TimestampType.withZone())) {\n+      Timestamp timestamp;\n+      // Hive2 stores Timestamps with local TZ, Hive3 stores Timestamps in UTC so we have to insert different timestamp\n+      // to get the same expected values in the Iceberg rows. The Hive query should return the same values as inserted\n+      // in both cases\n+      if (MetastoreUtil.hive3PresentOnClasspath()) {\n+        timestamp = Timestamp.from(((OffsetDateTime) value).toInstant());\n+      } else {\n+        timestamp = Timestamp.valueOf(((OffsetDateTime) value).withOffsetSameInstant(ZoneOffset.UTC).toLocalDateTime());\n+      }\n+      return String.format(template, timestamp.toString());\n+    } else if (type.equals(Types.BooleanType.get())) {\n+      // in hive2 boolean type values must not be surrounded in apostrophes. Otherwise the value is translated to true.\n+      return value.toString();\n+    } else {\n+      return String.format(template, value.toString());\n+    }\n+  }\n }"
  },
  {
    "sha": "a911ec692b14bc2535e10b1ec5ab5ee78931ecf4",
    "filename": "mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergOutputCommitter.java",
    "status": "modified",
    "additions": 8,
    "deletions": 8,
    "changes": 16,
    "blob_url": "https://github.com/apache/iceberg/blob/ef6cbafd81434b9db11377fa6c65eded0afc583b/mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergOutputCommitter.java",
    "raw_url": "https://github.com/apache/iceberg/raw/ef6cbafd81434b9db11377fa6c65eded0afc583b/mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergOutputCommitter.java",
    "contents_url": "https://api.github.com/repos/apache/iceberg/contents/mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergOutputCommitter.java?ref=ef6cbafd81434b9db11377fa6c65eded0afc583b",
    "patch": "@@ -111,7 +111,7 @@ public void testSuccessfulUnpartitionedWrite() throws IOException {\n     committer.commitJob(new JobContextImpl(conf, JOB_ID));\n \n     HiveIcebergTestUtils.validateFiles(table, conf, JOB_ID, 1);\n-    HiveIcebergTestUtils.validateData(table, expected, 0);\n+    HiveIcebergTestUtils.validateDataWithIceberg(table, expected, 0);\n   }\n \n   @Test\n@@ -123,7 +123,7 @@ public void testSuccessfulPartitionedWrite() throws IOException {\n     committer.commitJob(new JobContextImpl(conf, JOB_ID));\n \n     HiveIcebergTestUtils.validateFiles(table, conf, JOB_ID, 3);\n-    HiveIcebergTestUtils.validateData(table, expected, 0);\n+    HiveIcebergTestUtils.validateDataWithIceberg(table, expected, 0);\n   }\n \n   @Test\n@@ -135,7 +135,7 @@ public void testSuccessfulMultipleTasksUnpartitionedWrite() throws IOException {\n     committer.commitJob(new JobContextImpl(conf, JOB_ID));\n \n     HiveIcebergTestUtils.validateFiles(table, conf, JOB_ID, 2);\n-    HiveIcebergTestUtils.validateData(table, expected, 0);\n+    HiveIcebergTestUtils.validateDataWithIceberg(table, expected, 0);\n   }\n \n   @Test\n@@ -147,7 +147,7 @@ public void testSuccessfulMultipleTasksPartitionedWrite() throws IOException {\n     committer.commitJob(new JobContextImpl(conf, JOB_ID));\n \n     HiveIcebergTestUtils.validateFiles(table, conf, JOB_ID, 6);\n-    HiveIcebergTestUtils.validateData(table, expected, 0);\n+    HiveIcebergTestUtils.validateDataWithIceberg(table, expected, 0);\n   }\n \n   @Test\n@@ -159,19 +159,19 @@ public void testRetryTask() throws IOException {\n     // Write records and abort the tasks\n     writeRecords(2, 0, false, true, conf);\n     HiveIcebergTestUtils.validateFiles(table, conf, JOB_ID, 0);\n-    HiveIcebergTestUtils.validateData(table, Collections.emptyList(), 0);\n+    HiveIcebergTestUtils.validateDataWithIceberg(table, Collections.emptyList(), 0);\n \n     // Write records but do not abort the tasks\n     // The data files remain since we can not identify them but should not be read\n     writeRecords(2, 1, false, false, conf);\n     HiveIcebergTestUtils.validateFiles(table, conf, JOB_ID, 2);\n-    HiveIcebergTestUtils.validateData(table, Collections.emptyList(), 0);\n+    HiveIcebergTestUtils.validateDataWithIceberg(table, Collections.emptyList(), 0);\n \n     // Write and commit the records\n     List<Record> expected = writeRecords(2, 2, true, false, conf);\n     committer.commitJob(new JobContextImpl(conf, JOB_ID));\n     HiveIcebergTestUtils.validateFiles(table, conf, JOB_ID, 4);\n-    HiveIcebergTestUtils.validateData(table, expected, 0);\n+    HiveIcebergTestUtils.validateDataWithIceberg(table, expected, 0);\n   }\n \n   @Test\n@@ -183,7 +183,7 @@ public void testAbortJob() throws IOException {\n     committer.abortJob(new JobContextImpl(conf, JOB_ID), JobStatus.State.FAILED);\n \n     HiveIcebergTestUtils.validateFiles(table, conf, JOB_ID, 0);\n-    HiveIcebergTestUtils.validateData(table, Collections.emptyList(), 0);\n+    HiveIcebergTestUtils.validateDataWithIceberg(table, Collections.emptyList(), 0);\n   }\n \n   @Test"
  },
  {
    "sha": "a4276e3e6b93ccc3e3054751176237bff11c2d22",
    "filename": "mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerTimezone.java",
    "status": "modified",
    "additions": 90,
    "deletions": 4,
    "changes": 94,
    "blob_url": "https://github.com/apache/iceberg/blob/ef6cbafd81434b9db11377fa6c65eded0afc583b/mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerTimezone.java",
    "raw_url": "https://github.com/apache/iceberg/raw/ef6cbafd81434b9db11377fa6c65eded0afc583b/mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerTimezone.java",
    "contents_url": "https://api.github.com/repos/apache/iceberg/contents/mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerTimezone.java?ref=ef6cbafd81434b9db11377fa6c65eded0afc583b",
    "patch": "@@ -23,16 +23,23 @@\n import java.text.DateFormat;\n import java.time.LocalDate;\n import java.time.LocalDateTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneId;\n+import java.time.ZoneOffset;\n+import java.time.ZonedDateTime;\n import java.util.Collection;\n import java.util.List;\n import java.util.Optional;\n import java.util.TimeZone;\n import org.apache.hadoop.hive.serde2.io.DateWritable;\n import org.apache.hadoop.hive.serde2.io.TimestampWritable;\n+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.common.DynFields;\n import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hive.MetastoreUtil;\n import org.apache.iceberg.mr.TestHelper;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.types.Types;\n@@ -47,26 +54,38 @@\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n \n+import static org.apache.iceberg.mr.hive.HiveIcebergTestUtils.TIMESTAMP_WITH_TZ_FORMATTER;\n import static org.apache.iceberg.types.Types.NestedField.optional;\n import static org.junit.runners.Parameterized.Parameter;\n import static org.junit.runners.Parameterized.Parameters;\n \n @RunWith(Parameterized.class)\n public class TestHiveIcebergStorageHandlerTimezone {\n-  private static final Optional<ThreadLocal<DateFormat>> dateFormat =\n+  private static final Optional<ThreadLocal<DateFormat>> DATE_FORMAT =\n       Optional.ofNullable((ThreadLocal<DateFormat>) DynFields.builder()\n           .hiddenImpl(TimestampWritable.class, \"threadLocalDateFormat\")\n           .defaultAlwaysNull()\n           .buildStatic()\n           .get());\n \n-  private static final Optional<ThreadLocal<TimeZone>> localTimeZone =\n+  private static final Optional<ThreadLocal<TimeZone>> LOCAL_TIMEZONE =\n       Optional.ofNullable((ThreadLocal<TimeZone>) DynFields.builder()\n           .hiddenImpl(DateWritable.class, \"LOCAL_TIMEZONE\")\n           .defaultAlwaysNull()\n           .buildStatic()\n           .get());\n \n+  private static final PrimitiveTypeInfo TIMESTAMP_LOCAL_TZ_TYPE_INFO =\n+      (PrimitiveTypeInfo) DynFields.builder().hiddenImpl(TypeInfoFactory.class, \"timestampLocalTZTypeInfo\")\n+          .defaultAlwaysNull()\n+          .buildStatic().get();\n+\n+  private static final Optional<DynFields.BoundField<ZoneId>> ZONE_ID =\n+      Optional.ofNullable(TIMESTAMP_LOCAL_TZ_TYPE_INFO == null ? null : DynFields.builder()\n+          .hiddenImpl(\"org.apache.hadoop.hive.serde2.typeinfo.TimestampLocalTZTypeInfo\", \"timeZone\")\n+          .defaultAlwaysNull()\n+          .build(TIMESTAMP_LOCAL_TZ_TYPE_INFO));\n+\n   @Parameters(name = \"timezone={0}\")\n   public static Collection<Object[]> parameters() {\n     return ImmutableList.of(\n@@ -102,8 +121,9 @@ public void before() throws IOException {\n \n     // Magic to clean cached date format and local timezone for Hive where the default timezone is used/stored in the\n     // cached object\n-    dateFormat.ifPresent(ThreadLocal::remove);\n-    localTimeZone.ifPresent(ThreadLocal::remove);\n+    DATE_FORMAT.ifPresent(ThreadLocal::remove);\n+    LOCAL_TIMEZONE.ifPresent(ThreadLocal::remove);\n+    ZONE_ID.ifPresent(field -> field.set(TimeZone.getDefault().toZoneId()));\n \n     this.testTables = HiveIcebergStorageHandlerTestUtils.testTables(shell, TestTables.TestTableType.HIVE_CATALOG, temp);\n     // Uses spark as an engine so we can detect if we unintentionally try to use any execution engines\n@@ -169,4 +189,70 @@ public void testTimestampQuery() throws IOException {\n     result = shell.executeStatement(\"SELECT * FROM ts_test WHERE d_ts='2017-01-01 22:30:57.3'\");\n     Assert.assertEquals(0, result.size());\n   }\n+\n+  @Test\n+  public void testTimestampTzQuery() throws IOException {\n+    Schema timestampSchema = new Schema(optional(1, \"d_ts\", Types.TimestampType.withZone()));\n+\n+    if (MetastoreUtil.hive3PresentOnClasspath()) {\n+      LocalDateTime localDateTime = LocalDateTime.of(2019, 1, 22, 9, 44, 54, 100000000);\n+      ZoneOffset zoneOffSet = TimeZone.getDefault().toZoneId().getRules().getOffset(localDateTime);\n+\n+      OffsetDateTime offsetDateTime1 = localDateTime.atOffset(zoneOffSet);\n+\n+      localDateTime = LocalDateTime.of(2019, 2, 22, 9, 44, 54, 200000000);\n+      zoneOffSet = TimeZone.getDefault().toZoneId().getRules().getOffset(localDateTime);\n+\n+      OffsetDateTime offsetDateTime2 = localDateTime.atOffset(zoneOffSet);\n+\n+      List<Record> records = TestHelper.RecordsBuilder.newInstance(timestampSchema)\n+          .add(offsetDateTime1)\n+          .add(offsetDateTime2)\n+          .build();\n+\n+      testTables.createTable(shell, \"ts_test\", timestampSchema, FileFormat.PARQUET, records);\n+\n+      List<Object[]> result = shell.executeStatement(\"SELECT d_ts FROM ts_test WHERE d_ts='2019-02-22 09:44:54.2'\");\n+      Assert.assertEquals(1, result.size());\n+      Assert.assertEquals(offsetDateTime2.toInstant(),\n+          ZonedDateTime.parse(result.get(0)[0].toString(), TIMESTAMP_WITH_TZ_FORMATTER).toInstant());\n+\n+      // Skip testing `in` as I was not able come up with the constants that would work (maybe some Hive bug?)\n+\n+      result = shell.executeStatement(\"SELECT d_ts FROM ts_test WHERE d_ts < '2019-02-22 09:44:54.2'\");\n+      Assert.assertEquals(1, result.size());\n+      Assert.assertEquals(offsetDateTime1.toInstant(),\n+          ZonedDateTime.parse(result.get(0)[0].toString(), TIMESTAMP_WITH_TZ_FORMATTER).toInstant());\n+\n+      result = shell.executeStatement(\"SELECT d_ts FROM ts_test WHERE d_ts='2017-01-01 22:30:57.3'\");\n+      Assert.assertEquals(0, result.size());\n+    } else {\n+      List<Record> records = TestHelper.RecordsBuilder.newInstance(timestampSchema)\n+          .add(OffsetDateTime.of(\n+              LocalDateTime.of(2019, 1, 22, 9, 44, 54, 100000000),\n+              ZoneOffset.UTC))\n+          .add(OffsetDateTime.of(\n+              LocalDateTime.of(2019, 2, 22, 9, 44, 54, 200000000),\n+              ZoneOffset.UTC))\n+          .build();\n+\n+      testTables.createTable(shell, \"ts_test\", timestampSchema, FileFormat.PARQUET, records);\n+\n+      List<Object[]> result = shell.executeStatement(\"SELECT d_ts FROM ts_test WHERE d_ts='2019-02-22 09:44:54.2'\");\n+      Assert.assertEquals(1, result.size());\n+      Assert.assertEquals(\"2019-02-22 09:44:54.2\", result.get(0)[0]);\n+\n+      result = shell.executeStatement(\n+          \"SELECT d_ts FROM ts_test WHERE d_ts in ('2017-01-01 22:30:57.1', '2019-02-22 09:44:54.2')\");\n+      Assert.assertEquals(1, result.size());\n+      Assert.assertEquals(\"2019-02-22 09:44:54.2\", result.get(0)[0]);\n+\n+      result = shell.executeStatement(\"SELECT d_ts FROM ts_test WHERE d_ts < '2019-02-22 09:44:54.2'\");\n+      Assert.assertEquals(1, result.size());\n+      Assert.assertEquals(\"2019-01-22 09:44:54.1\", result.get(0)[0]);\n+\n+      result = shell.executeStatement(\"SELECT d_ts FROM ts_test WHERE d_ts='2017-01-01 22:30:57.3'\");\n+      Assert.assertEquals(0, result.size());\n+    }\n+  }\n }"
  },
  {
    "sha": "f6c0747714a14aa24ff6738f06ffbf3f257f76bb",
    "filename": "mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerWithEngine.java",
    "status": "modified",
    "additions": 15,
    "deletions": 13,
    "changes": 28,
    "blob_url": "https://github.com/apache/iceberg/blob/ef6cbafd81434b9db11377fa6c65eded0afc583b/mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerWithEngine.java",
    "raw_url": "https://github.com/apache/iceberg/raw/ef6cbafd81434b9db11377fa6c65eded0afc583b/mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerWithEngine.java",
    "contents_url": "https://api.github.com/repos/apache/iceberg/contents/mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerWithEngine.java?ref=ef6cbafd81434b9db11377fa6c65eded0afc583b",
    "patch": "@@ -306,7 +306,7 @@ public void testInsert() throws IOException {\n \n     shell.executeStatement(query.toString());\n \n-    HiveIcebergTestUtils.validateData(table, HiveIcebergStorageHandlerTestUtils.CUSTOMER_RECORDS, 0);\n+    HiveIcebergTestUtils.validateDataWithIceberg(table, HiveIcebergStorageHandlerTestUtils.CUSTOMER_RECORDS, 0);\n   }\n \n   @Test\n@@ -323,14 +323,16 @@ public void testInsertSupportedTypes() throws IOException {\n         continue;\n       }\n       String columnName = type.typeId().toString().toLowerCase() + \"_column\";\n+      String tableName = type.typeId().toString().toLowerCase() + \"_table_\" + i;\n \n       Schema schema = new Schema(required(1, \"id\", Types.LongType.get()), required(2, columnName, type));\n       List<Record> expected = TestHelper.generateRandomRecords(schema, 5, 0L);\n \n-      Table table = testTables.createTable(shell, type.typeId().toString().toLowerCase() + \"_table_\" + i,\n-          schema, PartitionSpec.unpartitioned(), fileFormat, expected);\n+      Table table = testTables.createTable(shell, tableName, schema, PartitionSpec.unpartitioned(), fileFormat,\n+          expected);\n \n-      HiveIcebergTestUtils.validateData(table, expected, 0);\n+      HiveIcebergTestUtils.validateDataWithIceberg(table, expected, 0);\n+      HiveIcebergTestUtils.validateDataWithSql(shell, tableName, expected);\n     }\n   }\n \n@@ -350,7 +352,7 @@ public void testInsertFromSelect() throws IOException {\n     // Check that everything is duplicated as expected\n     List<Record> records = new ArrayList<>(HiveIcebergStorageHandlerTestUtils.CUSTOMER_RECORDS);\n     records.addAll(HiveIcebergStorageHandlerTestUtils.CUSTOMER_RECORDS);\n-    HiveIcebergTestUtils.validateData(table, records, 0);\n+    HiveIcebergTestUtils.validateDataWithIceberg(table, records, 0);\n   }\n \n   /**\n@@ -370,7 +372,7 @@ public void testInsertFromSelectWithOrderBy() throws IOException {\n     // Check that everything is duplicated as expected\n     List<Record> records = new ArrayList<>(HiveIcebergStorageHandlerTestUtils.CUSTOMER_RECORDS);\n     records.addAll(HiveIcebergStorageHandlerTestUtils.CUSTOMER_RECORDS);\n-    HiveIcebergTestUtils.validateData(table, records, 0);\n+    HiveIcebergTestUtils.validateDataWithIceberg(table, records, 0);\n   }\n \n   @Test\n@@ -389,7 +391,7 @@ public void testInsertFromSelectWithProjection() throws IOException {\n         .add(1L, null, \"test\")\n         .build();\n \n-    HiveIcebergTestUtils.validateData(table, expected, 0);\n+    HiveIcebergTestUtils.validateDataWithIceberg(table, expected, 0);\n   }\n \n   @Test\n@@ -414,7 +416,7 @@ public void testInsertUsingSourceTableWithSharedColumnsNames() throws IOExceptio\n       copy.setField(\"first_name\", \"Sam\");\n       expected.add(copy);\n     });\n-    HiveIcebergTestUtils.validateData(table, expected, 0);\n+    HiveIcebergTestUtils.validateDataWithIceberg(table, expected, 0);\n   }\n \n   @Test\n@@ -433,7 +435,7 @@ public void testInsertFromJoiningTwoIcebergTables() throws IOException {\n     shell.executeStatement(\"INSERT INTO target_customers SELECT a.customer_id, b.first_name, a.last_name FROM \" +\n             \"source_customers_1 a JOIN source_customers_2 b ON a.last_name = b.last_name\");\n \n-    HiveIcebergTestUtils.validateData(table, HiveIcebergStorageHandlerTestUtils.CUSTOMER_RECORDS, 0);\n+    HiveIcebergTestUtils.validateDataWithIceberg(table, HiveIcebergStorageHandlerTestUtils.CUSTOMER_RECORDS, 0);\n   }\n \n   @Test\n@@ -585,7 +587,7 @@ public void testPartitionedWrite() throws IOException {\n     Table table = testTables.createTable(shell, \"partitioned_customers\",\n         HiveIcebergStorageHandlerTestUtils.CUSTOMER_SCHEMA, spec, fileFormat, records);\n \n-    HiveIcebergTestUtils.validateData(table, records, 0);\n+    HiveIcebergTestUtils.validateDataWithIceberg(table, records, 0);\n   }\n \n   @Test\n@@ -601,7 +603,7 @@ public void testIdentityPartitionedWrite() throws IOException {\n     Table table = testTables.createTable(shell, \"partitioned_customers\",\n         HiveIcebergStorageHandlerTestUtils.CUSTOMER_SCHEMA, spec, fileFormat, records);\n \n-    HiveIcebergTestUtils.validateData(table, records, 0);\n+    HiveIcebergTestUtils.validateDataWithIceberg(table, records, 0);\n   }\n \n   @Test\n@@ -618,7 +620,7 @@ public void testMultilevelIdentityPartitionedWrite() throws IOException {\n     Table table = testTables.createTable(shell, \"partitioned_customers\",\n         HiveIcebergStorageHandlerTestUtils.CUSTOMER_SCHEMA, spec, fileFormat, records);\n \n-    HiveIcebergTestUtils.validateData(table, records, 0);\n+    HiveIcebergTestUtils.validateDataWithIceberg(table, records, 0);\n   }\n \n   private void testComplexTypeWrite(Schema schema, List<Record> records) throws IOException {\n@@ -629,7 +631,7 @@ private void testComplexTypeWrite(Schema schema, List<Record> records) throws IO\n     shell.executeStatement(\"CREATE TABLE default.\" + dummyTableName + \"(a int)\");\n     shell.executeStatement(\"INSERT INTO TABLE default.\" + dummyTableName + \" VALUES(1)\");\n     records.forEach(r -> shell.executeStatement(insertQueryForComplexType(tableName, dummyTableName, schema, r)));\n-    HiveIcebergTestUtils.validateData(table, records, 0);\n+    HiveIcebergTestUtils.validateDataWithIceberg(table, records, 0);\n   }\n \n   private String insertQueryForComplexType(String tableName, String dummyTableName, Schema schema, Record record) {"
  },
  {
    "sha": "29dad7953e75f84e16f1258279a29a19d93c4a3a",
    "filename": "mr/src/test/java/org/apache/iceberg/mr/hive/TestTables.java",
    "status": "modified",
    "additions": 2,
    "deletions": 21,
    "changes": 23,
    "blob_url": "https://github.com/apache/iceberg/blob/ef6cbafd81434b9db11377fa6c65eded0afc583b/mr/src/test/java/org/apache/iceberg/mr/hive/TestTables.java",
    "raw_url": "https://github.com/apache/iceberg/raw/ef6cbafd81434b9db11377fa6c65eded0afc583b/mr/src/test/java/org/apache/iceberg/mr/hive/TestTables.java",
    "contents_url": "https://api.github.com/repos/apache/iceberg/contents/mr/src/test/java/org/apache/iceberg/mr/hive/TestTables.java?ref=ef6cbafd81434b9db11377fa6c65eded0afc583b",
    "patch": "@@ -22,9 +22,6 @@\n import java.io.File;\n import java.io.IOException;\n import java.io.UncheckedIOException;\n-import java.sql.Timestamp;\n-import java.time.LocalDateTime;\n-import java.time.OffsetDateTime;\n import java.util.Collections;\n import java.util.List;\n import java.util.Map;\n@@ -54,8 +51,6 @@\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.ObjectArrays;\n-import org.apache.iceberg.types.Type;\n-import org.apache.iceberg.types.Types;\n import org.junit.Assert;\n import org.junit.rules.TemporaryFolder;\n \n@@ -186,8 +181,8 @@ public Table createTable(TestHiveShell shell, String tableName, Schema schema, P\n       records.forEach(record -> {\n         query.append(\"(\");\n         query.append(record.struct().fields().stream()\n-                .map(field -> getStringValueForInsert(record.getField(field.name()), field.type()))\n-                .collect(Collectors.joining(\",\")));\n+            .map(field -> HiveIcebergTestUtils.getStringValueForInsert(record.getField(field.name()), field.type()))\n+            .collect(Collectors.joining(\",\")));\n         query.append(\"),\");\n       });\n       query.setLength(query.length() - 1);\n@@ -405,20 +400,6 @@ private static String tablePath(TableIdentifier identifier) {\n     return \"/\" + Joiner.on(\"/\").join(identifier.namespace().levels()) + \"/\" + identifier.name();\n   }\n \n-  private String getStringValueForInsert(Object value, Type type) {\n-    String template = \"\\'%s\\'\";\n-    if (type.equals(Types.TimestampType.withoutZone())) {\n-      return String.format(template, Timestamp.valueOf((LocalDateTime) value).toString());\n-    } else if (type.equals(Types.TimestampType.withZone())) {\n-      return String.format(template, Timestamp.from(((OffsetDateTime) value).toInstant()).toString());\n-    } else if (type.equals(Types.BooleanType.get())) {\n-      // in hive2 boolean type values must not be surrounded in apostrophes. Otherwise the value is translated to true.\n-      return value.toString();\n-    } else {\n-      return String.format(template, value.toString());\n-    }\n-  }\n-\n   enum TestTableType {\n     HADOOP_TABLE {\n       public TestTables instance(Configuration conf, TemporaryFolder temporaryFolder) {"
  },
  {
    "sha": "ddcf9f6ba88b265de0362b60ca80a5374d68c826",
    "filename": "mr/src/test/java/org/apache/iceberg/mr/hive/serde/objectinspector/TestIcebergTimestampWithZoneObjectInspector.java",
    "status": "modified",
    "additions": 1,
    "deletions": 5,
    "changes": 6,
    "blob_url": "https://github.com/apache/iceberg/blob/ef6cbafd81434b9db11377fa6c65eded0afc583b/mr/src/test/java/org/apache/iceberg/mr/hive/serde/objectinspector/TestIcebergTimestampWithZoneObjectInspector.java",
    "raw_url": "https://github.com/apache/iceberg/raw/ef6cbafd81434b9db11377fa6c65eded0afc583b/mr/src/test/java/org/apache/iceberg/mr/hive/serde/objectinspector/TestIcebergTimestampWithZoneObjectInspector.java",
    "contents_url": "https://api.github.com/repos/apache/iceberg/contents/mr/src/test/java/org/apache/iceberg/mr/hive/serde/objectinspector/TestIcebergTimestampWithZoneObjectInspector.java?ref=ef6cbafd81434b9db11377fa6c65eded0afc583b",
    "patch": "@@ -52,7 +52,7 @@ public void testIcebergTimestampObjectInspectorWithUTCAdjustment() {\n \n     LocalDateTime local = LocalDateTime.of(2020, 1, 1, 16, 45, 33, 456000);\n     OffsetDateTime offsetDateTime = OffsetDateTime.of(local, ZoneOffset.ofHours(-5));\n-    Timestamp ts = Timestamp.from(offsetDateTime.toInstant());\n+    Timestamp ts = Timestamp.valueOf(offsetDateTime.withOffsetSameInstant(ZoneOffset.UTC).toLocalDateTime());\n \n     Assert.assertEquals(ts, oi.getPrimitiveJavaObject(offsetDateTime));\n     Assert.assertEquals(new TimestampWritable(ts), oi.getPrimitiveWritableObject(offsetDateTime));\n@@ -66,9 +66,5 @@ public void testIcebergTimestampObjectInspectorWithUTCAdjustment() {\n \n     Assert.assertEquals(OffsetDateTime.ofInstant(local.toInstant(ZoneOffset.ofHours(-5)), ZoneOffset.UTC),\n             oi.convert(ts));\n-\n-    Assert.assertEquals(offsetDateTime.withOffsetSameInstant(ZoneOffset.UTC),\n-            oi.convert(Timestamp.from(offsetDateTime.toInstant())));\n   }\n-\n }"
  }
]
