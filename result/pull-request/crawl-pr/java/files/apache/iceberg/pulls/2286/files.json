[
  {
    "sha": "279263d69788e25c750cdaf6a7ad6ce44e3280f9",
    "filename": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/ArrowBatch.java",
    "status": "added",
    "additions": 82,
    "deletions": 0,
    "changes": 82,
    "blob_url": "https://github.com/apache/iceberg/blob/da10fe2cab636f24dff800c31c586b846ca10a41/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/ArrowBatch.java",
    "raw_url": "https://github.com/apache/iceberg/raw/da10fe2cab636f24dff800c31c586b846ca10a41/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/ArrowBatch.java",
    "contents_url": "https://api.github.com/repos/apache/iceberg/contents/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/ArrowBatch.java?ref=da10fe2cab636f24dff800c31c586b846ca10a41",
    "patch": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import java.util.Arrays;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+\n+/**\n+ * This class is inspired by Spark's {@code ColumnarBatch}.\n+ * This class wraps a columnar batch in the result set of an Iceberg table query.\n+ */\n+public class ArrowBatch implements AutoCloseable {\n+\n+  private final int numRows;\n+  private final ArrowVector[] columns;\n+\n+  ArrowBatch(int numRows, ArrowVector[] columns) {\n+    this.numRows = numRows;\n+    this.columns = columns;\n+  }\n+\n+  /**\n+   * Create a new instance of {@link VectorSchemaRoot}\n+   * from the arrow vectors stored in this arrow batch.\n+   * The arrow vectors are owned by the reader.\n+   */\n+  public VectorSchemaRoot createVectorSchemaRootFromVectors() {\n+    return VectorSchemaRoot.of(Arrays.stream(columns)\n+        .map(ArrowVector::getFieldVector)\n+        .toArray(FieldVector[]::new));\n+  }\n+\n+  /**\n+   * Called to close all the columns in this batch. It is not valid to access the data after calling this. This must be\n+   * called at the end to clean up memory allocations.\n+   */\n+  @Override\n+  public void close() {\n+    for (ArrowVector c : columns) {\n+      c.close();\n+    }\n+  }\n+\n+  /**\n+   * Returns the number of columns that make up this batch.\n+   */\n+  public int numCols() {\n+    return columns.length;\n+  }\n+\n+  /**\n+   * Returns the number of rows for read, including filtered rows.\n+   */\n+  public int numRows() {\n+    return numRows;\n+  }\n+\n+  /**\n+   * Returns the column at `ordinal`.\n+   */\n+  public ArrowVector column(int ordinal) {\n+    return columns[ordinal];\n+  }\n+}"
  },
  {
    "sha": "bc963e555114cabc02087a17cefc4a571ffa86a6",
    "filename": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/ArrowBatchReader.java",
    "status": "added",
    "additions": 106,
    "deletions": 0,
    "changes": 106,
    "blob_url": "https://github.com/apache/iceberg/blob/da10fe2cab636f24dff800c31c586b846ca10a41/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/ArrowBatchReader.java",
    "raw_url": "https://github.com/apache/iceberg/raw/da10fe2cab636f24dff800c31c586b846ca10a41/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/ArrowBatchReader.java",
    "contents_url": "https://api.github.com/repos/apache/iceberg/contents/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/ArrowBatchReader.java?ref=da10fe2cab636f24dff800c31c586b846ca10a41",
    "patch": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+\n+/**\n+ * A collection of vectorized readers per column (in the expected read schema) and Arrow Vector holders. This class owns\n+ * the Arrow vectors and is responsible for closing the Arrow vectors.\n+ */\n+class ArrowBatchReader implements VectorizedReader<ArrowBatch> {\n+\n+  private final VectorizedArrowReader[] readers;\n+  private final VectorHolder[] vectorHolders;\n+\n+  ArrowBatchReader(List<VectorizedReader<?>> readers) {\n+    this.readers = readers.stream()\n+        .map(VectorizedArrowReader.class::cast)\n+        .toArray(VectorizedArrowReader[]::new);\n+    this.vectorHolders = new VectorHolder[readers.size()];\n+  }\n+\n+  @Override\n+  public final void setRowGroupInfo(\n+      PageReadStore pageStore, Map<ColumnPath, ColumnChunkMetaData> metaData, long rowPosition) {\n+    for (VectorizedArrowReader reader : readers) {\n+      if (reader != null) {\n+        reader.setRowGroupInfo(pageStore, metaData, rowPosition);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public final ArrowBatch read(ArrowBatch reuse, int numRowsToRead) {\n+    Preconditions.checkArgument(numRowsToRead > 0, \"Invalid number of rows to read: %s\", numRowsToRead);\n+\n+    if (reuse == null) {\n+      closeVectors();\n+    }\n+\n+    ArrowVector[] arrowVectors = new ArrowVector[readers.length];\n+    for (int i = 0; i < readers.length; i += 1) {\n+      vectorHolders[i] = readers[i].read(vectorHolders[i], numRowsToRead);\n+      int numRowsInVector = vectorHolders[i].numValues();\n+      Preconditions.checkState(\n+          numRowsInVector == numRowsToRead,\n+          \"Number of rows in the vector %s didn't match expected %s \", numRowsInVector,\n+          numRowsToRead);\n+      // Handle null vector for constant case\n+      arrowVectors[i] = new ArrowVector(vectorHolders[i]);\n+    }\n+    return new ArrowBatch(numRowsToRead, arrowVectors);\n+  }\n+\n+  private void closeVectors() {\n+    for (int i = 0; i < vectorHolders.length; i++) {\n+      if (vectorHolders[i] != null) {\n+        // Release any resources used by the vector\n+        if (vectorHolders[i].vector() != null) {\n+          vectorHolders[i].vector().close();\n+        }\n+        vectorHolders[i] = null;\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void close() {\n+    for (VectorizedReader<?> reader : readers) {\n+      reader.close();\n+    }\n+    closeVectors();\n+  }\n+\n+  @Override\n+  public void setBatchSize(int batchSize) {\n+    for (VectorizedArrowReader reader : readers) {\n+      if (reader != null) {\n+        reader.setBatchSize(batchSize);\n+      }\n+    }\n+  }\n+}"
  },
  {
    "sha": "9b41c1050c4dd7f45901457b765047b91255ac93",
    "filename": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/ArrowReader.java",
    "status": "added",
    "additions": 179,
    "deletions": 0,
    "changes": 179,
    "blob_url": "https://github.com/apache/iceberg/blob/da10fe2cab636f24dff800c31c586b846ca10a41/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/ArrowReader.java",
    "raw_url": "https://github.com/apache/iceberg/raw/da10fe2cab636f24dff800c31c586b846ca10a41/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/ArrowReader.java",
    "contents_url": "https://api.github.com/repos/apache/iceberg/contents/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/ArrowReader.java?ref=da10fe2cab636f24dff800c31c586b846ca10a41",
    "patch": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import org.apache.arrow.vector.types.Types.MinorType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableGroup;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.types.Types;\n+\n+/**\n+ * Vectorized reader that returns an iterator of {@link ArrowBatch}.\n+ * See {@link #open(CloseableIterable)} ()} to learn about the\n+ * behavior of the iterator.\n+ *\n+ * <p>The following Iceberg data types are supported and have been tested:\n+ * <ul>\n+ *     <li>Iceberg: {@link Types.BooleanType}, Arrow: {@link MinorType#BIT}</li>\n+ *     <li>Iceberg: {@link Types.IntegerType}, Arrow: {@link MinorType#INT}</li>\n+ *     <li>Iceberg: {@link Types.LongType}, Arrow: {@link MinorType#BIGINT}</li>\n+ *     <li>Iceberg: {@link Types.FloatType}, Arrow: {@link MinorType#FLOAT4}</li>\n+ *     <li>Iceberg: {@link Types.DoubleType}, Arrow: {@link MinorType#FLOAT8}</li>\n+ *     <li>Iceberg: {@link Types.StringType}, Arrow: {@link MinorType#VARCHAR}</li>\n+ *     <li>Iceberg: {@link Types.TimestampType} (both with and without timezone),\n+ *         Arrow: {@link MinorType#TIMEMICRO}</li>\n+ *     <li>Iceberg: {@link Types.BinaryType}, Arrow: {@link MinorType#VARBINARY}</li>\n+ *     <li>Iceberg: {@link Types.DateType}, Arrow: {@link MinorType#DATEDAY}</li>\n+ * </ul>\n+ *\n+ * <p>Features that don't work in this implementation:\n+ * <ul>\n+ *     <li>Type promotion: In case of type promotion, the Arrow vector corresponding to\n+ *     the data type in the parquet file is returned instead of the data type in the latest schema.</li>\n+ *     <li>Columns with constant values are physically encoded as a dictionary. The Arrow vector\n+ *     type is int32 instead of the type as per the schema.</li>\n+ *     <li>Data types: {@link Types.TimeType}, {@link Types.ListType}, {@link Types.MapType},\n+ *     {@link Types.StructType}</li>\n+ * </ul>\n+ *\n+ * <p>Data types not tested: {@link Types.UUIDType}, {@link Types.FixedType}, {@link Types.DecimalType}.\n+ */\n+public class ArrowReader extends CloseableGroup {\n+\n+  private final Schema schema;\n+  private final FileIO io;\n+  private final EncryptionManager encryption;\n+  private final int batchSize;\n+  private final boolean reuseContainers;\n+\n+  /**\n+   * Create a new instance of the reader.\n+   *\n+   * @param scan the table scan object.\n+   * @param batchSize the maximum number of rows per Arrow batch.\n+   * @param reuseContainers whether to reuse Arrow vectors when iterating through the data.\n+   *                        If set to {@code false}, every {@link Iterator#next()} call creates\n+   *                        new instances of Arrow vectors.\n+   *                        If set to {@code true}, the Arrow vectors in the previous\n+   *                        {@link Iterator#next()} may be reused for the data returned\n+   *                        in the current {@link Iterator#next()}.\n+   *                        This option avoids allocating memory again and again.\n+   *                        Irrespective of the value of {@code reuseContainers}, the Arrow vectors\n+   *                        in the previous {@link Iterator#next()} call are closed before creating\n+   *                        new instances if the current {@link Iterator#next()}.\n+   */\n+  public ArrowReader(TableScan scan, int batchSize, boolean reuseContainers) {\n+    this.schema = scan.schema();\n+    this.io = scan.table().io();\n+    this.encryption = scan.table().encryption();\n+    this.batchSize = batchSize;\n+    // start planning tasks in the background\n+    this.reuseContainers = reuseContainers;\n+  }\n+\n+  /**\n+   * Returns a new iterator of {@link ArrowBatch} objects.\n+   * <p>\n+   * Note that the reader owns the {@link ArrowBatch} objects and takes care of closing them.\n+   * The caller should not hold onto a {@link ArrowBatch} or try to close them.\n+   *\n+   * <p>If {@code reuseContainers} is {@code false}, the Arrow vectors in the\n+   * previous {@link ArrowBatch} are closed before returning the next {@link ArrowBatch} object.\n+   * This implies that the caller should either use the {@link ArrowBatch} or transfer the ownership of\n+   * {@link ArrowBatch} before getting the next {@link ArrowBatch}.\n+   *\n+   * <p>If {@code reuseContainers} is {@code true}, the Arrow vectors in the\n+   * previous {@link ArrowBatch} may be reused for the next {@link ArrowBatch}.\n+   * This implies that the caller should either use the {@link ArrowBatch} or deep copy the\n+   * {@link ArrowBatch} before getting the next {@link ArrowBatch}.\n+   */\n+  public CloseableIterator<ArrowBatch> open(CloseableIterable<CombinedScanTask> tasks) {\n+    CloseableIterator<ArrowBatch> itr = new ConcatIterator(tasks);\n+    addCloseable(itr);\n+    return itr;\n+  }\n+\n+  private CloseableIterator<ArrowBatch> open(CombinedScanTask task) {\n+    return new VectorizedCombinedScanIterator(\n+        task,\n+        schema,\n+        null,\n+        io,\n+        encryption,\n+        true,\n+        batchSize,\n+        reuseContainers\n+    );\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    super.close(); // close data files\n+  }\n+\n+  private final class ConcatIterator implements CloseableIterator<ArrowBatch> {\n+    private CloseableIterator<ArrowBatch> currentIterator;\n+    private final CloseableIterator<CombinedScanTask> tasksIterator;\n+    private ArrowBatch current;\n+\n+    private ConcatIterator(CloseableIterable<CombinedScanTask> tasks) {\n+      this.currentIterator = CloseableIterator.empty();\n+      this.tasksIterator = tasks.iterator();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      this.currentIterator.close();\n+    }\n+\n+    @Override\n+    public boolean hasNext() {\n+      try {\n+        while (true) {\n+          if (currentIterator.hasNext()) {\n+            this.current = currentIterator.next();\n+            return true;\n+          } else if (tasksIterator.hasNext()) {\n+            this.currentIterator.close();\n+            this.currentIterator = open(tasksIterator.next());\n+          } else {\n+            this.currentIterator.close();\n+            return false;\n+          }\n+        }\n+      } catch (IOException e) {\n+        throw new RuntimeException(e);\n+      }\n+    }\n+\n+    @Override\n+    public ArrowBatch next() {\n+      return current;\n+    }\n+  }\n+}"
  },
  {
    "sha": "87247496d3838a1017fce1e6e2ab73afe1e92237",
    "filename": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/ArrowVector.java",
    "status": "added",
    "additions": 513,
    "deletions": 0,
    "changes": 513,
    "blob_url": "https://github.com/apache/iceberg/blob/da10fe2cab636f24dff800c31c586b846ca10a41/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/ArrowVector.java",
    "raw_url": "https://github.com/apache/iceberg/raw/da10fe2cab636f24dff800c31c586b846ca10a41/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/ArrowVector.java",
    "contents_url": "https://api.github.com/repos/apache/iceberg/contents/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/ArrowVector.java?ref=da10fe2cab636f24dff800c31c586b846ca10a41",
    "patch": "@@ -0,0 +1,513 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.stream.IntStream;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TimeStampMicroVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.PrimitiveType;\n+\n+/**\n+ * This class is inspired by Spark's {@code ColumnVector}.\n+ * This class represents the column data for an Iceberg table query.\n+ * It wraps an arrow {@link FieldVector} and provides simple\n+ * accessors for the row values. Advanced users can access\n+ * the {@link FieldVector}.\n+ * <p>\n+ *   Supported Iceberg data types:\n+ *   <ul>\n+ *     <li>{@link Types.BooleanType}</li>\n+ *     <li>{@link Types.IntegerType}</li>\n+ *     <li>{@link Types.LongType}</li>\n+ *     <li>{@link Types.FloatType}</li>\n+ *     <li>{@link Types.DoubleType}</li>\n+ *     <li>{@link Types.StringType}</li>\n+ *     <li>{@link Types.BinaryType}</li>\n+ *     <li>{@link Types.TimestampType} (with and without timezone)</li>\n+ *     <li>{@link Types.DateType}</li>\n+ *   </ul>\n+ */\n+public class ArrowVector implements AutoCloseable {\n+  private final VectorHolder vectorHolder;\n+  private final ArrowVectorAccessor accessor;\n+\n+  ArrowVector(VectorHolder vectorHolder) {\n+    this.vectorHolder = vectorHolder;\n+    this.accessor = getVectorAccessor(vectorHolder);\n+  }\n+\n+  public FieldVector getFieldVector() {\n+    // TODO Convert dictionary encoded vectors to correctly typed arrow vector.\n+    //   e.g. convert long dictionary encoded vector to a BigIntVector.\n+    return vectorHolder.vector();\n+  }\n+\n+  public boolean hasNull() {\n+    return accessor.getNullCount() > 0;\n+  }\n+\n+  public int numNulls() {\n+    return accessor.getNullCount();\n+  }\n+\n+  @Override\n+  public void close() {\n+    accessor.close();\n+  }\n+\n+  public boolean isNullAt(int rowId) {\n+    return accessor.isNullAt(rowId);\n+  }\n+\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  public String getString(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getString(rowId);\n+  }\n+\n+  public byte[] getBinary(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getBinary(rowId);\n+  }\n+\n+  private static ArrowVectorAccessor getVectorAccessor(VectorHolder holder) {\n+    Dictionary dictionary = holder.dictionary();\n+    boolean isVectorDictEncoded = holder.isDictionaryEncoded();\n+    FieldVector vector = holder.vector();\n+    if (isVectorDictEncoded) {\n+      ColumnDescriptor desc = holder.descriptor();\n+      PrimitiveType primitive = desc.getPrimitiveType();\n+      return getDictionaryVectorAccessor(dictionary, desc, vector, primitive);\n+    } else {\n+      return getPlainVectorAccessor(vector);\n+    }\n+  }\n+\n+  private static ArrowVectorAccessor getDictionaryVectorAccessor(\n+      Dictionary dictionary,\n+      ColumnDescriptor desc,\n+      FieldVector vector,\n+      PrimitiveType primitive) {\n+    Preconditions.checkState(vector instanceof IntVector, \"Dictionary ids should be stored in IntVectors only\");\n+    if (primitive.getOriginalType() != null) {\n+      switch (desc.getPrimitiveType().getOriginalType()) {\n+        case ENUM:\n+        case JSON:\n+        case UTF8:\n+        case BSON:\n+          return new DictionaryStringAccessor((IntVector) vector, dictionary);\n+        case INT_64:\n+        case TIMESTAMP_MILLIS:\n+        case TIMESTAMP_MICROS:\n+          return new DictionaryLongAccessor((IntVector) vector, dictionary);\n+        default:\n+          throw new UnsupportedOperationException(\n+              \"Unsupported logical type: \" + primitive.getOriginalType());\n+      }\n+    } else {\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new DictionaryBinaryAccessor((IntVector) vector, dictionary);\n+        case FLOAT:\n+          return new DictionaryFloatAccessor((IntVector) vector, dictionary);\n+        case INT64:\n+          return new DictionaryLongAccessor((IntVector) vector, dictionary);\n+        case DOUBLE:\n+          return new DictionaryDoubleAccessor((IntVector) vector, dictionary);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static ArrowVectorAccessor getPlainVectorAccessor(FieldVector vector) {\n+    if (vector instanceof BitVector) {\n+      return new BooleanAccessor((BitVector) vector);\n+    } else if (vector instanceof IntVector) {\n+      return new IntAccessor((IntVector) vector);\n+    } else if (vector instanceof BigIntVector) {\n+      return new LongAccessor((BigIntVector) vector);\n+    } else if (vector instanceof Float4Vector) {\n+      return new FloatAccessor((Float4Vector) vector);\n+    } else if (vector instanceof Float8Vector) {\n+      return new DoubleAccessor((Float8Vector) vector);\n+    } else if (vector instanceof VarCharVector) {\n+      return new StringAccessor((VarCharVector) vector);\n+    } else if (vector instanceof VarBinaryVector) {\n+      return new BinaryAccessor((VarBinaryVector) vector);\n+    } else if (vector instanceof DateDayVector) {\n+      return new DateAccessor((DateDayVector) vector);\n+    } else if (vector instanceof TimeStampMicroTZVector) {\n+      return new TimestampMicroTzAccessor((TimeStampMicroTZVector) vector);\n+    } else if (vector instanceof TimeStampMicroVector) {\n+      return new TimestampMicroAccessor((TimeStampMicroVector) vector);\n+    }\n+    throw new UnsupportedOperationException(\"Unsupported vector: \" + vector.getClass());\n+  }\n+\n+  private abstract static class ArrowVectorAccessor {\n+\n+    private final ValueVector vector;\n+\n+    ArrowVectorAccessor(ValueVector vector) {\n+      this.vector = vector;\n+    }\n+\n+    boolean isNullAt(int rowId) {\n+      return vector.isNull(rowId);\n+    }\n+\n+    final int getNullCount() {\n+      return vector.getNullCount();\n+    }\n+\n+    final void close() {\n+      vector.close();\n+    }\n+\n+    boolean getBoolean(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    int getInt(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    long getLong(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    float getFloat(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    double getDouble(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    String getString(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte[] getBinary(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+  }\n+\n+  private static class BooleanAccessor extends ArrowVectorAccessor {\n+\n+    private final BitVector accessor;\n+\n+    BooleanAccessor(BitVector vector) {\n+      super(vector);\n+      this.accessor = vector;\n+    }\n+\n+    @Override\n+    final boolean getBoolean(int rowId) {\n+      return accessor.get(rowId) == 1;\n+    }\n+  }\n+\n+  private static class IntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector accessor;\n+\n+    IntAccessor(IntVector vector) {\n+      super(vector);\n+      this.accessor = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return accessor.get(rowId);\n+    }\n+  }\n+\n+  private static class LongAccessor extends ArrowVectorAccessor {\n+\n+    private final BigIntVector accessor;\n+\n+    LongAccessor(BigIntVector vector) {\n+      super(vector);\n+      this.accessor = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return accessor.get(rowId);\n+    }\n+  }\n+\n+  private static class FloatAccessor extends ArrowVectorAccessor {\n+\n+    private final Float4Vector accessor;\n+\n+    FloatAccessor(Float4Vector vector) {\n+      super(vector);\n+      this.accessor = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return accessor.get(rowId);\n+    }\n+  }\n+\n+  private static class DoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final Float8Vector accessor;\n+\n+    DoubleAccessor(Float8Vector vector) {\n+      super(vector);\n+      this.accessor = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return accessor.get(rowId);\n+    }\n+  }\n+\n+  private static class StringAccessor extends ArrowVectorAccessor {\n+\n+    private final VarCharVector accessor;\n+    private final NullableVarCharHolder stringResult = new NullableVarCharHolder();\n+\n+    StringAccessor(VarCharVector vector) {\n+      super(vector);\n+      this.accessor = vector;\n+    }\n+\n+    @Override\n+    final String getString(int rowId) {\n+      accessor.get(rowId, stringResult);\n+      if (stringResult.isSet == 0) {\n+        return null;\n+      } else {\n+        int strlen = stringResult.end - stringResult.start + 1;\n+        byte[] dst = new byte[strlen];\n+        stringResult.buffer.getBytes(stringResult.start, dst, 0, strlen);\n+        return new String(dst, StandardCharsets.UTF_8);\n+      }\n+    }\n+  }\n+\n+  private static class BinaryAccessor extends ArrowVectorAccessor {\n+\n+    private final VarBinaryVector accessor;\n+\n+    BinaryAccessor(VarBinaryVector vector) {\n+      super(vector);\n+      this.accessor = vector;\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      return accessor.getObject(rowId);\n+    }\n+  }\n+\n+  private static class DateAccessor extends ArrowVectorAccessor {\n+\n+    private final DateDayVector accessor;\n+\n+    DateAccessor(DateDayVector vector) {\n+      super(vector);\n+      this.accessor = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return accessor.get(rowId);\n+    }\n+  }\n+\n+  private static class TimestampMicroTzAccessor extends ArrowVectorAccessor {\n+\n+    private final TimeStampMicroTZVector accessor;\n+\n+    TimestampMicroTzAccessor(TimeStampMicroTZVector vector) {\n+      super(vector);\n+      this.accessor = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return accessor.get(rowId);\n+    }\n+  }\n+\n+  private static class TimestampMicroAccessor extends ArrowVectorAccessor {\n+\n+    private final TimeStampMicroVector accessor;\n+\n+    TimestampMicroAccessor(TimeStampMicroVector vector) {\n+      super(vector);\n+      this.accessor = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return accessor.get(rowId);\n+    }\n+  }\n+\n+  private static class DictionaryLongAccessor extends ArrowVectorAccessor {\n+    private final IntVector offsetVector;\n+    private final long[] decodedDictionary;\n+\n+    DictionaryLongAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector);\n+      this.offsetVector = vector;\n+      this.decodedDictionary = IntStream.rangeClosed(0, dictionary.getMaxId())\n+          .mapToLong(dictionary::decodeToLong)\n+          .toArray();\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return decodedDictionary[offsetVector.get(rowId)];\n+    }\n+  }\n+\n+  private static class DictionaryFloatAccessor extends ArrowVectorAccessor {\n+    private final IntVector offsetVector;\n+    private final float[] decodedDictionary;\n+\n+    DictionaryFloatAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector);\n+      this.offsetVector = vector;\n+      this.decodedDictionary = new float[dictionary.getMaxId() + 1];\n+      for (int i = 0; i <= dictionary.getMaxId(); i++) {\n+        decodedDictionary[i] = dictionary.decodeToFloat(i);\n+      }\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return decodedDictionary[offsetVector.get(rowId)];\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return getFloat(rowId);\n+    }\n+  }\n+\n+  private static class DictionaryDoubleAccessor extends ArrowVectorAccessor {\n+    private final IntVector offsetVector;\n+    private final double[] decodedDictionary;\n+\n+    DictionaryDoubleAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector);\n+      this.offsetVector = vector;\n+      this.decodedDictionary = IntStream.rangeClosed(0, dictionary.getMaxId())\n+          .mapToDouble(dictionary::decodeToDouble)\n+          .toArray();\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return decodedDictionary[offsetVector.get(rowId)];\n+    }\n+  }\n+\n+  private static class DictionaryStringAccessor extends ArrowVectorAccessor {\n+    private final String[] decodedDictionary;\n+    private final IntVector offsetVector;\n+\n+    DictionaryStringAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector);\n+      this.offsetVector = vector;\n+      this.decodedDictionary = IntStream.rangeClosed(0, dictionary.getMaxId())\n+          .mapToObj(dictionary::decodeToBinary)\n+          .map(binary -> new String(binary.getBytes(), StandardCharsets.UTF_8))\n+          .toArray(String[]::new);\n+    }\n+\n+    @Override\n+    final String getString(int rowId) {\n+      int offset = offsetVector.get(rowId);\n+      return decodedDictionary[offset];\n+    }\n+  }\n+\n+  private static class DictionaryBinaryAccessor extends ArrowVectorAccessor {\n+    private final IntVector offsetVector;\n+    private final byte[][] decodedDictionary;\n+\n+    DictionaryBinaryAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector);\n+      this.offsetVector = vector;\n+      this.decodedDictionary = IntStream.rangeClosed(0, dictionary.getMaxId())\n+          .mapToObj(dictionary::decodeToBinary)\n+          .map(Binary::getBytes)\n+          .toArray(byte[][]::new);\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      int offset = offsetVector.get(rowId);\n+      return decodedDictionary[offset];\n+    }\n+  }\n+}"
  },
  {
    "sha": "5ed323f765660ea34e0b92737744d0d86f0e0b30",
    "filename": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java",
    "status": "modified",
    "additions": 6,
    "deletions": 1,
    "changes": 7,
    "blob_url": "https://github.com/apache/iceberg/blob/da10fe2cab636f24dff800c31c586b846ca10a41/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java",
    "raw_url": "https://github.com/apache/iceberg/raw/da10fe2cab636f24dff800c31c586b846ca10a41/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java",
    "contents_url": "https://api.github.com/repos/apache/iceberg/contents/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java?ref=da10fe2cab636f24dff800c31c586b846ca10a41",
    "patch": "@@ -31,6 +31,7 @@\n import org.apache.arrow.vector.Float8Vector;\n import org.apache.arrow.vector.IntVector;\n import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TimeStampMicroVector;\n import org.apache.arrow.vector.types.FloatingPointPrecision;\n import org.apache.arrow.vector.types.pojo.ArrowType;\n import org.apache.arrow.vector.types.pojo.Field;\n@@ -231,7 +232,11 @@ private void allocateFieldVector(boolean dictionaryEncodedVector) {\n             break;\n           case TIMESTAMP_MICROS:\n             this.vec = arrowField.createVector(rootAlloc);\n-            ((TimeStampMicroTZVector) vec).allocateNew(batchSize);\n+            if (((Types.TimestampType) icebergField.type()).shouldAdjustToUTC()) {\n+              ((TimeStampMicroTZVector) vec).allocateNew(batchSize);\n+            } else {\n+              ((TimeStampMicroVector) vec).allocateNew(batchSize);\n+            }\n             this.readType = ReadType.LONG;\n             this.typeWidth = (int) BigIntVector.TYPE_WIDTH;\n             break;"
  },
  {
    "sha": "55aaf420ca10f478d56a611d2d0ac1c8e47064b5",
    "filename": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedCombinedScanIterator.java",
    "status": "added",
    "additions": 194,
    "deletions": 0,
    "changes": 194,
    "blob_url": "https://github.com/apache/iceberg/blob/da10fe2cab636f24dff800c31c586b846ca10a41/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedCombinedScanIterator.java",
    "raw_url": "https://github.com/apache/iceberg/raw/da10fe2cab636f24dff800c31c586b846ca10a41/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedCombinedScanIterator.java",
    "contents_url": "https://api.github.com/repos/apache/iceberg/contents/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedCombinedScanIterator.java?ref=da10fe2cab636f24dff800c31c586b846ca10a41",
    "patch": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.stream.Stream;\n+import org.apache.arrow.vector.NullCheckingForGet;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptedInputFile;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Reads the data file and returns an iterator of {@link VectorSchemaRoot}.\n+ * Only Parquet data file format is supported.\n+ */\n+class VectorizedCombinedScanIterator implements CloseableIterator<ArrowBatch> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(VectorizedCombinedScanIterator.class);\n+\n+  private final Iterator<FileScanTask> tasks;\n+  private final Map<String, InputFile> inputFiles;\n+  private final Schema expectedSchema;\n+  private final String nameMapping;\n+  private final boolean caseSensitive;\n+  private final int batchSize;\n+  private final boolean reuseContainers;\n+  private CloseableIterator<ArrowBatch> currentIterator;\n+  private ArrowBatch current;\n+  private FileScanTask currentTask;\n+\n+  /**\n+   * Create a new instance.\n+   *\n+   * @param task              Combined file scan task.\n+   * @param expectedSchema    Read schema. The returned data will have this schema.\n+   * @param nameMapping       Mapping from external schema names to Iceberg type IDs.\n+   * @param io                File I/O.\n+   * @param encryptionManager Encryption manager.\n+   * @param caseSensitive     If {@code true}, column names are case sensitive.\n+   *                          If {@code false}, column names are not case sensitive.\n+   * @param batchSize         Batch size in number of rows. Each Arrow batch contains\n+   *                          a maximum of {@code batchSize} rows.\n+   * @param reuseContainers   If set to {@code false}, every {@link Iterator#next()} call creates\n+   *                          new instances of Arrow vectors.\n+   *                          If set to {@code true}, the Arrow vectors in the previous\n+   *                          {@link Iterator#next()} may be reused for the data returned\n+   *                          in the current {@link Iterator#next()}.\n+   *                          This option avoids allocating memory again and again.\n+   *                          Irrespective of the value of {@code reuseContainers}, the Arrow vectors\n+   *                          in the previous {@link Iterator#next()} call are closed before creating\n+   *                          new instances if the current {@link Iterator#next()}.\n+   */\n+  VectorizedCombinedScanIterator(\n+          CombinedScanTask task,\n+          Schema expectedSchema,\n+          String nameMapping,\n+          FileIO io,\n+          EncryptionManager encryptionManager,\n+          boolean caseSensitive,\n+          int batchSize,\n+          boolean reuseContainers) {\n+    this.tasks = task.files().iterator();\n+    Map<String, ByteBuffer> keyMetadata = Maps.newHashMap();\n+    task.files().stream()\n+        .flatMap(fileScanTask -> Stream.concat(Stream.of(fileScanTask.file()), fileScanTask.deletes().stream()))\n+        .forEach(file -> keyMetadata.put(file.path().toString(), file.keyMetadata()));\n+    Stream<EncryptedInputFile> encrypted = keyMetadata.entrySet().stream()\n+        .map(entry -> EncryptedFiles.encryptedInput(io.newInputFile(entry.getKey()), entry.getValue()));\n+\n+    // decrypt with the batch call to avoid multiple RPCs to a key server, if possible\n+    Iterable<InputFile> decryptedFiles = encryptionManager.decrypt(encrypted::iterator);\n+\n+    Map<String, InputFile> files = Maps.newHashMapWithExpectedSize(task.files().size());\n+    decryptedFiles.forEach(decrypted -> files.putIfAbsent(decrypted.location(), decrypted));\n+    this.inputFiles = Collections.unmodifiableMap(files);\n+\n+    this.currentIterator = CloseableIterator.empty();\n+    this.expectedSchema = expectedSchema;\n+    this.nameMapping = nameMapping;\n+    this.caseSensitive = caseSensitive;\n+    this.batchSize = batchSize;\n+    this.reuseContainers = reuseContainers;\n+  }\n+\n+  @Override\n+  public boolean hasNext() {\n+    try {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          this.current = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          this.currentIterator.close();\n+          this.currentTask = tasks.next();\n+          this.currentIterator = open(currentTask);\n+        } else {\n+          this.currentIterator.close();\n+          return false;\n+        }\n+      }\n+    } catch (IOException | RuntimeException e) {\n+      if (currentTask != null && !currentTask.isDataTask()) {\n+        LOG.error(\"Error reading file: {}\", getInputFile(currentTask).location(), e);\n+      }\n+      throw new RuntimeException(e);\n+    }\n+  }\n+\n+  @Override\n+  public ArrowBatch next() {\n+    return current;\n+  }\n+\n+  CloseableIterator<ArrowBatch> open(FileScanTask task) {\n+    CloseableIterable<ArrowBatch> iter;\n+    InputFile location = getInputFile(task);\n+    Preconditions.checkNotNull(location, \"Could not find InputFile associated with FileScanTask\");\n+    if (task.file().format() == FileFormat.PARQUET) {\n+      Parquet.ReadBuilder builder = Parquet.read(location)\n+              .project(expectedSchema)\n+              .split(task.start(), task.length())\n+              .createBatchedReaderFunc(fileSchema -> VectorizedParquetReaders.buildReader(expectedSchema,\n+                      fileSchema, /* setArrowValidityVector */ NullCheckingForGet.NULL_CHECKING_ENABLED))\n+              .recordsPerBatch(batchSize)\n+              .filter(task.residual())\n+              .caseSensitive(caseSensitive);\n+\n+      if (reuseContainers) {\n+        builder.reuseContainers();\n+      }\n+      if (nameMapping != null) {\n+        builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n+      }\n+\n+      iter = builder.build();\n+    } else {\n+      throw new UnsupportedOperationException(\n+              \"Format: \" + task.file().format() + \" not supported for batched reads\");\n+    }\n+    return iter.iterator();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    // close the current iterator\n+    this.currentIterator.close();\n+\n+    // exhaust the task iterator\n+    while (tasks.hasNext()) {\n+      tasks.next();\n+    }\n+  }\n+\n+  private InputFile getInputFile(FileScanTask task) {\n+    Preconditions.checkArgument(!task.isDataTask(), \"Invalid task type\");\n+    return inputFiles.get(task.file().path().toString());\n+  }\n+}"
  },
  {
    "sha": "8677611d5cb5805676a17d510b6bb4fb4c13af63",
    "filename": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedParquetReaders.java",
    "status": "added",
    "additions": 51,
    "deletions": 0,
    "changes": 51,
    "blob_url": "https://github.com/apache/iceberg/blob/da10fe2cab636f24dff800c31c586b846ca10a41/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedParquetReaders.java",
    "raw_url": "https://github.com/apache/iceberg/raw/da10fe2cab636f24dff800c31c586b846ca10a41/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedParquetReaders.java",
    "contents_url": "https://api.github.com/repos/apache/iceberg/contents/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedParquetReaders.java?ref=da10fe2cab636f24dff800c31c586b846ca10a41",
    "patch": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.parquet.schema.MessageType;\n+\n+/**\n+ * Builds an {@link ArrowBatchReader}.\n+ */\n+class VectorizedParquetReaders {\n+\n+  private VectorizedParquetReaders() {\n+  }\n+\n+  /**\n+   * Build the {@link ArrowBatchReader} for the expected schema and file schema.\n+   *\n+   * @param expectedSchema         Expected schema of the data returned.\n+   * @param fileSchema             Schema of the data file.\n+   * @param setArrowValidityVector Indicates whether to set the validity vector in Arrow vectors.\n+   */\n+  public static ArrowBatchReader buildReader(\n+      Schema expectedSchema,\n+      MessageType fileSchema,\n+      boolean setArrowValidityVector) {\n+    return (ArrowBatchReader)\n+        TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+            new VectorizedReaderBuilder(\n+                expectedSchema, fileSchema, setArrowValidityVector, ImmutableMap.of(), ArrowBatchReader::new));\n+  }\n+}"
  },
  {
    "sha": "77bd08952ea9890602c0f5ef5eae8c86b668d633",
    "filename": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedReaderBuilder.java",
    "status": "added",
    "additions": 130,
    "deletions": 0,
    "changes": 130,
    "blob_url": "https://github.com/apache/iceberg/blob/da10fe2cab636f24dff800c31c586b846ca10a41/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedReaderBuilder.java",
    "raw_url": "https://github.com/apache/iceberg/raw/da10fe2cab636f24dff800c31c586b846ca10a41/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedReaderBuilder.java",
    "contents_url": "https://api.github.com/repos/apache/iceberg/contents/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedReaderBuilder.java?ref=da10fe2cab636f24dff800c31c586b846ca10a41",
    "patch": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import java.util.stream.IntStream;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.arrow.ArrowAllocation;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+\n+public class VectorizedReaderBuilder extends TypeWithSchemaVisitor<VectorizedReader<?>> {\n+  private final MessageType parquetSchema;\n+  private final Schema icebergSchema;\n+  private final BufferAllocator rootAllocator;\n+  private final Map<Integer, ?> idToConstant;\n+  private final boolean setArrowValidityVector;\n+  private final Function<List<VectorizedReader<?>>, VectorizedReader<?>> readerFactory;\n+\n+  public VectorizedReaderBuilder(\n+      Schema expectedSchema,\n+      MessageType parquetSchema,\n+      boolean setArrowValidityVector, Map<Integer, ?> idToConstant,\n+      Function<List<VectorizedReader<?>>, VectorizedReader<?>> readerFactory) {\n+    this.parquetSchema = parquetSchema;\n+    this.icebergSchema = expectedSchema;\n+    this.rootAllocator = ArrowAllocation.rootAllocator()\n+        .newChildAllocator(\"VectorizedReadBuilder\", 0, Long.MAX_VALUE);\n+    this.setArrowValidityVector = setArrowValidityVector;\n+    this.idToConstant = idToConstant;\n+    this.readerFactory = readerFactory;\n+  }\n+\n+  @Override\n+  public VectorizedReader<?> message(\n+      Types.StructType expected, MessageType message,\n+      List<VectorizedReader<?>> fieldReaders) {\n+    GroupType groupType = message.asGroupType();\n+    Map<Integer, VectorizedReader<?>> readersById = Maps.newHashMap();\n+    List<Type> fields = groupType.getFields();\n+\n+    IntStream.range(0, fields.size())\n+        .filter(pos -> fields.get(pos).getId() != null)\n+        .forEach(pos -> readersById.put(fields.get(pos).getId().intValue(), fieldReaders.get(pos)));\n+\n+    List<Types.NestedField> icebergFields = expected != null ?\n+        expected.fields() : ImmutableList.of();\n+\n+    List<VectorizedReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+        icebergFields.size());\n+\n+    for (Types.NestedField field : icebergFields) {\n+      int id = field.fieldId();\n+      VectorizedReader<?> reader = readersById.get(id);\n+      if (idToConstant.containsKey(id)) {\n+        reorderedFields.add(new VectorizedArrowReader.ConstantVectorReader<>(idToConstant.get(id)));\n+      } else if (id == MetadataColumns.ROW_POSITION.fieldId()) {\n+        reorderedFields.add(VectorizedArrowReader.positions());\n+      } else if (reader != null) {\n+        reorderedFields.add(reader);\n+      } else {\n+        reorderedFields.add(VectorizedArrowReader.nulls());\n+      }\n+    }\n+    return readerFactory.apply(reorderedFields);\n+  }\n+\n+  @Override\n+  public VectorizedReader<?> struct(\n+      Types.StructType expected, GroupType groupType,\n+      List<VectorizedReader<?>> fieldReaders) {\n+    if (expected != null) {\n+      throw new UnsupportedOperationException(\"Vectorized reads are not supported yet for struct fields\");\n+    }\n+    return null;\n+  }\n+\n+  @Override\n+  public VectorizedReader<?> primitive(\n+      org.apache.iceberg.types.Type.PrimitiveType expected,\n+      PrimitiveType primitive) {\n+\n+    // Create arrow vector for this field\n+    if (primitive.getId() == null) {\n+      return null;\n+    }\n+    int parquetFieldId = primitive.getId().intValue();\n+    ColumnDescriptor desc = parquetSchema.getColumnDescription(currentPath());\n+    // Nested types not yet supported for vectorized reads\n+    if (desc.getMaxRepetitionLevel() > 0) {\n+      return null;\n+    }\n+    Types.NestedField icebergField = icebergSchema.findField(parquetFieldId);\n+    if (icebergField == null) {\n+      return null;\n+    }\n+    // Set the validity buffer if null checking is enabled in arrow\n+    return new VectorizedArrowReader(desc, icebergField, rootAllocator, setArrowValidityVector);\n+  }\n+}"
  },
  {
    "sha": "5968233b30374b9d1ffb89ea439bb4e24fc0eef9",
    "filename": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedTableScanIterable.java",
    "status": "added",
    "additions": 73,
    "deletions": 0,
    "changes": 73,
    "blob_url": "https://github.com/apache/iceberg/blob/da10fe2cab636f24dff800c31c586b846ca10a41/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedTableScanIterable.java",
    "raw_url": "https://github.com/apache/iceberg/raw/da10fe2cab636f24dff800c31c586b846ca10a41/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedTableScanIterable.java",
    "contents_url": "https://api.github.com/repos/apache/iceberg/contents/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedTableScanIterable.java?ref=da10fe2cab636f24dff800c31c586b846ca10a41",
    "patch": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import java.io.IOException;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.io.CloseableGroup;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+\n+/**\n+ * A vectorized implementation of the Iceberg reader that iterates over the table scan.\n+ * See {@link ArrowReader} for details.\n+ */\n+public class VectorizedTableScanIterable extends CloseableGroup implements CloseableIterable<ArrowBatch> {\n+\n+  private static final int BATCH_SIZE_IN_NUM_ROWS = 1 << 16;\n+\n+  private final ArrowReader reader;\n+  private final CloseableIterable<CombinedScanTask> tasks;\n+\n+  /**\n+   * Create a new instance using default values for {@code batchSize} and {@code reuseContainers}.\n+   * The {@code batchSize} is set to {@link #BATCH_SIZE_IN_NUM_ROWS} and {@code reuseContainers}\n+   * is set to {@code false}.\n+   *\n+   */\n+  public VectorizedTableScanIterable(TableScan scan) {\n+    this(scan, BATCH_SIZE_IN_NUM_ROWS, false);\n+  }\n+\n+  /**\n+   * Create a new instance.\n+   *\n+   * See {@link ArrowReader#ArrowReader(TableScan, int, boolean)} for details.\n+   */\n+  public VectorizedTableScanIterable(TableScan scan, int batchSize, boolean reuseContainers) {\n+    this.reader = new ArrowReader(scan, batchSize, reuseContainers);\n+    // start planning tasks in the background\n+    this.tasks = scan.planTasks();\n+  }\n+\n+  @Override\n+  public CloseableIterator<ArrowBatch> iterator() {\n+    CloseableIterator<ArrowBatch> iter = reader.open(tasks);\n+    addCloseable(iter);\n+    return iter;\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    tasks.close(); // close manifests from scan planning\n+    super.close(); // close data files\n+  }\n+}"
  },
  {
    "sha": "9753f7aba04be09e4016a23f2918c2285a5e5aa9",
    "filename": "arrow/src/test/java/org/apache/iceberg/arrow/vectorized/ArrowReaderTest.java",
    "status": "added",
    "additions": 734,
    "deletions": 0,
    "changes": 734,
    "blob_url": "https://github.com/apache/iceberg/blob/da10fe2cab636f24dff800c31c586b846ca10a41/arrow/src/test/java/org/apache/iceberg/arrow/vectorized/ArrowReaderTest.java",
    "raw_url": "https://github.com/apache/iceberg/raw/da10fe2cab636f24dff800c31c586b846ca10a41/arrow/src/test/java/org/apache/iceberg/arrow/vectorized/ArrowReaderTest.java",
    "contents_url": "https://api.github.com/repos/apache/iceberg/contents/arrow/src/test/java/org/apache/iceberg/arrow/vectorized/ArrowReaderTest.java?ref=da10fe2cab636f24dff800c31c586b846ca10a41",
    "patch": "@@ -0,0 +1,734 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.BiFunction;\n+import java.util.stream.Collectors;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TimeStampMicroVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.VarCharVector;\n+import org.apache.arrow.vector.VectorSchemaRoot;\n+import org.apache.arrow.vector.types.Types.MinorType;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.OverwriteFiles;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.parquet.GenericParquetWriter;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Ignore;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.Files.localInput;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Test cases for {@link ArrowReader}.\n+ * <p>All tests create a table with monthly partitions and write 1 year of data to the table.\n+ */\n+public class ArrowReaderTest {\n+\n+  private static final int NUM_ROWS_PER_MONTH = 20;\n+  private static final ImmutableList<String> ALL_COLUMNS =\n+      ImmutableList.of(\n+          \"timestamp\",\n+          \"timestamp_nullable\",\n+          \"boolean\",\n+          \"boolean_nullable\",\n+          \"int\",\n+          \"int_nullable\",\n+          \"long\",\n+          \"long_nullable\",\n+          \"float\",\n+          \"float_nullable\",\n+          \"double\",\n+          \"double_nullable\",\n+          \"timestamp_tz\",\n+          \"timestamp_tz_nullable\",\n+          \"string\",\n+          \"string_nullable\",\n+          \"bytes\",\n+          \"bytes_nullable\",\n+          \"date\",\n+          \"date_nullable\",\n+          \"int_promotion\"\n+      );\n+\n+  @Rule\n+  public final TemporaryFolder temp = new TemporaryFolder();\n+\n+  private HadoopTables tables;\n+\n+  private String tableLocation;\n+  private List<GenericRecord> rowsWritten;\n+\n+  /**\n+   * Read all rows and columns from the table without any filter. The test asserts that the Arrow {@link\n+   * VectorSchemaRoot} contains the expected schema and expected vector types. Then the test asserts that the vectors\n+   * contains expected values. The test also asserts the total number of rows match the expected value.\n+   */\n+  @Test\n+  public void testReadAll() throws Exception {\n+    writeTableWithIncrementalRecords();\n+    Table table = tables.load(tableLocation);\n+    readAndCheckVectorSchemaRoots(table.newScan(), NUM_ROWS_PER_MONTH, 12 * NUM_ROWS_PER_MONTH, ALL_COLUMNS);\n+  }\n+\n+  /**\n+   * This test writes each partition with constant value rows. The Arrow vectors returned are mostly of type int32\n+   * which is unexpected. This is happening because of dictionary encoding at the storage level.\n+   * <p>\n+   * Following are the expected and actual Arrow schema:\n+   * <pre>\n+   * Expected Arrow Schema:\n+   * timestamp: Timestamp(MICROSECOND, null) not null,\n+   * timestamp_nullable: Timestamp(MICROSECOND, null),\n+   * boolean: Bool not null,\n+   * boolean_nullable: Bool,\n+   * int: Int(32, true) not null,\n+   * int_nullable: Int(32, true),\n+   * long: Int(64, true) not null,\n+   * long_nullable: Int(64, true),\n+   * float: FloatingPoint(SINGLE) not null,\n+   * float_nullable: FloatingPoint(SINGLE),\n+   * double: FloatingPoint(DOUBLE) not null,\n+   * double_nullable: FloatingPoint(DOUBLE),\n+   * timestamp_tz: Timestamp(MICROSECOND, UTC) not null,\n+   * timestamp_tz_nullable: Timestamp(MICROSECOND, UTC),\n+   * string: Utf8 not null,\n+   * string_nullable: Utf8,\n+   * bytes: Binary not null,\n+   * bytes_nullable: Binary,\n+   * date: Date(DAY) not null,\n+   * date_nullable: Date(DAY),\n+   * int_promotion: Int(32, true) not null\n+   *\n+   * Actual Arrow Schema:\n+   * timestamp: Int(32, true) not null,\n+   * timestamp_nullable: Int(32, true),\n+   * boolean: Bool not null,\n+   * boolean_nullable: Bool,\n+   * int: Int(32, true) not null,\n+   * int_nullable: Int(32, true),\n+   * long: Int(32, true) not null,\n+   * long_nullable: Int(32, true),\n+   * float: Int(32, true) not null,\n+   * float_nullable: Int(32, true),\n+   * double: Int(32, true) not null,\n+   * double_nullable: Int(32, true),\n+   * timestamp_tz: Int(32, true) not null,\n+   * timestamp_tz_nullable: Int(32, true),\n+   * string: Int(32, true) not null,\n+   * string_nullable: Int(32, true),\n+   * bytes: Int(32, true) not null,\n+   * bytes_nullable: Int(32, true),\n+   * date: Date(DAY) not null,\n+   * date_nullable: Date(DAY),\n+   * int_promotion: Int(32, true) not null\n+   * </pre>\n+   * <p>\n+   * TODO: fix the returned Arrow vectors to have vector types consistent with Iceberg types.\n+   * <p>\n+   * Read all rows and columns from the table without any filter. The test asserts that the Arrow {@link\n+   * VectorSchemaRoot} contains the expected schema and expected vector types. Then the test asserts that the vectors\n+   * contains expected values. The test also asserts the total number of rows match the expected value.\n+   */\n+  @Test\n+  @Ignore\n+  public void testReadAllWithConstantRecords() throws Exception {\n+    writeTableWithConstantRecords();\n+    Table table = tables.load(tableLocation);\n+    readAndCheckVectorSchemaRoots(table.newScan(), NUM_ROWS_PER_MONTH, 12 * NUM_ROWS_PER_MONTH, ALL_COLUMNS);\n+  }\n+\n+  /**\n+   * Read all rows and columns from the table without any filter. The test uses a batch size smaller than the number of\n+   * rows in a partition. The test asserts that the Arrow {@link VectorSchemaRoot} contains the expected schema and\n+   * expected vector types. Then the test asserts that the vectors contains expected values. The test also asserts the\n+   * total number of rows match the expected value.\n+   */\n+  @Test\n+  public void testReadAllWithSmallerBatchSize() throws Exception {\n+    writeTableWithIncrementalRecords();\n+    Table table = tables.load(tableLocation);\n+    TableScan scan = table.newScan();\n+    readAndCheckVectorSchemaRoots(scan, 10, 12 * NUM_ROWS_PER_MONTH, ALL_COLUMNS);\n+  }\n+\n+  /**\n+   * Read selected rows and all columns from the table using a time range row filter. The test asserts that the Arrow\n+   * {@link VectorSchemaRoot} contains the expected schema and expected vector types. Then the test asserts that the\n+   * vectors contains expected values. The test also asserts the total number of rows match the expected value.\n+   */\n+  @Test\n+  public void testReadRangeFilter() throws Exception {\n+    writeTableWithIncrementalRecords();\n+    Table table = tables.load(tableLocation);\n+    LocalDateTime beginTime = LocalDateTime.of(2020, 1, 1, 0, 0, 0);\n+    LocalDateTime endTime = LocalDateTime.of(2020, 2, 1, 0, 0, 0);\n+    TableScan scan = table.newScan()\n+        .filter(Expressions.and(\n+            Expressions.greaterThanOrEqual(\"timestamp\", timestampToMicros(beginTime)),\n+            Expressions.lessThan(\"timestamp\", timestampToMicros(endTime))));\n+    readAndCheckVectorSchemaRoots(scan, NUM_ROWS_PER_MONTH, NUM_ROWS_PER_MONTH, ALL_COLUMNS);\n+  }\n+\n+  /**\n+   * Read selected rows and all columns from the table using a time range row filter.\n+   * The test asserts that the result is empty.\n+   */\n+  @Test\n+  public void testReadRangeFilterEmptyResult() throws Exception {\n+    writeTableWithIncrementalRecords();\n+    Table table = tables.load(tableLocation);\n+    LocalDateTime beginTime = LocalDateTime.of(2021, 1, 1, 0, 0, 0);\n+    LocalDateTime endTime = LocalDateTime.of(2021, 2, 1, 0, 0, 0);\n+    TableScan scan = table.newScan()\n+            .filter(Expressions.and(\n+                    Expressions.greaterThanOrEqual(\"timestamp\", timestampToMicros(beginTime)),\n+                    Expressions.lessThan(\"timestamp\", timestampToMicros(endTime))));\n+    int numRoots = 0;\n+    for (ArrowBatch batch : new VectorizedTableScanIterable(scan, NUM_ROWS_PER_MONTH, false)) {\n+      numRoots++;\n+    }\n+    assertEquals(0, numRoots);\n+  }\n+\n+  /**\n+   * Read all rows and selected columns from the table with a column selection filter. The test asserts that the Arrow\n+   * {@link VectorSchemaRoot} contains the expected schema and expected vector types. Then the test asserts that the\n+   * vectors contains expected values. The test also asserts the total number of rows match the expected value.\n+   */\n+  @Test\n+  public void testReadColumnFilter1() throws Exception {\n+    writeTableWithIncrementalRecords();\n+    Table table = tables.load(tableLocation);\n+    TableScan scan = table.newScan()\n+        .select(\"timestamp\", \"int\", \"string\");\n+    readAndCheckVectorSchemaRoots(\n+        scan, NUM_ROWS_PER_MONTH, 12 * NUM_ROWS_PER_MONTH,\n+        ImmutableList.of(\"timestamp\", \"int\", \"string\"));\n+  }\n+\n+  /**\n+   * Read all rows and a single column from the table with a column selection filter. The test asserts that the Arrow\n+   * {@link VectorSchemaRoot} contains the expected schema and expected vector types. Then the test asserts that the\n+   * vectors contains expected values. The test also asserts the total number of rows match the expected value.\n+   */\n+  @Test\n+  public void testReadColumnFilter2() throws Exception {\n+    writeTableWithIncrementalRecords();\n+    Table table = tables.load(tableLocation);\n+    TableScan scan = table.newScan()\n+        .select(\"timestamp\");\n+    readAndCheckVectorSchemaRoots(\n+        scan, NUM_ROWS_PER_MONTH, 12 * NUM_ROWS_PER_MONTH,\n+        ImmutableList.of(\"timestamp\"));\n+  }\n+\n+  private void readAndCheckVectorSchemaRoots(\n+      TableScan scan,\n+      int numRowsPerRoot,\n+      int expectedTotalRows,\n+      List<String> columns) {\n+    Set<String> columnSet = ImmutableSet.copyOf(columns);\n+    int rowIndex = 0;\n+    int totalRows = 0;\n+    for (ArrowBatch batch : new VectorizedTableScanIterable(scan, numRowsPerRoot, false)) {\n+      VectorSchemaRoot root = batch.createVectorSchemaRootFromVectors();\n+      assertEquals(createExpectedArrowSchema(columnSet), root.getSchema());\n+      checkAllVectorTypes(root, columnSet);\n+      checkAllVectorValues(numRowsPerRoot, rowsWritten.subList(rowIndex, rowIndex + numRowsPerRoot), root, columnSet);\n+      rowIndex += numRowsPerRoot;\n+      totalRows += root.getRowCount();\n+    }\n+    assertEquals(expectedTotalRows, totalRows);\n+  }\n+\n+  private void writeTableWithConstantRecords() throws Exception {\n+    writeTable(true);\n+  }\n+\n+  private void writeTableWithIncrementalRecords() throws Exception {\n+    writeTable(false);\n+  }\n+\n+  private void writeTable(boolean constantRecords) throws Exception {\n+    rowsWritten = new ArrayList<>();\n+    tables = new HadoopTables();\n+    tableLocation = temp.newFolder(\"test\").toString();\n+\n+    Schema schema = new Schema(\n+        Types.NestedField.required(1, \"timestamp\", Types.TimestampType.withoutZone()),\n+        Types.NestedField.optional(2, \"timestamp_nullable\", Types.TimestampType.withoutZone()),\n+        Types.NestedField.required(3, \"boolean\", Types.BooleanType.get()),\n+        Types.NestedField.optional(4, \"boolean_nullable\", Types.BooleanType.get()),\n+        Types.NestedField.required(5, \"int\", Types.IntegerType.get()),\n+        Types.NestedField.optional(6, \"int_nullable\", Types.IntegerType.get()),\n+        Types.NestedField.required(7, \"long\", Types.LongType.get()),\n+        Types.NestedField.optional(8, \"long_nullable\", Types.LongType.get()),\n+        Types.NestedField.required(9, \"float\", Types.FloatType.get()),\n+        Types.NestedField.optional(10, \"float_nullable\", Types.FloatType.get()),\n+        Types.NestedField.required(11, \"double\", Types.DoubleType.get()),\n+        Types.NestedField.optional(12, \"double_nullable\", Types.DoubleType.get()),\n+        Types.NestedField.required(13, \"timestamp_tz\", Types.TimestampType.withZone()),\n+        Types.NestedField.optional(14, \"timestamp_tz_nullable\", Types.TimestampType.withZone()),\n+        Types.NestedField.required(15, \"string\", Types.StringType.get()),\n+        Types.NestedField.optional(16, \"string_nullable\", Types.StringType.get()),\n+        Types.NestedField.required(17, \"bytes\", Types.BinaryType.get()),\n+        Types.NestedField.optional(18, \"bytes_nullable\", Types.BinaryType.get()),\n+        Types.NestedField.required(19, \"date\", Types.DateType.get()),\n+        Types.NestedField.optional(20, \"date_nullable\", Types.DateType.get()),\n+        Types.NestedField.required(21, \"int_promotion\", Types.IntegerType.get())\n+    );\n+\n+    PartitionSpec spec = PartitionSpec.builderFor(schema)\n+        .month(\"timestamp\")\n+        .build();\n+\n+    Table table = tables.create(schema, spec, tableLocation);\n+\n+    OverwriteFiles overwrite = table.newOverwrite();\n+    for (int i = 1; i <= 12; i++) {\n+      final List<GenericRecord> records;\n+      if (constantRecords) {\n+        records = createConstantRecordsForDate(\n+            table.schema(), LocalDateTime.of(2020, i, 1, 0, 0, 0)\n+        );\n+      } else {\n+        records = createIncrementalRecordsForDate(\n+            table.schema(), LocalDateTime.of(2020, i, 1, 0, 0, 0)\n+        );\n+      }\n+      overwrite.addFile(writeParquetFile(table, records));\n+    }\n+    overwrite.commit();\n+\n+    // Perform a type promotion\n+    // TODO: The read Arrow vector should of type BigInt (promoted type) but it is Int (old type).\n+    Table tableLatest = tables.load(tableLocation);\n+    tableLatest.updateSchema()\n+        .updateColumn(\"int_promotion\", Types.LongType.get())\n+        .commit();\n+  }\n+\n+  private static org.apache.arrow.vector.types.pojo.Schema createExpectedArrowSchema(Set<String> columnSet) {\n+    List<Field> allFields = ImmutableList.of(\n+        new Field(\n+            \"timestamp\", new FieldType(false, MinorType.TIMESTAMPMICRO.getType(), null), null),\n+        new Field(\n+            \"timestamp_nullable\", new FieldType(true, MinorType.TIMESTAMPMICRO.getType(), null), null),\n+        new Field(\n+            \"boolean\", new FieldType(false, MinorType.BIT.getType(), null), null),\n+        new Field(\n+            \"boolean_nullable\", new FieldType(true, MinorType.BIT.getType(), null), null),\n+        new Field(\n+            \"int\", new FieldType(false, MinorType.INT.getType(), null), null),\n+        new Field(\n+            \"int_nullable\", new FieldType(true, MinorType.INT.getType(), null), null),\n+        new Field(\n+            \"long\", new FieldType(false, MinorType.BIGINT.getType(), null), null),\n+        new Field(\n+            \"long_nullable\", new FieldType(true, MinorType.BIGINT.getType(), null), null),\n+        new Field(\n+            \"float\", new FieldType(false, MinorType.FLOAT4.getType(), null), null),\n+        new Field(\n+            \"float_nullable\", new FieldType(true, MinorType.FLOAT4.getType(), null), null),\n+        new Field(\n+            \"double\", new FieldType(false, MinorType.FLOAT8.getType(), null), null),\n+        new Field(\n+            \"double_nullable\", new FieldType(true, MinorType.FLOAT8.getType(), null), null),\n+        new Field(\n+            \"timestamp_tz\", new FieldType(false, new ArrowType.Timestamp(\n+                org.apache.arrow.vector.types.TimeUnit.MICROSECOND, \"UTC\"), null), null),\n+        new Field(\n+            \"timestamp_tz_nullable\", new FieldType(true, new ArrowType.Timestamp(\n+                org.apache.arrow.vector.types.TimeUnit.MICROSECOND, \"UTC\"), null), null),\n+        new Field(\n+            \"string\", new FieldType(false, MinorType.VARCHAR.getType(), null), null),\n+        new Field(\n+            \"string_nullable\", new FieldType(true, MinorType.VARCHAR.getType(), null), null),\n+        new Field(\n+            \"bytes\", new FieldType(false, MinorType.VARBINARY.getType(), null), null),\n+        new Field(\n+            \"bytes_nullable\", new FieldType(true, MinorType.VARBINARY.getType(), null), null),\n+        new Field(\n+            \"date\", new FieldType(false, MinorType.DATEDAY.getType(), null), null),\n+        new Field(\n+            \"date_nullable\", new FieldType(true, MinorType.DATEDAY.getType(), null), null),\n+        new Field(\n+            \"int_promotion\", new FieldType(false, MinorType.INT.getType(), null), null)\n+    );\n+    List<Field> filteredFields = allFields.stream()\n+        .filter(f -> columnSet.contains(f.getName()))\n+        .collect(Collectors.toList());\n+    return new org.apache.arrow.vector.types.pojo.Schema(filteredFields);\n+  }\n+\n+  private List<GenericRecord> createIncrementalRecordsForDate(Schema schema, LocalDateTime datetime) {\n+    List<GenericRecord> records = new ArrayList<>();\n+    for (int i = 0; i < NUM_ROWS_PER_MONTH; i++) {\n+      GenericRecord rec = GenericRecord.create(schema);\n+      rec.setField(\"timestamp\", datetime.plus(i, ChronoUnit.DAYS));\n+      rec.setField(\"timestamp_nullable\", datetime.plus(i, ChronoUnit.DAYS));\n+      rec.setField(\"boolean\", i % 2 == 0);\n+      rec.setField(\"boolean_nullable\", i % 2 == 0);\n+      rec.setField(\"int\", i);\n+      rec.setField(\"int_nullable\", i);\n+      rec.setField(\"long\", (long) i * 2);\n+      rec.setField(\"long_nullable\", (long) i * 2);\n+      rec.setField(\"float\", (float) i * 3);\n+      rec.setField(\"float_nullable\", (float) i * 3);\n+      rec.setField(\"double\", (double) i * 4);\n+      rec.setField(\"double_nullable\", (double) i * 4);\n+      rec.setField(\"timestamp_tz\", datetime.plus(i, ChronoUnit.MINUTES).atOffset(ZoneOffset.UTC));\n+      rec.setField(\"timestamp_tz_nullable\", datetime.plus(i, ChronoUnit.MINUTES).atOffset(ZoneOffset.UTC));\n+      rec.setField(\"string\", \"String-\" + i);\n+      rec.setField(\"string_nullable\", \"String-\" + i);\n+      rec.setField(\"bytes\", ByteBuffer.wrap((\"Bytes-\" + i).getBytes(StandardCharsets.UTF_8)));\n+      rec.setField(\"bytes_nullable\", ByteBuffer.wrap((\"Bytes-\" + i).getBytes(StandardCharsets.UTF_8)));\n+      rec.setField(\"date\", LocalDate.of(2020, 1, 1).plus(i, ChronoUnit.DAYS));\n+      rec.setField(\"date_nullable\", LocalDate.of(2020, 1, 1).plus(i, ChronoUnit.DAYS));\n+      rec.setField(\"int_promotion\", i);\n+      records.add(rec);\n+    }\n+    return records;\n+  }\n+\n+  private List<GenericRecord> createConstantRecordsForDate(Schema schema, LocalDateTime datetime) {\n+    List<GenericRecord> records = new ArrayList<>();\n+    for (int i = 0; i < NUM_ROWS_PER_MONTH; i++) {\n+      GenericRecord rec = GenericRecord.create(schema);\n+      rec.setField(\"timestamp\", datetime);\n+      rec.setField(\"timestamp_nullable\", datetime);\n+      rec.setField(\"boolean\", true);\n+      rec.setField(\"boolean_nullable\", true);\n+      rec.setField(\"int\", 1);\n+      rec.setField(\"int_nullable\", 1);\n+      rec.setField(\"long\", 2L);\n+      rec.setField(\"long_nullable\", 2L);\n+      rec.setField(\"float\", 3.0f);\n+      rec.setField(\"float_nullable\", 3.0f);\n+      rec.setField(\"double\", 4.0);\n+      rec.setField(\"double_nullable\", 4.0);\n+      rec.setField(\"timestamp_tz\", datetime.atOffset(ZoneOffset.UTC));\n+      rec.setField(\"timestamp_tz_nullable\", datetime.atOffset(ZoneOffset.UTC));\n+      rec.setField(\"string\", \"String\");\n+      rec.setField(\"string_nullable\", \"String\");\n+      rec.setField(\"bytes\", ByteBuffer.wrap(\"Bytes\".getBytes(StandardCharsets.UTF_8)));\n+      rec.setField(\"bytes_nullable\", ByteBuffer.wrap(\"Bytes\".getBytes(StandardCharsets.UTF_8)));\n+      rec.setField(\"date\", LocalDate.of(2020, 1, 1));\n+      rec.setField(\"date_nullable\", LocalDate.of(2020, 1, 1));\n+      rec.setField(\"int_promotion\", 1);\n+      records.add(rec);\n+    }\n+    return records;\n+  }\n+\n+  private DataFile writeParquetFile(Table table, List<GenericRecord> records) throws IOException {\n+    rowsWritten.addAll(records);\n+    File parquetFile = temp.newFile();\n+    assertTrue(parquetFile.delete());\n+    FileAppender<GenericRecord> appender = Parquet.write(Files.localOutput(parquetFile))\n+        .schema(table.schema())\n+        .createWriterFunc(GenericParquetWriter::buildWriter)\n+        .build();\n+    try {\n+      appender.addAll(records);\n+    } finally {\n+      appender.close();\n+    }\n+\n+    PartitionKey partitionKey = new PartitionKey(table.spec(), table.schema());\n+    partitionKey.partition(new LocalDateTimeToLongMicros(records.get(0)));\n+\n+    return DataFiles.builder(table.spec())\n+        .withPartition(partitionKey)\n+        .withInputFile(localInput(parquetFile))\n+        .withMetrics(appender.metrics())\n+        .withFormat(FileFormat.PARQUET)\n+        .build();\n+  }\n+\n+  private static long timestampToMicros(LocalDateTime value) {\n+    Instant instant = value.toInstant(ZoneOffset.UTC);\n+    return ChronoUnit.MICROS.between(Instant.EPOCH, instant);\n+  }\n+\n+  private static long timestampToMicros(OffsetDateTime value) {\n+    Instant instant = value.toInstant();\n+    return ChronoUnit.MICROS.between(Instant.EPOCH, instant);\n+  }\n+\n+  private static LocalDateTime timestampFromMicros(long micros) {\n+    return LocalDateTime.ofEpochSecond(\n+        TimeUnit.MICROSECONDS.toSeconds(micros),\n+        (int) TimeUnit.MICROSECONDS.toNanos(micros % 1000),\n+        ZoneOffset.UTC\n+    );\n+  }\n+\n+  private static LocalDate dateFromDay(int day) {\n+    return LocalDate.ofEpochDay(day);\n+  }\n+\n+  private void checkAllVectorTypes(VectorSchemaRoot root, Set<String> columnSet) {\n+    assertEqualsForField(root, columnSet, \"timestamp\", TimeStampMicroVector.class);\n+    assertEqualsForField(root, columnSet, \"timestamp_nullable\", TimeStampMicroVector.class);\n+    assertEqualsForField(root, columnSet, \"boolean\", BitVector.class);\n+    assertEqualsForField(root, columnSet, \"boolean_nullable\", BitVector.class);\n+    assertEqualsForField(root, columnSet, \"int\", IntVector.class);\n+    assertEqualsForField(root, columnSet, \"int_nullable\", IntVector.class);\n+    assertEqualsForField(root, columnSet, \"long\", BigIntVector.class);\n+    assertEqualsForField(root, columnSet, \"long_nullable\", BigIntVector.class);\n+    assertEqualsForField(root, columnSet, \"float\", Float4Vector.class);\n+    assertEqualsForField(root, columnSet, \"float_nullable\", Float4Vector.class);\n+    assertEqualsForField(root, columnSet, \"double\", Float8Vector.class);\n+    assertEqualsForField(root, columnSet, \"double_nullable\", Float8Vector.class);\n+    assertEqualsForField(root, columnSet, \"timestamp_tz\", TimeStampMicroTZVector.class);\n+    assertEqualsForField(root, columnSet, \"timestamp_tz_nullable\", TimeStampMicroTZVector.class);\n+    assertEqualsForField(root, columnSet, \"string\", VarCharVector.class);\n+    assertEqualsForField(root, columnSet, \"string_nullable\", VarCharVector.class);\n+    assertEqualsForField(root, columnSet, \"bytes\", VarBinaryVector.class);\n+    assertEqualsForField(root, columnSet, \"bytes_nullable\", VarBinaryVector.class);\n+    assertEqualsForField(root, columnSet, \"date\", DateDayVector.class);\n+    assertEqualsForField(root, columnSet, \"date_nullable\", DateDayVector.class);\n+    assertEqualsForField(root, columnSet, \"int_promotion\", IntVector.class);\n+  }\n+\n+  private void assertEqualsForField(\n+      VectorSchemaRoot root, Set<String> columnSet, String columnName, Class<?> expected) {\n+    if (columnSet.contains(columnName)) {\n+      assertEquals(expected, root.getVector(columnName).getClass());\n+    }\n+  }\n+\n+  private void checkAllVectorValues(\n+      int expectedNumRows,\n+      List<GenericRecord> expectedRows,\n+      VectorSchemaRoot root,\n+      Set<String> columnSet) {\n+    assertEquals(expectedNumRows, root.getRowCount());\n+\n+    checkVectorValues(\n+        expectedNumRows, expectedRows, root, columnSet, \"timestamp\",\n+        (records, i) -> records.get(i).getField(\"timestamp\"),\n+        (vector, i) -> timestampFromMicros(((TimeStampMicroVector) vector).get(i))\n+    );\n+    checkVectorValues(\n+        expectedNumRows, expectedRows, root, columnSet, \"timestamp_nullable\",\n+        (records, i) -> records.get(i).getField(\"timestamp_nullable\"),\n+        (vector, i) -> timestampFromMicros(((TimeStampMicroVector) vector).get(i))\n+    );\n+    checkVectorValues(\n+        expectedNumRows, expectedRows, root, columnSet, \"boolean\",\n+        (records, i) -> records.get(i).getField(\"boolean\"),\n+        (vector, i) -> ((BitVector) vector).get(i) == 1\n+    );\n+    checkVectorValues(\n+        expectedNumRows, expectedRows, root, columnSet, \"boolean_nullable\",\n+        (records, i) -> records.get(i).getField(\"boolean_nullable\"),\n+        (vector, i) -> ((BitVector) vector).get(i) == 1\n+    );\n+    checkVectorValues(\n+        expectedNumRows, expectedRows, root, columnSet, \"int\",\n+        (records, i) -> records.get(i).getField(\"int\"),\n+        (vector, i) -> ((IntVector) vector).get(i)\n+    );\n+    checkVectorValues(\n+        expectedNumRows, expectedRows, root, columnSet, \"int_nullable\",\n+        (records, i) -> records.get(i).getField(\"int_nullable\"),\n+        (vector, i) -> ((IntVector) vector).get(i)\n+    );\n+    checkVectorValues(\n+        expectedNumRows, expectedRows, root, columnSet, \"long\",\n+        (records, i) -> records.get(i).getField(\"long\"),\n+        (vector, i) -> ((BigIntVector) vector).get(i)\n+    );\n+    checkVectorValues(\n+        expectedNumRows, expectedRows, root, columnSet, \"long_nullable\",\n+        (records, i) -> records.get(i).getField(\"long_nullable\"),\n+        (vector, i) -> ((BigIntVector) vector).get(i)\n+    );\n+    checkVectorValues(\n+        expectedNumRows, expectedRows, root, columnSet, \"float\",\n+        (records, i) -> Float.floatToIntBits((float) records.get(i).getField(\"float\")),\n+        (vector, i) -> Float.floatToIntBits(((Float4Vector) vector).get(i))\n+    );\n+    checkVectorValues(\n+        expectedNumRows, expectedRows, root, columnSet, \"float_nullable\",\n+        (records, i) -> Float.floatToIntBits((float) records.get(i).getField(\"float_nullable\")),\n+        (vector, i) -> Float.floatToIntBits(((Float4Vector) vector).get(i))\n+    );\n+    checkVectorValues(\n+        expectedNumRows, expectedRows, root, columnSet, \"double\",\n+        (records, i) -> Double.doubleToLongBits((double) records.get(i).getField(\"double\")),\n+        (vector, i) -> Double.doubleToLongBits(((Float8Vector) vector).get(i))\n+    );\n+    checkVectorValues(\n+        expectedNumRows, expectedRows, root, columnSet, \"double_nullable\",\n+        (records, i) -> Double.doubleToLongBits((double) records.get(i).getField(\"double_nullable\")),\n+        (vector, i) -> Double.doubleToLongBits(((Float8Vector) vector).get(i))\n+    );\n+    checkVectorValues(\n+        expectedNumRows, expectedRows, root, columnSet, \"timestamp_tz\",\n+        (records, i) -> timestampToMicros((OffsetDateTime) records.get(i).getField(\"timestamp_tz\")),\n+        (vector, i) -> ((TimeStampMicroTZVector) vector).get(i)\n+    );\n+    checkVectorValues(\n+        expectedNumRows, expectedRows, root, columnSet, \"timestamp_tz_nullable\",\n+        (records, i) -> timestampToMicros((OffsetDateTime) records.get(i).getField(\"timestamp_tz_nullable\")),\n+        (vector, i) -> ((TimeStampMicroTZVector) vector).get(i)\n+    );\n+    checkVectorValues(\n+        expectedNumRows, expectedRows, root, columnSet, \"string\",\n+        (records, i) -> records.get(i).getField(\"string\"),\n+        (vector, i) -> new String(((VarCharVector) vector).get(i), StandardCharsets.UTF_8)\n+    );\n+    checkVectorValues(\n+        expectedNumRows, expectedRows, root, columnSet, \"string_nullable\",\n+        (records, i) -> records.get(i).getField(\"string_nullable\"),\n+        (vector, i) -> new String(((VarCharVector) vector).get(i), StandardCharsets.UTF_8)\n+    );\n+    checkVectorValues(\n+        expectedNumRows, expectedRows, root, columnSet, \"bytes\",\n+        (records, i) -> records.get(i).getField(\"bytes\"),\n+        (vector, i) -> ByteBuffer.wrap(((VarBinaryVector) vector).get(i))\n+    );\n+    checkVectorValues(\n+        expectedNumRows, expectedRows, root, columnSet, \"bytes_nullable\",\n+        (records, i) -> records.get(i).getField(\"bytes_nullable\"),\n+        (vector, i) -> ByteBuffer.wrap(((VarBinaryVector) vector).get(i))\n+    );\n+    checkVectorValues(\n+        expectedNumRows, expectedRows, root, columnSet, \"date\",\n+        (records, i) -> records.get(i).getField(\"date\"),\n+        (vector, i) -> dateFromDay(((DateDayVector) vector).get(i))\n+    );\n+    checkVectorValues(\n+        expectedNumRows, expectedRows, root, columnSet, \"date_nullable\",\n+        (records, i) -> records.get(i).getField(\"date_nullable\"),\n+        (vector, i) -> dateFromDay(((DateDayVector) vector).get(i))\n+    );\n+    checkVectorValues(\n+        expectedNumRows, expectedRows, root, columnSet, \"int_promotion\",\n+        (records, i) -> records.get(i).getField(\"int_promotion\"),\n+        (vector, i) -> ((IntVector) vector).get(i)\n+    );\n+  }\n+\n+  private static void checkVectorValues(\n+      int expectedNumRows,\n+      List<GenericRecord> expectedRows,\n+      VectorSchemaRoot root,\n+      Set<String> columnSet,\n+      String columnName,\n+      BiFunction<List<GenericRecord>, Integer, Object> expectedValueExtractor,\n+      BiFunction<FieldVector, Integer, Object> vectorValueExtractor) {\n+    if (columnSet.contains(columnName)) {\n+      FieldVector vector = root.getVector(columnName);\n+      assertEquals(expectedNumRows, vector.getValueCount());\n+      for (int i = 0; i < expectedNumRows; i++) {\n+        Object expectedValue = expectedValueExtractor.apply(expectedRows, i);\n+        Object actualValue = vectorValueExtractor.apply(vector, i);\n+        assertEquals(\"Row#\" + i + \" mismatches\", expectedValue, actualValue);\n+      }\n+    }\n+  }\n+\n+  private static final class LocalDateTimeToLongMicros implements StructLike {\n+\n+    private final Record row;\n+\n+    LocalDateTimeToLongMicros(Record row) {\n+      this.row = row;\n+    }\n+\n+    @Override\n+    public int size() {\n+      return row.size();\n+    }\n+\n+    @Override\n+    public <T> T get(int pos, Class<T> javaClass) {\n+      Object value = row.get(pos);\n+      if (value instanceof LocalDateTime) {\n+        @SuppressWarnings(\"unchecked\")\n+        T result = (T) (Long) timestampToMicros((LocalDateTime) value);\n+        return result;\n+      } else if (value instanceof OffsetDateTime) {\n+        @SuppressWarnings(\"unchecked\")\n+        T result = (T) (Long) timestampToMicros(((OffsetDateTime) value).toLocalDateTime());\n+        return result;\n+      } else if (value != null) {\n+        throw new IllegalArgumentException(\"Unsupported value type: \" + value.getClass());\n+      } else {\n+        throw new IllegalArgumentException(\"Don't know how to handle null value\");\n+      }\n+    }\n+\n+    @Override\n+    public <T> void set(int pos, T value) {\n+      row.set(pos, value);\n+    }\n+  }\n+}"
  },
  {
    "sha": "748858af95804d419bb0a3162803cd1e856fa583",
    "filename": "build.gradle",
    "status": "modified",
    "additions": 7,
    "deletions": 0,
    "changes": 7,
    "blob_url": "https://github.com/apache/iceberg/blob/da10fe2cab636f24dff800c31c586b846ca10a41/build.gradle",
    "raw_url": "https://github.com/apache/iceberg/raw/da10fe2cab636f24dff800c31c586b846ca10a41/build.gradle",
    "contents_url": "https://api.github.com/repos/apache/iceberg/contents/build.gradle?ref=da10fe2cab636f24dff800c31c586b846ca10a41",
    "patch": "@@ -738,6 +738,13 @@ project(':iceberg-arrow') {\n       exclude group: 'io.netty', module: 'netty-common'\n       exclude group: 'com.google.code.findbugs', module: 'jsr305'\n     }\n+\n+    testCompile(\"org.apache.arrow:arrow-memory-netty\") {\n+      exclude group: 'com.google.code.findbugs', module: 'jsr305'\n+    }\n+    testCompile project(path: ':iceberg-core', configuration: 'testArtifacts')\n+    testCompile(\"org.apache.hadoop:hadoop-common\")\n+    testCompile(\"org.apache.hadoop:hadoop-mapreduce-client-core\")\n   }\n }\n "
  },
  {
    "sha": "b2d582352d74cf2a2773cc4a1f0a6f9cfdf236d7",
    "filename": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java",
    "status": "modified",
    "additions": 4,
    "deletions": 103,
    "changes": 107,
    "blob_url": "https://github.com/apache/iceberg/blob/da10fe2cab636f24dff800c31c586b846ca10a41/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java",
    "raw_url": "https://github.com/apache/iceberg/raw/da10fe2cab636f24dff800c31c586b846ca10a41/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java",
    "contents_url": "https://api.github.com/repos/apache/iceberg/contents/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java?ref=da10fe2cab636f24dff800c31c586b846ca10a41",
    "patch": "@@ -19,26 +19,12 @@\n \n package org.apache.iceberg.spark.data.vectorized;\n \n-import java.util.List;\n import java.util.Map;\n-import java.util.stream.IntStream;\n-import org.apache.arrow.memory.BufferAllocator;\n-import org.apache.iceberg.MetadataColumns;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.arrow.ArrowAllocation;\n-import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n-import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader.ConstantVectorReader;\n+import org.apache.iceberg.arrow.vectorized.VectorizedReaderBuilder;\n import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n-import org.apache.iceberg.parquet.VectorizedReader;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n-import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n-import org.apache.iceberg.types.Types;\n-import org.apache.parquet.column.ColumnDescriptor;\n-import org.apache.parquet.schema.GroupType;\n import org.apache.parquet.schema.MessageType;\n-import org.apache.parquet.schema.PrimitiveType;\n-import org.apache.parquet.schema.Type;\n \n public class VectorizedSparkParquetReaders {\n \n@@ -59,93 +45,8 @@ public static ColumnarBatchReader buildReader(\n       Map<Integer, ?> idToConstant) {\n     return (ColumnarBatchReader)\n         TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n-            new VectorizedReaderBuilder(expectedSchema, fileSchema, setArrowValidityVector, idToConstant));\n-  }\n-\n-  private static class VectorizedReaderBuilder extends TypeWithSchemaVisitor<VectorizedReader<?>> {\n-    private final MessageType parquetSchema;\n-    private final Schema icebergSchema;\n-    private final BufferAllocator rootAllocator;\n-    private final Map<Integer, ?> idToConstant;\n-    private final boolean setArrowValidityVector;\n-\n-    VectorizedReaderBuilder(\n-        Schema expectedSchema,\n-        MessageType parquetSchema,\n-        boolean setArrowValidityVector, Map<Integer, ?> idToConstant) {\n-      this.parquetSchema = parquetSchema;\n-      this.icebergSchema = expectedSchema;\n-      this.rootAllocator = ArrowAllocation.rootAllocator()\n-          .newChildAllocator(\"VectorizedReadBuilder\", 0, Long.MAX_VALUE);\n-      this.setArrowValidityVector = setArrowValidityVector;\n-      this.idToConstant = idToConstant;\n-    }\n-\n-    @Override\n-    public VectorizedReader<?> message(\n-            Types.StructType expected, MessageType message,\n-            List<VectorizedReader<?>> fieldReaders) {\n-      GroupType groupType = message.asGroupType();\n-      Map<Integer, VectorizedReader<?>> readersById = Maps.newHashMap();\n-      List<Type> fields = groupType.getFields();\n-\n-      IntStream.range(0, fields.size())\n-          .filter(pos -> fields.get(pos).getId() != null)\n-          .forEach(pos -> readersById.put(fields.get(pos).getId().intValue(), fieldReaders.get(pos)));\n-\n-      List<Types.NestedField> icebergFields = expected != null ?\n-          expected.fields() : ImmutableList.of();\n-\n-      List<VectorizedReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n-          icebergFields.size());\n-\n-      for (Types.NestedField field : icebergFields) {\n-        int id = field.fieldId();\n-        VectorizedReader<?> reader = readersById.get(id);\n-        if (idToConstant.containsKey(id)) {\n-          reorderedFields.add(new ConstantVectorReader<>(idToConstant.get(id)));\n-        } else if (id == MetadataColumns.ROW_POSITION.fieldId()) {\n-          reorderedFields.add(VectorizedArrowReader.positions());\n-        } else if (reader != null) {\n-          reorderedFields.add(reader);\n-        } else {\n-          reorderedFields.add(VectorizedArrowReader.nulls());\n-        }\n-      }\n-      return new ColumnarBatchReader(reorderedFields);\n-    }\n-\n-    @Override\n-    public VectorizedReader<?> struct(\n-        Types.StructType expected, GroupType groupType,\n-        List<VectorizedReader<?>> fieldReaders) {\n-      if (expected != null) {\n-        throw new UnsupportedOperationException(\"Vectorized reads are not supported yet for struct fields\");\n-      }\n-      return null;\n-    }\n-\n-    @Override\n-    public VectorizedReader<?> primitive(\n-        org.apache.iceberg.types.Type.PrimitiveType expected,\n-        PrimitiveType primitive) {\n-\n-      // Create arrow vector for this field\n-      if (primitive.getId() == null) {\n-        return null;\n-      }\n-      int parquetFieldId = primitive.getId().intValue();\n-      ColumnDescriptor desc = parquetSchema.getColumnDescription(currentPath());\n-      // Nested types not yet supported for vectorized reads\n-      if (desc.getMaxRepetitionLevel() > 0) {\n-        return null;\n-      }\n-      Types.NestedField icebergField = icebergSchema.findField(parquetFieldId);\n-      if (icebergField == null) {\n-        return null;\n-      }\n-      // Set the validity buffer if null checking is enabled in arrow\n-      return new VectorizedArrowReader(desc, icebergField, rootAllocator, setArrowValidityVector);\n-    }\n+            new VectorizedReaderBuilder(\n+                expectedSchema, fileSchema, setArrowValidityVector,\n+                idToConstant, ColumnarBatchReader::new));\n   }\n }"
  }
]
