[
  {
    "sha": "d6e36f5031c2e6b743d5aac72824a771b7a229db",
    "filename": "api/src/main/java/org/apache/iceberg/exceptions/CommitStateUnknownException.java",
    "status": "added",
    "additions": 39,
    "deletions": 0,
    "changes": 39,
    "blob_url": "https://github.com/apache/iceberg/blob/5ac3d0e6435436bb6a34f3fe3b2bea8ea35b19aa/api/src/main/java/org/apache/iceberg/exceptions/CommitStateUnknownException.java",
    "raw_url": "https://github.com/apache/iceberg/raw/5ac3d0e6435436bb6a34f3fe3b2bea8ea35b19aa/api/src/main/java/org/apache/iceberg/exceptions/CommitStateUnknownException.java",
    "contents_url": "https://api.github.com/repos/apache/iceberg/contents/api/src/main/java/org/apache/iceberg/exceptions/CommitStateUnknownException.java?ref=5ac3d0e6435436bb6a34f3fe3b2bea8ea35b19aa",
    "patch": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.exceptions;\n+\n+/**\n+ * Exception for a failure to confirm either affirmatively or negatively that a commit was applied. The client\n+ * cannot take any further action without possibly corrupting the table.\n+ */\n+public class CommitStateUnknownException extends RuntimeException {\n+\n+  private static final String COMMON_INFO =\n+      \"Cannot determine whether the commit was successful or not, the underlying data files may or \" +\n+      \"may not be needed. Manual intervention via the Remove Orphan Files Action can remove these \" +\n+      \"files when a connection to the Catalog can be re-established if the commit was actually unsuccessful.\\n\" +\n+      \"Please check to see whether or not your commit was successful before retrying this commit. Retrying \" +\n+      \"an already successful operation will result in duplicate records or unintentional modifications.\\n\" +\n+      \"At this time no files will be deleted including possibly unused manifest lists.\";\n+\n+  public CommitStateUnknownException(Throwable cause) {\n+    super(cause.getMessage() + \"\\n\" + COMMON_INFO, cause);\n+  }\n+}"
  },
  {
    "sha": "3a179f99fbe463d6569e4cb4354ee958d0a138db",
    "filename": "core/src/main/java/org/apache/iceberg/SnapshotProducer.java",
    "status": "modified",
    "additions": 3,
    "deletions": 0,
    "changes": 3,
    "blob_url": "https://github.com/apache/iceberg/blob/5ac3d0e6435436bb6a34f3fe3b2bea8ea35b19aa/core/src/main/java/org/apache/iceberg/SnapshotProducer.java",
    "raw_url": "https://github.com/apache/iceberg/raw/5ac3d0e6435436bb6a34f3fe3b2bea8ea35b19aa/core/src/main/java/org/apache/iceberg/SnapshotProducer.java",
    "contents_url": "https://api.github.com/repos/apache/iceberg/contents/core/src/main/java/org/apache/iceberg/SnapshotProducer.java?ref=5ac3d0e6435436bb6a34f3fe3b2bea8ea35b19aa",
    "patch": "@@ -32,6 +32,7 @@\n import java.util.function.Consumer;\n import org.apache.iceberg.events.Listeners;\n import org.apache.iceberg.exceptions.CommitFailedException;\n+import org.apache.iceberg.exceptions.CommitStateUnknownException;\n import org.apache.iceberg.exceptions.RuntimeIOException;\n import org.apache.iceberg.io.OutputFile;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n@@ -299,6 +300,8 @@ public void commit() {\n             taskOps.commit(base, updated.withUUID());\n           });\n \n+    } catch (CommitStateUnknownException commitStateUnknownException) {\n+      throw commitStateUnknownException;\n     } catch (RuntimeException e) {\n       Exceptions.suppressAndThrow(e, this::cleanAll);\n     }"
  },
  {
    "sha": "3648b1211ba3f27f6e429a394919b1549b8465f5",
    "filename": "core/src/main/java/org/apache/iceberg/TableOperations.java",
    "status": "modified",
    "additions": 7,
    "deletions": 0,
    "changes": 7,
    "blob_url": "https://github.com/apache/iceberg/blob/5ac3d0e6435436bb6a34f3fe3b2bea8ea35b19aa/core/src/main/java/org/apache/iceberg/TableOperations.java",
    "raw_url": "https://github.com/apache/iceberg/raw/5ac3d0e6435436bb6a34f3fe3b2bea8ea35b19aa/core/src/main/java/org/apache/iceberg/TableOperations.java",
    "contents_url": "https://api.github.com/repos/apache/iceberg/contents/core/src/main/java/org/apache/iceberg/TableOperations.java?ref=5ac3d0e6435436bb6a34f3fe3b2bea8ea35b19aa",
    "patch": "@@ -52,6 +52,13 @@\n    * Implementations must check that the base metadata is current to avoid overwriting updates.\n    * Once the atomic commit operation succeeds, implementations must not perform any operations that\n    * may fail because failure in this method cannot be distinguished from commit failure.\n+   * <p>\n+   * Implementations must throw a {@link org.apache.iceberg.exceptions.CommitStateUnknownException}\n+   * in cases where it cannot be determined if the commit succeeded or failed.\n+   * For example if a network partition causes the confirmation of the commit to be lost,\n+   * the implementation should throw a CommitStateUnknownException. This is important because downstream users of\n+   * this API need to know whether they can clean up the commit or not, if the state is unknown then it is not safe\n+   * to remove any files. All other exceptions will be treated as if the commit has failed.\n    *\n    * @param base     table metadata on which changes were based\n    * @param metadata new table metadata with updates"
  },
  {
    "sha": "bdc632b896028a9292462e7e27fc2e88a47cf07d",
    "filename": "core/src/main/java/org/apache/iceberg/TableProperties.java",
    "status": "modified",
    "additions": 3,
    "deletions": 0,
    "changes": 3,
    "blob_url": "https://github.com/apache/iceberg/blob/5ac3d0e6435436bb6a34f3fe3b2bea8ea35b19aa/core/src/main/java/org/apache/iceberg/TableProperties.java",
    "raw_url": "https://github.com/apache/iceberg/raw/5ac3d0e6435436bb6a34f3fe3b2bea8ea35b19aa/core/src/main/java/org/apache/iceberg/TableProperties.java",
    "contents_url": "https://api.github.com/repos/apache/iceberg/contents/core/src/main/java/org/apache/iceberg/TableProperties.java?ref=5ac3d0e6435436bb6a34f3fe3b2bea8ea35b19aa",
    "patch": "@@ -36,6 +36,9 @@ private TableProperties() {\n   public static final String COMMIT_TOTAL_RETRY_TIME_MS = \"commit.retry.total-timeout-ms\";\n   public static final int COMMIT_TOTAL_RETRY_TIME_MS_DEFAULT = 1800000; // 30 minutes\n \n+  public static final String COMMIT_NUM_STATUS_CHECKS = \"commit.num-status-checks\";\n+  public static final int COMMIT_NUM_STATUS_CHECKS_DEFAULT = 3;\n+\n   public static final String MANIFEST_TARGET_SIZE_BYTES = \"commit.manifest.target-size-bytes\";\n   public static final long MANIFEST_TARGET_SIZE_BYTES_DEFAULT = 8388608; // 8 MB\n "
  },
  {
    "sha": "2ec6cf36be08c4882c34c0a5b113d00ee2eb52a1",
    "filename": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java",
    "status": "modified",
    "additions": 82,
    "deletions": 11,
    "changes": 93,
    "blob_url": "https://github.com/apache/iceberg/blob/5ac3d0e6435436bb6a34f3fe3b2bea8ea35b19aa/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java",
    "raw_url": "https://github.com/apache/iceberg/raw/5ac3d0e6435436bb6a34f3fe3b2bea8ea35b19aa/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java",
    "contents_url": "https://api.github.com/repos/apache/iceberg/contents/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java?ref=5ac3d0e6435436bb6a34f3fe3b2bea8ea35b19aa",
    "patch": "@@ -54,24 +54,32 @@\n import org.apache.iceberg.common.DynMethods;\n import org.apache.iceberg.exceptions.AlreadyExistsException;\n import org.apache.iceberg.exceptions.CommitFailedException;\n+import org.apache.iceberg.exceptions.CommitStateUnknownException;\n import org.apache.iceberg.exceptions.NoSuchIcebergTableException;\n import org.apache.iceberg.exceptions.NoSuchTableException;\n import org.apache.iceberg.hadoop.ConfigProperties;\n import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.PropertyUtil;\n import org.apache.iceberg.util.Tasks;\n import org.apache.thrift.TException;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import static org.apache.iceberg.TableProperties.COMMIT_NUM_STATUS_CHECKS;\n+import static org.apache.iceberg.TableProperties.COMMIT_NUM_STATUS_CHECKS_DEFAULT;\n+\n /**\n  * TODO we should be able to extract some more commonalities to BaseMetastoreTableOperations to\n  * avoid code duplication between this class and Metacat Tables.\n  */\n public class HiveTableOperations extends BaseMetastoreTableOperations {\n   private static final Logger LOG = LoggerFactory.getLogger(HiveTableOperations.class);\n \n+  private static final int COMMIT_STATUS_RECHECK_SLEEP = 1000;\n+\n   private static final String HIVE_ACQUIRE_LOCK_TIMEOUT_MS = \"iceberg.hive.lock-timeout-ms\";\n   private static final String HIVE_LOCK_CHECK_MIN_WAIT_MS = \"iceberg.hive.lock-check-min-wait-ms\";\n   private static final String HIVE_LOCK_CHECK_MAX_WAIT_MS = \"iceberg.hive.lock-check-max-wait-ms\";\n@@ -91,6 +99,12 @@\n     }\n   }\n \n+  private enum CommitStatus {\n+    FAILURE,\n+    SUCCESS,\n+    UNKNOWN\n+  }\n+\n   private final HiveClientPool metaClients;\n   private final String fullName;\n   private final String database;\n@@ -153,12 +167,13 @@ protected void doRefresh() {\n     refreshFromMetadataLocation(metadataLocation);\n   }\n \n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n   @Override\n   protected void doCommit(TableMetadata base, TableMetadata metadata) {\n     String newMetadataLocation = writeNewMetadata(metadata, currentVersion() + 1);\n     boolean hiveEngineEnabled = hiveEngineEnabled(metadata, conf);\n \n-    boolean threw = true;\n+    CommitStatus commitStatus = CommitStatus.FAILURE;\n     boolean updateHiveTable = false;\n     Optional<Long> lockId = Optional.empty();\n     try {\n@@ -203,8 +218,23 @@ protected void doCommit(TableMetadata base, TableMetadata metadata) {\n           .orElseGet(ImmutableMap::of);\n       setHmsTableParameters(newMetadataLocation, tbl, metadata.properties(), removedProps, hiveEngineEnabled, summary);\n \n-      persistTable(tbl, updateHiveTable);\n-      threw = false;\n+      try {\n+        persistTable(tbl, updateHiveTable);\n+        commitStatus = CommitStatus.SUCCESS;\n+      } catch (Throwable persistFailure) {\n+        LOG.error(\"Cannot tell if commit to {}.{} succeeded, attempting to reconnect and check.\",\n+            database, tableName, persistFailure);\n+        commitStatus = checkCommitStatus(newMetadataLocation, metadata);\n+        switch (commitStatus) {\n+          case SUCCESS:\n+            return;\n+          case FAILURE:\n+            throw persistFailure;\n+          case UNKNOWN:\n+            throw new CommitStateUnknownException(persistFailure);\n+        }\n+      }\n+\n     } catch (org.apache.hadoop.hive.metastore.api.AlreadyExistsException e) {\n       throw new AlreadyExistsException(\"Table already exists: %s.%s\", database, tableName);\n \n@@ -222,11 +252,51 @@ protected void doCommit(TableMetadata base, TableMetadata metadata) {\n       throw new RuntimeException(\"Interrupted during commit\", e);\n \n     } finally {\n-      cleanupMetadataAndUnlock(threw, newMetadataLocation, lockId);\n+      cleanupMetadataAndUnlock(commitStatus, newMetadataLocation, lockId);\n+    }\n+  }\n+\n+  /**\n+   * Attempt to load the table and see if any current or past metadata location matches the one we were attempting\n+   * to set. This is used as a last resort when we are dealing with exceptions that may indicate the commit has\n+   * failed but are not proof that this is the case. Past locations must also be searched on the chance that a second\n+   * committer was able to successfully commit on top of our commit.\n+   *\n+   * @param newMetadataLocation the path of the new commit file\n+   * @param config metadata to use for configuration\n+   * @return Commit Status of Success, Failure or Unknown\n+   */\n+  private CommitStatus checkCommitStatus(String newMetadataLocation, TableMetadata config) {\n+    int maxAttempts = PropertyUtil.propertyAsInt(config.properties(), COMMIT_NUM_STATUS_CHECKS,\n+        COMMIT_NUM_STATUS_CHECKS_DEFAULT);\n+\n+    for (int attempt = 1; attempt <= maxAttempts; attempt++) {\n+      try {\n+        Thread.sleep(COMMIT_STATUS_RECHECK_SLEEP);\n+        TableMetadata metadata = refresh();\n+        String metadataLocation = metadata.metadataFileLocation();\n+        boolean commitSuccess = metadataLocation.equals(newMetadataLocation) ||\n+            metadata.previousFiles().stream().anyMatch(log -> log.file().equals(newMetadataLocation));\n+        if (commitSuccess) {\n+          LOG.info(\"Commit status check: Commit to {}.{} of {} succeeded\", database, tableName, newMetadataLocation);\n+          return CommitStatus.SUCCESS;\n+        } else {\n+          LOG.info(\"Commit status check: Commit to {}.{} of {} failed\", database, tableName, newMetadataLocation);\n+          return CommitStatus.FAILURE;\n+        }\n+      } catch (Throwable checkFailure) {\n+        LOG.error(\"Cannot check if commit to {}.{} exists. Retry attempt {} of {}.\",\n+            database, tableName, attempt, maxAttempts, checkFailure);\n+      }\n     }\n+\n+    LOG.error(\"Cannot determine commit state to {}.{}. Failed to check {} times. Treating commit state as unknown.\",\n+        database, tableName, maxAttempts);\n+    return CommitStatus.UNKNOWN;\n   }\n \n-  private void persistTable(Table hmsTable, boolean updateHiveTable) throws TException, InterruptedException {\n+  @VisibleForTesting\n+  void persistTable(Table hmsTable, boolean updateHiveTable) throws TException, InterruptedException {\n     if (updateHiveTable) {\n       metaClients.run(client -> {\n         EnvironmentContext envContext = new EnvironmentContext(\n@@ -335,7 +405,8 @@ private StorageDescriptor storageDescriptor(TableMetadata metadata, boolean hive\n     return storageDescriptor;\n   }\n \n-  private long acquireLock() throws UnknownHostException, TException, InterruptedException {\n+  @VisibleForTesting\n+  long acquireLock() throws UnknownHostException, TException, InterruptedException {\n     final LockComponent lockComponent = new LockComponent(LockType.EXCLUSIVE, LockLevel.TABLE, database);\n     lockComponent.setTablename(tableName);\n     final LockRequest lockRequest = new LockRequest(Lists.newArrayList(lockComponent),\n@@ -401,10 +472,10 @@ private long acquireLock() throws UnknownHostException, TException, InterruptedE\n     return lockId;\n   }\n \n-  private void cleanupMetadataAndUnlock(boolean errorThrown, String metadataLocation, Optional<Long> lockId) {\n+  private void cleanupMetadataAndUnlock(CommitStatus commitStatus, String metadataLocation, Optional<Long> lockId) {\n     try {\n-      if (errorThrown) {\n-        // if anything went wrong, clean up the uncommitted metadata file\n+      if (commitStatus == CommitStatus.FAILURE) {\n+        // If we are sure the commit failed, clean up the uncommitted metadata file\n         io().deleteFile(metadataLocation);\n       }\n     } catch (RuntimeException e) {\n@@ -425,8 +496,8 @@ private void unlock(Optional<Long> lockId) {\n     }\n   }\n \n-  // visible for testing\n-  protected void doUnlock(long lockId) throws TException, InterruptedException {\n+  @VisibleForTesting\n+  void doUnlock(long lockId) throws TException, InterruptedException {\n     metaClients.run(client -> {\n       client.unlock(lockId);\n       return null;"
  },
  {
    "sha": "bb1b0332e12013731ede7eb477f1f71a3d6bb0c4",
    "filename": "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommits.java",
    "status": "modified",
    "additions": 253,
    "deletions": 0,
    "changes": 253,
    "blob_url": "https://github.com/apache/iceberg/blob/5ac3d0e6435436bb6a34f3fe3b2bea8ea35b19aa/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommits.java",
    "raw_url": "https://github.com/apache/iceberg/raw/5ac3d0e6435436bb6a34f3fe3b2bea8ea35b19aa/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommits.java",
    "contents_url": "https://api.github.com/repos/apache/iceberg/contents/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommits.java?ref=5ac3d0e6435436bb6a34f3fe3b2bea8ea35b19aa",
    "patch": "@@ -19,17 +19,26 @@\n \n package org.apache.iceberg.hive;\n \n+import java.io.File;\n+import java.net.UnknownHostException;\n+import java.util.concurrent.atomic.AtomicLong;\n+import org.apache.iceberg.AssertHelpers;\n import org.apache.iceberg.HasTableOperations;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.exceptions.CommitStateUnknownException;\n import org.apache.iceberg.types.Types;\n import org.apache.thrift.TException;\n import org.junit.Assert;\n import org.junit.Test;\n import org.mockito.ArgumentCaptor;\n \n+import static org.mockito.Matchers.any;\n+import static org.mockito.Matchers.anyBoolean;\n+import static org.mockito.Mockito.doAnswer;\n import static org.mockito.Mockito.doThrow;\n import static org.mockito.Mockito.spy;\n+import static org.mockito.Mockito.when;\n \n public class TestHiveCommits extends HiveTableBaseTest {\n \n@@ -66,4 +75,248 @@ public void testSuppressUnlockExceptions() throws TException, InterruptedExcepti\n     // the commit must succeed\n     Assert.assertEquals(1, ops.current().schema().columns().size());\n   }\n+\n+  /**\n+   * Pretends we throw an error while persisting that actually fails to commit serverside\n+   */\n+  @Test\n+  public void testThriftExceptionFailureOnCommit() throws TException, InterruptedException {\n+    Table table = catalog.loadTable(TABLE_IDENTIFIER);\n+    HiveTableOperations ops = (HiveTableOperations) ((HasTableOperations) table).operations();\n+\n+    TableMetadata metadataV1 = ops.current();\n+\n+    table.updateSchema()\n+        .addColumn(\"n\", Types.IntegerType.get())\n+        .commit();\n+\n+    ops.refresh();\n+\n+    TableMetadata metadataV2 = ops.current();\n+\n+    Assert.assertEquals(2, ops.current().schema().columns().size());\n+\n+    HiveTableOperations spyOps = spy(ops);\n+\n+    failCommitAndThrowException(spyOps);\n+\n+    AssertHelpers.assertThrows(\"We should rethrow generic runtime errors if the \" +\n+        \"commit actually doesn't succeed\", RuntimeException.class, \"Metastore operation failed\",\n+        () -> spyOps.commit(metadataV2, metadataV1));\n+\n+    ops.refresh();\n+    Assert.assertEquals(\"Current metadata should not have changed\", metadataV2, ops.current());\n+    Assert.assertTrue(\"Current metadata should still exist\", metadataFileExists(metadataV2));\n+    Assert.assertEquals(\"No new metadata files should exist\", 2, metadataFileCount(ops.current()));\n+  }\n+\n+  /**\n+   * Pretends we throw an error while persisting that actually does commit serverside\n+   */\n+  @Test\n+  public void testThriftExceptionSuccessOnCommit() throws TException, InterruptedException {\n+    Table table = catalog.loadTable(TABLE_IDENTIFIER);\n+    HiveTableOperations ops = (HiveTableOperations) ((HasTableOperations) table).operations();\n+\n+    TableMetadata metadataV1 = ops.current();\n+\n+    table.updateSchema()\n+        .addColumn(\"n\", Types.IntegerType.get())\n+        .commit();\n+\n+    ops.refresh();\n+\n+    TableMetadata metadataV2 = ops.current();\n+\n+    Assert.assertEquals(2, ops.current().schema().columns().size());\n+\n+    HiveTableOperations spyOps = spy(ops);\n+\n+    // Simulate a communication error after a successful commit\n+    commitAndThrowException(ops, spyOps);\n+\n+    // Shouldn't throw because the commit actually succeeds even though persistTable throws an exception\n+    spyOps.commit(metadataV2, metadataV1);\n+\n+    ops.refresh();\n+    Assert.assertNotEquals(\"Current metadata should have changed\", metadataV2, ops.current());\n+    Assert.assertTrue(\"Current metadata file should still exist\", metadataFileExists(ops.current()));\n+    Assert.assertEquals(\"Commit should have been successful and new metadata file should be made\",\n+        3, metadataFileCount(ops.current()));\n+  }\n+\n+  /**\n+   * Pretends we throw an exception while persisting and don't know what happened, can't check to find out,\n+   * but in reality the commit failed\n+   */\n+  @Test\n+  public void testThriftExceptionUnknownFailedCommit() throws TException, InterruptedException {\n+    Table table = catalog.loadTable(TABLE_IDENTIFIER);\n+    HiveTableOperations ops = (HiveTableOperations) ((HasTableOperations) table).operations();\n+\n+    TableMetadata metadataV1 = ops.current();\n+\n+    table.updateSchema()\n+        .addColumn(\"n\", Types.IntegerType.get())\n+        .commit();\n+\n+    ops.refresh();\n+\n+    TableMetadata metadataV2 = ops.current();\n+\n+    Assert.assertEquals(2, ops.current().schema().columns().size());\n+\n+    HiveTableOperations spyOps = spy(ops);\n+\n+    failCommitAndThrowException(spyOps);\n+    breakFallbackCatalogCommitCheck(spyOps);\n+\n+    AssertHelpers.assertThrows(\"Should throw CommitStateUnknownException since the catalog check was blocked\",\n+        CommitStateUnknownException.class, \"Datacenter on fire\",\n+        () -> spyOps.commit(metadataV2, metadataV1));\n+\n+    ops.refresh();\n+\n+    Assert.assertEquals(\"Current metadata should not have changed\", metadataV2, ops.current());\n+    Assert.assertTrue(\"Current metadata file should still exist\", metadataFileExists(ops.current()));\n+    Assert.assertEquals(\"Client could not determine outcome so new metadata file should also exist\",\n+        3, metadataFileCount(ops.current()));\n+  }\n+\n+  /**\n+   * Pretends we throw an exception while persisting and don't know what happened, can't check to find out,\n+   * but in reality the commit succeeded\n+   */\n+  @Test\n+  public void testThriftExceptionsUnknownSuccessCommit() throws TException, InterruptedException {\n+    Table table = catalog.loadTable(TABLE_IDENTIFIER);\n+    HiveTableOperations ops = (HiveTableOperations) ((HasTableOperations) table).operations();\n+\n+    TableMetadata metadataV1 = ops.current();\n+\n+    table.updateSchema()\n+        .addColumn(\"n\", Types.IntegerType.get())\n+        .commit();\n+\n+    ops.refresh();\n+\n+    TableMetadata metadataV2 = ops.current();\n+\n+    Assert.assertEquals(2, ops.current().schema().columns().size());\n+\n+    HiveTableOperations spyOps = spy(ops);\n+\n+    commitAndThrowException(ops, spyOps);\n+    breakFallbackCatalogCommitCheck(spyOps);\n+\n+    AssertHelpers.assertThrows(\"Should throw CommitStateUnknownException since the catalog check was blocked\",\n+        CommitStateUnknownException.class, \"Datacenter on fire\",\n+        () -> spyOps.commit(metadataV2, metadataV1));\n+\n+    ops.refresh();\n+\n+    Assert.assertFalse(\"Current metadata should have changed\", ops.current().equals(metadataV2));\n+    Assert.assertTrue(\"Current metadata file should still exist\", metadataFileExists(ops.current()));\n+  }\n+\n+  /**\n+   * Pretends we threw an exception while persisting, the commit succeeded, the lock expired,\n+   * and a second committer placed a commit on top of ours before the first committer was able to check\n+   * if their commit succeeded or not\n+   *\n+   * Timeline:\n+   *   Client 1 commits which throws an exception but suceeded\n+   *   Client 1's lock expires while waiting to do the recheck for commit success\n+   *   Client 2 acquires a lock, commits successfully on top of client 1's commit and release lock\n+   *   Client 1 check's to see if their commit was successful\n+   *\n+   * This tests to make sure a disconnected client 1 doesn't think their commit failed just because it isn't the\n+   * current one during the recheck phase.\n+   */\n+  @Test\n+  public void testThriftExceptionConcurrentCommit() throws TException, InterruptedException, UnknownHostException {\n+    Table table = catalog.loadTable(TABLE_IDENTIFIER);\n+    HiveTableOperations ops = (HiveTableOperations) ((HasTableOperations) table).operations();\n+\n+    TableMetadata metadataV1 = ops.current();\n+\n+    table.updateSchema()\n+        .addColumn(\"n\", Types.IntegerType.get())\n+        .commit();\n+\n+    ops.refresh();\n+\n+    TableMetadata metadataV2 = ops.current();\n+\n+    Assert.assertEquals(2, ops.current().schema().columns().size());\n+\n+    HiveTableOperations spyOps = spy(ops);\n+\n+    AtomicLong lockId = new AtomicLong();\n+    doAnswer(i -> {\n+      lockId.set(ops.acquireLock());\n+      return lockId.get();\n+    }).when(spyOps).acquireLock();\n+\n+    concurrentCommitAndThrowException(ops, spyOps, table, lockId);\n+\n+    /*\n+    This commit and our concurrent commit should succeed even though this commit throws an exception\n+    after the persist operation succeeds\n+     */\n+    spyOps.commit(metadataV2, metadataV1);\n+\n+    ops.refresh();\n+    Assert.assertNotEquals(\"Current metadata should have changed\", metadataV2, ops.current());\n+    Assert.assertTrue(\"Current metadata file should still exist\", metadataFileExists(ops.current()));\n+    Assert.assertEquals(\"The column addition from the concurrent commit should have been successful\",\n+        2, ops.current().schema().columns().size());\n+  }\n+\n+  private void commitAndThrowException(HiveTableOperations realOperations, HiveTableOperations spyOperations)\n+      throws TException, InterruptedException {\n+    // Simulate a communication error after a successful commit\n+    doAnswer(i -> {\n+      org.apache.hadoop.hive.metastore.api.Table tbl =\n+          i.getArgumentAt(0, org.apache.hadoop.hive.metastore.api.Table.class);\n+      realOperations.persistTable(tbl, true);\n+      throw new TException(\"Datacenter on fire\");\n+    }).when(spyOperations).persistTable(any(), anyBoolean());\n+  }\n+\n+  private void concurrentCommitAndThrowException(HiveTableOperations realOperations, HiveTableOperations spyOperations,\n+                                                 Table table, AtomicLong lockId)\n+      throws TException, InterruptedException {\n+    // Simulate a communication error after a successful commit\n+    doAnswer(i -> {\n+      org.apache.hadoop.hive.metastore.api.Table tbl =\n+          i.getArgumentAt(0, org.apache.hadoop.hive.metastore.api.Table.class);\n+      realOperations.persistTable(tbl, true);\n+      // Simulate lock expiration or removal\n+      realOperations.doUnlock(lockId.get());\n+      table.refresh();\n+      table.updateSchema().addColumn(\"newCol\", Types.IntegerType.get()).commit();\n+      throw new TException(\"Datacenter on fire\");\n+    }).when(spyOperations).persistTable(any(), anyBoolean());\n+  }\n+\n+  private void failCommitAndThrowException(HiveTableOperations spyOperations) throws TException, InterruptedException {\n+    doThrow(new TException(\"Datacenter on fire\"))\n+        .when(spyOperations)\n+        .persistTable(any(), anyBoolean());\n+  }\n+\n+  private void breakFallbackCatalogCommitCheck(HiveTableOperations spyOperations) {\n+    when(spyOperations.refresh())\n+        .thenThrow(new RuntimeException(\"Still on fire\")); // Failure on commit check\n+  }\n+\n+  private boolean metadataFileExists(TableMetadata metadata) {\n+    return new File(metadata.metadataFileLocation().replace(\"file:\", \"\")).exists();\n+  }\n+\n+  private int metadataFileCount(TableMetadata metadata) {\n+    return new File(metadata.metadataFileLocation().replace(\"file:\", \"\")).getParentFile()\n+        .listFiles(file -> file.getName().endsWith(\"metadata.json\")).length;\n+  }\n }"
  }
]
