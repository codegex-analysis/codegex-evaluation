[
  {
    "sha": "74d78aea3f18c081fdc7bc8234a2382323afebee",
    "filename": "build.gradle",
    "status": "modified",
    "additions": 16,
    "deletions": 0,
    "changes": 16,
    "blob_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/blob/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/build.gradle",
    "raw_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/raw/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/build.gradle",
    "contents_url": "https://api.github.com/repos/ibm-messaging/kafka-connect-ibmcos-sink/contents/build.gradle?ref=6ae26d0b4a687fc5f575fc5454002af1a235f9a3",
    "patch": "@@ -21,12 +21,28 @@ plugins {\n \n repositories {\n    mavenCentral()\n+   maven {\n+     url \"http://packages.confluent.io/maven/\"\n+   }\n }\n \n dependencies {\n   compile 'com.eclipsesource.minimal-json:minimal-json:0.9.5'\n   compile 'com.ibm.cos:ibm-cos-java-sdk:2.4.5'\n   compile 'org.apache.kafka:connect-api:2.2.1'\n+  compile group: 'org.apache.avro', name: 'avro', version: '1.8.1'\n+  compile (group: 'io.confluent', name: 'kafka-connect-avro-converter', version: '5.3.2') {\n+    exclude group: 'org.apache.avro', module: 'avro'\n+  }\n+  compile (group: 'io.confluent', name: 'kafka-connect-storage-format', version: '5.3.2') {\n+    exclude group: 'org.apache.avro', module: 'avro'\n+  }\n+  compile (group: 'org.apache.parquet', name: 'parquet-avro', version: '1.11.0') {\n+    exclude group: 'org.apache.avro', module: 'avro'\n+  }\n+  compile (group: 'org.apache.hadoop', name: 'hadoop-common', version: '3.3.0') {\n+    exclude group: 'org.apache.avro', module: 'avro'\n+  }\n \n   testCompile 'org.mockito:mockito-all:1.10.19'\n   testCompile 'junit:junit:4.12'"
  },
  {
    "sha": "35348a8cd7cbd51d0946e881b5209e4a14c5e9e1",
    "filename": "config/cos-sink.json",
    "status": "modified",
    "additions": 15,
    "deletions": 1,
    "changes": 16,
    "blob_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/blob/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/config/cos-sink.json",
    "raw_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/raw/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/config/cos-sink.json",
    "contents_url": "https://api.github.com/repos/ibm-messaging/kafka-connect-ibmcos-sink/contents/config/cos-sink.json?ref=6ae26d0b4a687fc5f575fc5454002af1a235f9a3",
    "patch": "@@ -16,6 +16,20 @@\n         \"cos.object.deadline.seconds\": \"5\",\n         \"cos.object.interval.seconds\": \"5\",\n         \"cos.object.records\": \"5\",\n-        \"cos.service.crn\": \"<COS_CRN>\"\n+        \"cos.service.crn\": \"<COS_CRN>\",\n+\n+        \"cos.enable.parquet\": false,\n+        \"cos.schema.registry.url\": \"<SCHEMA_REGISTRY_URL>\",\n+        \"cos.schema.registry.apikey\": \"<SCHEMA_REGISTRY_APIKEY>\",\n+        \"cos.schema.subject\": \"<SCHEMA_SUBJECT>\",\n+        \"cos.schema.version\": \"<SCHEMA_VERSION>\",\n+        \"cos.schema.cache.size\": \"1000\",\n+        \"cos.enhanced.avro.schema.support\": true,\n+        \"cos.parquet.output.buffer.size\": \"26214400\",\n+        \"cos.parquet.write.mode\": \"create\",\n+        \"cos.parquet.compression.codec\": \"snappy\",\n+        \"cos.parquet.row.group.size\": \"268435356\",\n+        \"cos.parquet.page.size\": \"65536\",\n+        \"cos.parquet.dictionary.encoding\": true\n     }\n }"
  },
  {
    "sha": "3c0afccb5f0df060d5e3dbacee24696e898d901b",
    "filename": "config/cos-sink.properties",
    "status": "modified",
    "additions": 27,
    "deletions": 0,
    "changes": 27,
    "blob_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/blob/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/config/cos-sink.properties",
    "raw_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/raw/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/config/cos-sink.properties",
    "contents_url": "https://api.github.com/repos/ibm-messaging/kafka-connect-ibmcos-sink/contents/config/cos-sink.properties?ref=6ae26d0b4a687fc5f575fc5454002af1a235f9a3",
    "patch": "@@ -29,3 +29,30 @@ cos.object.interval.seconds=5\n cos.object.records=5\n # (required) - CRN for the Object Storage service instance.\n cos.service.crn=<COS_CRN>\n+\n+# (optional) - Enable Parquet output.\n+cos.enable.parquet=false\n+# (optional) - IBM Cloud EventStream schema registry URL.\n+cos.schema.registry.url=<SCHEMA_REGISTRY_URL>\n+# (optional) - IBM Cloud EventStream API Key.\n+cos.schema.registry.apikey=<SCHEMA_REGISTRY_APIKEY>\n+# (optional) - Avro schema subject.\n+cos.schema.subject=<SCHEMA_SUBJECT>\n+# (optional) - Avro schema version.\n+cos.schema.version=<SCHEMA_VERSION>\n+# (optional) - Avro schema cache size.\n+cos.schema.cache.size=1000\n+# (optional) - Enable more elaborate support for Avro schema.\n+cos.enhanced.avro.schema.support=true\n+# (optional) - Parquet output buffer size, stored in bytes in memory.\n+cos.parquet.output.buffer.size=26214400\n+# (optional) - Parquet write mode, can be create or overwrite.\n+cos.parquet.write.mode=create\n+# (optional) - Compression codec, can be empty (uncompressed), snappy, gz, lzo, br, lz4, zstd.\n+cos.parquet.compression.codec=snappy\n+# (optional) - Row group size, a Parquet file consists of multiple row groups.\n+cos.parquet.row.group.size=268435356\n+# (optional) - Page size, a row group consists of multiple column chunks and each column chuck consists of multiple pages.\n+cos.parquet.page.size=65536\n+# (optional) - Enable dictionary encoding.\n+cos.parquet.dictionary.encoding=true"
  },
  {
    "sha": "63fa526fc576a4c271d7ead084e6760c4bea01ff",
    "filename": "src/main/java/com/ibm/eventstreams/connect/cossink/COSSinkConnectorConfig.java",
    "status": "modified",
    "additions": 118,
    "deletions": 1,
    "changes": 119,
    "blob_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/blob/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/COSSinkConnectorConfig.java",
    "raw_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/raw/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/COSSinkConnectorConfig.java",
    "contents_url": "https://api.github.com/repos/ibm-messaging/kafka-connect-ibmcos-sink/contents/src/main/java/com/ibm/eventstreams/connect/cossink/COSSinkConnectorConfig.java?ref=6ae26d0b4a687fc5f575fc5454002af1a235f9a3",
    "patch": "@@ -91,6 +91,71 @@\n     private static final String CONFIG_DISPLAY_COS_ENDPOINTS_URL = \"Endpoints URL\";\n     private static final String CONFIG_VALUE_COS_ENDPOINTS_URL = \"https://control.cloud-object-storage.cloud.ibm.com/v2/endpoints\";\n \n+    static final String CONFIG_NAME_COS_ENABLE_PARQUET = \"cos.enable.parquet\";\n+    private static final String CONFIG_DOCUMENTATION_COS_ENABLE_PARQUET =\n+            \"Enable sending records in Parquet format to Cloud Object Storage.\";\n+    private static final String CONFIG_DISPLAY_COS_ENABLE_PARQUET = \"COS Enable Parquet\";\n+\n+    static final String CONFIG_NAME_COS_SCHEMA_REGISTRY_URL = \"cos.schema.registry.url\";\n+    private static final String CONFIG_DOCUMENTATION_COS_SCHEMA_REGISTRY_URL =\n+            \"EventStream Schema Registry URL where the schema is stored.\";\n+    private static final String CONFIG_DISPLAY_COS_SCHEMA_REGISTRY_URL = \"COS Schema Registry URL\";\n+\n+    static final String CONFIG_NAME_COS_SCHEMA_REGISTRY_APIKEY = \"cos.schema.registry.apikey\";\n+    private static final String CONFIG_DOCUMENTATION_COS_SCHEMA_REGISTRY_APIKEY =\n+            \"EventStream Schema Registry API Key to access schema.\";\n+    private static final String CONFIG_DISPLAY_COS_SCHEMA_REGISTRY_APIKEY = \"COS Schema Registry API Key\";\n+\n+    static final String CONFIG_NAME_COS_SCHEMA_SUBJECT = \"cos.schema.subject\";\n+    private static final String CONFIG_DOCUMENTATION_COS_SCHEMA_SUBJECT =\n+            \"Avro schema subject name. It should have either key or value ending.\";\n+    private static final String CONFIG_DISPLAY_COS_SCHEMA_SUBJECT = \"COS Schema Subject Name\";\n+\n+    static final String CONFIG_NAME_COS_SCHEMA_VERSION = \"cos.schema.version\";\n+    private static final String CONFIG_DOCUMENTATION_COS_SCHEMA_VERSION =\n+            \"Avro schema version. It should be an integer starting from 1.\";\n+    private static final String CONFIG_DISPLAY_COS_SCHEMA_VERSION = \"COS Schema Version\";\n+\n+    static final String CONFIG_NAME_COS_SCHEMA_CACHE_SIZE = \"cos.schema.cache.size\";\n+    private static final String CONFIG_DOCUMENTATION_COS_SCHEMA_CACHE_SIZE =\n+            \"Size of Avro schema cache. Default to 1000.\";\n+    private static final String CONFIG_DISPLAY_COS_SCHEMA_CACHE_SIZE = \"COS Schema Cache Size\";\n+\n+    static final String CONFIG_NAME_COS_ENHANCED_AVRO_SCHEMA_SUPPORT = \"cos.enhanced.avro.schema.support\";\n+    private static final String CONFIG_DOCUMENTATION_COS_ENHANCED_AVRO_SCHEMA_SUPPORT =\n+            \"Toggle for enabling/disabling enhanced avro schema support: Enum symbol preservation and Package Name awareness.\";\n+    private static final String CONFIG_DISPLAY_COS_ENHANCED_AVRO_SCHEMA_SUPPORT = \"COS Enhanced Avro Schema Support\";\n+\n+    static final String CONFIG_NAME_COS_PARQUET_OUTPUT_BUFFER_SIZE = \"cos.parquet.output.buffer.size\";\n+    private static final String CONFIG_DOCUMENTATION_COS_PARQUET_OUTPUT_BUFFER_SIZE =\n+            \"Size of output stream buffer for writing parquet data.\";\n+    private static final String CONFIG_DISPLAY_COS_PARQUET_OUTPUT_BUFFER_SIZE = \"COS Parquet Output Buffer Size\";\n+\n+    static final String CONFIG_NAME_COS_PARQUET_WRITE_MODE = \"cos.parquet.write.mode\";\n+    private static final String CONFIG_DOCUMENTATION_COS_PARQUET_WRITE_MODE =\n+            \"Write mode for parquet output.\";\n+    private static final String CONFIG_DISPLAY_COS_PARQUET_WRITE_MODE = \"COS Parquet Write Mode\";\n+\n+    static final String CONFIG_NAME_COS_PARQUET_COMPRESSION_CODEC = \"cos.parquet.compression.codec\";\n+    private static final String CONFIG_DOCUMENTATION_COS_PARQUET_COMPRESSION_CODEC =\n+            \"Compression codec for parquet output.\";\n+    private static final String CONFIG_DISPLAY_COS_PARQUET_COMPRESSION_CODEC = \"COS Parquet Compression Codec\";\n+\n+    static final String CONFIG_NAME_COS_PARQUET_ROW_GROUP_SIZE = \"cos.parquet.row.group.size\";\n+    private static final String CONFIG_DOCUMENTATION_COS_PARQUET_ROW_GROUP_SIZE =\n+            \"Block size threshold for parquet output.\";\n+    private static final String CONFIG_DISPLAY_COS_PARQUET_ROW_GROUP_SIZE = \"COS Parquet Row Group Size\";\n+\n+    static final String CONFIG_NAME_COS_PARQUET_PAGE_SIZE = \"cos.parquet.page.size\";\n+    private static final String CONFIG_DOCUMENTATION_COS_PARQUET_PAGE_SIZE =\n+            \"Blocks are subdivided into pages for alignment and other purposes.\";\n+    private static final String CONFIG_DISPLAY_COS_PARQUET_PAGE_SIZE = \"COS Parquet Page Size\";\n+\n+    static final String CONFIG_NAME_COS_PARQUET_DICTIONARY_ENCODING = \"cos.parquet.dictionary.encoding\";\n+    private static final String CONFIG_DOCUMENTATION_COS_PARQUET_DICTIONARY_ENCODING =\n+            \"Whether to use a dictionary to compress columns.\";\n+    private static final String CONFIG_DISPLAY_COS_PARQUET_DICTIONARY_ENCODING = \"COS Parquet Dictionary Encoding\";\n+\n     public static final ConfigDef CONFIG_DEF = new ConfigDef()\n         .define(CONFIG_NAME_COS_API_KEY, Type.PASSWORD, ConfigDef.NO_DEFAULT_VALUE, Importance.HIGH,\n                 CONFIG_DOCUMENTATION_COS_API_KEY, CONFIG_GROUP_COS, 1, Width.MEDIUM,\n@@ -141,7 +206,59 @@\n \n         .define(CONFIG_NAME_COS_ENDPOINTS_URL, Type.STRING, CONFIG_VALUE_COS_ENDPOINTS_URL, Importance.LOW,\n                 CONFIG_DOCUMENTATION_COS_ENDPOINTS_URL, CONFIG_GROUP_COS, 11, Width.MEDIUM,\n-                CONFIG_DISPLAY_COS_ENDPOINTS_URL);\n+                CONFIG_DISPLAY_COS_ENDPOINTS_URL)\n+\n+        .define(CONFIG_NAME_COS_ENABLE_PARQUET, Type.BOOLEAN, false, Importance.MEDIUM,\n+                CONFIG_DOCUMENTATION_COS_ENABLE_PARQUET, CONFIG_GROUP_COS, 12, Width.MEDIUM,\n+                CONFIG_DISPLAY_COS_ENABLE_PARQUET)\n+\n+        .define(CONFIG_NAME_COS_SCHEMA_REGISTRY_URL, Type.STRING, \"\", Importance.MEDIUM,\n+                CONFIG_DOCUMENTATION_COS_SCHEMA_REGISTRY_URL, CONFIG_GROUP_COS, 13, Width.MEDIUM,\n+                CONFIG_DISPLAY_COS_SCHEMA_REGISTRY_URL)\n+\n+        .define(CONFIG_NAME_COS_SCHEMA_REGISTRY_APIKEY, Type.STRING, \"\", Importance.MEDIUM,\n+                CONFIG_DOCUMENTATION_COS_SCHEMA_REGISTRY_APIKEY, CONFIG_GROUP_COS, 14, Width.MEDIUM,\n+                CONFIG_DISPLAY_COS_SCHEMA_REGISTRY_APIKEY)\n+\n+        .define(CONFIG_NAME_COS_SCHEMA_SUBJECT, Type.STRING, \"\", Importance.MEDIUM,\n+                CONFIG_DOCUMENTATION_COS_SCHEMA_SUBJECT, CONFIG_GROUP_COS, 15, Width.MEDIUM,\n+                CONFIG_DISPLAY_COS_SCHEMA_SUBJECT)\n+\n+        .define(CONFIG_NAME_COS_SCHEMA_VERSION, Type.INT, 1, Importance.MEDIUM,\n+                CONFIG_DOCUMENTATION_COS_SCHEMA_VERSION, CONFIG_GROUP_COS, 16, Width.MEDIUM,\n+                CONFIG_DISPLAY_COS_SCHEMA_VERSION)\n+\n+        .define(CONFIG_NAME_COS_SCHEMA_CACHE_SIZE, Type.INT, ConfigDef.NO_DEFAULT_VALUE, Importance.MEDIUM,\n+                CONFIG_DOCUMENTATION_COS_SCHEMA_CACHE_SIZE, CONFIG_GROUP_COS, 17, Width.MEDIUM,\n+                CONFIG_DISPLAY_COS_SCHEMA_CACHE_SIZE)\n+\n+        .define(CONFIG_NAME_COS_ENHANCED_AVRO_SCHEMA_SUPPORT, Type.BOOLEAN, false, Importance.MEDIUM,\n+                CONFIG_DOCUMENTATION_COS_ENHANCED_AVRO_SCHEMA_SUPPORT, CONFIG_GROUP_COS, 18, Width.MEDIUM,\n+                CONFIG_DISPLAY_COS_ENHANCED_AVRO_SCHEMA_SUPPORT)\n+\n+        .define(CONFIG_NAME_COS_PARQUET_OUTPUT_BUFFER_SIZE, Type.INT, 262144, Importance.MEDIUM,\n+                CONFIG_DOCUMENTATION_COS_PARQUET_OUTPUT_BUFFER_SIZE, CONFIG_GROUP_COS, 19, Width.MEDIUM,\n+                CONFIG_DISPLAY_COS_PARQUET_OUTPUT_BUFFER_SIZE)\n+\n+        .define(CONFIG_NAME_COS_PARQUET_WRITE_MODE, Type.STRING, \"create\", Importance.MEDIUM,\n+                CONFIG_DOCUMENTATION_COS_PARQUET_WRITE_MODE, CONFIG_GROUP_COS, 20, Width.MEDIUM,\n+                CONFIG_DISPLAY_COS_PARQUET_WRITE_MODE)\n+\n+        .define(CONFIG_NAME_COS_PARQUET_COMPRESSION_CODEC, Type.STRING, \"uncompressed\", Importance.MEDIUM,\n+                CONFIG_DOCUMENTATION_COS_PARQUET_COMPRESSION_CODEC, CONFIG_GROUP_COS, 21, Width.MEDIUM,\n+                CONFIG_DISPLAY_COS_PARQUET_COMPRESSION_CODEC)\n+\n+        .define(CONFIG_NAME_COS_PARQUET_ROW_GROUP_SIZE, Type.INT, 268435356, Importance.MEDIUM,\n+                CONFIG_DOCUMENTATION_COS_PARQUET_ROW_GROUP_SIZE, CONFIG_GROUP_COS, 22, Width.MEDIUM,\n+                CONFIG_DISPLAY_COS_PARQUET_ROW_GROUP_SIZE)\n+\n+        .define(CONFIG_NAME_COS_PARQUET_PAGE_SIZE, Type.INT, 65536, Importance.MEDIUM,\n+                CONFIG_DOCUMENTATION_COS_PARQUET_PAGE_SIZE, CONFIG_GROUP_COS, 23, Width.MEDIUM,\n+                CONFIG_DISPLAY_COS_PARQUET_PAGE_SIZE)\n+\n+        .define(CONFIG_NAME_COS_PARQUET_DICTIONARY_ENCODING, Type.BOOLEAN, true, Importance.MEDIUM,\n+                CONFIG_DOCUMENTATION_COS_PARQUET_DICTIONARY_ENCODING, CONFIG_GROUP_COS, 24, Width.MEDIUM,\n+                CONFIG_DISPLAY_COS_PARQUET_DICTIONARY_ENCODING);\n \n     public COSSinkConnectorConfig(ConfigDef definition, Map<?, ?> originals) {\n         super(definition, originals);"
  },
  {
    "sha": "0b463e10289cb2fec8e99fb15479ff7962abd5a2",
    "filename": "src/main/java/com/ibm/eventstreams/connect/cossink/COSSinkTask.java",
    "status": "modified",
    "additions": 37,
    "deletions": 1,
    "changes": 38,
    "blob_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/blob/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/COSSinkTask.java",
    "raw_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/raw/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/COSSinkTask.java",
    "contents_url": "https://api.github.com/repos/ibm-messaging/kafka-connect-ibmcos-sink/contents/src/main/java/com/ibm/eventstreams/connect/cossink/COSSinkTask.java?ref=6ae26d0b4a687fc5f575fc5454002af1a235f9a3",
    "patch": "@@ -19,6 +19,7 @@\n import java.util.HashMap;\n import java.util.Map;\n \n+import com.ibm.eventstreams.connect.cossink.parquet.COSParquetConfig;\n import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n import org.apache.kafka.common.TopicPartition;\n import org.apache.kafka.common.config.ConfigException;\n@@ -59,6 +60,9 @@\n     private int deadlineSec;\n     private int intervalSec;\n \n+    private COSParquetConfig cosParquetConfig;\n+    private boolean enableParquet;\n+\n     // Connect framework requires no-value constructor.\n     public COSSinkTask() {\n         this(new ClientFactoryImpl(), new COSPartitionWriterFactory(), new HashMap<>(), new DeadlineServiceImpl());\n@@ -118,6 +122,37 @@ public void start(Map<String, String> props) {\n                             \"to a value that is greater than zero\");\n         }\n \n+        enableParquet = connectorConfig.getBoolean(COSSinkConnectorConfig.CONFIG_NAME_COS_ENABLE_PARQUET);\n+\n+        if (enableParquet) {\n+            cosParquetConfig = COSParquetConfig.newBuilder()\n+                    .setSchemaRegistryUrl(connectorConfig.getString(\n+                            COSSinkConnectorConfig.CONFIG_NAME_COS_SCHEMA_REGISTRY_URL))\n+                    .setSchemaRegistryApiKey(connectorConfig.getString(\n+                            COSSinkConnectorConfig.CONFIG_NAME_COS_SCHEMA_REGISTRY_APIKEY))\n+                    .setSchemaSubject(connectorConfig.getString(\n+                            COSSinkConnectorConfig.CONFIG_NAME_COS_SCHEMA_SUBJECT))\n+                    .setSchemaVersion(connectorConfig.getInt(\n+                            COSSinkConnectorConfig.CONFIG_NAME_COS_SCHEMA_VERSION))\n+                    .setSchemaCacheSize(connectorConfig.getInt(\n+                            COSSinkConnectorConfig.CONFIG_NAME_COS_SCHEMA_CACHE_SIZE))\n+                    .setEnhancedSchemaSupport(connectorConfig.getBoolean(\n+                            COSSinkConnectorConfig.CONFIG_NAME_COS_ENHANCED_AVRO_SCHEMA_SUPPORT))\n+                    .setParquetBufferSize(connectorConfig.getInt(\n+                            COSSinkConnectorConfig.CONFIG_NAME_COS_PARQUET_OUTPUT_BUFFER_SIZE))\n+                    .setParquetWriteMode(connectorConfig.getString(\n+                            COSSinkConnectorConfig.CONFIG_NAME_COS_PARQUET_WRITE_MODE))\n+                    .setParquetCompressionCodec(connectorConfig.getString(\n+                            COSSinkConnectorConfig.CONFIG_NAME_COS_PARQUET_COMPRESSION_CODEC))\n+                    .setParquetRowGroupSize(connectorConfig.getInt(\n+                            COSSinkConnectorConfig.CONFIG_NAME_COS_PARQUET_ROW_GROUP_SIZE))\n+                    .setParquetPageSize(connectorConfig.getInt(\n+                            COSSinkConnectorConfig.CONFIG_NAME_COS_PARQUET_PAGE_SIZE))\n+                    .setParquetDictionaryEncoding(connectorConfig.getBoolean(\n+                            COSSinkConnectorConfig.CONFIG_NAME_COS_PARQUET_DICTIONARY_ENCODING))\n+                    .build();\n+        }\n+\n         open(context.assignment());\n         LOG.trace(\"< start\");\n     }\n@@ -150,7 +185,8 @@ public void open(Collection<TopicPartition> partitions) {\n             if (assignedWriters.containsKey(tp)) {\n                 LOG.info(\"A PartitionWriter already exists for {}\", tp);\n             } else {\n-                PartitionWriter pw = pwFactory.newPartitionWriter(bucket, buildCompletionCriteriaSet(), delimitRecords);\n+                PartitionWriter pw = pwFactory.newPartitionWriter(bucket,\n+                        buildCompletionCriteriaSet(), delimitRecords, cosParquetConfig);\n                 assignedWriters.put(tp, pw);\n             }\n         }"
  },
  {
    "sha": "9e733d2b55d7cf53c4be2c13a78d19b6c31b2510",
    "filename": "src/main/java/com/ibm/eventstreams/connect/cossink/parquet/COSOutputStream.java",
    "status": "added",
    "additions": 104,
    "deletions": 0,
    "changes": 104,
    "blob_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/blob/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/parquet/COSOutputStream.java",
    "raw_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/raw/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/parquet/COSOutputStream.java",
    "contents_url": "https://api.github.com/repos/ibm-messaging/kafka-connect-ibmcos-sink/contents/src/main/java/com/ibm/eventstreams/connect/cossink/parquet/COSOutputStream.java?ref=6ae26d0b4a687fc5f575fc5454002af1a235f9a3",
    "patch": "@@ -0,0 +1,104 @@\n+/*\n+ * IBM Confidential Source Materials\n+ * (C) Copyright IBM Corp. 2020\n+ *\n+ * The source code for this program is not published or otherwise\n+ * divested of its trade secrets, irrespective of what has\n+ * been deposited with the U.S. Copyright Office.\n+ *\n+ */\n+package com.ibm.eventstreams.connect.cossink.parquet;\n+\n+import com.ibm.cloud.objectstorage.services.s3.model.ObjectMetadata;\n+import com.ibm.cos.Bucket;\n+import org.apache.parquet.io.PositionOutputStream;\n+\n+import java.io.ByteArrayInputStream;\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * This class interacts directly with COS API to write Parquet output.\n+ * It is the parent class of COSParquetOutputStream and is instantiated\n+ * when COSParquetOutputStream is instantiated\n+ *\n+ * @author Bill Li\n+ */\n+public class COSOutputStream extends PositionOutputStream {\n+\n+    private long position;\n+    private ByteBuffer byteBuffer;\n+    private Bucket bucket;\n+    private String key;\n+\n+    /**\n+     * Prepare values for writing Parquet to COS\n+     * @param bucket COS bucket object\n+     * @param filename File name of Parquet file\n+     * @param parquetBufferSize Allocated Parquet buffer size\n+     */\n+    public COSOutputStream(final Bucket bucket, final String filename, final int parquetBufferSize) {\n+        this.position = 0L;\n+        this.byteBuffer = ByteBuffer.allocate(parquetBufferSize);\n+        this.bucket = bucket;\n+        this.key = filename;\n+    }\n+\n+    @Override\n+    public long getPos() {\n+        return position;\n+    }\n+\n+    /**\n+     * Place one byte at a time into Byte Buffer and increment position.\n+     * This method involves converting integer type to byte type. We have\n+     * verified that there is no issue in accepting Unicode character.\n+     * This method is inherited from parent class OutputStream. For detailed\n+     * information, please refer to the original source Java Doc.\n+     * @param b: Parquet bytes\n+     */\n+    @Override\n+    public void write(int b) {\n+        byteBuffer.put((byte) b);\n+        position++;\n+    }\n+\n+    /**\n+     * Ready to commit to COS by calling uploadToCOS method\n+     */\n+    public void commit() {\n+        uploadToCOS(getBufferBytes());\n+    }\n+\n+    /**\n+     * Upload Parquet byte array to COS\n+     * @param value: Byte array to be committed to COS\n+     */\n+    private void uploadToCOS(byte[] value) {\n+        bucket.putObject(key, new ByteArrayInputStream(value), createMetadata(value));\n+        byteBuffer.clear();\n+    }\n+\n+    /**\n+     * Turn valid byte content from Byte Buffer into byte array\n+     * Note that we need to do rewind operation in order to retrieve\n+     * bytes from the beginning to the end of valid bytes\n+     * @return return Parquet byte array\n+     */\n+    private byte[] getBufferBytes() {\n+        byte[] value = new byte[byteBuffer.position()];\n+        byteBuffer.rewind();\n+        byteBuffer.get(value);\n+        return value;\n+    }\n+\n+    /**\n+     * Create COS object metadata\n+     * @param value Byte array containing Parquet output bytes\n+     * @return Object metadata to commit record to COS\n+     */\n+    private static ObjectMetadata createMetadata(final byte[] value) {\n+        ObjectMetadata metadata = new ObjectMetadata();\n+        metadata.setContentLength(value.length);\n+        return metadata;\n+    }\n+}"
  },
  {
    "sha": "e567770acfba52f42f32db8f6606cecd5cae8dd4",
    "filename": "src/main/java/com/ibm/eventstreams/connect/cossink/parquet/COSParquetConfig.java",
    "status": "added",
    "additions": 171,
    "deletions": 0,
    "changes": 171,
    "blob_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/blob/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/parquet/COSParquetConfig.java",
    "raw_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/raw/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/parquet/COSParquetConfig.java",
    "contents_url": "https://api.github.com/repos/ibm-messaging/kafka-connect-ibmcos-sink/contents/src/main/java/com/ibm/eventstreams/connect/cossink/parquet/COSParquetConfig.java?ref=6ae26d0b4a687fc5f575fc5454002af1a235f9a3",
    "patch": "@@ -0,0 +1,171 @@\n+package com.ibm.eventstreams.connect.cossink.parquet;\n+\n+import org.apache.parquet.hadoop.ParquetFileWriter;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+\n+public class COSParquetConfig {\n+\n+    private String schemaRegistryUrl;\n+    private String schemaRegistryApiKey;\n+    private String schemaSubject;\n+    private int schemaVersion;\n+    private int schemaCacheSize;\n+    private boolean enhancedSchemaSupport;\n+    private int parquetBufferSize;\n+    private String parquetWriteMode;\n+    private String parquetCompressionCodec;\n+    private int parquetRowGroupSize;\n+    private int parquetPageSize;\n+    private boolean parquetDictionaryEncoding;\n+\n+    public static Builder newBuilder() {\n+        return new Builder();\n+    }\n+\n+    private COSParquetConfig() {}\n+\n+    public String getSchemaRegistryUrl() {\n+        return schemaRegistryUrl;\n+    }\n+\n+    public String getSchemaRegistryApiKey() {\n+        return schemaRegistryApiKey;\n+    }\n+\n+    public String getSchemaSubject() {\n+        return schemaSubject;\n+    }\n+\n+    public int getSchemaVersion() {\n+        return schemaVersion;\n+    }\n+\n+    public int getSchemaCacheSize() {\n+        return schemaCacheSize;\n+    }\n+\n+    public boolean isEnhancedSchemaSupport() {\n+        return enhancedSchemaSupport;\n+    }\n+\n+    public int getParquetBufferSize() {\n+        return parquetBufferSize;\n+    }\n+\n+    public ParquetFileWriter.Mode getParquetWriteMode() throws Exception {\n+        switch (parquetWriteMode) {\n+            case \"create\":\n+                return ParquetFileWriter.Mode.CREATE;\n+            case \"overwrite\":\n+                return ParquetFileWriter.Mode.OVERWRITE;\n+            default:\n+                throw new Exception(\"Parquet write mode not found.\");\n+        }\n+    }\n+\n+    public CompressionCodecName getParquetCompressionCodec() throws Exception {\n+        switch (parquetCompressionCodec) {\n+            case \"uncompressed\":\n+                return CompressionCodecName.UNCOMPRESSED;\n+            case \"snappy\":\n+                return CompressionCodecName.SNAPPY;\n+            case \"gzip\":\n+                return CompressionCodecName.GZIP;\n+            case \"lzo\":\n+                return CompressionCodecName.LZO;\n+            case \"brotli\":\n+                return CompressionCodecName.BROTLI;\n+            case \"lz4\":\n+                return CompressionCodecName.LZ4;\n+            case \"zstd\":\n+                return CompressionCodecName.ZSTD;\n+            default:\n+                throw new Exception(\"Compression codec name not found.\");\n+        }\n+    }\n+\n+    public int getParquetRowGroupSize() {\n+        return parquetRowGroupSize;\n+    }\n+\n+    public int getParquetPageSize() {\n+        return parquetPageSize;\n+    }\n+\n+    public boolean isParquetDictionaryEncoding() {\n+        return parquetDictionaryEncoding;\n+    }\n+\n+    public static class Builder {\n+\n+        private COSParquetConfig cosAvroParquetConfig;\n+\n+        private Builder() {\n+            cosAvroParquetConfig = new COSParquetConfig();\n+        }\n+\n+        public Builder setSchemaRegistryUrl(String schemaRegistryUrl) {\n+            cosAvroParquetConfig.schemaRegistryUrl = schemaRegistryUrl;\n+            return this;\n+        }\n+\n+        public Builder setSchemaRegistryApiKey(String schemaRegistryApiKey) {\n+            cosAvroParquetConfig.schemaRegistryApiKey = schemaRegistryApiKey;\n+            return this;\n+        }\n+\n+        public Builder setSchemaSubject(String schemaSubject) {\n+            cosAvroParquetConfig.schemaSubject = schemaSubject;\n+            return this;\n+        }\n+\n+        public Builder setSchemaVersion(int schemaVersion) {\n+            cosAvroParquetConfig.schemaVersion = schemaVersion;\n+            return this;\n+        }\n+\n+        public Builder setSchemaCacheSize(int schemaCacheSize) {\n+            cosAvroParquetConfig.schemaCacheSize = schemaCacheSize;\n+            return this;\n+        }\n+\n+        public Builder setEnhancedSchemaSupport(boolean enhancedSchemaSupport) {\n+            cosAvroParquetConfig.enhancedSchemaSupport = enhancedSchemaSupport;\n+            return this;\n+        }\n+\n+        public Builder setParquetBufferSize(int parquetBufferSize) {\n+            cosAvroParquetConfig.parquetBufferSize = parquetBufferSize;\n+            return this;\n+        }\n+\n+        public Builder setParquetWriteMode(String parquetWriteMode) {\n+            cosAvroParquetConfig.parquetWriteMode = parquetWriteMode;\n+            return this;\n+        }\n+\n+        public Builder setParquetCompressionCodec(String parquetCompressionCodec) {\n+            cosAvroParquetConfig.parquetCompressionCodec = parquetCompressionCodec;\n+            return this;\n+        }\n+\n+        public Builder setParquetRowGroupSize(int parquetRowGroupSize) {\n+            cosAvroParquetConfig.parquetRowGroupSize = parquetRowGroupSize;\n+            return this;\n+        }\n+\n+        public Builder setParquetPageSize(int parquetPageSize) {\n+            cosAvroParquetConfig.parquetPageSize = parquetPageSize;\n+            return this;\n+        }\n+\n+        public Builder setParquetDictionaryEncoding(boolean parquetDictionaryEncoding) {\n+            cosAvroParquetConfig.parquetDictionaryEncoding = parquetDictionaryEncoding;\n+            return this;\n+        }\n+\n+        public COSParquetConfig build() {\n+            return cosAvroParquetConfig;\n+        }\n+    }\n+}"
  },
  {
    "sha": "d789c24f0ed85bb20a86f10ca8b02b3c7f69fee5",
    "filename": "src/main/java/com/ibm/eventstreams/connect/cossink/parquet/COSParquetOutputFile.java",
    "status": "added",
    "additions": 89,
    "deletions": 0,
    "changes": 89,
    "blob_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/blob/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/parquet/COSParquetOutputFile.java",
    "raw_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/raw/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/parquet/COSParquetOutputFile.java",
    "contents_url": "https://api.github.com/repos/ibm-messaging/kafka-connect-ibmcos-sink/contents/src/main/java/com/ibm/eventstreams/connect/cossink/parquet/COSParquetOutputFile.java?ref=6ae26d0b4a687fc5f575fc5454002af1a235f9a3",
    "patch": "@@ -0,0 +1,89 @@\n+/*\n+ * IBM Confidential Source Materials\n+ * (C) Copyright IBM Corp. 2020\n+ *\n+ * The source code for this program is not published or otherwise\n+ * divested of its trade secrets, irrespective of what has\n+ * been deposited with the U.S. Copyright Office.\n+ *\n+ */\n+package com.ibm.eventstreams.connect.cossink.parquet;\n+\n+import com.ibm.cos.Bucket;\n+import org.apache.parquet.io.OutputFile;\n+import org.apache.parquet.io.PositionOutputStream;\n+\n+/**\n+ * This class is an implementation of interface OutputFile mainly used for\n+ * instantiating class AvroParquetWriter.\n+ * It is instantiated inside class ParquetRecordWriterProvider.\n+ *\n+ * @author Bill Li\n+ */\n+public class COSParquetOutputFile implements OutputFile {\n+\n+    private Bucket bucket;\n+    private String filename;\n+    private int parquetBufferSize;\n+    private COSParquetOutputStream cosOut;\n+\n+    /**\n+     * Setting values for creating COSParquetOutputStream\n+     * @param bucket COS bucket object\n+     * @param filename File name of Parquet file\n+     * @param parquetBufferSize Allocated Parquet buffer size\n+     */\n+    public COSParquetOutputFile(Bucket bucket, String filename, int parquetBufferSize) {\n+        this.bucket = bucket;\n+        this.filename = filename;\n+        this.parquetBufferSize = parquetBufferSize;\n+    }\n+\n+    /**\n+     * Getter method for COSParquetOutputStream object\n+     * @return COSParquetOutputStream object\n+     */\n+    public COSParquetOutputStream getCosOut() {\n+        return cosOut;\n+    }\n+\n+    /**\n+     * Create COSParquetOutputStream instance for writing Parquet output with\n+     * COS bucket object, file name, and Parquet buffer size\n+     * @param blockSizeHint Block size hint is not used\n+     * @return Position output stream\n+     */\n+    @Override\n+    public PositionOutputStream create(long blockSizeHint) {\n+        cosOut = new COSParquetOutputStream(bucket, filename, parquetBufferSize);\n+        return cosOut;\n+    }\n+\n+    /**\n+     * Calls create method to create COSParquetOutputStream\n+     * @param blockSizeHint Block size hint is not used\n+     * @return PositionOutputStream\n+     */\n+    @Override\n+    public PositionOutputStream createOrOverwrite(long blockSizeHint) {\n+        return create(blockSizeHint);\n+    }\n+\n+    /**\n+     * Not used\n+     * @return true or false\n+     */\n+    @Override\n+    public boolean supportsBlockSize() {\n+        return false;\n+    }\n+\n+    /**\n+     * Not used\n+     * @return block size of type long\n+     */\n+    @Override\n+    public long defaultBlockSize() {\n+        return 0;\n+    }\n+}"
  },
  {
    "sha": "027b47d5d591541e7414b7f1661115269f671e32",
    "filename": "src/main/java/com/ibm/eventstreams/connect/cossink/parquet/COSParquetOutputStream.java",
    "status": "added",
    "additions": 52,
    "deletions": 0,
    "changes": 52,
    "blob_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/blob/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/parquet/COSParquetOutputStream.java",
    "raw_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/raw/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/parquet/COSParquetOutputStream.java",
    "contents_url": "https://api.github.com/repos/ibm-messaging/kafka-connect-ibmcos-sink/contents/src/main/java/com/ibm/eventstreams/connect/cossink/parquet/COSParquetOutputStream.java?ref=6ae26d0b4a687fc5f575fc5454002af1a235f9a3",
    "patch": "@@ -0,0 +1,52 @@\n+/*\n+ * IBM Confidential Source Materials\n+ * (C) Copyright IBM Corp. 2020\n+ *\n+ * The source code for this program is not published or otherwise\n+ * divested of its trade secrets, irrespective of what has\n+ * been deposited with the U.S. Copyright Office.\n+ *\n+ */\n+package com.ibm.eventstreams.connect.cossink.parquet;\n+\n+import com.ibm.cos.Bucket;\n+\n+import java.io.IOException;\n+\n+/**\n+ * This class is responsible for passing values to COSOutputStream.\n+ * It is instantiated inside class COSParquetOutputFile.\n+ *\n+ * @author Bill Li\n+ */\n+public class COSParquetOutputStream extends COSOutputStream {\n+\n+    private volatile boolean commit;\n+\n+    /**\n+     * Instantiate parent class COSOutputStream\n+     * @param bucket COS bucket object\n+     * @param filename File name of Parquet file\n+     * @param parquetBufferSize Allocated Parquet buffer size\n+     */\n+    public COSParquetOutputStream(Bucket bucket, String filename, int parquetBufferSize) {\n+        super(bucket, filename, parquetBufferSize);\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+        if (commit) {\n+            super.commit();\n+            commit = false;\n+        } else {\n+            super.close();\n+        }\n+    }\n+\n+    /**\n+     * Setter method for commit\n+     */\n+    public void setCommit() {\n+        commit = true;\n+    }\n+}"
  },
  {
    "sha": "5390efd35280243d0c9c561e5a79719901c5d7c3",
    "filename": "src/main/java/com/ibm/eventstreams/connect/cossink/parquet/ParquetRecordWriterProvider.java",
    "status": "added",
    "additions": 150,
    "deletions": 0,
    "changes": 150,
    "blob_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/blob/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/parquet/ParquetRecordWriterProvider.java",
    "raw_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/raw/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/parquet/ParquetRecordWriterProvider.java",
    "contents_url": "https://api.github.com/repos/ibm-messaging/kafka-connect-ibmcos-sink/contents/src/main/java/com/ibm/eventstreams/connect/cossink/parquet/ParquetRecordWriterProvider.java?ref=6ae26d0b4a687fc5f575fc5454002af1a235f9a3",
    "patch": "@@ -0,0 +1,150 @@\n+/*\n+ * IBM Confidential Source Materials\n+ * (C) Copyright IBM Corp. 2020\n+ *\n+ * The source code for this program is not published or otherwise\n+ * divested of its trade secrets, irrespective of what has\n+ * been deposited with the U.S. Copyright Office.\n+ *\n+ */\n+package com.ibm.eventstreams.connect.cossink.parquet;\n+\n+import com.ibm.cos.Bucket;\n+import com.ibm.eventstreams.connect.cossink.registry.CachedSchemaRegistryClient;\n+import com.ibm.eventstreams.connect.cossink.registry.SchemaFileClient;\n+import com.ibm.eventstreams.connect.cossink.registry.SchemaRegistryClient;\n+import io.confluent.connect.avro.AvroData;\n+import io.confluent.connect.storage.format.RecordWriter;\n+import io.confluent.connect.storage.format.RecordWriterProvider;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+import org.apache.parquet.avro.AvroParquetWriter;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+\n+/**\n+ * This class prepares required configuration before writing Parquet to COS.\n+ * It is instantiated inside COSObject class and called from method writeParquet().\n+ *\n+ * @author Bill Li\n+ */\n+public class ParquetRecordWriterProvider implements RecordWriterProvider<COSParquetConfig> {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(ParquetRecordWriterProvider.class);\n+    private static final String EXTENSION = \".parquet\";\n+    private final Bucket bucket;\n+    private final AvroData avroDataHelper;\n+\n+    /**\n+     * Constructor for ParquetRecordWriterProvider\n+     * @param bucket COS bucket object for writing parquet output\n+     * @param avroData Confluent utility class for converting Connect data to Parquet data\n+     */\n+    public ParquetRecordWriterProvider(Bucket bucket, AvroData avroData) {\n+        this.bucket = bucket;\n+        this.avroDataHelper = avroData;\n+    }\n+\n+    /**\n+     * Returns Parquet Extension\n+     * @return '.parquet' to append to all file names\n+     */\n+    @Override\n+    public String getExtension() {\n+        return EXTENSION;\n+    }\n+\n+    /**\n+     * Define Parquet output format and write Parquet output\n+     * @param cosAvroParquetConfig: Configuration parameters to write parquet output\n+     * @param filename: File name generated from createKeys() in COSObject\n+     * @return Record Writer that writes parquet output\n+     */\n+    @Override\n+    public RecordWriter getRecordWriter(COSParquetConfig cosParquetConfig, String filename) {\n+\n+        SchemaRegistryClient schemaRegistryClient = new CachedSchemaRegistryClient(\n+                cosParquetConfig.getSchemaRegistryUrl(),\n+                cosParquetConfig.getSchemaRegistryApiKey()\n+        );\n+        org.apache.avro.Schema avroSchema = schemaRegistryClient.getBySubject(cosParquetConfig.getSchemaSubject());\n+\n+        return new RecordWriter() {\n+            Schema connectSchema = null;\n+            ParquetWriter<GenericRecord> writer;\n+            COSParquetOutputFile cosParquetOutputFile;\n+\n+            /**\n+             * Write record as Parquet to COS\n+             * @param record record to be written to COS (format: SinkRecord)\n+             */\n+            @Override\n+            public void write(SinkRecord record) {\n+                if (connectSchema == null) {\n+                    connectSchema = record.valueSchema();\n+                }\n+\n+                try {\n+                    String fileFullName = filename\n+                            + cosParquetConfig.getParquetCompressionCodec().getExtension()\n+                            + getExtension();\n+\n+                    cosParquetOutputFile = new COSParquetOutputFile(bucket,\n+                            fileFullName, cosParquetConfig.getParquetBufferSize());\n+\n+                    writer = AvroParquetWriter\n+                            .<GenericRecord>builder(cosParquetOutputFile)\n+                            .withSchema(avroSchema)\n+                            .withWriteMode(cosParquetConfig.getParquetWriteMode())\n+                            .withCompressionCodec(cosParquetConfig.getParquetCompressionCodec())\n+                            .withRowGroupSize(cosParquetConfig.getParquetRowGroupSize())\n+                            .withPageSize(cosParquetConfig.getParquetPageSize())\n+                            .withDictionaryEncoding(cosParquetConfig.isParquetDictionaryEncoding())\n+                            .build();\n+                } catch (Exception e) {\n+                    throw new IllegalStateException(e);\n+                }\n+\n+                LOG.trace(\"Sink record: {}\", record.toString());\n+                Object value = avroDataHelper.fromConnectData(connectSchema, record.value());\n+                try {\n+                    writer.write((GenericRecord) value);\n+                } catch (IOException e) {\n+                    throw new ConnectException(e);\n+                }\n+            }\n+\n+            /**\n+             * Called when commit method is executed\n+             */\n+            @Override\n+            public void close() {\n+                try {\n+                    writer.close();\n+                } catch (IOException e) {\n+                    throw new ConnectException(e);\n+                }\n+            }\n+\n+            /**\n+             * Called when write operation completes\n+             */\n+            @Override\n+            public void commit() {\n+                try {\n+                    cosParquetOutputFile.getCosOut().setCommit();\n+                    if (writer != null) {\n+                        writer.close();\n+                    }\n+                } catch (IOException e) {\n+                    throw new ConnectException(e);\n+                }\n+            }\n+        };\n+    }\n+}"
  },
  {
    "sha": "6afea1a13e4c291fdecc5bde35c47a7e51ae1b77",
    "filename": "src/main/java/com/ibm/eventstreams/connect/cossink/partitionwriter/COSObject.java",
    "status": "modified",
    "additions": 33,
    "deletions": 1,
    "changes": 34,
    "blob_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/blob/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/partitionwriter/COSObject.java",
    "raw_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/raw/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/partitionwriter/COSObject.java",
    "contents_url": "https://api.github.com/repos/ibm-messaging/kafka-connect-ibmcos-sink/contents/src/main/java/com/ibm/eventstreams/connect/cossink/partitionwriter/COSObject.java?ref=6ae26d0b4a687fc5f575fc5454002af1a235f9a3",
    "patch": "@@ -24,6 +24,12 @@\n import java.util.LinkedList;\n import java.util.List;\n \n+import com.ibm.eventstreams.connect.cossink.parquet.COSParquetConfig;\n+import com.ibm.eventstreams.connect.cossink.parquet.ParquetRecordWriterProvider;\n+import io.confluent.connect.avro.AvroData;\n+import io.confluent.connect.avro.AvroDataConfig;\n+import io.confluent.connect.storage.format.RecordWriter;\n+import io.confluent.connect.storage.format.RecordWriterProvider;\n import org.apache.kafka.connect.data.Schema;\n import org.apache.kafka.connect.data.Schema.Type;\n import org.apache.kafka.connect.sink.SinkRecord;\n@@ -59,7 +65,7 @@ void put(SinkRecord record) {\n         LOG.trace(\"< put\");\n     }\n \n-    void write(final Bucket bucket) {\n+    void writeString(final Bucket bucket) {\n         LOG.trace(\"> write, records.size={} lastOffset={}\", records.size(), lastOffset);\n         if (records.isEmpty()) {\n             throw new IllegalStateException(\"Attempting to write an empty object\");\n@@ -72,6 +78,32 @@ void write(final Bucket bucket) {\n         LOG.trace(\"< write, key={}\", key);\n     }\n \n+    void writeParquet(final Bucket bucket, final COSParquetConfig cosParquetConfig) {\n+        LOG.trace(\"> write parquet, records.size={} lastOffset={}\", records.size(), lastOffset);\n+        if (records.isEmpty()) {\n+            throw new IllegalStateException(\"Attempting to write an empty Parquet object\");\n+        }\n+\n+        final String key = createKey();\n+\n+        AvroDataConfig avroDataConfig = new AvroDataConfig.Builder()\n+                .with(\"enhanced.avro.schema.support\", cosParquetConfig.isEnhancedSchemaSupport())\n+                .with(\"schemas.cache.config\", cosParquetConfig.getSchemaCacheSize())\n+                .build();\n+        AvroData avroDataHelper = new AvroData(avroDataConfig);\n+\n+        RecordWriterProvider<COSParquetConfig> recordWriterProvider = new ParquetRecordWriterProvider(bucket, avroDataHelper);\n+        RecordWriter recordWriter = recordWriterProvider.getRecordWriter(cosParquetConfig, key);\n+\n+        for (SinkRecord record : records) {\n+            recordWriter.write(record);\n+        }\n+\n+        recordWriter.commit();\n+\n+        LOG.trace(\"< write parquet, key={}\", key);\n+    }\n+\n     Long lastOffset() {\n         return lastOffset;\n     }"
  },
  {
    "sha": "a724915e4c0d8e2ac0642c143806cb85e93f77f9",
    "filename": "src/main/java/com/ibm/eventstreams/connect/cossink/partitionwriter/COSPartitionWriter.java",
    "status": "modified",
    "additions": 13,
    "deletions": 2,
    "changes": 15,
    "blob_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/blob/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/partitionwriter/COSPartitionWriter.java",
    "raw_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/raw/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/partitionwriter/COSPartitionWriter.java",
    "contents_url": "https://api.github.com/repos/ibm-messaging/kafka-connect-ibmcos-sink/contents/src/main/java/com/ibm/eventstreams/connect/cossink/partitionwriter/COSPartitionWriter.java?ref=6ae26d0b4a687fc5f575fc5454002af1a235f9a3",
    "patch": "@@ -17,6 +17,7 @@\n \n import java.util.concurrent.atomic.AtomicReference;\n \n+import com.ibm.eventstreams.connect.cossink.parquet.COSParquetConfig;\n import org.apache.kafka.connect.sink.SinkRecord;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n@@ -37,13 +38,19 @@\n     private Long objectCount = 0L;\n     private Boolean delimitRecords;\n \n+    private COSParquetConfig cosParquetConfig;\n+\n     private AtomicReference<Long> lastOffset = new AtomicReference<>();\n \n-    COSPartitionWriter(final Bucket bucket, final CompletionCriteriaSet completionCriteria, final Boolean delimitRecords) {\n+    COSPartitionWriter(final Bucket bucket,\n+                       final CompletionCriteriaSet completionCriteria,\n+                       final Boolean delimitRecords,\n+                       final COSParquetConfig cosParquetConfig) {\n         super(RequestType.CLOSE);\n         this.bucket = bucket;\n         this.completionCriteria = completionCriteria;\n         this.delimitRecords = delimitRecords;\n+        this.cosParquetConfig = cosParquetConfig;\n     }\n \n     @Override\n@@ -76,7 +83,11 @@ public void close() {\n \n     private void writeObject() {\n         objectCount++;\n-        osObject.write(bucket);\n+        if (cosParquetConfig == null) {\n+            osObject.writeString(bucket);\n+        } else {\n+            osObject.writeParquet(bucket, cosParquetConfig);\n+        }\n         lastOffset.set(osObject.lastOffset());\n         osObject = null;\n         completionCriteria.complete();"
  },
  {
    "sha": "07029ff042178c2699fb29155c48d5017d25cf9c",
    "filename": "src/main/java/com/ibm/eventstreams/connect/cossink/partitionwriter/COSPartitionWriterFactory.java",
    "status": "modified",
    "additions": 6,
    "deletions": 2,
    "changes": 8,
    "blob_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/blob/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/partitionwriter/COSPartitionWriterFactory.java",
    "raw_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/raw/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/partitionwriter/COSPartitionWriterFactory.java",
    "contents_url": "https://api.github.com/repos/ibm-messaging/kafka-connect-ibmcos-sink/contents/src/main/java/com/ibm/eventstreams/connect/cossink/partitionwriter/COSPartitionWriterFactory.java?ref=6ae26d0b4a687fc5f575fc5454002af1a235f9a3",
    "patch": "@@ -17,15 +17,19 @@\n \n import com.ibm.cos.Bucket;\n import com.ibm.eventstreams.connect.cossink.completion.CompletionCriteriaSet;\n+import com.ibm.eventstreams.connect.cossink.parquet.COSParquetConfig;\n \n public class COSPartitionWriterFactory implements PartitionWriterFactory {\n \n     public COSPartitionWriterFactory() {\n     }\n \n     @Override\n-    public PartitionWriter newPartitionWriter(final Bucket bucket, final CompletionCriteriaSet completionCriteria, final Boolean delimitRecords) {\n-        return new COSPartitionWriter(bucket, completionCriteria, delimitRecords);\n+    public PartitionWriter newPartitionWriter(final Bucket bucket,\n+                                              final CompletionCriteriaSet completionCriteria,\n+                                              final Boolean delimitRecords,\n+                                              final COSParquetConfig cosParquetConfig) {\n+        return new COSPartitionWriter(bucket, completionCriteria, delimitRecords, cosParquetConfig);\n     }\n \n }"
  },
  {
    "sha": "99a260568270f19d7ff4e2a26f635caa39d22e7f",
    "filename": "src/main/java/com/ibm/eventstreams/connect/cossink/partitionwriter/PartitionWriterFactory.java",
    "status": "modified",
    "additions": 5,
    "deletions": 2,
    "changes": 7,
    "blob_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/blob/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/partitionwriter/PartitionWriterFactory.java",
    "raw_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/raw/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/partitionwriter/PartitionWriterFactory.java",
    "contents_url": "https://api.github.com/repos/ibm-messaging/kafka-connect-ibmcos-sink/contents/src/main/java/com/ibm/eventstreams/connect/cossink/partitionwriter/PartitionWriterFactory.java?ref=6ae26d0b4a687fc5f575fc5454002af1a235f9a3",
    "patch": "@@ -17,6 +17,7 @@\n \n import com.ibm.cos.Bucket;\n import com.ibm.eventstreams.connect.cossink.completion.CompletionCriteriaSet;\n+import com.ibm.eventstreams.connect.cossink.parquet.COSParquetConfig;\n \n public interface PartitionWriterFactory {\n \n@@ -27,6 +28,8 @@\n      *           have been read and can be written as an object storage object.\n      * @return\n      */\n-    PartitionWriter newPartitionWriter(\n-            final Bucket bucket, final CompletionCriteriaSet completionCriteira, final Boolean recordDelimiter);\n+    PartitionWriter newPartitionWriter(final Bucket bucket,\n+                                       final CompletionCriteriaSet completionCriteira,\n+                                       final Boolean recordDelimiter,\n+                                       final COSParquetConfig cosParquetConfig);\n }"
  },
  {
    "sha": "52e27cb441dc03f3eb5eb3e8bd3abff6930dad00",
    "filename": "src/main/java/com/ibm/eventstreams/connect/cossink/registry/CachedSchemaRegistryClient.java",
    "status": "added",
    "additions": 47,
    "deletions": 0,
    "changes": 47,
    "blob_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/blob/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/registry/CachedSchemaRegistryClient.java",
    "raw_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/raw/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/registry/CachedSchemaRegistryClient.java",
    "contents_url": "https://api.github.com/repos/ibm-messaging/kafka-connect-ibmcos-sink/contents/src/main/java/com/ibm/eventstreams/connect/cossink/registry/CachedSchemaRegistryClient.java?ref=6ae26d0b4a687fc5f575fc5454002af1a235f9a3",
    "patch": "@@ -0,0 +1,47 @@\n+package com.ibm.eventstreams.connect.cossink.registry;\n+\n+import org.apache.avro.Schema;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class CachedSchemaRegistryClient implements SchemaRegistryClient {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(CachedSchemaRegistryClient.class);\n+\n+    private EventStreamRestService eventStreamRestService;\n+    private Map<String, Schema> subjectSchemaMap;\n+\n+    public CachedSchemaRegistryClient(String baseUrl, String apiKey) {\n+\n+        this.eventStreamRestService = new EventStreamRestService(baseUrl, apiKey);\n+        this.subjectSchemaMap = new HashMap<>();\n+    }\n+\n+    @Override\n+    public synchronized Schema getSchemaBySubjectFromRegistry(String subject) throws IOException {\n+\n+        return eventStreamRestService.getSchemaBySubject(subject);\n+    }\n+\n+    @Override\n+    public synchronized Schema getBySubject(String subject) {\n+\n+        Schema cachedSchema = subjectSchemaMap.get(subject);\n+        if (cachedSchema != null) {\n+            return cachedSchema;\n+        } else {\n+            Schema retrievedSchema = null;\n+            try {\n+                retrievedSchema = this.getSchemaBySubjectFromRegistry(subject);\n+            } catch (IOException e) {\n+                throw new IllegalStateException(\"Unable to parse Avro schema\", e);\n+            }\n+            subjectSchemaMap.put(subject, retrievedSchema);\n+            return retrievedSchema;\n+        }\n+    }\n+}"
  },
  {
    "sha": "c91218ead7a0400be5f686a615ae662f7739cf43",
    "filename": "src/main/java/com/ibm/eventstreams/connect/cossink/registry/EventStreamRestService.java",
    "status": "added",
    "additions": 77,
    "deletions": 0,
    "changes": 77,
    "blob_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/blob/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/registry/EventStreamRestService.java",
    "raw_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/raw/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/registry/EventStreamRestService.java",
    "contents_url": "https://api.github.com/repos/ibm-messaging/kafka-connect-ibmcos-sink/contents/src/main/java/com/ibm/eventstreams/connect/cossink/registry/EventStreamRestService.java?ref=6ae26d0b4a687fc5f575fc5454002af1a235f9a3",
    "patch": "@@ -0,0 +1,77 @@\n+package com.ibm.eventstreams.connect.cossink.registry;\n+\n+import org.apache.avro.Schema;\n+import org.apache.http.HttpHeaders;\n+import org.apache.http.HttpResponse;\n+import org.apache.http.client.HttpClient;\n+import org.apache.http.client.methods.HttpGet;\n+import org.apache.http.impl.client.HttpClientBuilder;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.Base64;\n+\n+public class EventStreamRestService {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(EventStreamRestService.class);\n+\n+    private String baseUrl;\n+    private String apiKey;\n+    private Schema.Parser schemaParser;\n+\n+    public EventStreamRestService(String baseUrl, String apiKey) {\n+\n+        this.baseUrl = baseUrl;\n+        this.apiKey = apiKey;\n+        this.schemaParser = new Schema.Parser();\n+    }\n+\n+    public Schema getSchemaBySubject(String subject) throws IOException {\n+\n+        String schemaPath = getSchemaPath(subject);\n+        return this.httpRequest(schemaPath);\n+    }\n+\n+    private String getSchemaPath(String subject) {\n+\n+        return String.format(\"%s/artifacts/%s\", baseUrl, subject);\n+    }\n+\n+    private Schema httpRequest(String schemaPath) throws IOException {\n+\n+        HttpClient httpClient = HttpClientBuilder.create().build();\n+\n+        Base64.Encoder encoder = Base64.getEncoder();\n+        String encoderString = String.format(\"token:%s\", apiKey);\n+        String encoding = encoder.encodeToString(encoderString.getBytes());\n+\n+        HttpGet httpGet = new HttpGet(schemaPath);\n+        httpGet.setHeader(HttpHeaders.AUTHORIZATION, \"Basic \" + encoding);\n+\n+        return this.sendRequest(httpClient, httpGet);\n+    }\n+\n+    private Schema sendRequest(HttpClient httpClient, HttpGet httpGet) throws IOException {\n+\n+        HttpResponse response;\n+\n+        try {\n+            response = httpClient.execute(httpGet);\n+        } catch (IOException e) {\n+            throw new IllegalStateException(\"Unable to get response from client\", e);\n+        }\n+\n+        InputStream schemaStream = response.getEntity().getContent();\n+        Schema avroSchema = toAvroSchema(schemaStream);\n+        schemaStream.close();\n+\n+        return avroSchema;\n+    }\n+\n+    private Schema toAvroSchema(InputStream schemaStream) throws IOException {\n+\n+        return schemaParser.parse(schemaStream);\n+    }\n+}"
  },
  {
    "sha": "7c3ea5149f12d7552925c43589b9d2fc9d660aa6",
    "filename": "src/main/java/com/ibm/eventstreams/connect/cossink/registry/SchemaFileClient.java",
    "status": "added",
    "additions": 36,
    "deletions": 0,
    "changes": 36,
    "blob_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/blob/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/registry/SchemaFileClient.java",
    "raw_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/raw/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/registry/SchemaFileClient.java",
    "contents_url": "https://api.github.com/repos/ibm-messaging/kafka-connect-ibmcos-sink/contents/src/main/java/com/ibm/eventstreams/connect/cossink/registry/SchemaFileClient.java?ref=6ae26d0b4a687fc5f575fc5454002af1a235f9a3",
    "patch": "@@ -0,0 +1,36 @@\n+package com.ibm.eventstreams.connect.cossink.registry;\n+\n+import org.apache.avro.Schema;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+import java.io.IOException;\n+\n+public class SchemaFileClient {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(SchemaFileClient.class);\n+\n+    private String filePath;\n+    private Schema.Parser schemaParser;\n+\n+    public SchemaFileClient(String filePath) {\n+\n+        this.filePath = filePath;\n+        this.schemaParser = new Schema.Parser();\n+    }\n+\n+    public Schema getByFile() {\n+\n+        Schema avroSchema;\n+\n+        try {\n+            File avroSchemaFile = new File(filePath);\n+            avroSchema = schemaParser.parse(avroSchemaFile);\n+        } catch (IOException e) {\n+            throw new IllegalStateException(\"Unable to parse Avro schema\", e);\n+        }\n+\n+        return avroSchema;\n+    }\n+}"
  },
  {
    "sha": "69d139b9d443b65d95a983cd089abe4da1051305",
    "filename": "src/main/java/com/ibm/eventstreams/connect/cossink/registry/SchemaRegistryClient.java",
    "status": "added",
    "additions": 12,
    "deletions": 0,
    "changes": 12,
    "blob_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/blob/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/registry/SchemaRegistryClient.java",
    "raw_url": "https://github.com/ibm-messaging/kafka-connect-ibmcos-sink/raw/6ae26d0b4a687fc5f575fc5454002af1a235f9a3/src/main/java/com/ibm/eventstreams/connect/cossink/registry/SchemaRegistryClient.java",
    "contents_url": "https://api.github.com/repos/ibm-messaging/kafka-connect-ibmcos-sink/contents/src/main/java/com/ibm/eventstreams/connect/cossink/registry/SchemaRegistryClient.java?ref=6ae26d0b4a687fc5f575fc5454002af1a235f9a3",
    "patch": "@@ -0,0 +1,12 @@\n+package com.ibm.eventstreams.connect.cossink.registry;\n+\n+import org.apache.avro.Schema;\n+\n+import java.io.IOException;\n+\n+public interface SchemaRegistryClient {\n+\n+    Schema getSchemaBySubjectFromRegistry(String subject) throws IOException;\n+\n+    Schema getBySubject(String subject);\n+}"
  }
]
