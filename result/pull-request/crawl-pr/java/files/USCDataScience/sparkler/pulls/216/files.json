[
  {
    "sha": "aa1034c4a23aa6dd65af004be2c6bf86e28f2baf",
    "filename": "sparkler-core/conf/sparkler-default.yaml",
    "status": "modified",
    "additions": 12,
    "deletions": 8,
    "changes": 20,
    "blob_url": "https://github.com/USCDataScience/sparkler/blob/6e546171082fd11e562442e3568222f94343046f/sparkler-core/conf/sparkler-default.yaml",
    "raw_url": "https://github.com/USCDataScience/sparkler/raw/6e546171082fd11e562442e3568222f94343046f/sparkler-core/conf/sparkler-default.yaml",
    "contents_url": "https://api.github.com/repos/USCDataScience/sparkler/contents/sparkler-core/conf/sparkler-default.yaml?ref=6e546171082fd11e562442e3568222f94343046f",
    "patch": "@@ -17,16 +17,20 @@\n \n ##################### General Properties ################################\n \n-# Crawl Database URL. Stores crawl metadata and status updates.\n-# Type: String. Default: http://localhost:8983/solr/crawldb\n-# for standalone server\n-crawldb.uri: http://localhost:8983/solr/crawldb\n+# uri - Crawl Database URL. Stores crawl metadata and status updates.\n \n-# for quick test crawls using embedded solr\n-#crawldb.uri: file://conf/solr/crawldb\n+crawldb.backend: solr  # \"solr\" is default until \"elasticsearch\" becomes usable.\n \n-# for cloudmode with zookeepers; Format = collectionName::zkhost1:port1,zkhost2:port2,zkhost3:port3\n-# crawldb.uri: crawldb::localhost:9983\n+# Type: String. Default: http://localhost:8983/solr/crawldb\n+# for standalone server\n+# For quick test crawls using embedded solr\n+# solr.uri: file://conf/solr/crawldb\n+# For cloudmode with zookeepers; Format = collectionName::zkhost1:port1,zkhost2:port2,zkhost3:port3\n+# solr.uri: crawldb::localhost:9983\n+solr.uri: http://localhost:8983/solr/crawldb\n+\n+# elasticsearch settings\n+elasticsearch.uri: http://localhost:9200\n \n \n ##################### Apache Spark Properties ###########################"
  },
  {
    "sha": "26c671357439f76c8f90fd0ae826e0dd89206abe",
    "filename": "sparkler-core/sparkler-api/src/main/java/edu/usc/irds/sparkler/Constants.java",
    "status": "modified",
    "additions": 2,
    "deletions": 2,
    "changes": 4,
    "blob_url": "https://github.com/USCDataScience/sparkler/blob/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-api/src/main/java/edu/usc/irds/sparkler/Constants.java",
    "raw_url": "https://github.com/USCDataScience/sparkler/raw/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-api/src/main/java/edu/usc/irds/sparkler/Constants.java",
    "contents_url": "https://api.github.com/repos/USCDataScience/sparkler/contents/sparkler-core/sparkler-api/src/main/java/edu/usc/irds/sparkler/Constants.java?ref=6e546171082fd11e562442e3568222f94343046f",
    "patch": "@@ -38,7 +38,7 @@\n \n         // General Properties\n         @ConfigKey\n-        String CRAWLDB = \"crawldb.uri\";\n+        String CRAWLDB_BACKEND = \"crawldb.backend\";\n \n         // Apache Spark Properties\n         @ConfigKey\n@@ -122,7 +122,7 @@ public static SparklerConfiguration newDefaultConfig(){\n         String SPARKLER_DEFAULT = \"sparkler-default.yaml\";\n     }\n \n-    interface solr { // Solr Fields\n+    interface storage { // Storage Fields\n         String ID = \"id\";\n         String CRAWL_ID = \"crawl_id\";\n         String URL = \"url\";"
  },
  {
    "sha": "b8b19950643e42970e4eb446e933cd273fa6d050",
    "filename": "sparkler-core/sparkler-api/src/main/java/edu/usc/irds/sparkler/SparklerConfiguration.java",
    "status": "modified",
    "additions": 5,
    "deletions": 0,
    "changes": 5,
    "blob_url": "https://github.com/USCDataScience/sparkler/blob/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-api/src/main/java/edu/usc/irds/sparkler/SparklerConfiguration.java",
    "raw_url": "https://github.com/USCDataScience/sparkler/raw/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-api/src/main/java/edu/usc/irds/sparkler/SparklerConfiguration.java",
    "contents_url": "https://api.github.com/repos/USCDataScience/sparkler/contents/sparkler-core/sparkler-api/src/main/java/edu/usc/irds/sparkler/SparklerConfiguration.java?ref=6e546171082fd11e562442e3568222f94343046f",
    "patch": "@@ -102,4 +102,9 @@ private static Map deepMerge(Map original, Map newMap) {\n         return original;\n     }\n \n+    public String getDatabaseURI() {\n+        String dbToUse = (String) this.getOrDefault(Constants.key.CRAWLDB_BACKEND, \"solr\"); // solr is default\n+        return (String) this.get(dbToUse+\".uri\");\n+    }\n+\n }"
  },
  {
    "sha": "37020ff40c0b02c5c05bfe61b42bd01e4d545e30",
    "filename": "sparkler-core/sparkler-api/src/main/java/edu/usc/irds/sparkler/storage/solr/schema/FieldMapper.java",
    "status": "renamed",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/USCDataScience/sparkler/blob/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-api/src/main/java/edu/usc/irds/sparkler/storage/solr/schema/FieldMapper.java",
    "raw_url": "https://github.com/USCDataScience/sparkler/raw/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-api/src/main/java/edu/usc/irds/sparkler/storage/solr/schema/FieldMapper.java",
    "contents_url": "https://api.github.com/repos/USCDataScience/sparkler/contents/sparkler-core/sparkler-api/src/main/java/edu/usc/irds/sparkler/storage/solr/schema/FieldMapper.java?ref=6e546171082fd11e562442e3568222f94343046f",
    "patch": "@@ -15,7 +15,7 @@\n  * limitations under the License.\n  */\n \n-package edu.usc.irds.sparkler.solr.schema;\n+package edu.usc.irds.sparkler.storage.solr.schema;\n \n import org.json.simple.parser.ParseException;\n import org.slf4j.Logger;",
    "previous_filename": "sparkler-core/sparkler-api/src/main/java/edu/usc/irds/sparkler/solr/schema/FieldMapper.java"
  },
  {
    "sha": "3856330681cb16f289c2b53ebbdd0c1caf9b4cfd",
    "filename": "sparkler-core/sparkler-api/src/main/java/edu/usc/irds/sparkler/storage/solr/schema/StringEvaluator.java",
    "status": "renamed",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/USCDataScience/sparkler/blob/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-api/src/main/java/edu/usc/irds/sparkler/storage/solr/schema/StringEvaluator.java",
    "raw_url": "https://github.com/USCDataScience/sparkler/raw/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-api/src/main/java/edu/usc/irds/sparkler/storage/solr/schema/StringEvaluator.java",
    "contents_url": "https://api.github.com/repos/USCDataScience/sparkler/contents/sparkler-core/sparkler-api/src/main/java/edu/usc/irds/sparkler/storage/solr/schema/StringEvaluator.java?ref=6e546171082fd11e562442e3568222f94343046f",
    "patch": "@@ -15,7 +15,7 @@\n  * limitations under the License.\n  */\n \n-package edu.usc.irds.sparkler.solr.schema;\n+package edu.usc.irds.sparkler.storage.solr.schema;\n \n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;",
    "previous_filename": "sparkler-core/sparkler-api/src/main/java/edu/usc/irds/sparkler/solr/schema/StringEvaluator.java"
  },
  {
    "sha": "0e93f22e81efa940ce207c82e390f1b73bb5219a",
    "filename": "sparkler-core/sparkler-api/src/test/resources/sparkler-default.yaml",
    "status": "modified",
    "additions": 13,
    "deletions": 3,
    "changes": 16,
    "blob_url": "https://github.com/USCDataScience/sparkler/blob/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-api/src/test/resources/sparkler-default.yaml",
    "raw_url": "https://github.com/USCDataScience/sparkler/raw/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-api/src/test/resources/sparkler-default.yaml",
    "contents_url": "https://api.github.com/repos/USCDataScience/sparkler/contents/sparkler-core/sparkler-api/src/test/resources/sparkler-default.yaml?ref=6e546171082fd11e562442e3568222f94343046f",
    "patch": "@@ -17,10 +17,20 @@\n \n ##################### General Properties ################################\n \n-# Crawl Database URL. Stores crawl metadata and status updates.\n-# Type: String. Default: http://localhost:8983/solr/crawldb\n-crawldb.uri: file://conf/solr/crawldb\n+# uri - Crawl Database URL. Stores crawl metadata and status updates.\n+\n+crawldb.backend: solr  # \"solr\" is default until \"elasticsearch\" becomes usable.\n \n+# Type: String. Default: http://localhost:8983/solr/crawldb\n+# for standalone server\n+# For quick test crawls using embedded solr\n+# solr.uri: file://conf/solr/crawldb\n+# For cloudmode with zookeepers; Format = collectionName::zkhost1:port1,zkhost2:port2,zkhost3:port3\n+# solr.uri: crawldb::localhost:9983\n+solr.uri: http://localhost:8983/solr/crawldb\n+\n+# elasticsearch settings\n+elasticsearch.uri: http://localhost:9200\n \n \n ##################### Apache Spark Properties ###########################"
  },
  {
    "sha": "2e8e37311bc661feee1dfd6e76f152c0f72a8c74",
    "filename": "sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/CrawlDbRDD.scala",
    "status": "modified",
    "additions": 10,
    "deletions": 10,
    "changes": 20,
    "blob_url": "https://github.com/USCDataScience/sparkler/blob/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/CrawlDbRDD.scala",
    "raw_url": "https://github.com/USCDataScience/sparkler/raw/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/CrawlDbRDD.scala",
    "contents_url": "https://api.github.com/repos/USCDataScience/sparkler/contents/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/CrawlDbRDD.scala?ref=6e546171082fd11e562442e3568222f94343046f",
    "patch": "@@ -19,7 +19,7 @@ package edu.usc.irds.sparkler\n \n import edu.usc.irds.sparkler.base.Loggable\n import edu.usc.irds.sparkler.model.{Resource, ResourceStatus, SparklerJob}\n-import edu.usc.irds.sparkler.solr.SolrGroupPartition\n+import edu.usc.irds.sparkler.storage.solr.SolrGroupPartition\n import edu.usc.irds.sparkler.util.SolrResultIterator\n import org.apache.solr.client.solrj.SolrQuery\n import org.apache.solr.client.solrj.util.ClientUtils\n@@ -50,27 +50,27 @@ class CrawlDbRDD(sc: SparkContext,\n     val partition: SolrGroupPartition = split.asInstanceOf[SolrGroupPartition]\n     val batchSize = 100\n     val query = new SolrQuery(generateQry)\n-    query.addFilterQuery(s\"\"\"${Constants.solr.GROUP}:\"${escapeQueryChars(partition.group)}\"\"\"\")\n-    query.addFilterQuery(s\"${Constants.solr.CRAWL_ID}:${job.id}\")\n+    query.addFilterQuery(s\"\"\"${Constants.storage.GROUP}:\"${escapeQueryChars(partition.group)}\"\"\"\")\n+    query.addFilterQuery(s\"${Constants.storage.CRAWL_ID}:${job.id}\")\n     query.set(\"sort\", sortBy)\n     query.setRows(batchSize)\n \n-    new SolrResultIterator[Resource](job.newCrawlDbSolrClient().crawlDb, query,\n+    new SolrResultIterator[Resource](job.newStorageProxy().getClient(), query,\n       batchSize, classOf[Resource], closeClient = true, limit = topN)\n   }\n \n   override protected def getPartitions: Array[Partition] = {\n     val qry = new SolrQuery(generateQry)\n-    qry.addFilterQuery(s\"${Constants.solr.CRAWL_ID}:${job.id}\")\n+    qry.addFilterQuery(s\"${Constants.storage.CRAWL_ID}:${job.id}\")\n     qry.set(\"sort\", sortBy)\n     qry.set(\"group\", true)\n     qry.set(\"group.ngroups\", true)\n     qry.set(\"group.field\", groupBy)\n     qry.set(\"group.limit\", 0)\n     qry.setRows(maxGroups)\n-    val proxy = job.newCrawlDbSolrClient()\n-    val solr = proxy.crawlDb\n-    val groupRes = solr.query(qry).getGroupResponse.getValues.get(0)\n+    val proxy = job.newStorageProxy()\n+    val client = proxy.getClient()\n+    val groupRes = client.query(qry).getGroupResponse.getValues.get(0)\n     val grps = groupRes.getValues\n     CrawlDbRDD.LOG.info(s\"selecting ${grps.size()} out of ${groupRes.getNGroups}\")\n     val res = new Array[Partition](grps.size())\n@@ -86,8 +86,8 @@ class CrawlDbRDD(sc: SparkContext,\n \n object CrawlDbRDD extends Loggable {\n \n-  val DEFAULT_ORDER = Constants.solr.DISCOVER_DEPTH + \" asc,\" + Constants.solr.SCORE + \" asc\"\n-  val DEFAULT_FILTER_QRY = Constants.solr.STATUS + \":\" + ResourceStatus.UNFETCHED\n+  val DEFAULT_ORDER = Constants.storage.DISCOVER_DEPTH + \" asc,\" + Constants.storage.SCORE + \" asc\"\n+  val DEFAULT_FILTER_QRY = Constants.storage.STATUS + \":\" + ResourceStatus.UNFETCHED\n   val DEFAULT_GROUPS = 1000\n   val DEFAULT_TOPN = 1000\n   val DEFAULT_GROUPBY = \"group\""
  },
  {
    "sha": "5b60e76c7aceabd96e65177576042e7a74b8131e",
    "filename": "sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/MemexCrawlDbRDD.scala",
    "status": "modified",
    "additions": 11,
    "deletions": 11,
    "changes": 22,
    "blob_url": "https://github.com/USCDataScience/sparkler/blob/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/MemexCrawlDbRDD.scala",
    "raw_url": "https://github.com/USCDataScience/sparkler/raw/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/MemexCrawlDbRDD.scala",
    "contents_url": "https://api.github.com/repos/USCDataScience/sparkler/contents/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/MemexCrawlDbRDD.scala?ref=6e546171082fd11e562442e3568222f94343046f",
    "patch": "@@ -2,7 +2,7 @@ package edu.usc.irds.sparkler\n \n import edu.usc.irds.sparkler.base.Loggable\n import edu.usc.irds.sparkler.model.{Resource, ResourceStatus, SparklerJob}\n-import edu.usc.irds.sparkler.solr.SolrGroupPartition\n+import edu.usc.irds.sparkler.storage.solr.SolrGroupPartition\n import edu.usc.irds.sparkler.util.SolrResultIterator\n import org.apache.solr.client.solrj.SolrQuery\n import org.apache.solr.client.solrj.util.ClientUtils.escapeQueryChars\n@@ -29,27 +29,27 @@ class MemexCrawlDbRDD(sc: SparkContext,\n     val partition: SolrGroupPartition = split.asInstanceOf[SolrGroupPartition]\n     val batchSize = 100\n     val query = new SolrQuery(generateQry)\n-    query.addFilterQuery(s\"\"\"${Constants.solr.PARENT}:\"${escapeQueryChars(partition.group)}\"\"\"\")\n-    query.addFilterQuery(s\"${Constants.solr.CRAWL_ID}:${job.id}\")\n+    query.addFilterQuery(s\"\"\"${Constants.storage.PARENT}:\"${escapeQueryChars(partition.group)}\"\"\"\")\n+    query.addFilterQuery(s\"${Constants.storage.CRAWL_ID}:${job.id}\")\n     query.set(\"sort\", sortBy)\n     query.setRows(batchSize)\n \n-    new SolrResultIterator[Resource](job.newCrawlDbSolrClient().crawlDb, query,\n+    new SolrResultIterator[Resource](job.newStorageProxy().getClient(), query,\n       batchSize, classOf[Resource], closeClient = true, limit = topN)\n   }\n \n   override protected def getPartitions: Array[Partition] = {\n     val qry = new SolrQuery(generateQry)\n-    qry.addFilterQuery(s\"${Constants.solr.CRAWL_ID}:${job.id}\")\n+    qry.addFilterQuery(s\"${Constants.storage.CRAWL_ID}:${job.id}\")\n     qry.set(\"sort\", sortBy)\n     qry.set(\"group\", true)\n     qry.set(\"group.ngroups\", true)\n-    qry.set(\"group.field\", Constants.solr.PARENT)\n+    qry.set(\"group.field\", Constants.storage.PARENT)\n     qry.set(\"group.limit\", 0)\n     qry.setRows(maxGroups)\n-    val proxy = job.newCrawlDbSolrClient()\n-    val solr = proxy.crawlDb\n-    val groupRes = solr.query(qry).getGroupResponse.getValues.get(0)\n+    val proxy = job.newStorageProxy()\n+    val client = proxy.getClient()\n+    val groupRes = client.query(qry).getGroupResponse.getValues.get(0)\n     val grps = groupRes.getValues\n     MemexCrawlDbRDD.LOG.info(s\"selecting ${grps.size()} out of ${groupRes.getNGroups}\")\n     val res = new Array[Partition](grps.size())\n@@ -65,8 +65,8 @@ class MemexCrawlDbRDD(sc: SparkContext,\n \n object MemexCrawlDbRDD extends Loggable {\n \n-  val DEFAULT_ORDER = Constants.solr.DISCOVER_DEPTH + \" asc,\" + Constants.solr.SCORE + \" desc\"\n-  val DEFAULT_FILTER_QRY = Constants.solr.STATUS + \":\" + ResourceStatus.UNFETCHED\n+  val DEFAULT_ORDER = Constants.storage.DISCOVER_DEPTH + \" asc,\" + Constants.storage.SCORE + \" desc\"\n+  val DEFAULT_FILTER_QRY = Constants.storage.STATUS + \":\" + ResourceStatus.UNFETCHED\n   val DEFAULT_GROUPS = 1000\n   val DEFAULT_TOPN = 1000\n }"
  },
  {
    "sha": "7ed4e96cc7d1415e215f827152ef516a915126df",
    "filename": "sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/MemexDeepCrawlRDD.scala",
    "status": "modified",
    "additions": 11,
    "deletions": 11,
    "changes": 22,
    "blob_url": "https://github.com/USCDataScience/sparkler/blob/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/MemexDeepCrawlRDD.scala",
    "raw_url": "https://github.com/USCDataScience/sparkler/raw/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/MemexDeepCrawlRDD.scala",
    "contents_url": "https://api.github.com/repos/USCDataScience/sparkler/contents/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/MemexDeepCrawlRDD.scala?ref=6e546171082fd11e562442e3568222f94343046f",
    "patch": "@@ -4,7 +4,7 @@ import java.net.URL\n \n import edu.usc.irds.sparkler.base.Loggable\n import edu.usc.irds.sparkler.model.{Resource, ResourceStatus, SparklerJob}\n-import edu.usc.irds.sparkler.solr.SolrGroupPartition\n+import edu.usc.irds.sparkler.storage.solr.SolrGroupPartition\n import edu.usc.irds.sparkler.util.SolrResultIterator\n import org.apache.solr.client.solrj.SolrQuery\n import org.apache.solr.client.solrj.util.ClientUtils.escapeQueryChars\n@@ -38,28 +38,28 @@ class MemexDeepCrawlDbRDD(sc: SparkContext,\n       }\n     }\n     query.addFilterQuery(hostnameFilter)\n-    query.addFilterQuery(s\"\"\"${Constants.solr.PARENT}:\"${escapeQueryChars(partition.group)}\"\"\"\")\n-    query.addFilterQuery(s\"${Constants.solr.CRAWL_ID}:${job.id}\")\n+    query.addFilterQuery(s\"\"\"${Constants.storage.PARENT}:\"${escapeQueryChars(partition.group)}\"\"\"\")\n+    query.addFilterQuery(s\"${Constants.storage.CRAWL_ID}:${job.id}\")\n     query.set(\"sort\", sortBy)\n     query.setRows(batchSize)\n \n-    new SolrResultIterator[Resource](job.newCrawlDbSolrClient().crawlDb, query,\n+    new SolrResultIterator[Resource](job.newStorageProxy().getClient(), query,\n       batchSize, classOf[Resource], closeClient = true, limit = topN)\n   }\n \n   override protected def getPartitions: Array[Partition] = {\n     val qry = new SolrQuery(generateQry)\n \n-    qry.addFilterQuery(s\"${Constants.solr.CRAWL_ID}:${job.id}\")\n+    qry.addFilterQuery(s\"${Constants.storage.CRAWL_ID}:${job.id}\")\n     qry.set(\"sort\", sortBy)\n     qry.set(\"group\", true)\n     qry.set(\"group.ngroups\", true)\n-    qry.set(\"group.field\", Constants.solr.PARENT)\n+    qry.set(\"group.field\", Constants.storage.PARENT)\n     qry.set(\"group.limit\", 0)\n     qry.setRows(maxGroups)\n-    val proxy = job.newCrawlDbSolrClient()\n-    val solr = proxy.crawlDb\n-    val groupRes = solr.query(qry).getGroupResponse.getValues.get(0)\n+    val proxy = job.newStorageProxy()\n+    val client = proxy.getClient()\n+    val groupRes = client.query(qry).getGroupResponse.getValues.get(0)\n     val grps = groupRes.getValues\n     MemexDeepCrawlDbRDD.LOG.info(s\"selecting ${grps.size()} out of ${groupRes.getNGroups}\")\n     val res = new Array[Partition](grps.size())\n@@ -75,8 +75,8 @@ class MemexDeepCrawlDbRDD(sc: SparkContext,\n \n object MemexDeepCrawlDbRDD extends Loggable {\n \n-  val DEFAULT_ORDER = Constants.solr.DISCOVER_DEPTH + \" asc,\" + Constants.solr.SCORE + \" desc\"\n-  val DEFAULT_FILTER_QRY = Constants.solr.STATUS + \":\" + ResourceStatus.UNFETCHED\n+  val DEFAULT_ORDER = Constants.storage.DISCOVER_DEPTH + \" asc,\" + Constants.storage.SCORE + \" desc\"\n+  val DEFAULT_FILTER_QRY = Constants.storage.STATUS + \":\" + ResourceStatus.UNFETCHED\n   val DEFAULT_GROUPS = 10\n   val DEFAULT_TOPN = 1000\n }"
  },
  {
    "sha": "061e41ec49a343d4fb2d78770ae0c0dc96a042c5",
    "filename": "sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/model/SparklerJob.scala",
    "status": "modified",
    "additions": 8,
    "deletions": 51,
    "changes": 59,
    "blob_url": "https://github.com/USCDataScience/sparkler/blob/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/model/SparklerJob.scala",
    "raw_url": "https://github.com/USCDataScience/sparkler/raw/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/model/SparklerJob.scala",
    "contents_url": "https://api.github.com/repos/USCDataScience/sparkler/contents/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/model/SparklerJob.scala?ref=6e546171082fd11e562442e3568222f94343046f",
    "patch": "@@ -19,17 +19,14 @@ package edu.usc.irds.sparkler.model\n \n import java.io.File\n \n+import edu.usc.irds.sparkler.storage.StorageProxyFactory\n+\n import edu.usc.irds.sparkler.base.Loggable\n-import edu.usc.irds.sparkler.service.{RejectingURLFilterChain, SolrProxy}\n+import edu.usc.irds.sparkler.service.RejectingURLFilterChain\n import edu.usc.irds.sparkler.util.JobUtil\n import edu.usc.irds.sparkler._\n-import org.apache.solr.client.solrj.SolrClient\n-import org.apache.solr.client.solrj.embedded.EmbeddedSolrServer\n-import org.apache.solr.client.solrj.impl.{CloudSolrClient}\n-import org.apache.solr.core.CoreContainer\n \n import scala.collection.mutable\n-import org.apache.solr.client.solrj.impl.HttpSolrClient\n \n /**\n   *\n@@ -40,61 +37,21 @@ class SparklerJob(val id: String,\n                   var currentTask: String)\n   extends Serializable with JobContext with Loggable {\n \n-  var crawlDbUri: String = config.get(Constants.key.CRAWLDB).toString\n-\n   /*\n    * mappings from extension point to extension chain\n    */\n   //TODO: we should be able to overwrite these from config file\n   val extChain: collection.mutable.HashMap[Class[_<:ExtensionPoint], Class[_<:ExtensionChain[_]]] =\n-    mutable.HashMap(\n-      (classOf[URLFilter], classOf[RejectingURLFilterChain])\n-    )\n-\n-  /**\n-    * Creates solr client based on the crawldburi\n-    * @return Solr Client\n-    */\n-  def newSolrClient(): SolrClient = {\n-    if (crawlDbUri.startsWith(\"http://\") || crawlDbUri.startsWith(\"https://\")) {\n-      new HttpSolrClient.Builder(crawlDbUri).build\n-    } else if (crawlDbUri.startsWith(\"file://\")) {\n-      var solrHome = crawlDbUri.replace(\"file://\", \"\")\n-      LOG.info(\"Embedded Solr, Solr Core={}\", solrHome)\n-      val solrHomeFile = new File(solrHome)\n-      if (!solrHomeFile.exists()) {\n-        val msg = s\"Solr Core $solrHome doesn't exists\"\n-        LOG.warn(msg)\n-        throw new SparklerException(msg)\n-      }\n-\n-      //parent directory is solr home\n-      solrHome = solrHomeFile.getParent\n-      //directory name is the core name\n-      val coreName = solrHomeFile.getName\n-      LOG.info(s\"Loading Embedded Solr, Home=$solrHome, Core=$coreName\")\n-      val coreContainer: CoreContainer = new CoreContainer(solrHome)\n-      coreContainer.load()\n-      new EmbeddedSolrServer(coreContainer, coreName)\n-    } else if (crawlDbUri.contains(\"::\")){\n-      //Expected format = collection::zkhost1:port1,zkhost2:port2\n-      // usually cloud uri has multi ZK hosts separated by comma(,)\n-      val Array(collectionName, zkhosts) = crawlDbUri.split(\"::\")\n-      LOG.info(\"Solr crawldb.uri:{}, Cloud Client: Collection:{} ZKHost={}\", crawlDbUri, collectionName, zkhosts)\n-      val client = new CloudSolrClient.Builder().withZkHost(zkhosts).build()\n-      client.setDefaultCollection(collectionName)\n-      client\n-    } else {\n-      throw new RuntimeException(s\"$crawlDbUri not supported\")\n-    }\n-  }\n+  mutable.HashMap(\n+    (classOf[URLFilter], classOf[RejectingURLFilterChain])\n+  )\n \n   def this(id: String, conf: SparklerConfiguration) {\n     this(id, conf, JobUtil.newSegmentId())\n   }\n \n-  def newCrawlDbSolrClient(): SolrProxy = {\n-    new SolrProxy(newSolrClient())\n+  def newStorageProxy() = {\n+    new StorageProxyFactory(config).getProxy()\n   }\n \n   override def getConfiguration: SparklerConfiguration ={"
  },
  {
    "sha": "9e903b4519c6919d0e595c02fd21b1acfcf46c29",
    "filename": "sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/pipeline/Crawler.scala",
    "status": "modified",
    "additions": 9,
    "deletions": 10,
    "changes": 19,
    "blob_url": "https://github.com/USCDataScience/sparkler/blob/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/pipeline/Crawler.scala",
    "raw_url": "https://github.com/USCDataScience/sparkler/raw/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/pipeline/Crawler.scala",
    "contents_url": "https://api.github.com/repos/USCDataScience/sparkler/contents/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/pipeline/Crawler.scala?ref=6e546171082fd11e562442e3568222f94343046f",
    "patch": "@@ -24,9 +24,8 @@ import edu.usc.irds.sparkler._\n import edu.usc.irds.sparkler.base.{CliTool, Loggable}\n import edu.usc.irds.sparkler.model.ResourceStatus._\n import edu.usc.irds.sparkler.model.{CrawlData, Resource, ResourceStatus, SparklerJob}\n-import edu.usc.irds.sparkler.service.SolrProxy\n \n-import edu.usc.irds.sparkler.solr.{SolrStatusUpdate, SolrUpsert}\n+import edu.usc.irds.sparkler.storage.solr.{SolrProxy, SolrStatusUpdate, SolrUpsert, StatusUpdateSolrTransformer, ScoreUpdateSolrTransformer}\n import edu.usc.irds.sparkler.util.{JobUtil, NutchBridge}\n import org.apache.hadoop.conf.Configuration\n import org.apache.hadoop.io.Text\n@@ -61,7 +60,7 @@ class Crawler extends CliTool {\n \n   @Option(name = \"-cdb\", aliases = Array(\"--crawldb\"),\n     usage = \"Crawl DB URI.\")\n-  var sparkSolr: String = sparklerConf.get(Constants.key.CRAWLDB).asInstanceOf[String]\n+  var sparkStorage: String = sparklerConf.getDatabaseURI()\n \n   @Option(name = \"-id\", aliases = Array(\"--id\"), required = true,\n     usage = \"Job id. When not sure, get the job id from injector command\")\n@@ -140,8 +139,8 @@ class Crawler extends CliTool {\n     if (!sparkMaster.isEmpty) {\n       conf.setMaster(sparkMaster)\n     }\n-    if (!sparkSolr.isEmpty){\n-      sparklerConf.asInstanceOf[java.util.HashMap[String,String]].put(\"crawldb.uri\", sparkSolr)\n+    if (!sparkStorage.isEmpty){\n+      sparklerConf.asInstanceOf[java.util.HashMap[String,String]].put(\"crawldb.uri\", sparkStorage)\n     }\n \n     if (databricksEnable) {\n@@ -171,9 +170,9 @@ class Crawler extends CliTool {\n     //STEP : Initialize environment\n     init()\n \n-    val solrc = this.job.newCrawlDbSolrClient()\n+    val storageProxy = this.job.newStorageProxy()\n     LOG.info(\"Committing crawldb..\")\n-    solrc.commitCrawlDb()\n+    storageProxy.commitCrawlDb()\n     val localFetchDelay = fetchDelay\n     val job = this.job // local variable to bypass serialization\n \n@@ -212,7 +211,7 @@ class Crawler extends CliTool {\n         storeContent(outputPath, scoredRdd)\n \n         LOG.info(\"Committing crawldb..\")\n-        solrc.commitCrawlDb()\n+        storageProxy.commitCrawlDb()\n       }\n \n       var taskId = JobUtil.newSegmentId(true)\n@@ -236,9 +235,9 @@ class Crawler extends CliTool {\n       storeContent(outputPath, scoredRdd)\n \n       LOG.info(\"Committing crawldb..\")\n-      solrc.commitCrawlDb()\n+      storageProxy.commitCrawlDb()\n     }\n-    solrc.close()\n+    storageProxy.close()\n     //PluginService.shutdown(job)\n     LOG.info(\"Shutting down Spark CTX..\")\n     sc.stop()"
  },
  {
    "sha": "697aa8eecdad2c5c07f62772e15f8ff9922d8a1e",
    "filename": "sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/pipeline/CrawlerRunner.scala",
    "status": "modified",
    "additions": 8,
    "deletions": 7,
    "changes": 15,
    "blob_url": "https://github.com/USCDataScience/sparkler/blob/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/pipeline/CrawlerRunner.scala",
    "raw_url": "https://github.com/USCDataScience/sparkler/raw/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/pipeline/CrawlerRunner.scala",
    "contents_url": "https://api.github.com/repos/USCDataScience/sparkler/contents/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/pipeline/CrawlerRunner.scala?ref=6e546171082fd11e562442e3568222f94343046f",
    "patch": "@@ -10,14 +10,15 @@ import edu.usc.irds.sparkler.model.{Resource, SparklerJob}\n import edu.usc.irds.sparkler.{MemexDeepCrawlDbRDD, MemexCrawlDbRDD}\n import edu.usc.irds.sparkler.model.ResourceStatus.UNFETCHED\n import edu.usc.irds.sparkler.model.{CrawlData, Resource, ResourceStatus, SparklerJob}\n-import edu.usc.irds.sparkler.solr._\n+import edu.usc.irds.sparkler.storage.solr._\n import java.io.File\n import scala.collection.mutable\n import scala.io.Source\n import edu.usc.irds.sparkler.util.JobUtil\n import org.apache.solr.common.SolrInputDocument\n import org.apache.spark.rdd.RDD\n-import edu.usc.irds.sparkler.solr.{SolrStatusUpdate, SolrUpsert}\n+\n+\n class CrawlerRunner {\n \n   import Crawler.LOG\n@@ -39,9 +40,9 @@ class CrawlerRunner {\n     this.outputPath = op\n     val job = init(configOverride, jobId, sparkSolr, databricksEnable = false, sparklerConf, outputPath, sparkMaster, jarPath)\n \n-    val solrc = job.newCrawlDbSolrClient()\n+    val storageProxy = job.newStorageProxy()\n     LOG.info(\"Committing crawldb..\")\n-    solrc.commitCrawlDb()\n+    storageProxy.commitCrawlDb()\n     val localFetchDelay = fetchDelay\n     for (_ <- 1 to iterations) {\n       var deepCrawlHosts: mutable.Set[String] = new mutable.HashSet[String]()\n@@ -78,7 +79,7 @@ class CrawlerRunner {\n         Crawler.storeContent(outputPath, scoredRdd)\n \n         LOG.info(\"Committing crawldb..\")\n-        solrc.commitCrawlDb()\n+        storageProxy.commitCrawlDb()\n       }\n \n       var taskId = JobUtil.newSegmentId(true)\n@@ -102,9 +103,9 @@ class CrawlerRunner {\n       Crawler.storeContent(this.outputPath, scoredRdd)\n \n       LOG.info(\"Committing crawldb..\")\n-      solrc.commitCrawlDb()\n+      storageProxy.commitCrawlDb()\n     }\n-    solrc.close()\n+    storageProxy.close()\n     //PluginService.shutdown(job)\n     LOG.info(\"Shutting down Spark CTX..\")\n     this.sc.stop()"
  },
  {
    "sha": "74cdc5bf45a77eec3dfa1fd88aadaa6fefc48b07",
    "filename": "sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/pipeline/FairFetcher.scala",
    "status": "modified",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/USCDataScience/sparkler/blob/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/pipeline/FairFetcher.scala",
    "raw_url": "https://github.com/USCDataScience/sparkler/raw/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/pipeline/FairFetcher.scala",
    "contents_url": "https://api.github.com/repos/USCDataScience/sparkler/contents/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/pipeline/FairFetcher.scala?ref=6e546171082fd11e562442e3568222f94343046f",
    "patch": "@@ -69,7 +69,7 @@ class FairFetcher(val job: SparklerJob, val resources: Iterator[Resource], val d\n     data.parsedData.outlinks = outLinkFilterFunc(job, data)\n     val doc = solrUpdateFunction(data)\n     LOG.info(\"Adding doc to SOLR\")\n-    job.newCrawlDbSolrClient().addResource(doc)\n+    job.newStorageProxy().addResource(doc)\n     data\n   }\n }"
  },
  {
    "sha": "2929fd3762922f2c0acaaca48c8fb35c19be87f6",
    "filename": "sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/pipeline/UrlInjectorFunction.scala",
    "status": "modified",
    "additions": 2,
    "deletions": 0,
    "changes": 2,
    "blob_url": "https://github.com/USCDataScience/sparkler/blob/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/pipeline/UrlInjectorFunction.scala",
    "raw_url": "https://github.com/USCDataScience/sparkler/raw/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/pipeline/UrlInjectorFunction.scala",
    "contents_url": "https://api.github.com/repos/USCDataScience/sparkler/contents/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/pipeline/UrlInjectorFunction.scala?ref=6e546171082fd11e562442e3568222f94343046f",
    "patch": "@@ -8,6 +8,8 @@ import scala.collection.JavaConversions._\n import java.util.ArrayList\n import java.util\n import edu.usc.irds.sparkler.UrlInjectorObj\n+\n+\n object UrlInjectorFunction extends ((SparklerJob, util.Collection[String]) => util.Collection[UrlInjectorObj]) with Serializable with Loggable{\n \n  override def apply(job: SparklerJob, resources: util.Collection[String])"
  },
  {
    "sha": "ef6643be67ab9e4d05ba78edc84fc5d5962c9c02",
    "filename": "sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/service/Injector.scala",
    "status": "modified",
    "additions": 7,
    "deletions": 7,
    "changes": 14,
    "blob_url": "https://github.com/USCDataScience/sparkler/blob/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/service/Injector.scala",
    "raw_url": "https://github.com/USCDataScience/sparkler/raw/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/service/Injector.scala",
    "contents_url": "https://api.github.com/repos/USCDataScience/sparkler/contents/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/service/Injector.scala?ref=6e546171082fd11e562442e3568222f94343046f",
    "patch": "@@ -66,7 +66,7 @@ class Injector extends CliTool {\n \n   @Option(name = \"-cdb\", aliases = Array(\"--crawldb\"),\n     usage = \"Crawdb URI.\")\n-  var sparkSolr: String = conf.get(Constants.key.CRAWLDB).asInstanceOf[String]\n+  var sparkStorage: String = conf.getDatabaseURI()\n \n   @Option(name = \"-co\", aliases = Array(\"--config-override\"),\n     handler = classOf[StringArrayOptionHandler],\n@@ -77,9 +77,9 @@ class Injector extends CliTool {\n     if (configOverride != \"\"){\n       conf.overloadConfig(configOverride.mkString(\" \"));\n     }\n-    if (!sparkSolr.isEmpty) {\n+    if (!sparkStorage.isEmpty) {\n       val uri = conf.asInstanceOf[java.util.HashMap[String, String]]\n-      uri.put(\"crawldb.uri\", sparkSolr)\n+      uri.put(\"crawldb.uri\", sparkStorage)\n     }\n \n     if (jobId.isEmpty) {\n@@ -109,10 +109,10 @@ class Injector extends CliTool {\n     })\n     LOG.info(\"Injecting {} seeds\", seeds.size())\n \n-    val solrClient = job.newCrawlDbSolrClient()\n-    solrClient.addResources(seeds.iterator())\n-    solrClient.commitCrawlDb()\n-    solrClient.close()\n+    val storageProxy = job.newStorageProxy()\n+    storageProxy.addResources(seeds.iterator())\n+    storageProxy.commitCrawlDb()\n+    storageProxy.close()\n   }\n \n   override def parseArgs(args: Array[String]): Unit = {"
  },
  {
    "sha": "917468bad4499d30d285de7f6f1e6fc2b9187d6e",
    "filename": "sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/service/SolrProxy.scala",
    "status": "removed",
    "additions": 0,
    "deletions": 68,
    "changes": 68,
    "blob_url": "https://github.com/USCDataScience/sparkler/blob/6ec6d100642de44a35528f6ba174a07337009686/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/service/SolrProxy.scala",
    "raw_url": "https://github.com/USCDataScience/sparkler/raw/6ec6d100642de44a35528f6ba174a07337009686/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/service/SolrProxy.scala",
    "contents_url": "https://api.github.com/repos/USCDataScience/sparkler/contents/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/service/SolrProxy.scala?ref=6ec6d100642de44a35528f6ba174a07337009686",
    "patch": "@@ -1,68 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements.  See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package edu.usc.irds.sparkler.service\n-\n-import java.io.Closeable\n-\n-import edu.usc.irds.sparkler.base.Loggable\n-import edu.usc.irds.sparkler.model.Resource\n-import org.apache.solr.client.solrj.SolrClient\n-import org.apache.solr.common.SolrInputDocument\n-\n-/**\n-  *\n-  * @since 5/28/16\n-  */\n-class SolrProxy(var crawlDb: SolrClient) extends Closeable with Loggable {\n-\n-\n-  def addResourceDocs(docs: java.util.Iterator[SolrInputDocument]): Unit = {\n-    crawlDb.add(docs)\n-  }\n-\n-  def addResources(beans: java.util.Iterator[_]): Unit = {\n-    try {\n-      crawlDb.addBeans(beans)\n-    } catch {\n-      case e: Exception =>\n-        LOG.warn(\"Caught {} while adding beans, trying to add one by one\", e.getMessage)\n-        while (beans.hasNext) {\n-          val bean = beans.next()\n-          try { // to add one by one\n-            crawlDb.addBean(bean)\n-          } catch {\n-            case e2: Exception =>\n-              LOG.warn(\"(SKIPPED) {} while adding {}\", e2.getMessage, bean)\n-              LOG.debug(e2.getMessage, e2)\n-          }\n-        }\n-    }\n-  }\n-\n-  def addResource(doc: SolrInputDocument): Unit = {\n-    crawlDb.add(doc)\n-  }\n-\n-  def commitCrawlDb(): Unit = {\n-    crawlDb.commit()\n-  }\n-\n-  override def close(): Unit = {\n-    crawlDb.close()\n-  }\n-}"
  },
  {
    "sha": "3c70712325d18ef7a3091b39aefbf326f14dc7da",
    "filename": "sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/StorageProxy.scala",
    "status": "added",
    "additions": 33,
    "deletions": 0,
    "changes": 33,
    "blob_url": "https://github.com/USCDataScience/sparkler/blob/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/StorageProxy.scala",
    "raw_url": "https://github.com/USCDataScience/sparkler/raw/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/StorageProxy.scala",
    "contents_url": "https://api.github.com/repos/USCDataScience/sparkler/contents/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/StorageProxy.scala?ref=6e546171082fd11e562442e3568222f94343046f",
    "patch": "@@ -0,0 +1,33 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package edu.usc.irds.sparkler.storage\n+\n+/**\n+  *\n+  * @since 3/2/2021\n+  */\n+abstract class StorageProxy() {\n+\n+  // TODO: Change return Any??\n+  def newClient(crawlDbUri: String): Any\n+  def getClient(): Any\n+  def commitCrawlDb(): Unit\n+  def close(): Unit\n+\n+}\n+"
  },
  {
    "sha": "08521e2a99fcd5712a7c689073ef47aae57dda04",
    "filename": "sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/StorageProxyFactory.scala",
    "status": "added",
    "additions": 38,
    "deletions": 0,
    "changes": 38,
    "blob_url": "https://github.com/USCDataScience/sparkler/blob/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/StorageProxyFactory.scala",
    "raw_url": "https://github.com/USCDataScience/sparkler/raw/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/StorageProxyFactory.scala",
    "contents_url": "https://api.github.com/repos/USCDataScience/sparkler/contents/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/StorageProxyFactory.scala?ref=6e546171082fd11e562442e3568222f94343046f",
    "patch": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package edu.usc.irds.sparkler.storage\n+\n+import edu.usc.irds.sparkler.storage.solr.SolrProxy\n+import edu.usc.irds.sparkler.{Constants, SparklerConfiguration}\n+\n+/**\n+  *\n+  * @since 3/2/2021\n+  */\n+class StorageProxyFactory(var config: SparklerConfiguration) {\n+\n+  val dbToUse: String = config.get(Constants.key.CRAWLDB_BACKEND).asInstanceOf[String]\n+\n+  def getProxy() = {\n+    dbToUse match {\n+      case \"solr\" => new SolrProxy(config)\n+      case _ => new SolrProxy(config)\n+    }\n+  }\n+\n+}\n\\ No newline at end of file"
  },
  {
    "sha": "b612c375226f33dbadd6fa10900688c479e44d16",
    "filename": "sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/solr/ScoreUpdateSolrTransformer.scala",
    "status": "renamed",
    "additions": 4,
    "deletions": 4,
    "changes": 8,
    "blob_url": "https://github.com/USCDataScience/sparkler/blob/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/solr/ScoreUpdateSolrTransformer.scala",
    "raw_url": "https://github.com/USCDataScience/sparkler/raw/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/solr/ScoreUpdateSolrTransformer.scala",
    "contents_url": "https://api.github.com/repos/USCDataScience/sparkler/contents/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/solr/ScoreUpdateSolrTransformer.scala?ref=6e546171082fd11e562442e3568222f94343046f",
    "patch": "@@ -15,7 +15,7 @@\n  * limitations under the License.\n  */\n \n-package edu.usc.irds.sparkler.pipeline\n+package edu.usc.irds.sparkler.storage.solr\n \n import java.util\n import java.util.Iterator\n@@ -24,7 +24,7 @@ import com.google.common.hash.{HashFunction, Hashing}\n import edu.usc.irds.sparkler.Constants\n import edu.usc.irds.sparkler.base.Loggable\n import edu.usc.irds.sparkler.model.CrawlData\n-import edu.usc.irds.sparkler.solr.schema.FieldMapper\n+import edu.usc.irds.sparkler.storage.solr.schema.FieldMapper\n import org.apache.solr.common.SolrInputDocument\n \n import scala.collection.JavaConverters._\n@@ -41,9 +41,9 @@ object ScoreUpdateSolrTransformer extends (CrawlData => SolrInputDocument ) with\n     val hashFunction: HashFunction = Hashing.sha256()\n     val sUpdate = new SolrInputDocument()\n     //FIXME: handle failure case\n-    sUpdate.setField(Constants.solr.ID, data.fetchedData.getResource.getId)\n+    sUpdate.setField(Constants.storage.ID, data.fetchedData.getResource.getId)\n \n-    sUpdate.setField(Constants.solr.GENERATE_SCORE, Map(\"set\" -> data.fetchedData.getResource.getGenerateScore()).asJava)\n+    sUpdate.setField(Constants.storage.GENERATE_SCORE, Map(\"set\" -> data.fetchedData.getResource.getGenerateScore()).asJava)\n \n     sUpdate\n   }",
    "previous_filename": "sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/pipeline/ScoreUpdateSolrTransformer.scala"
  },
  {
    "sha": "dcfff2b03f2a34105eb8259f6b62dc8000fe4609",
    "filename": "sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/solr/SolrGroupPartition.scala",
    "status": "renamed",
    "additions": 1,
    "deletions": 1,
    "changes": 2,
    "blob_url": "https://github.com/USCDataScience/sparkler/blob/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/solr/SolrGroupPartition.scala",
    "raw_url": "https://github.com/USCDataScience/sparkler/raw/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/solr/SolrGroupPartition.scala",
    "contents_url": "https://api.github.com/repos/USCDataScience/sparkler/contents/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/solr/SolrGroupPartition.scala?ref=6e546171082fd11e562442e3568222f94343046f",
    "patch": "@@ -15,7 +15,7 @@\n  * limitations under the License.\n  */\n \n-package edu.usc.irds.sparkler.solr\n+package edu.usc.irds.sparkler.storage.solr\n \n import org.apache.spark.Partition\n ",
    "previous_filename": "sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/solr/SolrGroupPartition.scala"
  },
  {
    "sha": "1d9660b253d44563d39eb3484081e70613b80b7d",
    "filename": "sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/solr/SolrProxy.scala",
    "status": "added",
    "additions": 118,
    "deletions": 0,
    "changes": 118,
    "blob_url": "https://github.com/USCDataScience/sparkler/blob/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/solr/SolrProxy.scala",
    "raw_url": "https://github.com/USCDataScience/sparkler/raw/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/solr/SolrProxy.scala",
    "contents_url": "https://api.github.com/repos/USCDataScience/sparkler/contents/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/solr/SolrProxy.scala?ref=6e546171082fd11e562442e3568222f94343046f",
    "patch": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package edu.usc.irds.sparkler.storage.solr\n+\n+import java.io.{Closeable, File}\n+import edu.usc.irds.sparkler.base.Loggable\n+import edu.usc.irds.sparkler.storage.StorageProxy\n+import edu.usc.irds.sparkler._\n+\n+import org.apache.solr.client.solrj.SolrClient\n+import org.apache.solr.client.solrj.embedded.EmbeddedSolrServer\n+import org.apache.solr.client.solrj.impl.{CloudSolrClient}\n+import org.apache.solr.core.CoreContainer\n+import org.apache.solr.common.SolrInputDocument\n+\n+import org.apache.solr.client.solrj.impl.HttpSolrClient\n+\n+/**\n+  *\n+  * @since 5/28/16\n+  */\n+class SolrProxy(var config: SparklerConfiguration) extends StorageProxy with Closeable with Loggable {\n+\n+  /**\n+    * Creates solr client based on the crawldburi\n+    * @return Solr Client\n+    */\n+  def newClient(crawlDbUri: String): SolrClient = {\n+    if (crawlDbUri.startsWith(\"http://\") || crawlDbUri.startsWith(\"https://\")) {\n+      new HttpSolrClient.Builder(crawlDbUri).build\n+    } else if (crawlDbUri.startsWith(\"file://\")) {\n+      var solrHome = crawlDbUri.replace(\"file://\", \"\")\n+      LOG.info(\"Embedded Solr, Solr Core={}\", solrHome)\n+      val solrHomeFile = new File(solrHome)\n+      if (!solrHomeFile.exists()) {\n+        val msg = s\"Solr Core $solrHome doesn't exists\"\n+        LOG.warn(msg)\n+        throw new SparklerException(msg)\n+      }\n+\n+      //parent directory is solr home\n+      solrHome = solrHomeFile.getParent\n+      //directory name is the core name\n+      val coreName = solrHomeFile.getName\n+      LOG.info(s\"Loading Embedded Solr, Home=$solrHome, Core=$coreName\")\n+      val coreContainer: CoreContainer = new CoreContainer(solrHome)\n+      coreContainer.load()\n+      new EmbeddedSolrServer(coreContainer, coreName)\n+    } else if (crawlDbUri.contains(\"::\")){\n+      //Expected format = collection::zkhost1:port1,zkhost2:port2\n+      // usually cloud uri has multi ZK hosts separated by comma(,)\n+      val Array(collectionName, zkhosts) = crawlDbUri.split(\"::\")\n+      LOG.info(\"Solr crawldb.uri:{}, Cloud Client: Collection:{} ZKHost={}\", crawlDbUri, collectionName, zkhosts)\n+      val client = new CloudSolrClient.Builder().withZkHost(zkhosts).build()\n+      client.setDefaultCollection(collectionName)\n+      client\n+    } else {\n+      throw new RuntimeException(s\"$crawlDbUri not supported\")\n+    }\n+  }\n+\n+  // creates the solr client\n+  private var crawlDb = newClient(config.getDatabaseURI())\n+\n+  def getClient(): SolrClient = {\n+    return crawlDb\n+  }\n+\n+  def addResourceDocs(docs: java.util.Iterator[SolrInputDocument]): Unit = {\n+    crawlDb.add(docs)\n+  }\n+\n+  def addResources(beans: java.util.Iterator[_]): Unit = {\n+    try {\n+      crawlDb.addBeans(beans)\n+    } catch {\n+      case e: Exception =>\n+        LOG.warn(\"Caught {} while adding beans, trying to add one by one\", e.getMessage)\n+        while (beans.hasNext) {\n+          val bean = beans.next()\n+          try { // to add one by one\n+            crawlDb.addBean(bean)\n+          } catch {\n+            case e2: Exception =>\n+              LOG.warn(\"(SKIPPED) {} while adding {}\", e2.getMessage, bean)\n+              LOG.debug(e2.getMessage, e2)\n+          }\n+        }\n+    }\n+  }\n+\n+  def addResource(doc: SolrInputDocument): Unit = {\n+    crawlDb.add(doc)\n+  }\n+\n+  def commitCrawlDb(): Unit = {\n+    crawlDb.commit()\n+  }\n+\n+  override def close(): Unit = {\n+    crawlDb.close()\n+  }\n+}"
  },
  {
    "sha": "d42503dcce97a5a55f5a6f42fe874741b760a4d9",
    "filename": "sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/solr/SolrStatusUpdate.scala",
    "status": "renamed",
    "additions": 2,
    "deletions": 2,
    "changes": 4,
    "blob_url": "https://github.com/USCDataScience/sparkler/blob/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/solr/SolrStatusUpdate.scala",
    "raw_url": "https://github.com/USCDataScience/sparkler/raw/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/solr/SolrStatusUpdate.scala",
    "contents_url": "https://api.github.com/repos/USCDataScience/sparkler/contents/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/solr/SolrStatusUpdate.scala?ref=6e546171082fd11e562442e3568222f94343046f",
    "patch": "@@ -15,7 +15,7 @@\n  * limitations under the License.\n  */\n \n-package edu.usc.irds.sparkler.solr\n+package edu.usc.irds.sparkler.storage.solr\n \n import SolrStatusUpdate.LOG\n import edu.usc.irds.sparkler.base.Loggable\n@@ -32,7 +32,7 @@ class SolrStatusUpdate(job: SparklerJob) extends ((TaskContext, Iterator[SolrInp\n \n   override def apply(context: TaskContext, docs: Iterator[SolrInputDocument]): Any = {\n     LOG.debug(\"Updating document status into CrawlDb\")\n-    val solrClient = job.newCrawlDbSolrClient()\n+    val solrClient = job.newStorageProxy()\n     solrClient.addResourceDocs(docs)\n     solrClient.close()\n   }",
    "previous_filename": "sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/solr/SolrStatusUpdate.scala"
  },
  {
    "sha": "f381454a8262b8cbb5cb0dceb3b2f6e62f25d324",
    "filename": "sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/solr/SolrUpsert.scala",
    "status": "renamed",
    "additions": 5,
    "deletions": 5,
    "changes": 10,
    "blob_url": "https://github.com/USCDataScience/sparkler/blob/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/solr/SolrUpsert.scala",
    "raw_url": "https://github.com/USCDataScience/sparkler/raw/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/solr/SolrUpsert.scala",
    "contents_url": "https://api.github.com/repos/USCDataScience/sparkler/contents/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/solr/SolrUpsert.scala?ref=6e546171082fd11e562442e3568222f94343046f",
    "patch": "@@ -15,7 +15,7 @@\n  * limitations under the License.\n  */\n \n-package edu.usc.irds.sparkler.solr\n+package edu.usc.irds.sparkler.storage.solr\n \n import edu.usc.irds.sparkler.Constants\n import edu.usc.irds.sparkler.model.{Resource, SparklerJob}\n@@ -31,19 +31,19 @@ import scala.collection.JavaConversions._\n  */\n class SolrUpsert(job: SparklerJob) extends ((TaskContext, Iterator[Resource]) => Any) with Serializable {\n \n-  import edu.usc.irds.sparkler.solr.SolrUpsert.LOG\n+  import edu.usc.irds.sparkler.storage.solr.SolrUpsert.LOG\n \n   override def apply(context: TaskContext, docs: Iterator[Resource]): Any = {\n     LOG.debug(\"Inserting new resources into CrawlDb\")\n-    val solrClient = job.newCrawlDbSolrClient()\n+    val solrClient = job.newStorageProxy()\n \n     //TODO: handle this in server side - tell solr to skip docs if they already exist\n \n     //This filter function returns true if there is no other resource  with the same dedupe_id\n     val newLinksFilter: (Resource => Boolean) = doc => {\n-      val qry = new SolrQuery(s\"${Constants.solr.DEDUPE_ID}:${doc.getDedupeId}\")\n+      val qry = new SolrQuery(s\"${Constants.storage.DEDUPE_ID}:${doc.getDedupeId}\")\n       qry.setRows(0) //we are interested in counts only and not the contents\n-      solrClient.crawlDb.query(qry).getResults.getNumFound == 0\n+      solrClient.getClient().query(qry).getResults.getNumFound == 0\n       // if zero hits, then there are no duplicates\n     }\n     val newResources = docs.withFilter(newLinksFilter)",
    "previous_filename": "sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/solr/SolrUpsert.scala"
  },
  {
    "sha": "779aa864f4ea1ea13dd44da673796360612d1f93",
    "filename": "sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/solr/StatusUpdateSolrTransformer.scala",
    "status": "renamed",
    "additions": 21,
    "deletions": 21,
    "changes": 42,
    "blob_url": "https://github.com/USCDataScience/sparkler/blob/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/solr/StatusUpdateSolrTransformer.scala",
    "raw_url": "https://github.com/USCDataScience/sparkler/raw/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/solr/StatusUpdateSolrTransformer.scala",
    "contents_url": "https://api.github.com/repos/USCDataScience/sparkler/contents/sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/storage/solr/StatusUpdateSolrTransformer.scala?ref=6e546171082fd11e562442e3568222f94343046f",
    "patch": "@@ -15,15 +15,15 @@\n  * limitations under the License.\n  */\n \n-package edu.usc.irds.sparkler.pipeline\n+package edu.usc.irds.sparkler.storage.solr\n \n import java.util\n \n import com.google.common.hash.{HashFunction, Hashing}\n import edu.usc.irds.sparkler.Constants\n import edu.usc.irds.sparkler.base.Loggable\n import edu.usc.irds.sparkler.model.CrawlData\n-import edu.usc.irds.sparkler.solr.schema.FieldMapper\n+import edu.usc.irds.sparkler.storage.solr.schema.FieldMapper\n import edu.usc.irds.sparkler.util.URLUtil\n import org.apache.solr.common.SolrInputDocument\n \n@@ -43,33 +43,33 @@ object StatusUpdateSolrTransformer extends (CrawlData => SolrInputDocument ) wit\n     val sUpdate = new SolrInputDocument()\n     //FIXME: handle failure case\n     //val x:java.util.Map[String, Object] = Map(\"ss\" -> new Object).asJava\n-    sUpdate.setField(Constants.solr.ID, data.fetchedData.getResource.getId)\n-    sUpdate.setField(Constants.solr.STATUS, Map(\"set\" -> data.fetchedData.getResource.getStatus).asJava)\n-    sUpdate.setField(Constants.solr.FETCH_TIMESTAMP, Map(\"set\" -> data.fetchedData.getFetchedAt).asJava)\n-    sUpdate.setField(Constants.solr.LAST_UPDATED_AT, Map(\"set\" -> new util.Date()).asJava)\n-    sUpdate.setField(Constants.solr.RETRIES_SINCE_FETCH, Map(\"inc\" -> 1).asJava)\n-    sUpdate.setField(Constants.solr.EXTRACTED_TEXT, data.parsedData.extractedText)\n-    sUpdate.setField(Constants.solr.CONTENT_TYPE, data.fetchedData.getContentType.split(\"; \")(0))\n-    sUpdate.setField(Constants.solr.FETCH_STATUS_CODE, data.fetchedData.getResponseCode)\n-    sUpdate.setField(Constants.solr.SIGNATURE, hashFunction.hashBytes(data.fetchedData.getContent).toString)\n-    sUpdate.setField(Constants.solr.RELATIVE_PATH, URLUtil.reverseUrl(data.fetchedData.getResource.getUrl))\n-    sUpdate.setField(Constants.solr.OUTLINKS, data.parsedData.outlinks.toArray)\n-    sUpdate.setField(Constants.solr.SEGMENT, data.fetchedData.getSegment)\n+    sUpdate.setField(Constants.storage.ID, data.fetchedData.getResource.getId)\n+    sUpdate.setField(Constants.storage.STATUS, Map(\"set\" -> data.fetchedData.getResource.getStatus).asJava)\n+    sUpdate.setField(Constants.storage.FETCH_TIMESTAMP, Map(\"set\" -> data.fetchedData.getFetchedAt).asJava)\n+    sUpdate.setField(Constants.storage.LAST_UPDATED_AT, Map(\"set\" -> new util.Date()).asJava)\n+    sUpdate.setField(Constants.storage.RETRIES_SINCE_FETCH, Map(\"inc\" -> 1).asJava)\n+    sUpdate.setField(Constants.storage.EXTRACTED_TEXT, data.parsedData.extractedText)\n+    sUpdate.setField(Constants.storage.CONTENT_TYPE, data.fetchedData.getContentType.split(\"; \")(0))\n+    sUpdate.setField(Constants.storage.FETCH_STATUS_CODE, data.fetchedData.getResponseCode)\n+    sUpdate.setField(Constants.storage.SIGNATURE, hashFunction.hashBytes(data.fetchedData.getContent).toString)\n+    sUpdate.setField(Constants.storage.RELATIVE_PATH, URLUtil.reverseUrl(data.fetchedData.getResource.getUrl))\n+    sUpdate.setField(Constants.storage.OUTLINKS, data.parsedData.outlinks.toArray)\n+    sUpdate.setField(Constants.storage.SEGMENT, data.fetchedData.getSegment)\n     val splitMimeTypes = data.fetchedData.getContentType.toLowerCase().split(\";\")\n-    if (splitMimeTypes.contains(Constants.solr.WEBPAGE_MIMETYPE.toLowerCase())) {\n-      sUpdate.setField(Constants.solr.RAW_CONTENT, new String(data.fetchedData.getContent))\n-    } else if (splitMimeTypes.contains(Constants.solr.JSON_MIMETYPE.toLowerCase())){\n-      sUpdate.setField(Constants.solr.RAW_CONTENT, new String(data.fetchedData.getContent))\n+    if (splitMimeTypes.contains(Constants.storage.WEBPAGE_MIMETYPE.toLowerCase())) {\n+      sUpdate.setField(Constants.storage.RAW_CONTENT, new String(data.fetchedData.getContent))\n+    } else if (splitMimeTypes.contains(Constants.storage.JSON_MIMETYPE.toLowerCase())){\n+      sUpdate.setField(Constants.storage.RAW_CONTENT, new String(data.fetchedData.getContent))\n     }\n-    sUpdate.setField(Constants.solr.RESPONSE_TIME, data.fetchedData.getResponseTime)\n+    sUpdate.setField(Constants.storage.RESPONSE_TIME, data.fetchedData.getResponseTime)\n     for ((scoreKey, score) <- data.fetchedData.getResource.getScore) {\n       sUpdate.setField(scoreKey, Map(\"set\" -> score).asJava)\n     }\n \n     val md = data.parsedData.metadata\n     val mdFields = md.names().map(name => (name, if (md.isMultiValued(name)) md.getValues(name) else md.get(name))).toMap\n-    updateFields(mdFields, Constants.solr.MD_SUFFIX, sUpdate)\n-    updateFields(data.parsedData.headers, Constants.solr.HDR_SUFFIX, sUpdate)\n+    updateFields(mdFields, Constants.storage.MD_SUFFIX, sUpdate)\n+    updateFields(data.parsedData.headers, Constants.storage.HDR_SUFFIX, sUpdate)\n     sUpdate\n   }\n ",
    "previous_filename": "sparkler-core/sparkler-app/src/main/scala/edu/usc/irds/sparkler/pipeline/StatusUpdateSolrTransformer.scala"
  },
  {
    "sha": "9668bab57f8b0792d9865362e41e8184265d1f35",
    "filename": "sparkler-core/sparkler-app/src/test/resources/sparkler-default.yaml",
    "status": "modified",
    "additions": 12,
    "deletions": 3,
    "changes": 15,
    "blob_url": "https://github.com/USCDataScience/sparkler/blob/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/test/resources/sparkler-default.yaml",
    "raw_url": "https://github.com/USCDataScience/sparkler/raw/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-app/src/test/resources/sparkler-default.yaml",
    "contents_url": "https://api.github.com/repos/USCDataScience/sparkler/contents/sparkler-core/sparkler-app/src/test/resources/sparkler-default.yaml?ref=6e546171082fd11e562442e3568222f94343046f",
    "patch": "@@ -17,11 +17,20 @@\n \n ##################### General Properties ################################\n \n-# Crawl Database URL. Stores crawl metadata and status updates.\n-# Type: String. Default: http://localhost:8983/solr/crawldb\n-crawldb.uri: file://conf/solr/crawldb\n+# uri - Crawl Database URL. Stores crawl metadata and status updates.\n \n+crawldb.backend: solr  # \"solr\" is default until \"elasticsearch\" becomes usable.\n \n+# Type: String. Default: http://localhost:8983/solr/crawldb\n+# for standalone server\n+# For quick test crawls using embedded solr\n+# solr.uri: file://conf/solr/crawldb\n+# For cloudmode with zookeepers; Format = collectionName::zkhost1:port1,zkhost2:port2,zkhost3:port3\n+# solr.uri: crawldb::localhost:9983\n+solr.uri: http://localhost:8983/solr/crawldb\n+\n+# elasticsearch settings\n+elasticsearch.uri: http://localhost:9200\n \n ##################### Apache Spark Properties ###########################\n "
  },
  {
    "sha": "a3e1778b2100f03d51d873e2cb9e92eb6a103259",
    "filename": "sparkler-core/sparkler-deployment/docker/conf/sparkler-default.yaml",
    "status": "modified",
    "additions": 12,
    "deletions": 8,
    "changes": 20,
    "blob_url": "https://github.com/USCDataScience/sparkler/blob/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-deployment/docker/conf/sparkler-default.yaml",
    "raw_url": "https://github.com/USCDataScience/sparkler/raw/6e546171082fd11e562442e3568222f94343046f/sparkler-core/sparkler-deployment/docker/conf/sparkler-default.yaml",
    "contents_url": "https://api.github.com/repos/USCDataScience/sparkler/contents/sparkler-core/sparkler-deployment/docker/conf/sparkler-default.yaml?ref=6e546171082fd11e562442e3568222f94343046f",
    "patch": "@@ -17,16 +17,20 @@\n \n ##################### General Properties ################################\n \n-# Crawl Database URL. Stores crawl metadata and status updates.\n-# Type: String. Default: http://localhost:8983/solr/crawldb\n-# for standalone server\n-crawldb.uri: http://localhost:8983/solr/crawldb\n+# uri - Crawl Database URL. Stores crawl metadata and status updates.\n \n-# for quick test crawls using embedded solr\n-#crawldb.uri: file://conf/solr/crawldb\n+crawldb.backend: solr  # \"solr\" is default until \"elasticsearch\" becomes usable.\n \n-# for cloudmode with zookeepers; Format = collectionName::zkhost1:port1,zkhost2:port2,zkhost3:port3\n-# crawldb.uri: crawldb::localhost:9983\n+# Type: String. Default: http://localhost:8983/solr/crawldb\n+# for standalone server\n+# For quick test crawls using embedded solr\n+# solr.uri: file://conf/solr/crawldb\n+# For cloudmode with zookeepers; Format = collectionName::zkhost1:port1,zkhost2:port2,zkhost3:port3\n+# solr.uri: crawldb::localhost:9983\n+solr.uri: http://localhost:8983/solr/crawldb\n+\n+# elasticsearch settings\n+elasticsearch.uri: http://localhost:9300\n \n \n ##################### Apache Spark Properties ###########################"
  }
]
