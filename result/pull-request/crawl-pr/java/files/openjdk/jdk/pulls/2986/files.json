[
  {
    "sha": "9ba81d29532cbee0e6abec435fcd744b702ddcbc",
    "filename": "src/hotspot/share/gc/g1/g1DirtyCardQueue.cpp",
    "status": "modified",
    "additions": 4,
    "deletions": 111,
    "changes": 115,
    "blob_url": "https://github.com/openjdk/jdk/blob/91f22bbd229beca281a4a0de58bdf66c57648eeb/src/hotspot/share/gc/g1/g1DirtyCardQueue.cpp",
    "raw_url": "https://github.com/openjdk/jdk/raw/91f22bbd229beca281a4a0de58bdf66c57648eeb/src/hotspot/share/gc/g1/g1DirtyCardQueue.cpp",
    "contents_url": "https://api.github.com/repos/openjdk/jdk/contents/src/hotspot/share/gc/g1/g1DirtyCardQueue.cpp?ref=91f22bbd229beca281a4a0de58bdf66c57648eeb",
    "patch": "@@ -45,7 +45,9 @@\n #include \"runtime/thread.inline.hpp\"\n #include \"runtime/threadSMR.hpp\"\n #include \"utilities/globalCounter.inline.hpp\"\n+#include \"utilities/lockFreeQueue.inline.hpp\"\n #include \"utilities/macros.hpp\"\n+#include \"utilities/pair.hpp\"\n #include \"utilities/quickSort.hpp\"\n #include \"utilities/ticks.hpp\"\n \n@@ -116,115 +118,6 @@ void G1DirtyCardQueueSet::handle_zero_index_for_thread(Thread* t) {\n   G1BarrierSet::dirty_card_queue_set().handle_zero_index(queue);\n }\n \n-#ifdef ASSERT\n-G1DirtyCardQueueSet::Queue::~Queue() {\n-  assert(_head == NULL, \"precondition\");\n-  assert(_tail == NULL, \"precondition\");\n-}\n-#endif // ASSERT\n-\n-BufferNode* G1DirtyCardQueueSet::Queue::top() const {\n-  return Atomic::load(&_head);\n-}\n-\n-// An append operation atomically exchanges the new tail with the queue tail.\n-// It then sets the \"next\" value of the old tail to the head of the list being\n-// appended; it is an invariant that the old tail's \"next\" value is NULL.\n-// But if the old tail is NULL then the queue was empty.  In this case the\n-// head of the list being appended is instead stored in the queue head; it is\n-// an invariant that the queue head is NULL in this case.\n-//\n-// This means there is a period between the exchange and the old tail update\n-// where the queue sequence is split into two parts, the list from the queue\n-// head to the old tail, and the list being appended.  If there are concurrent\n-// push/append operations, each may introduce another such segment.  But they\n-// all eventually get resolved by their respective updates of their old tail's\n-// \"next\" value.  This also means that pop operations must handle a buffer\n-// with a NULL \"next\" value specially.\n-//\n-// A push operation is just a degenerate append, where the buffer being pushed\n-// is both the head and the tail of the list being appended.\n-void G1DirtyCardQueueSet::Queue::append(BufferNode& first, BufferNode& last) {\n-  assert(last.next() == NULL, \"precondition\");\n-  BufferNode* old_tail = Atomic::xchg(&_tail, &last);\n-  if (old_tail == NULL) {       // Was empty.\n-    Atomic::store(&_head, &first);\n-  } else {\n-    assert(old_tail->next() == NULL, \"invariant\");\n-    old_tail->set_next(&first);\n-  }\n-}\n-\n-BufferNode* G1DirtyCardQueueSet::Queue::pop() {\n-  Thread* current_thread = Thread::current();\n-  while (true) {\n-    // Use a critical section per iteration, rather than over the whole\n-    // operation.  We're not guaranteed to make progress.  Lingering in one\n-    // CS could lead to excessive allocation of buffers, because the CS\n-    // blocks return of released buffers to the free list for reuse.\n-    GlobalCounter::CriticalSection cs(current_thread);\n-\n-    BufferNode* result = Atomic::load_acquire(&_head);\n-    if (result == NULL) return NULL; // Queue is empty.\n-\n-    BufferNode* next = Atomic::load_acquire(BufferNode::next_ptr(*result));\n-    if (next != NULL) {\n-      // The \"usual\" lock-free pop from the head of a singly linked list.\n-      if (result == Atomic::cmpxchg(&_head, result, next)) {\n-        // Former head successfully taken; it is not the last.\n-        assert(Atomic::load(&_tail) != result, \"invariant\");\n-        assert(result->next() != NULL, \"invariant\");\n-        result->set_next(NULL);\n-        return result;\n-      }\n-      // Lost the race; try again.\n-      continue;\n-    }\n-\n-    // next is NULL.  This case is handled differently from the \"usual\"\n-    // lock-free pop from the head of a singly linked list.\n-\n-    // If _tail == result then result is the only element in the list. We can\n-    // remove it from the list by first setting _tail to NULL and then setting\n-    // _head to NULL, the order being important.  We set _tail with cmpxchg in\n-    // case of a concurrent push/append/pop also changing _tail.  If we win\n-    // then we've claimed result.\n-    if (Atomic::cmpxchg(&_tail, result, (BufferNode*)NULL) == result) {\n-      assert(result->next() == NULL, \"invariant\");\n-      // Now that we've claimed result, also set _head to NULL.  But we must\n-      // be careful of a concurrent push/append after we NULLed _tail, since\n-      // it may have already performed its list-was-empty update of _head,\n-      // which we must not overwrite.\n-      Atomic::cmpxchg(&_head, result, (BufferNode*)NULL);\n-      return result;\n-    }\n-\n-    // If _head != result then we lost the race to take result; try again.\n-    if (result != Atomic::load_acquire(&_head)) {\n-      continue;\n-    }\n-\n-    // An in-progress concurrent operation interfered with taking the head\n-    // element when it was the only element.  A concurrent pop may have won\n-    // the race to clear the tail but not yet cleared the head. Alternatively,\n-    // a concurrent push/append may have changed the tail but not yet linked\n-    // result->next().  We cannot take result in either case.  We don't just\n-    // try again, because we could spin for a long time waiting for that\n-    // concurrent operation to finish.  In the first case, returning NULL is\n-    // fine; we lost the race for the only element to another thread.  We\n-    // also return NULL for the second case, and let the caller cope.\n-    return NULL;\n-  }\n-}\n-\n-G1DirtyCardQueueSet::HeadTail G1DirtyCardQueueSet::Queue::take_all() {\n-  assert_at_safepoint();\n-  HeadTail result(Atomic::load(&_head), Atomic::load(&_tail));\n-  Atomic::store(&_head, (BufferNode*)NULL);\n-  Atomic::store(&_tail, (BufferNode*)NULL);\n-  return result;\n-}\n-\n void G1DirtyCardQueueSet::enqueue_completed_buffer(BufferNode* cbn) {\n   assert(cbn != NULL, \"precondition\");\n   // Increment _num_cards before adding to queue, so queue removal doesn't\n@@ -425,10 +318,10 @@ void G1DirtyCardQueueSet::merge_bufferlists(G1RedirtyCardsQueueSet* src) {\n G1BufferNodeList G1DirtyCardQueueSet::take_all_completed_buffers() {\n   enqueue_all_paused_buffers();\n   verify_num_cards();\n-  HeadTail buffers = _completed.take_all();\n+  Pair<BufferNode*, BufferNode*> pair = _completed.take_all();\n   size_t num_cards = Atomic::load(&_num_cards);\n   Atomic::store(&_num_cards, size_t(0));\n-  return G1BufferNodeList(buffers._head, buffers._tail, num_cards);\n+  return G1BufferNodeList(pair.first, pair.second, num_cards);\n }\n \n class G1RefineBufferedCards : public StackObj {"
  },
  {
    "sha": "b67e6f8e71eda6e08132ab49bedadf805e2a7b98",
    "filename": "src/hotspot/share/gc/g1/g1DirtyCardQueue.hpp",
    "status": "modified",
    "additions": 9,
    "deletions": 35,
    "changes": 44,
    "blob_url": "https://github.com/openjdk/jdk/blob/91f22bbd229beca281a4a0de58bdf66c57648eeb/src/hotspot/share/gc/g1/g1DirtyCardQueue.hpp",
    "raw_url": "https://github.com/openjdk/jdk/raw/91f22bbd229beca281a4a0de58bdf66c57648eeb/src/hotspot/share/gc/g1/g1DirtyCardQueue.hpp",
    "contents_url": "https://api.github.com/repos/openjdk/jdk/contents/src/hotspot/share/gc/g1/g1DirtyCardQueue.hpp?ref=91f22bbd229beca281a4a0de58bdf66c57648eeb",
    "patch": "@@ -32,6 +32,7 @@\n #include \"gc/shared/ptrQueue.hpp\"\n #include \"memory/allocation.hpp\"\n #include \"memory/padded.hpp\"\n+#include \"utilities/lockFreeQueue.hpp\"\n \n class G1ConcurrentRefineThread;\n class G1DirtyCardQueueSet;\n@@ -76,41 +77,14 @@ class G1DirtyCardQueueSet: public PtrQueueSet {\n     HeadTail(BufferNode* head, BufferNode* tail) : _head(head), _tail(tail) {}\n   };\n \n-  // A lock-free FIFO of BufferNodes, linked through their next() fields.\n-  // This class has a restriction that pop() may return NULL when there are\n-  // buffers in the queue if there is a concurrent push/append operation.\n-  class Queue {\n-    BufferNode* volatile _head;\n-    DEFINE_PAD_MINUS_SIZE(1, DEFAULT_CACHE_LINE_SIZE, sizeof(BufferNode*));\n-    BufferNode* volatile _tail;\n-    DEFINE_PAD_MINUS_SIZE(2, DEFAULT_CACHE_LINE_SIZE, sizeof(BufferNode*));\n-\n-    NONCOPYABLE(Queue);\n-\n+  class CompletedQueue: public LockFreeQueue<BufferNode, &BufferNode::next_ptr> {\n   public:\n-    Queue() : _head(NULL), _tail(NULL) {}\n-    DEBUG_ONLY(~Queue();)\n-\n-    // Return the first buffer in the queue.\n-    // Thread-safe, but the result may change immediately.\n-    BufferNode* top() const;\n-\n-    // Thread-safe add the buffer to the end of the queue.\n-    void push(BufferNode& node) { append(node, node); }\n-\n-    // Thread-safe add the buffers from first to last to the end of the queue.\n-    void append(BufferNode& first, BufferNode& last);\n-\n-    // Thread-safe attempt to remove and return the first buffer in the queue.\n-    // Returns NULL if the queue is empty, or if a concurrent push/append\n-    // interferes.  Uses GlobalCounter critical sections to address the ABA\n-    // problem; this works with the buffer allocator's use of GlobalCounter\n-    // synchronization.\n-    BufferNode* pop();\n-\n-    // Take all the buffers from the queue, leaving the queue empty.\n-    // Not thread-safe.\n-    HeadTail take_all();\n+    BufferNode* pop() {\n+      // Use GlobalCounter critical section to avoid ABA problem.\n+      // The release of a BufferNode to its allocator's free list uses\n+      // GlobalCounter::write_synchronize() to coordinate with the pop().\n+      return LockFreeQueue<BufferNode, &BufferNode::next_ptr>::pop<true /*use_rcu*/>();\n+    }\n   };\n \n   // Concurrent refinement may stop processing in the middle of a buffer if\n@@ -200,7 +174,7 @@ class G1DirtyCardQueueSet: public PtrQueueSet {\n   volatile size_t _num_cards;\n   DEFINE_PAD_MINUS_SIZE(2, DEFAULT_CACHE_LINE_SIZE, sizeof(size_t));\n   // Buffers ready for refinement.\n-  Queue _completed;           // Has inner padding, including trailer.\n+  CompletedQueue _completed;  // Has inner padding, including trailer.\n   // Buffers for which refinement is temporarily paused.\n   PausedBuffers _paused;      // Has inner padding, including trailer.\n "
  },
  {
    "sha": "a541038e8a26cd3cf55ebe452ae756d669604d11",
    "filename": "src/hotspot/share/utilities/globalCounter.hpp",
    "status": "modified",
    "additions": 3,
    "deletions": 0,
    "changes": 3,
    "blob_url": "https://github.com/openjdk/jdk/blob/91f22bbd229beca281a4a0de58bdf66c57648eeb/src/hotspot/share/utilities/globalCounter.hpp",
    "raw_url": "https://github.com/openjdk/jdk/raw/91f22bbd229beca281a4a0de58bdf66c57648eeb/src/hotspot/share/utilities/globalCounter.hpp",
    "contents_url": "https://api.github.com/repos/openjdk/jdk/contents/src/hotspot/share/utilities/globalCounter.hpp?ref=91f22bbd229beca281a4a0de58bdf66c57648eeb",
    "patch": "@@ -85,6 +85,9 @@ class GlobalCounter : public AllStatic {\n \n   // A scoped object for a read-side critical-section.\n   class CriticalSection;\n+\n+  // Similar to CriticalSection, but only enabled if \"enable\" is true.\n+  template<bool enable> class ConditionalCriticalSection;\n };\n \n #endif // SHARE_UTILITIES_GLOBALCOUNTER_HPP"
  },
  {
    "sha": "db5ab9a1aa8a71c22e935c914f131c41099548e6",
    "filename": "src/hotspot/share/utilities/globalCounter.inline.hpp",
    "status": "modified",
    "additions": 18,
    "deletions": 2,
    "changes": 20,
    "blob_url": "https://github.com/openjdk/jdk/blob/91f22bbd229beca281a4a0de58bdf66c57648eeb/src/hotspot/share/utilities/globalCounter.inline.hpp",
    "raw_url": "https://github.com/openjdk/jdk/raw/91f22bbd229beca281a4a0de58bdf66c57648eeb/src/hotspot/share/utilities/globalCounter.inline.hpp",
    "contents_url": "https://api.github.com/repos/openjdk/jdk/contents/src/hotspot/share/utilities/globalCounter.inline.hpp?ref=91f22bbd229beca281a4a0de58bdf66c57648eeb",
    "patch": "@@ -53,10 +53,9 @@ GlobalCounter::critical_section_end(Thread *thread, CSContext context) {\n }\n \n class GlobalCounter::CriticalSection {\n- private:\n   Thread* _thread;\n   CSContext _context;\n- public:\n+public:\n   inline CriticalSection(Thread* thread) :\n     _thread(thread),\n     _context(GlobalCounter::critical_section_begin(_thread))\n@@ -67,4 +66,21 @@ class GlobalCounter::CriticalSection {\n   }\n };\n \n+template<bool enable> class GlobalCounter::ConditionalCriticalSection {\n+  Thread* _thread;\n+  CSContext _context;\n+public:\n+  inline ConditionalCriticalSection(Thread* thread) {\n+    if (enable) {\n+      _thread = thread;\n+      _context = GlobalCounter::critical_section_begin(thread);\n+    }\n+  }\n+  inline ~ConditionalCriticalSection() {\n+    if (enable) {\n+      GlobalCounter::critical_section_end(_thread, _context);\n+    }\n+  }\n+};\n+\n #endif // SHARE_UTILITIES_GLOBALCOUNTER_INLINE_HPP"
  },
  {
    "sha": "ade2325a03c04dd25c3349b484dccf79fc86b2d3",
    "filename": "src/hotspot/share/utilities/lockFreeQueue.hpp",
    "status": "added",
    "additions": 100,
    "deletions": 0,
    "changes": 100,
    "blob_url": "https://github.com/openjdk/jdk/blob/91f22bbd229beca281a4a0de58bdf66c57648eeb/src/hotspot/share/utilities/lockFreeQueue.hpp",
    "raw_url": "https://github.com/openjdk/jdk/raw/91f22bbd229beca281a4a0de58bdf66c57648eeb/src/hotspot/share/utilities/lockFreeQueue.hpp",
    "contents_url": "https://api.github.com/repos/openjdk/jdk/contents/src/hotspot/share/utilities/lockFreeQueue.hpp?ref=91f22bbd229beca281a4a0de58bdf66c57648eeb",
    "patch": "@@ -0,0 +1,100 @@\n+/*\n+ * Copyright (c) 2021, Oracle and/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ */\n+\n+#ifndef SHARE_UTILITIES_LOCKFREEQUEUE_HPP\n+#define SHARE_UTILITIES_LOCKFREEQUEUE_HPP\n+\n+#include \"memory/padded.hpp\"\n+#include \"utilities/globalDefinitions.hpp\"\n+#include \"utilities/pair.hpp\"\n+\n+// The LockFreeQueue template provides a lock-free FIFO. Its structure\n+// and usage is similar to LockFreeStack. It has inner paddings, and\n+// optionally use GlobalCounter critical section in pop() to address\n+// the ABA problem. This class has a restriction that pop() may return\n+// NULL when there are objects in the queue if there is a concurrent\n+// push/append operation.\n+//\n+// \\tparam T is the class of the elements in the queue.\n+//\n+// \\tparam next_ptr is a function pointer.  Applying this function to\n+// an object of type T must return a pointer to the list entry member\n+// of the object associated with the LockFreeQueue type.\n+template<typename T, T* volatile* (*next_ptr)(T&)>\n+class LockFreeQueue {\n+  T* volatile _head;\n+  DEFINE_PAD_MINUS_SIZE(1, DEFAULT_CACHE_LINE_SIZE, sizeof(T*));\n+  T* volatile _tail;\n+  DEFINE_PAD_MINUS_SIZE(2, DEFAULT_CACHE_LINE_SIZE, sizeof(T*));\n+\n+  NONCOPYABLE(LockFreeQueue);\n+\n+  // Return the entry following node in the list used by the\n+  // specialized LockFreeQueue class.\n+  static T* next(const T& node);\n+\n+  // Set the entry following node to new_next in the list used by the\n+  // specialized LockFreeQueue class. Not thread-safe, as it cannot\n+  // concurrently run with push or pop operations that modify this\n+  // node.\n+  static void set_next(T& node, T* new_next);\n+\n+public:\n+  LockFreeQueue();\n+  DEBUG_ONLY(~LockFreeQueue();)\n+\n+  // Return the first object in the queue.\n+  // Thread-safe, but the result may change immediately.\n+  T* top() const;\n+\n+  // Return true if the queue is empty.\n+  bool empty() const { return top() == NULL; }\n+\n+  // Return the number of objects in the queue.\n+  // Not thread-safe. There must be no concurrent modification\n+  // while the length is being determined.\n+  size_t length() const;\n+\n+  // Thread-safe add the object to the end of the queue.\n+  void push(T& node) { append(node, node); }\n+\n+  // Thread-safe add the objects from first to last to the end of the queue.\n+  void append(T& first, T& last);\n+\n+  // Thread-safe attempt to remove and return the first object in the queue.\n+  // Returns NULL if the queue is empty, or if a concurrent push/append\n+  // interferes.\n+  // If use_rcu is true, it applies GlobalCounter critical sections to\n+  // address the ABA problem. This requires the object's\n+  // allocator use GlobalCounter synchronization to defer reusing object.\n+  template<bool use_rcu> T* pop();\n+\n+  // Take all the objects from the queue, leaving the queue empty.\n+  // Not thread-safe. It should only be used when there is no concurrent\n+  // push/append/pop operation.\n+  // Returns a pair of <head, tail> pointers to the current queue.\n+  Pair<T*, T*> take_all();\n+};\n+\n+#endif // SHARE_UTILITIES_LOCKFREEQUEUE_HPP"
  },
  {
    "sha": "85b6ebb13b2ba822e0ac4e25e413b3b3f1bbcd96",
    "filename": "src/hotspot/share/utilities/lockFreeQueue.inline.hpp",
    "status": "added",
    "additions": 177,
    "deletions": 0,
    "changes": 177,
    "blob_url": "https://github.com/openjdk/jdk/blob/91f22bbd229beca281a4a0de58bdf66c57648eeb/src/hotspot/share/utilities/lockFreeQueue.inline.hpp",
    "raw_url": "https://github.com/openjdk/jdk/raw/91f22bbd229beca281a4a0de58bdf66c57648eeb/src/hotspot/share/utilities/lockFreeQueue.inline.hpp",
    "contents_url": "https://api.github.com/repos/openjdk/jdk/contents/src/hotspot/share/utilities/lockFreeQueue.inline.hpp?ref=91f22bbd229beca281a4a0de58bdf66c57648eeb",
    "patch": "@@ -0,0 +1,177 @@\n+/*\n+ * Copyright (c) 2021, Oracle and/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ */\n+\n+#ifndef SHARE_UTILITIES_LOCKFREEQUEUE_INLINE_HPP\n+#define SHARE_UTILITIES_LOCKFREEQUEUE_INLINE_HPP\n+\n+#include \"runtime/atomic.hpp\"\n+#include \"runtime/thread.inline.hpp\"\n+#include \"utilities/globalCounter.inline.hpp\"\n+#include \"utilities/lockFreeQueue.hpp\"\n+#include \"logging/log.hpp\"\n+\n+template<typename T, T* volatile* (*next_ptr)(T&)>\n+T* LockFreeQueue<T, next_ptr>::next(const T& node) {\n+  return Atomic::load(next_ptr(const_cast<T&>(node)));\n+}\n+\n+template<typename T, T* volatile* (*next_ptr)(T&)>\n+void LockFreeQueue<T, next_ptr>::set_next(T& node, T* new_next) {\n+    Atomic::store(next_ptr(node), new_next);\n+}\n+\n+template<typename T, T* volatile* (*next_ptr)(T&)>\n+LockFreeQueue<T, next_ptr>::LockFreeQueue() : _head(NULL), _tail(NULL) {}\n+\n+#ifdef ASSERT\n+template<typename T, T* volatile* (*next_ptr)(T&)>\n+LockFreeQueue<T, next_ptr>::~LockFreeQueue() {\n+  assert(_head == NULL, \"precondition\");\n+  assert(_tail == NULL, \"precondition\");\n+}\n+#endif\n+\n+template<typename T, T* volatile* (*next_ptr)(T&)>\n+T* LockFreeQueue<T, next_ptr>::top() const {\n+  return Atomic::load(&_head);\n+}\n+\n+template<typename T, T* volatile* (*next_ptr)(T&)>\n+size_t LockFreeQueue<T, next_ptr>::length() const {\n+  size_t result = 0;\n+  for (const T* current = top(); current != NULL; current = next(*current)) {\n+    ++result;\n+  }\n+  return result;\n+}\n+\n+// An append operation atomically exchanges the new tail with the queue tail.\n+// It then sets the \"next\" value of the old tail to the head of the list being\n+// appended; it is an invariant that the old tail's \"next\" value is NULL.\n+// But if the old tail is NULL then the queue was empty.  In this case the\n+// head of the list being appended is instead stored in the queue head; it is\n+// an invariant that the queue head is NULL in this case.\n+//\n+// This means there is a period between the exchange and the old tail update\n+// where the queue sequence is split into two parts, the list from the queue\n+// head to the old tail, and the list being appended.  If there are concurrent\n+// push/append operations, each may introduce another such segment.  But they\n+// all eventually get resolved by their respective updates of their old tail's\n+// \"next\" value.  This also means that pop operations must handle an object\n+// with a NULL \"next\" value specially.\n+//\n+// A push operation is just a degenerate append, where the object being pushed\n+// is both the head and the tail of the list being appended.\n+template<typename T, T* volatile* (*next_ptr)(T&)>\n+void LockFreeQueue<T, next_ptr>::append(T& first, T& last) {\n+  assert(next(last) == NULL, \"precondition\");\n+  T* old_tail = Atomic::xchg(&_tail, &last);\n+  if (old_tail == NULL) {       // Was empty.\n+    Atomic::store(&_head, &first);\n+  } else {\n+    assert(next(*old_tail) == NULL, \"invariant\");\n+    set_next(*old_tail, &first);\n+  }\n+}\n+\n+template<typename T, T* volatile* (*next_ptr)(T&)>\n+template<bool use_rcu>\n+T* LockFreeQueue<T, next_ptr>::pop() {\n+  while (true) {\n+    // Use a critical section per iteration, rather than over the whole\n+    // operation. We're not guaranteed to make progress. Lingering in one\n+    // CS could defer the write-side operation of RCU synchronization\n+    // too long, leading to unwanted effect. E.g., if the write-side\n+    // returns released objects to a free list for reuse, it could cause\n+    // excessive allocations.\n+    GlobalCounter::ConditionalCriticalSection<use_rcu> cs(use_rcu ?\n+                                                          Thread::current():\n+                                                          NULL);\n+\n+    // We only need memory_order_consume. Upgrade it to \"load_acquire\"\n+    // as the memory_order_consume API is not ready for use yet.\n+    T* result = Atomic::load_acquire(&_head);\n+    if (result == NULL) return NULL; // Queue is empty.\n+\n+    // This relaxed load is always followed by a cmpxchg(), thus it\n+    // is OK as the reader-side of the release-acquire ordering.\n+    T* next_node = Atomic::load(next_ptr(*result));\n+    if (next_node != NULL) {\n+      // The \"usual\" lock-free pop from the head of a singly linked list.\n+      if (result == Atomic::cmpxchg(&_head, result, next_node)) {\n+        // Former head successfully taken; it is not the last.\n+        assert(Atomic::load(&_tail) != result, \"invariant\");\n+        assert(next(*result) != NULL, \"invariant\");\n+        set_next(*result, NULL);\n+        return result;\n+      }\n+      // Lost the race; try again.\n+      continue;\n+    }\n+\n+    // next is NULL.  This case is handled differently from the \"usual\"\n+    // lock-free pop from the head of a singly linked list.\n+\n+    // If _tail == result then result is the only element in the list. We can\n+    // remove it from the list by first setting _tail to NULL and then setting\n+    // _head to NULL, the order being important.  We set _tail with cmpxchg in\n+    // case of a concurrent push/append/pop also changing _tail.  If we win\n+    // then we've claimed result.\n+    if (Atomic::cmpxchg(&_tail, result, (T*)NULL) == result) {\n+      assert(next(*result) == NULL, \"invariant\");\n+      // Now that we've claimed result, also set _head to NULL.  But we must\n+      // be careful of a concurrent push/append after we NULLed _tail, since\n+      // it may have already performed its list-was-empty update of _head,\n+      // which we must not overwrite.\n+      Atomic::cmpxchg(&_head, result, (T*)NULL);\n+      return result;\n+    }\n+\n+    // If _head != result then we lost the race to take result; try again.\n+    if (result != Atomic::load_acquire(&_head)) {\n+      continue;\n+    }\n+\n+    // An in-progress concurrent operation interfered with taking the head\n+    // element when it was the only element.  A concurrent pop may have won\n+    // the race to clear the tail but not yet cleared the head. Alternatively,\n+    // a concurrent push/append may have changed the tail but not yet linked\n+    // result->next().  We cannot take result in either case.  We don't just\n+    // try again, because we could spin for a long time waiting for that\n+    // concurrent operation to finish.  In the first case, returning NULL is\n+    // fine; we lost the race for the only element to another thread.  We\n+    // also return NULL for the second case, and let the caller cope.\n+    return NULL;\n+  }\n+}\n+\n+template<typename T, T* volatile* (*next_ptr)(T&)>\n+Pair<T*, T*> LockFreeQueue<T, next_ptr>::take_all() {\n+  Pair<T*, T*> result(Atomic::load(&_head), Atomic::load(&_tail));\n+  Atomic::store(&_head, (T*)NULL);\n+  Atomic::store(&_tail, (T*)NULL);\n+  return result;\n+}\n+\n+#endif // SHARE_UTILITIES_LOCKFREEQUEUE_INLINE_HPP"
  },
  {
    "sha": "db2658f079ea0292796133148f9a5916a477057d",
    "filename": "test/hotspot/gtest/utilities/test_lockFreeQueue.cpp",
    "status": "added",
    "additions": 310,
    "deletions": 0,
    "changes": 310,
    "blob_url": "https://github.com/openjdk/jdk/blob/91f22bbd229beca281a4a0de58bdf66c57648eeb/test/hotspot/gtest/utilities/test_lockFreeQueue.cpp",
    "raw_url": "https://github.com/openjdk/jdk/raw/91f22bbd229beca281a4a0de58bdf66c57648eeb/test/hotspot/gtest/utilities/test_lockFreeQueue.cpp",
    "contents_url": "https://api.github.com/repos/openjdk/jdk/contents/test/hotspot/gtest/utilities/test_lockFreeQueue.cpp?ref=91f22bbd229beca281a4a0de58bdf66c57648eeb",
    "patch": "@@ -0,0 +1,310 @@\n+/*\n+ * Copyright (c) 2021, Oracle and/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ */\n+\n+#include \"precompiled.hpp\"\n+#include \"memory/allocation.inline.hpp\"\n+#include \"runtime/atomic.hpp\"\n+#include \"utilities/globalDefinitions.hpp\"\n+#include \"utilities/lockFreeQueue.inline.hpp\"\n+#include \"utilities/pair.hpp\"\n+#include \"threadHelper.inline.hpp\"\n+#include \"unittest.hpp\"\n+#include <new>\n+\n+class LockFreeQueueTestElement {\n+  typedef LockFreeQueueTestElement Element;\n+\n+  Element* volatile _entry;\n+  Element* volatile _entry1;\n+  size_t _id;\n+\n+  static Element* volatile* entry_ptr(Element& e) { return &e._entry; }\n+  static Element* volatile* entry1_ptr(Element& e) { return &e._entry1; }\n+\n+public:\n+  class TestQueue: public LockFreeQueue<Element, &entry_ptr> {\n+  public:\n+    Element* pop() {\n+      return LockFreeQueue<Element, &entry_ptr>::pop<false>();\n+    }\n+    Element* pop_rcu() {\n+      return LockFreeQueue<Element, &entry_ptr>::pop<true>();\n+    }\n+  };\n+  class TestQueue1: public LockFreeQueue<Element, &entry1_ptr> {\n+  public:\n+    Element* pop() {\n+      return LockFreeQueue<Element, &entry1_ptr>::pop<false>();\n+    }\n+  };\n+\n+  LockFreeQueueTestElement(size_t id = 0) : _entry(), _entry1(), _id(id) {}\n+  size_t id() const { return _id; }\n+  void set_id(size_t value) { _id = value; }\n+  Element* next() { return _entry; }\n+  Element* next1() { return _entry1; }\n+};\n+\n+typedef LockFreeQueueTestElement Element;\n+typedef Element::TestQueue TestQueue;\n+typedef Element::TestQueue1 TestQueue1;\n+\n+static void initialize(Element* elements, size_t size, TestQueue* queue) {\n+  for (size_t i = 0; i < size; ++i) {\n+    elements[i].set_id(i);\n+  }\n+  ASSERT_TRUE(queue->empty());\n+  ASSERT_EQ(0u, queue->length());\n+  ASSERT_TRUE(queue->pop() == NULL);\n+  ASSERT_TRUE(queue->top() == NULL);\n+\n+  for (size_t id = 0; id < size; ++id) {\n+    ASSERT_EQ(id, queue->length());\n+    Element* e = &elements[id];\n+    ASSERT_EQ(id, e->id());\n+    queue->push(*e);\n+    ASSERT_FALSE(queue->empty());\n+    // top() is always the oldest element.\n+    ASSERT_EQ(&elements[0], queue->top());\n+  }\n+}\n+\n+class LockFreeQueueTestBasics : public ::testing::Test {\n+public:\n+  LockFreeQueueTestBasics();\n+\n+  static const size_t nelements = 10;\n+  Element elements[nelements];\n+  TestQueue queue;\n+};\n+\n+const size_t LockFreeQueueTestBasics::nelements;\n+\n+LockFreeQueueTestBasics::LockFreeQueueTestBasics() : queue() {\n+  initialize(elements, nelements, &queue);\n+}\n+\n+TEST_F(LockFreeQueueTestBasics, pop) {\n+  for (size_t i = 0; i < nelements; ++i) {\n+    ASSERT_FALSE(queue.empty());\n+    ASSERT_EQ(nelements - i, queue.length());\n+    Element* e = queue.pop();\n+    ASSERT_TRUE(e != NULL);\n+    ASSERT_EQ(&elements[i], e);\n+    ASSERT_EQ(i, e->id());\n+  }\n+  ASSERT_TRUE(queue.empty());\n+  ASSERT_EQ(0u, queue.length());\n+  ASSERT_TRUE(queue.pop() == NULL);\n+}\n+\n+TEST_VM(LockFreeQueueTestPopRCU, pop_rcu) {\n+  // We have to run this test in a JVM, so that Thread::current() can work.\n+  const size_t nelements = 10;\n+  Element elements[nelements];\n+  TestQueue queue;\n+  initialize(elements, nelements, &queue);\n+  for (size_t i = 0; i < nelements; ++i) {\n+    ASSERT_FALSE(queue.empty());\n+    ASSERT_EQ(nelements - i, queue.length());\n+    Element* e = queue.pop_rcu();\n+    ASSERT_TRUE(e != NULL);\n+    ASSERT_EQ(&elements[i], e);\n+    ASSERT_EQ(i, e->id());\n+  }\n+  ASSERT_TRUE(queue.empty());\n+  ASSERT_EQ(0u, queue.length());\n+  ASSERT_TRUE(queue.pop() == NULL);\n+}\n+\n+TEST_F(LockFreeQueueTestBasics, append) {\n+  TestQueue other_queue;\n+  ASSERT_TRUE(other_queue.empty());\n+  ASSERT_EQ(0u, other_queue.length());\n+  ASSERT_TRUE(other_queue.top() == NULL);\n+  ASSERT_TRUE(other_queue.pop() == NULL);\n+\n+  Pair<Element*, Element*> pair = queue.take_all();\n+  other_queue.append(*pair.first, *pair.second);\n+  ASSERT_EQ(nelements, other_queue.length());\n+  ASSERT_TRUE(queue.empty());\n+  ASSERT_EQ(0u, queue.length());\n+  ASSERT_TRUE(queue.pop() == NULL);\n+  ASSERT_TRUE(queue.top() == NULL);\n+\n+  for (size_t i = 0; i < nelements; ++i) {\n+    ASSERT_EQ(nelements - i, other_queue.length());\n+    Element* e = other_queue.pop();\n+    ASSERT_TRUE(e != NULL);\n+    ASSERT_EQ(&elements[i], e);\n+    ASSERT_EQ(i, e->id());\n+  }\n+  ASSERT_EQ(0u, other_queue.length());\n+  ASSERT_TRUE(other_queue.pop() == NULL);\n+}\n+\n+TEST_F(LockFreeQueueTestBasics, two_queues) {\n+  TestQueue1 queue1;\n+  ASSERT_TRUE(queue1.pop() == NULL);\n+\n+  for (size_t id = 0; id < nelements; ++id) {\n+    queue1.push(elements[id]);\n+  }\n+  ASSERT_EQ(nelements, queue1.length());\n+  Element* e0 = queue.top();\n+  Element* e1 = queue1.top();\n+  while (true) {\n+    ASSERT_EQ(e0, e1);\n+    if (e0 == NULL) break;\n+    e0 = e0->next();\n+    e1 = e1->next1();\n+  }\n+\n+  for (size_t i = 0; i < nelements; ++i) {\n+    ASSERT_EQ(nelements - i, queue.length());\n+    ASSERT_EQ(nelements - i, queue1.length());\n+\n+    Element* e = queue.pop();\n+    ASSERT_TRUE(e != NULL);\n+    ASSERT_EQ(&elements[i], e);\n+    ASSERT_EQ(i, e->id());\n+\n+    Element* e1 = queue1.pop();\n+    ASSERT_TRUE(e1 != NULL);\n+    ASSERT_EQ(&elements[i], e1);\n+    ASSERT_EQ(i, e1->id());\n+\n+    ASSERT_EQ(e, e1);\n+  }\n+  ASSERT_EQ(0u, queue.length());\n+  ASSERT_EQ(0u, queue1.length());\n+  ASSERT_TRUE(queue.pop() == NULL);\n+  ASSERT_TRUE(queue1.pop() == NULL);\n+}\n+\n+class LockFreeQueueTestThread : public JavaTestThread {\n+  uint _id;\n+  TestQueue* _from;\n+  TestQueue* _to;\n+  volatile size_t* _processed;\n+  size_t _process_limit;\n+  size_t _local_processed;\n+  volatile bool _ready;\n+\n+public:\n+  LockFreeQueueTestThread(Semaphore* post,\n+                          uint id,\n+                          TestQueue* from,\n+                          TestQueue* to,\n+                          volatile size_t* processed,\n+                          size_t process_limit) :\n+    JavaTestThread(post),\n+    _id(id),\n+    _from(from),\n+    _to(to),\n+    _processed(processed),\n+    _process_limit(process_limit),\n+    _local_processed(0),\n+    _ready(false)\n+  {}\n+\n+  virtual void main_run() {\n+    Atomic::release_store_fence(&_ready, true);\n+    while (true) {\n+      Element* e = _from->pop();\n+      if (e != NULL) {\n+        _to->push(*e);\n+        Atomic::inc(_processed);\n+        ++_local_processed;\n+      } else if (Atomic::load_acquire(_processed) == _process_limit) {\n+        tty->print_cr(\"thread %u processed \" SIZE_FORMAT, _id, _local_processed);\n+        return;\n+      }\n+    }\n+  }\n+\n+  bool ready() const { return Atomic::load_acquire(&_ready); }\n+};\n+\n+TEST_VM(LockFreeQueueTest, stress) {\n+  Semaphore post;\n+  TestQueue initial_queue;\n+  TestQueue start_queue;\n+  TestQueue middle_queue;\n+  TestQueue final_queue;\n+  volatile size_t stage1_processed = 0;\n+  volatile size_t stage2_processed = 0;\n+\n+  const size_t nelements = 10000;\n+  Element* elements = NEW_C_HEAP_ARRAY(Element, nelements, mtOther);\n+  for (size_t id = 0; id < nelements; ++id) {\n+    ::new (&elements[id]) Element(id);\n+    initial_queue.push(elements[id]);\n+  }\n+  ASSERT_EQ(nelements, initial_queue.length());\n+\n+  // - stage1 threads pop from start_queue and push to middle_queue.\n+  // - stage2 threads pop from middle_queue and push to final_queue.\n+  // - all threads in a stage count the number of elements processed in\n+  //   their corresponding stageN_processed counter.\n+\n+  const uint stage1_threads = 2;\n+  const uint stage2_threads = 2;\n+  const uint nthreads = stage1_threads + stage2_threads;\n+  LockFreeQueueTestThread* threads[nthreads] = {};\n+\n+  for (uint i = 0; i < ARRAY_SIZE(threads); ++i) {\n+    TestQueue* from = &start_queue;\n+    TestQueue* to = &middle_queue;\n+    volatile size_t* processed = &stage1_processed;\n+    if (i >= stage1_threads) {\n+      from = &middle_queue;\n+      to = &final_queue;\n+      processed = &stage2_processed;\n+    }\n+    threads[i] =\n+      new LockFreeQueueTestThread(&post, i, from, to, processed, nelements);\n+    threads[i]->doit();\n+    while (!threads[i]->ready()) {} // Wait until ready to start test.\n+  }\n+\n+  // Transfer elements to start_queue to start test.\n+  Pair<Element*, Element*> pair = initial_queue.take_all();\n+  start_queue.append(*pair.first, *pair.second);\n+\n+  // Wait for all threads to complete.\n+  for (uint i = 0; i < nthreads; ++i) {\n+    post.wait();\n+  }\n+\n+  // Verify expected state.\n+  ASSERT_EQ(nelements, stage1_processed);\n+  ASSERT_EQ(nelements, stage2_processed);\n+  ASSERT_EQ(0u, initial_queue.length());\n+  ASSERT_EQ(0u, start_queue.length());\n+  ASSERT_EQ(0u, middle_queue.length());\n+  ASSERT_EQ(nelements, final_queue.length());\n+  while (final_queue.pop() != NULL) {}\n+\n+  FREE_C_HEAP_ARRAY(Element, elements);\n+}"
  }
]
